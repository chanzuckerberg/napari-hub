[
  "{\"authors\":[{\"name\":\"Marc Boucsein\"},{\"name\":\"Robin Koch\"}],\"code_repository\":\"https://github.com/MBPhys/Image-Part-Selecter\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"image-part-selecter\"}],\"description\":\"# Image-Part-Selecter\\n\\n[![License](https://img.shields.io/pypi/l/napari-medical-image-formats.svg?color=green)](https://github.com/MBPhys/Image-Part-Selecter/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/Image-Part-Selecter.svg?color=green)](https://pypi.org/project/Image-Part-Selecter)\\n[![Python Version](https://img.shields.io/pypi/pyversions/Image-Part-Selecter.svg?color=green)](https://python.org)\\n\\n\\nA napari plugin in order to select parts of images\\n\\n----------------------------------\\n\\n## Installation\\n\\nYou can install `Image-Part-Selecter` via [pip]:\\n\\n    pip install Image-Part-Selecter\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"Image-Part-Selecter\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/MBPhys/Image-Part-Selecter/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"Image-Part-Selecter\\n\\n\\n\\nA napari plugin in order to select parts of images\\n\\nInstallation\\nYou can install Image-Part-Selecter via pip:\\npip install Image-Part-Selecter\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"Image-Part-Selecter\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"Image-Part-Selecter\",\"documentation\":\"\",\"first_released\":\"2022-01-12T10:42:55.967575Z\",\"license\":\"BSD-3-Clause\",\"name\":\"Image-Part-Selecter\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/MBPhys/Image-Part-Selecter\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-01-12T10:42:55.967575Z\",\"report_issues\":\"\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\"],\"summary\":\"A napari plugin in order to select parts of images\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.0.7\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Jessy Lauer\"}],\"code_repository\":\"https://github.com/DeepLabCut/napari-deeplabcut\",\"description\":\"# napari-deeplabcut\\n\\n\\n<img src=\\\"https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1d409ffe-c9f4-47e1-bde2-3010c1c40455/naparidlc.png?format=750w\\\" width=\\\"250\\\" title=\\\"napari-deeplabcut\\\" alt=\\\"napari+deeplabcut\\\" align=\\\"right\\\" vspace = \\\"80\\\">\\n\\n[![License: BSD-3](https://img.shields.io/badge/License-BSD3-blue.svg)](https://www.gnu.org/licenses/bsd3)\\n[![PyPI](https://img.shields.io/pypi/v/napari-deeplabcut.svg?color=green)](https://pypi.org/project/napari-deeplabcut)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-deeplabcut.svg?color=green)](https://python.org)\\n[![tests](https://github.com/DeepLabCut/napari-deeplabcut/workflows/tests/badge.svg)](https://github.com/DeepLabCut/napari-deeplabcut/actions)\\n[![codecov](https://codecov.io/gh/DeepLabCut/napari-deeplabcut/branch/main/graph/badge.svg)](https://codecov.io/gh/DeepLabCut/napari-deeplabcut)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-deeplabcut)](https://napari-hub.org/plugins/napari-deeplabcut)\\n\\nA napari plugin for keypoint annotation with DeepLabCut\\n\\n\\n## Installation\\n\\nStart by installing PySide6 with `pip install \\\"pyside6<6.3.2\\\"`; this is the library we now use to build GUIs.\\n\\nYou can then install `napari-deeplabcut` via [pip]:\\n\\n    pip install napari-deeplabcut\\n\\n\\n\\nAlternatively, to install the latest development version, run:\\n\\n    pip install git+https://github.com/DeepLabCut/napari-deeplabcut.git\\n\\n\\n## Usage\\n\\nTo use the plugin, please run:\\n\\n    napari\\n\\nThen, activate the plugin in Plugins > napari-deeplabcut: Keypoint controls.\\n\\nAll accepted files (config.yaml, images, h5 data files) can be loaded\\neither by dropping them directly onto the canvas or via the File menu.\\n\\nThe easiest way to get started is to drop a folder (typically a folder from within a DeepLabCut's `labeled-data` directory), and, if labeling from scratch, drop the corresponding `config.yaml` to automatically add a `Points layer` and populate the dropdown menus.\\n\\n**Tools & shortcuts are:**\\n\\n- `2` and `3`, to easily switch between labeling and selection mode\\n- `4`, to enable pan & zoom (which is achieved using the mouse wheel or finger scrolling on the Trackpad)\\n- `M`, to cycle through regular (sequential), quick, and cycle annotation mode (see the description [here](https://github.com/DeepLabCut/DeepLabCut-label/blob/ee71b0e15018228c98db3b88769e8a8f4e2c0454/dlclabel/layers.py#L9-L19))\\n- `E`, to enable edge coloring (by default, if using this in refinement GUI mode, points with a confidence lower than 0.6 are marked\\nin red)\\n- `F`, to toggle between animal and body part color scheme.\\n- `backspace` to delete a point.\\n- Check the box \\\"display text\\\" to show the label names on the canvas.\\n- To move to another folder, be sure to save (Ctrl+S), then delete the layers, and re-drag/drop the next folder.\\n\\n\\n### Save Layers\\n\\nAnnotations and segmentations are saved with `File > Save Selected Layer(s)...` (or its shortcut `Ctrl+S`).\\nOnly when saving segmentation masks does a save file dialog pop up to name the destination folder;\\nkeypoint annotations are otherwise automatically saved in the corresponding folder as `CollectedData_<ScorerName>.h5`.\\n- As a reminder, DLC will only use the H5 file; so be sure if you open already labeled images you save/overwrite the H5.\\n- Note, before saving a layer, make sure the points layer is selected. If the user clicked on the image(s) layer first, does `Save As`, then closes the window, any labeling work during that session will be lost!\\n\\n\\n### Video frame extraction and prediction refinement\\n\\nSince v0.0.4, videos can be viewed in the GUI.\\n\\nSince v0.0.5, trailing points can be visualized; e.g., helping in the identification\\nof swaps or outlier, jittery predictions.\\n\\nLoading a video (and its corresponding output h5 file) will enable the video actions\\nat the top of the dock widget: they offer the option to manually extract video\\nframes from the GUI, or to define cropping coordinates.\\nNote that keypoints can be displaced and saved, as when annotating individual frames.\\n\\n\\n## Workflow\\n\\nSuggested workflows, depending on the image folder contents:\\n\\n1. **Labeling from scratch** – the image folder does not contain `CollectedData_<ScorerName>.h5` file.\\n\\n    Open *napari* as described in [Usage](#usage) and open an image folder together with the DeepLabCut project's `config.yaml`.\\n    The image folder creates an *image layer* with the images to label.\\n    Supported image formats are: `jpg`, `jpeg`, `png`.\\n    The `config.yaml` file creates a *Points layer*, which holds metadata (such as keypoints read from the config file) necessary for labeling.\\n    Select the *Points layer* in the layer list (lower left pane on the GUI) and click on the *+*-symbol in the layer controls menu (upper left pane) to start labeling.\\n    The current keypoint can be viewed/selected in the keypoints dropdown menu (right pane).\\n    The slider below the displayed image (or the left/right arrow keys) allows selecting the image to label.\\n\\n    To save the labeling progress refer to [Save Layers](#save-layers).\\n    `Data successfully saved` should be shown in the status bar, and the image folder should now contain a `CollectedData_<ScorerName>.h5` file.\\n    (Note: For convenience, a CSV file with the same name is also saved.)\\n\\n2. **Resuming labeling** – the image folder contains a `CollectedData_<ScorerName>.h5` file.\\n\\n    Open *napari* and open an image folder (which needs to contain a `CollectedData_<ScorerName>.h5` file).\\n    In this case, it is not necessary to open the DLC project's `config.yaml` file, as all necessary metadata is read from the `h5` data file.\\n\\n    Saving works as described in *1*.\\n\\n3. **Refining labels** – the image folder contains a `machinelabels-iter<#>.h5` file.\\n\\n    The process is analog to *2*.\\n\\n4. **Drawing segmentation masks**\\n\\n    Drop an image folder as in *1*, manually add a *shapes layer*. Then select the *rectangle* in the layer controls (top left pane),\\n    and start drawing rectangles over the images. Masks and rectangle vertices are saved as described in [Save Layers](#save-layers).\\n    Note that masks can be reloaded and edited at a later stage by dropping the `vertices.csv` file onto the canvas.\\n\\n\\n### Labeling multiple image folders\\n\\nLabeling multiple image folders has to be done in sequence; i.e., only one image folder can be opened at a time.\\nAfter labeling the images of a particular folder is done and the associated *Points layer* has been saved, *all* layers should be removed from the layers list (lower left pane on the GUI) by selecting them and clicking on the trashcan icon.\\nNow, another image folder can be labeled, following the process described in *1*, *2*, or *3*, depending on the particular image folder.\\n\\n\\n### Defining cropping coordinates\\n\\nPrior to defining cropping coordinates, two elements should be loaded in the GUI:\\na video and the DLC project's `config.yaml` file (into which the crop dimensions will be stored).\\nThen it suffices to add a `Shapes layer`, draw a `rectangle` in it with the desired area,\\nand hit the button `Store crop coordinates`; coordinates are automatically written to the configuration file.\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\nTo locally install the code, please git clone the repo and then run `pip install -e .`\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-deeplabcut\\\" is free and open source software.\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[file an issue]: https://github.com/DeepLabCut/napari-deeplabcut/issues\\n\\n\\n## Acknowledgements\\n\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template. We thank the Chan Zuckerberg Initiative (CZI) for funding this work!\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/plugins/stable/index.html\\n-->\\n\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-deeplabcut\\n\\n\\n\\n\\n\\n\\n\\nA napari plugin for keypoint annotation with DeepLabCut\\nInstallation\\nStart by installing PySide6 with pip install \\\"pyside6<6.3.2\\\"; this is the library we now use to build GUIs.\\nYou can then install napari-deeplabcut via pip:\\npip install napari-deeplabcut\\n\\nAlternatively, to install the latest development version, run:\\npip install git+https://github.com/DeepLabCut/napari-deeplabcut.git\\n\\nUsage\\nTo use the plugin, please run:\\nnapari\\n\\nThen, activate the plugin in Plugins > napari-deeplabcut: Keypoint controls.\\nAll accepted files (config.yaml, images, h5 data files) can be loaded\\neither by dropping them directly onto the canvas or via the File menu.\\nThe easiest way to get started is to drop a folder (typically a folder from within a DeepLabCut's labeled-data directory), and, if labeling from scratch, drop the corresponding config.yaml to automatically add a Points layer and populate the dropdown menus.\\nTools & shortcuts are:\\n\\n2 and 3, to easily switch between labeling and selection mode\\n4, to enable pan & zoom (which is achieved using the mouse wheel or finger scrolling on the Trackpad)\\nM, to cycle through regular (sequential), quick, and cycle annotation mode (see the description here)\\nE, to enable edge coloring (by default, if using this in refinement GUI mode, points with a confidence lower than 0.6 are marked\\nin red)\\nF, to toggle between animal and body part color scheme.\\nbackspace to delete a point.\\nCheck the box \\\"display text\\\" to show the label names on the canvas.\\nTo move to another folder, be sure to save (Ctrl+S), then delete the layers, and re-drag/drop the next folder.\\n\\nSave Layers\\nAnnotations and segmentations are saved with File > Save Selected Layer(s)... (or its shortcut Ctrl+S).\\nOnly when saving segmentation masks does a save file dialog pop up to name the destination folder;\\nkeypoint annotations are otherwise automatically saved in the corresponding folder as CollectedData_<ScorerName>.h5.\\n- As a reminder, DLC will only use the H5 file; so be sure if you open already labeled images you save/overwrite the H5.\\n- Note, before saving a layer, make sure the points layer is selected. If the user clicked on the image(s) layer first, does Save As, then closes the window, any labeling work during that session will be lost!\\nVideo frame extraction and prediction refinement\\nSince v0.0.4, videos can be viewed in the GUI.\\nSince v0.0.5, trailing points can be visualized; e.g., helping in the identification\\nof swaps or outlier, jittery predictions.\\nLoading a video (and its corresponding output h5 file) will enable the video actions\\nat the top of the dock widget: they offer the option to manually extract video\\nframes from the GUI, or to define cropping coordinates.\\nNote that keypoints can be displaced and saved, as when annotating individual frames.\\nWorkflow\\nSuggested workflows, depending on the image folder contents:\\n\\n\\nLabeling from scratch – the image folder does not contain CollectedData_<ScorerName>.h5 file.\\nOpen napari as described in Usage and open an image folder together with the DeepLabCut project's config.yaml.\\nThe image folder creates an image layer with the images to label.\\nSupported image formats are: jpg, jpeg, png.\\nThe config.yaml file creates a Points layer, which holds metadata (such as keypoints read from the config file) necessary for labeling.\\nSelect the Points layer in the layer list (lower left pane on the GUI) and click on the +-symbol in the layer controls menu (upper left pane) to start labeling.\\nThe current keypoint can be viewed/selected in the keypoints dropdown menu (right pane).\\nThe slider below the displayed image (or the left/right arrow keys) allows selecting the image to label.\\nTo save the labeling progress refer to Save Layers.\\nData successfully saved should be shown in the status bar, and the image folder should now contain a CollectedData_<ScorerName>.h5 file.\\n(Note: For convenience, a CSV file with the same name is also saved.)\\n\\n\\nResuming labeling – the image folder contains a CollectedData_<ScorerName>.h5 file.\\nOpen napari and open an image folder (which needs to contain a CollectedData_<ScorerName>.h5 file).\\nIn this case, it is not necessary to open the DLC project's config.yaml file, as all necessary metadata is read from the h5 data file.\\nSaving works as described in 1.\\n\\n\\nRefining labels – the image folder contains a machinelabels-iter<#>.h5 file.\\nThe process is analog to 2.\\n\\n\\nDrawing segmentation masks\\nDrop an image folder as in 1, manually add a shapes layer. Then select the rectangle in the layer controls (top left pane),\\nand start drawing rectangles over the images. Masks and rectangle vertices are saved as described in Save Layers.\\nNote that masks can be reloaded and edited at a later stage by dropping the vertices.csv file onto the canvas.\\n\\n\\nLabeling multiple image folders\\nLabeling multiple image folders has to be done in sequence; i.e., only one image folder can be opened at a time.\\nAfter labeling the images of a particular folder is done and the associated Points layer has been saved, all layers should be removed from the layers list (lower left pane on the GUI) by selecting them and clicking on the trashcan icon.\\nNow, another image folder can be labeled, following the process described in 1, 2, or 3, depending on the particular image folder.\\nDefining cropping coordinates\\nPrior to defining cropping coordinates, two elements should be loaded in the GUI:\\na video and the DLC project's config.yaml file (into which the crop dimensions will be stored).\\nThen it suffices to add a Shapes layer, draw a rectangle in it with the desired area,\\nand hit the button Store crop coordinates; coordinates are automatically written to the configuration file.\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nTo locally install the code, please git clone the repo and then run pip install -e .\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-deeplabcut\\\" is free and open source software.\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\\nAcknowledgements\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template. We thank the Chan Zuckerberg Initiative (CZI) for funding this work!\\n\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari DeepLabCut\",\"documentation\":\"https://github.com/DeepLabCut/napari-deeplabcut#README.md\",\"first_released\":\"2022-06-03T18:54:47.600892Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-deeplabcut\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\",\"writer\",\"widget\"],\"project_site\":\"https://github.com/DeepLabCut/napari-deeplabcut\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[\"*.mov\",\"*.mp4\",\"*.jpeg\",\"*.avi\",\"*\",\"*.png\",\"*.h5\",\"*.jpg\",\"*.yaml\"],\"release_date\":\"2022-12-06T10:12:20.065869Z\",\"report_issues\":\"https://github.com/DeepLabCut/napari-deeplabcut/issues\",\"requirements\":[\"dask-image\",\"napari (==0.4.17rc8)\",\"numpy\",\"opencv-python-headless\",\"pandas\",\"pyyaml\",\"qtpy\",\"scikit-image\",\"tables\",\"napari ; extra == 'testing'\",\"pyside6 (<6.3.2) ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"tox ; extra == 'testing'\"],\"summary\":\"napari + DeepLabCut annotation tool\",\"support\":\"https://github.com/DeepLabCut/napari-deeplabcut/issues\",\"twitter\":\"\",\"version\":\"0.0.9\",\"visibility\":\"public\",\"writer_file_extensions\":[\".h5\",\".csv\"],\"writer_save_layers\":[\"points\",\"shapes\"]}",
  "{\"authors\":[{\"name\":\"Pradeep Rajasekhar\"},{\"name\":\"Lachlan Whitehead\"},{\"name\":\"Robert Haase\"}],\"code_repository\":\"https://github.com/BioimageAnalysisCoreWEHI/napari_lattice\",\"description\":\"# napari-lattice\\n\\n[![License](https://img.shields.io/pypi/l/napari-lattice.svg?color=green)](https://github.com/githubuser/napari_lattice/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-lattice.svg?color=green)](https://pypi.org/project/napari_lattice)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-lattice.svg?color=green)](https://python.org)\\n[![tests](https://github.com/BioimageAnalysisCoreWEHI/napari_lattice/actions/workflows/test_and_deploy.yml/badge.svg)](https://github.com/BioimageAnalysisCoreWEHI/napari_lattice/actions/workflows/test_and_deploy.yml)\\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/napari-lattice)](https://pypistats.org/packages/napari-lattice)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-lattice)](https://napari-hub.org/plugins/napari-lattice)\\n\\nThis napari plugin allows deskewing, cropping, visualisation and designing custom analysis pipelines for lattice lightsheet data, particularly from the Zeiss Lattice Lightsheet. The plugin has also been otpimixed to run in headless mode.\\n\\n\\n## **Documentation**\\n\\nCheck the [Wiki page](https://github.com/BioimageAnalysisCoreWEHI/napari_lattice/wiki) for documentation on how to get started.\\n\\n\\n*************\\n\\n\\n<p align=\\\"left\\\">\\n<img src=\\\"https://raw.githubusercontent.com/BioimageAnalysisCoreWEHI/napari_lattice/master/resources/LLSZ_window.png\\\" alt=\\\"LLSZ_overview\\\" width=\\\"500\\\" >\\n</p>\\n\\n**Functions**\\n\\n* Deskewing and deconvolution of Zeiss lattice lightsheet images\\n  * Ability to preview deskewed image at channel or timepoint of interest\\n* Crop and process only a small portion of the image \\n* Import ImageJ ROIs for cropping\\n* Create image processing workflows using napari-workflows\\n* Run deskewing, deconvolution and custom image processing workflows from the terminal\\n* Files can be saved as h5 (BigDataViewer/BigStitcher) or tiff files\\n* Run in terminal without napari, enabling processing workflows on the HPC\\n\\n**Key Features**\\n\\nApply custom image processing workflows using `napari-workflows`. \\n* [Interactive workflow generation (no coding experience needed)](https://github.com/BioimageAnalysisCoreWEHI/napari_lattice/wiki/5.-Workflows-(Interactive:-no-coding)#workflow)\\n* [Use custom python functions/modules within workflows](https://github.com/BioimageAnalysisCoreWEHI/napari_lattice/wiki/5.1-Workflows-(Custom-workflow))\\n* [How to use Cellpose for cell segmentation](https://github.com/BioimageAnalysisCoreWEHI/napari_lattice/wiki/5.1-Workflows-(Custom-workflow)#cellpose)\\n\\n\\nSupport will be added for more file formats in the future.\\n\\nSample lattice lightsheet data download: https://doi.org/10.5281/zenodo.7117784\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/plugins/index.html\\n-->\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [GPL-3.0 License] license,\\n\\\"napari_lattice\\\" is free and open source software\\n\\n## Acknowledgment\\n\\n This project was supported by funding from the [Rogers Lab at the Centre for Dynamic Imaging at the Walter and Eliza Hall Institute of Medical Research](https://imaging.wehi.edu.au/). This project has been made possible in part by [Napari plugin accelerator grant](https://chanzuckerberg.com/science/programs-resources/imaging/napari/lattice-light-sheet-data-analysis-toolset/) from the Chan Zuckerberg Initiative DAF, an advised fund of the Silicon Valley Community Foundation.\\n\\n Thanks to the developers and maintainers of the amazing open-source plugins such as [pyclesperanto](https://github.com/clEsperanto/pyclesperanto_prototype), [aicsimageio](https://github.com/AllenCellModeling/aicsimageio), [dask](https://github.com/dask/dask) and [pycudadecon](https://github.com/tlambert03/pycudadecon).\\n Thanks in particular to the developers of open source projects: [LLSpy](https://github.com/tlambert03/LLSpy) and [lls_dd](https://github.com/VolkerH/Lattice_Lightsheet_Deskew_Deconv) as they were referred to extensively for developing napari-lattice.\\n Thanks to the imagesc forum!\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue](https://github.com/BioimageAnalysisCoreWEHI/napari_lattice/issues) along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GGPL-3.0 License]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-lattice\\n\\n\\n\\n\\n\\n\\nThis napari plugin allows deskewing, cropping, visualisation and designing custom analysis pipelines for lattice lightsheet data, particularly from the Zeiss Lattice Lightsheet. The plugin has also been otpimixed to run in headless mode.\\nDocumentation\\nCheck the Wiki page for documentation on how to get started.\\n\\n\\n\\n\\nFunctions\\n\\nDeskewing and deconvolution of Zeiss lattice lightsheet images\\nAbility to preview deskewed image at channel or timepoint of interest\\nCrop and process only a small portion of the image \\nImport ImageJ ROIs for cropping\\nCreate image processing workflows using napari-workflows\\nRun deskewing, deconvolution and custom image processing workflows from the terminal\\nFiles can be saved as h5 (BigDataViewer/BigStitcher) or tiff files\\nRun in terminal without napari, enabling processing workflows on the HPC\\n\\nKey Features\\nApply custom image processing workflows using napari-workflows. \\n* Interactive workflow generation (no coding experience needed)\\n* Use custom python functions/modules within workflows\\n* How to use Cellpose for cell segmentation\\nSupport will be added for more file formats in the future.\\nSample lattice lightsheet data download: https://doi.org/10.5281/zenodo.7117784\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the [GPL-3.0 License] license,\\n\\\"napari_lattice\\\" is free and open source software\\nAcknowledgment\\nThis project was supported by funding from the Rogers Lab at the Centre for Dynamic Imaging at the Walter and Eliza Hall Institute of Medical Research. This project has been made possible in part by Napari plugin accelerator grant from the Chan Zuckerberg Initiative DAF, an advised fund of the Silicon Valley Community Foundation.\\nThanks to the developers and maintainers of the amazing open-source plugins such as pyclesperanto, aicsimageio, dask and pycudadecon.\\n Thanks in particular to the developers of open source projects: LLSpy and lls_dd as they were referred to extensively for developing napari-lattice.\\n Thanks to the imagesc forum!\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"Lattice Lightsheet Analysis\",\"documentation\":\"https://github.com/BioimageAnalysisCoreWEHI/napari_lattice#readme\",\"first_released\":\"2022-07-19T06:02:29.231754Z\",\"license\":\"GPL-3.0\",\"name\":\"napari-lattice\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\",\"widget\"],\"project_site\":\"https://github.com/BioimageAnalysisCoreWEHI/napari_lattice\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[\"*.h5\"],\"release_date\":\"2022-12-22T03:18:01.288110Z\",\"report_issues\":\"https://github.com/BioimageAnalysisCoreWEHI/napari_lattice/issues\",\"requirements\":[\"aicsimageio (>=4.9.1)\",\"aicspylibczi (>=3.0.5)\",\"dask\",\"dask-image\",\"dask[distributed]\",\"magic-class (>=0.6.13)\",\"magicgui\",\"napari[all]\",\"pyopencl\",\"read-roi\",\"gputools\",\"pyclesperanto-prototype (>=0.20.0)\",\"napari-aicsimageio (>=0.7.2)\",\"napari-spreadsheet\",\"napari-workflows (>=0.2.8)\",\"napari-workflow-inspector\",\"npy2bdv\",\"redlionfish\",\"tifffile\",\"fsspec (>=2022.8.2)\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\"],\"summary\":\"Napari plugin for analysing and visualizing lattice lightsheet and Oblique Plane Microscopy data.\",\"support\":\"https://github.com/BioimageAnalysisCoreWEHI/napari_lattice/issues\",\"twitter\":\"\",\"version\":\"0.2.6\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Hanjin Liu\"}],\"code_repository\":\"https://github.com/hanjinliu/napari-power-widgets\",\"description\":\"# napari-power-widgets\\n\\n[![License BSD-3](https://img.shields.io/pypi/l/napari-power-widgets.svg?color=green)](https://github.com/hanjinliu/napari-power-widgets/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-power-widgets.svg?color=green)](https://pypi.org/project/napari-power-widgets)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-power-widgets.svg?color=green)](https://python.org)\\n[![tests](https://github.com/hanjinliu/napari-power-widgets/workflows/tests/badge.svg)](https://github.com/hanjinliu/napari-power-widgets/actions)\\n[![codecov](https://codecov.io/gh/hanjinliu/napari-power-widgets/branch/main/graph/badge.svg)](https://codecov.io/gh/hanjinliu/napari-power-widgets)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-power-widgets)](https://napari-hub.org/plugins/napari-power-widgets)\\n\\nPowerful `magicgui` widgets and type annotations for general-purpose napari plugin development.\\n\\n`napari-power-widgets` makes the full use of type-to-widget mapping strategy of `magicgui` to provide napari-specific types and value-widgets, which will be very useful to improve UI/UX of your napari plugins with simple codes.\\n\\nCurrently, `napari-power-widgets` does not provide any reader, writer or widget. It is supposed to be used programmatically.\\n\\n### Examples\\n\\nSome types/widgets and the possible usage are picked up here ([&rarr; check all](https://github.com/hanjinliu/napari-power-widgets/blob/main/src/napari_power_widgets/types.py)). If you have any neat ideas, please open an issue.\\n\\n#### 1. `BoxSelection`\\n\\nAlias of a four-float tuple for 2D selection. You can set the value by drawing a interaction box in the viewer.\\n\\n*e. g. : image cropper, rectangular labeling etc.*\\n\\n```python\\n@magicgui\\ndef f(box: BoxSelection):\\n    print(box)\\nviewer.window.add_dock_widget(f)\\n```\\n\\n![](images/BoxSelection.gif)\\n\\n#### 2. `OneOfRectangles`\\n\\nAlias of `np.ndarray` for one of rectangles in a `Shapes` layer.\\n\\n*e. g. : image cropper, rectangular labeling etc.*\\n\\n```python\\n@magicgui\\ndef f(rect: OneOfRectangles):\\n    print(rect)\\nviewer.window.add_dock_widget(f)\\n```\\n\\n![](images/OneOfRectangles.gif)\\n\\n#### 3. `LineData`\\n\\nAlias of `np.ndarray` for a line data. You can obtain the data by manually drawing a line in the viewer.\\n\\n*e. g. : line profiling, kymograph etc.*\\n\\n```python\\n@magicgui\\ndef f(line: LineData):\\n    print(line)\\nviewer.window.add_dock_widget(f)\\n```\\n\\n![](images/LineData.gif)\\n\\n#### 4. `OneOfLabels`\\n\\nAlias of boolean `np.ndarray` for a labeled region. You can choose ones by directly clicking the viewer.\\n\\n*e. g. : image masking, feature measurement etc.*\\n\\n```python\\n@magicgui\\ndef f(label: OneOfLabels):\\n    pass\\nviewer.window.add_dock_widget(f)\\n```\\n\\n![](images/OneOfLabels.gif)\\n\\n\\n#### 5. `ZRange`\\n\\nAlias of a tuple of float that represents the limit of the third dimension. You can select the values by moving the dimension slider.\\n\\n*e. g. : movie trimming, partial image projection etc.*\\n\\n```python\\n@magicgui\\ndef f(zrange: ZRange):\\n    print(zrange)\\nviewer.window.add_dock_widget(f)\\n```\\n\\n![](images/ZRange.gif)\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/stable/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-power-widgets` via [pip]:\\n\\n    pip install napari-power-widgets\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/hanjinliu/napari-power-widgets.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-power-widgets\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/hanjinliu/napari-power-widgets/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-power-widgets\\n\\n\\n\\n\\n\\n\\nPowerful magicgui widgets and type annotations for general-purpose napari plugin development.\\nnapari-power-widgets makes the full use of type-to-widget mapping strategy of magicgui to provide napari-specific types and value-widgets, which will be very useful to improve UI/UX of your napari plugins with simple codes.\\nCurrently, napari-power-widgets does not provide any reader, writer or widget. It is supposed to be used programmatically.\\nExamples\\nSome types/widgets and the possible usage are picked up here (→ check all). If you have any neat ideas, please open an issue.\\n1. BoxSelection\\nAlias of a four-float tuple for 2D selection. You can set the value by drawing a interaction box in the viewer.\\ne. g. : image cropper, rectangular labeling etc.\\npython\\n@magicgui\\ndef f(box: BoxSelection):\\n    print(box)\\nviewer.window.add_dock_widget(f)\\n\\n2. OneOfRectangles\\nAlias of np.ndarray for one of rectangles in a Shapes layer.\\ne. g. : image cropper, rectangular labeling etc.\\npython\\n@magicgui\\ndef f(rect: OneOfRectangles):\\n    print(rect)\\nviewer.window.add_dock_widget(f)\\n\\n3. LineData\\nAlias of np.ndarray for a line data. You can obtain the data by manually drawing a line in the viewer.\\ne. g. : line profiling, kymograph etc.\\npython\\n@magicgui\\ndef f(line: LineData):\\n    print(line)\\nviewer.window.add_dock_widget(f)\\n\\n4. OneOfLabels\\nAlias of boolean np.ndarray for a labeled region. You can choose ones by directly clicking the viewer.\\ne. g. : image masking, feature measurement etc.\\npython\\n@magicgui\\ndef f(label: OneOfLabels):\\n    pass\\nviewer.window.add_dock_widget(f)\\n\\n5. ZRange\\nAlias of a tuple of float that represents the limit of the third dimension. You can select the values by moving the dimension slider.\\ne. g. : movie trimming, partial image projection etc.\\npython\\n@magicgui\\ndef f(zrange: ZRange):\\n    print(zrange)\\nviewer.window.add_dock_widget(f)\\n\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-power-widgets via pip:\\npip install napari-power-widgets\\n\\nTo install latest development version :\\npip install git+https://github.com/hanjinliu/napari-power-widgets.git\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-power-widgets\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari-power-widgets\",\"documentation\":\"https://github.com/hanjinliu/napari-power-widgets#README.md\",\"first_released\":\"2022-11-11T15:48:22.469969Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-power-widgets\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[],\"project_site\":\"https://github.com/hanjinliu/napari-power-widgets\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-11-11T15:48:22.469969Z\",\"report_issues\":\"https://github.com/hanjinliu/napari-power-widgets/issues\",\"requirements\":[\"numpy\",\"pandas\",\"typing-extensions\",\"magicgui\",\"napari\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\"],\"summary\":\"Powerful widgets and type annotations for napari plugin widgets\",\"support\":\"https://github.com/hanjinliu/napari-power-widgets/issues\",\"twitter\":\"\",\"version\":\"0.0.1\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Robert Haase\"},{\"name\":\"Martin Schätz\"}],\"code_repository\":\"https://github.com/haesleinhuepf/the-segmentation-game\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"the-segmentation-game\"}],\"description\":\"# The segmentation game - for napari\\n\\n[![License](https://img.shields.io/pypi/l/the-segmentation-game.svg?color=green)](https://github.com/haesleinhuepf/the-segmentation-game/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/the-segmentation-game.svg?color=green)](https://pypi.org/project/the-segmentation-game)\\n[![Python Version](https://img.shields.io/pypi/pyversions/the-segmentation-game.svg?color=green)](https://python.org)\\n[![tests](https://github.com/haesleinhuepf/the-segmentation-game/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/the-segmentation-game/actions)\\n[![codecov](https://codecov.io/gh/haesleinhuepf/the-segmentation-game/branch/main/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/the-segmentation-game)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/the-segmentation-game)](https://napari-hub.org/plugins/the-segmentation-game)\\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.6588373.svg)](https://doi.org/10.5281/zenodo.6588373)\\n\\nGamified image segmentation quality estimation\\n\\n![img.png](https://github.com/haesleinhuepf/the-segmentation-game/raw/main/images/screencast.gif)\\n\\n----------------------------------\\n\\n## Usage\\n\\nThe Segmentation Game allows to quantitatively compare segmentation results to a given ground truth annotation.\\nThis allows fine-tuning parameters of image processing workflows to get optimal segmentation quality. \\nIt also allows comparing different segmentation algorithms and identify which algorithm performs best objectively.\\n\\nThe game can be found in napari's `Tools > Games > The Segmentation Game` menu.\\n\\n### Ground Truth Annotation\\n\\nBefore you can start playing the game, some annotated cells/nuclei are necessary to later compute segmentation quality from.\\nDepending on the used metric, it might be sufficient to annotate a hand full of objects. \\nUse napari's annotation tools as shown below. \\nUse the `+` and `-` keys on your keyboard to increase and decrease the label number that is currently drawn.\\nNote: Avoid label gaps. The labels must be continuously subsequent. If there are pixels annotated with value 2, there must be pixels annotated with value 1.\\n\\n![](https://github.com/haesleinhuepf/the-segmentation-game/raw/main/images/annotation.gif)\\n\\n### Parameter tuning\\n\\nIf you work with one of [napari's segmentation plugins](https://www.napari-hub.org/?search=segmentation&sort=relevance&page=1) that produce labels layers,\\nyou can tune their parameters and see how this influences segmentation quality.\\n\\n![](https://github.com/haesleinhuepf/the-segmentation-game/raw/main/images/parameter_tuning.gif)\\n\\n### Segmentation algorithm comparison\\n\\nIf you aim at comparing different segmentation algorithms, you can collect their results in label layers in the napari viewer.\\nYou can then select the segmentation result from the corresponding pulldown and save quantitative comparison results in the Highscore table.\\n\\n![](https://github.com/haesleinhuepf/the-segmentation-game/raw/main/images/algorithm_comparison.gif)\\n\\n## Metrics\\n\\nCurrently, these metrics are implemented:\\n* Jaccard Index (sparse): The [Jaccard Index](https://en.wikipedia.org/wiki/Jaccard_index) is a measure of overlap. \\n  It lies between 0 (no overlap) and 1 (perfect overlap). \\n  For each annotated ground truth label, the maximum overlap of any segmented label is determined. \\n  The mean overlap of all annotated labels serves as metric result.\\n* Jaccard Index (binary): The annotated ground truth labels and the segmentation result are first binarized considering all annotated pixels as positive and all other labels as negative.\\n  Afterwards, the overlap between the two binary images is computed. This allows comparing binary segmentation algorithms, such as thresholding techniques.\\n* Jaccard Index (binary, sparse): The annotated ground truth image can contain values 1 (negative, false) and 2 (positive, true) and\\n  the segmentation result image will be binarized (0: False, otherwise: True). This allows comparing object/no-object annotations with label images.\\n \\n \\nReceiver operating characteristic ([ROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic))\\n  \\nConsider a two-class thresholding problem (binary pixel-wise classification object/background), in which the outcomes are labeled either as positive (p) or negative (n). There are four possible outcomes from a binary classifier. If the outcome from a prediction is p and the actual value is also p, then it is called a true positive (TP); however if the actual value is n then it is said to be a false positive (FP). Conversely, a true negative (TN) has occurred when both the prediction outcome and the actual value are n, and false negative (FN) is when the prediction outcome is n while the actual value is p. We can organize result in table called confusion matrix, based on positive/neagtive results in row and true and false result in columns. From the confucsion matrix we can get many metrics with various usefulness. The curently implemented used for classification evaluation are:\\n\\n* Sensitivity, recall, hit rate, or true positive rate (TPR): (TP)/ (TP + FP), Sensitivity and specificity mathematically describe the accuracy of a test which reports the presence or absence of a condition. Individuals for which the condition is satisfied are considered \\\"positive\\\" and those for which it is not are considered \\\"negative\\\".\\n* Specificity, selectivity or true negative rate (TNR): (TN)/ (TN + FN), Sensitivity and specificity mathematically describe the accuracy of a test which reports the presence or absence of a condition. Individuals for which the condition is satisfied are considered \\\"positive\\\" and those for which it is not are considered \\\"negative\\\".\\n* Precision or positive predictive value (PPV): (TP)/ (TP + FP), in computing and information science is the process of obtaining information system resources that are relevant to an information need from a collection of those resources. It is quantification for the TP events.\\n* Accuracy: (TP + TN)/ (TP + FP + TN + FN), Accuracy measures observational error. Accuracy is how close or far off a given set of measurements are to their true value. However, it usually fails in imbalanced sets.\\n* Balanced Accuracy: (TP/(TP+FN) + TN/(TN+FP))/2, Balanced Accuracy is trying to even out problems of accuracy in imbalanced sets.\\n* F1 Score: 2*TP/(2*TP + FP + TN + FN), In statistical analysis of binary classification, the F-score or F-measure is a measure of a test's accuracy. It is calculated from the precision and recall of the test, where the precision is the number of true positive results divided by the number of all positive results, including those not identified.\\n* Threat score (TS) or critical success index (CSI): TP/(TP + FP + FN), TC is another name for Jaccard Index (binary).\\n\\nThe ROC measures or confusion matrix is invaluable in cases when when our binary classifier is not ideal (which is often) and we are aiming to not get a general good result but specified low error. In that case we usually need to decide for some trade off, for example we need all (as many as possible) classified true positive objects, but we do not mind getting (usually as few as possible) false positive objects.\\n\\n**What we want to achieve**\\n\\n![Precision-versus-accuracy, source: 10.13140/RG.2.1.1668.7603](https://github.com/martinschatz-cz/the-segmentation-game/blob/main/images/Precision-versus-accuracy.png)\\n\\nWhen we are doing semantic segmentation, we are aiming to classify each pixel (ideally correctly) to each of our classes. But that can be hugr ammount of information, and our object might have significantly much less pixels then number of pixels belonging to background and/or other classes. Before choosing right metrics, we need to set up goal for our classification results. Idealy, we would like to have high accuracy and precission for ach class (as is on pictur above), but we might be happy getting high accuracy with good precision. Realisticaly we might need to be more specific, as to choose how big error we are prepared to accept, or decide if it is acceptable to have FN findings but no FP.\\n\\nPicking up a metric for highly unbalanced classification as in semantic segmentation is challenging. Most of the classic metrics wil fail (but they are stil usable object-wise). And we usually stick up with Jaccard Index/Threat score, F1 Score or anything that will tell us result for TP rate (as we expect we will have less pixels for objects then background and/or other classes).\\n\\n## Literature recommendation\\n\\nHow to choose the right metric for comparing segmentation results is explained in this paper:\\n* [Metrics reloaded: Pitfalls and recommendations for image analysis validation. Maier-Hein L. and Reinke A. et al.](https://arxiv.org/abs/2206.01653)\\n\\n## Related plugins\\n\\nIf you aim at automatically optimizing segmentation quality, there are also napari plugins available with this capability:\\n\\n* [napari-accelerated-pixel-and-object-classification](https://www.napari-hub.org/plugins/napari-accelerated-pixel-and-object-classification)\\n* [napari-workflow-optimizer](https://www.napari-hub.org/plugins/napari-workflow-optimizer)\\n\\n## Installation\\n\\nYou can install `the-segmentation-game` via [pip]:\\n\\n    pip install the-segmentation-game\\n\\n## Contributing\\n\\nContributions - especially new image segmentation quality metrics - are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"the-segmentation-game\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please open a thread on [image.sc](https://image.sc) along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/haesleinhuepf/the-segmentation-game/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"The segmentation game - for napari\\n\\n\\n\\n\\n\\n\\n\\nGamified image segmentation quality estimation\\n\\n\\nUsage\\nThe Segmentation Game allows to quantitatively compare segmentation results to a given ground truth annotation.\\nThis allows fine-tuning parameters of image processing workflows to get optimal segmentation quality. \\nIt also allows comparing different segmentation algorithms and identify which algorithm performs best objectively.\\nThe game can be found in napari's Tools > Games > The Segmentation Game menu.\\nGround Truth Annotation\\nBefore you can start playing the game, some annotated cells/nuclei are necessary to later compute segmentation quality from.\\nDepending on the used metric, it might be sufficient to annotate a hand full of objects. \\nUse napari's annotation tools as shown below. \\nUse the + and - keys on your keyboard to increase and decrease the label number that is currently drawn.\\nNote: Avoid label gaps. The labels must be continuously subsequent. If there are pixels annotated with value 2, there must be pixels annotated with value 1.\\n\\nParameter tuning\\nIf you work with one of napari's segmentation plugins that produce labels layers,\\nyou can tune their parameters and see how this influences segmentation quality.\\n\\nSegmentation algorithm comparison\\nIf you aim at comparing different segmentation algorithms, you can collect their results in label layers in the napari viewer.\\nYou can then select the segmentation result from the corresponding pulldown and save quantitative comparison results in the Highscore table.\\n\\nMetrics\\nCurrently, these metrics are implemented:\\n* Jaccard Index (sparse): The Jaccard Index is a measure of overlap. \\n  It lies between 0 (no overlap) and 1 (perfect overlap). \\n  For each annotated ground truth label, the maximum overlap of any segmented label is determined. \\n  The mean overlap of all annotated labels serves as metric result.\\n* Jaccard Index (binary): The annotated ground truth labels and the segmentation result are first binarized considering all annotated pixels as positive and all other labels as negative.\\n  Afterwards, the overlap between the two binary images is computed. This allows comparing binary segmentation algorithms, such as thresholding techniques.\\n* Jaccard Index (binary, sparse): The annotated ground truth image can contain values 1 (negative, false) and 2 (positive, true) and\\n  the segmentation result image will be binarized (0: False, otherwise: True). This allows comparing object/no-object annotations with label images.\\nReceiver operating characteristic (ROC)\\nConsider a two-class thresholding problem (binary pixel-wise classification object/background), in which the outcomes are labeled either as positive (p) or negative (n). There are four possible outcomes from a binary classifier. If the outcome from a prediction is p and the actual value is also p, then it is called a true positive (TP); however if the actual value is n then it is said to be a false positive (FP). Conversely, a true negative (TN) has occurred when both the prediction outcome and the actual value are n, and false negative (FN) is when the prediction outcome is n while the actual value is p. We can organize result in table called confusion matrix, based on positive/neagtive results in row and true and false result in columns. From the confucsion matrix we can get many metrics with various usefulness. The curently implemented used for classification evaluation are:\\n\\nSensitivity, recall, hit rate, or true positive rate (TPR): (TP)/ (TP + FP), Sensitivity and specificity mathematically describe the accuracy of a test which reports the presence or absence of a condition. Individuals for which the condition is satisfied are considered \\\"positive\\\" and those for which it is not are considered \\\"negative\\\".\\nSpecificity, selectivity or true negative rate (TNR): (TN)/ (TN + FN), Sensitivity and specificity mathematically describe the accuracy of a test which reports the presence or absence of a condition. Individuals for which the condition is satisfied are considered \\\"positive\\\" and those for which it is not are considered \\\"negative\\\".\\nPrecision or positive predictive value (PPV): (TP)/ (TP + FP), in computing and information science is the process of obtaining information system resources that are relevant to an information need from a collection of those resources. It is quantification for the TP events.\\nAccuracy: (TP + TN)/ (TP + FP + TN + FN), Accuracy measures observational error. Accuracy is how close or far off a given set of measurements are to their true value. However, it usually fails in imbalanced sets.\\nBalanced Accuracy: (TP/(TP+FN) + TN/(TN+FP))/2, Balanced Accuracy is trying to even out problems of accuracy in imbalanced sets.\\nF1 Score: 2TP/(2TP + FP + TN + FN), In statistical analysis of binary classification, the F-score or F-measure is a measure of a test's accuracy. It is calculated from the precision and recall of the test, where the precision is the number of true positive results divided by the number of all positive results, including those not identified.\\nThreat score (TS) or critical success index (CSI): TP/(TP + FP + FN), TC is another name for Jaccard Index (binary).\\n\\nThe ROC measures or confusion matrix is invaluable in cases when when our binary classifier is not ideal (which is often) and we are aiming to not get a general good result but specified low error. In that case we usually need to decide for some trade off, for example we need all (as many as possible) classified true positive objects, but we do not mind getting (usually as few as possible) false positive objects.\\nWhat we want to achieve\\n\\nWhen we are doing semantic segmentation, we are aiming to classify each pixel (ideally correctly) to each of our classes. But that can be hugr ammount of information, and our object might have significantly much less pixels then number of pixels belonging to background and/or other classes. Before choosing right metrics, we need to set up goal for our classification results. Idealy, we would like to have high accuracy and precission for ach class (as is on pictur above), but we might be happy getting high accuracy with good precision. Realisticaly we might need to be more specific, as to choose how big error we are prepared to accept, or decide if it is acceptable to have FN findings but no FP.\\nPicking up a metric for highly unbalanced classification as in semantic segmentation is challenging. Most of the classic metrics wil fail (but they are stil usable object-wise). And we usually stick up with Jaccard Index/Threat score, F1 Score or anything that will tell us result for TP rate (as we expect we will have less pixels for objects then background and/or other classes).\\nLiterature recommendation\\nHow to choose the right metric for comparing segmentation results is explained in this paper:\\n* Metrics reloaded: Pitfalls and recommendations for image analysis validation. Maier-Hein L. and Reinke A. et al.\\nRelated plugins\\nIf you aim at automatically optimizing segmentation quality, there are also napari plugins available with this capability:\\n\\nnapari-accelerated-pixel-and-object-classification\\nnapari-workflow-optimizer\\n\\nInstallation\\nYou can install the-segmentation-game via pip:\\npip install the-segmentation-game\\n\\nContributing\\nContributions - especially new image segmentation quality metrics - are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"the-segmentation-game\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please open a thread on image.sc along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"the-segmentation-game\",\"documentation\":\"https://github.com/haesleinhuepf/the-segmentation-game#README.md\",\"first_released\":\"2022-05-27T19:11:51.793388Z\",\"license\":\"BSD-3-Clause\",\"name\":\"the-segmentation-game\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/haesleinhuepf/the-segmentation-game\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-10-02T21:24:14.068301Z\",\"report_issues\":\"https://github.com/haesleinhuepf/the-segmentation-game/issues\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"napari-tools-menu\",\"napari-skimage-regionprops\",\"scikit-learn\"],\"summary\":\"Gamified image segmentation quality estimation\",\"support\":\"https://github.com/haesleinhuepf/the-segmentation-game/issues\",\"twitter\":\"\",\"version\":\"0.2.0\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"jordao.bragantini@czbiohub.org\",\"name\":\"Jordao Bragantini\"}],\"category\":{\"Image modality\":[\"Fluorescence microscopy\"],\"Supported data\":[\"2D\",\"3D\",\"Time series\"],\"Workflow step\":[\"Image reconstruction\",\"Image enhancement\",\"Image registration\",\"Image fusion\",\"Image correction\",\"Visualization\"]},\"category_hierarchy\":{\"Image modality\":[[\"Fluorescence microscopy\",\"Light-sheet microscopy\"]],\"Supported data\":[[\"2D\"],[\"3D\"],[\"Time series\"]],\"Workflow step\":[[\"Image reconstruction\",\"Image denoising\"],[\"Image enhancement\",\"Image denoising\"],[\"Image registration\"],[\"Image fusion\"],[\"Image correction\"],[\"Image reconstruction\",\"Image deconvolution\"],[\"Visualization\",\"Image visualisation\"]]},\"code_repository\":\"https://github.com/royerlab/napari-dexp\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-dexp\"}],\"description\":\"# napari-DEXP\\n\\n[![License](https://img.shields.io/pypi/l/napari-dexp.svg?color=green)](https://github.com/royerlab/napari-dexp/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-dexp.svg?color=green)](https://pypi.org/project/napari-dexp)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-dexp.svg?color=green)](https://python.org)\\n[![tests](https://github.com/royerlab/napari-dexp/workflows/tests/badge.svg)](https://github.com/royerlab/napari-dexp/actions)\\n[![codecov](https://codecov.io/gh/royerlab/napari-dexp/branch/master/graph/badge.svg)](https://codecov.io/gh/royerlab/napari-dexp)\\n\\nA plugin to interface [DEXP](https://github.com/royerlab/dexp) with [napari](https://github.com/napari/napari).\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-dexp` via [pip]:\\n\\n    pip install napari-dexp\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-dexp\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/royerlab/napari-dexp/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-DEXP\\n\\n\\n\\n\\n\\nA plugin to interface DEXP with napari.\\n\\nThis napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-dexp via pip:\\npip install napari-dexp\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-dexp\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"DEXP\",\"documentation\":\"\",\"first_released\":\"2021-07-01T18:14:41.391237Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-dexp\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\",\"writer\"],\"project_site\":\"https://github.com/royerlab/napari-dexp\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[\"*.zarr\",\"*.zarr.zip\"],\"release_date\":\"2022-06-23T19:50:58.841329Z\",\"report_issues\":\"\",\"requirements\":[\"napari\",\"napari-plugin-engine (>=0.1.4)\",\"dexp\",\"numpy\"],\"summary\":\"A simple plugin to use with napari\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.0.7\",\"visibility\":\"public\",\"writer_file_extensions\":[\".zarr\"],\"writer_save_layers\":[\"image\",\"labels\"]}",
  "{\"authors\":[{\"name\":\"Hanjin Liu\"}],\"code_repository\":\"https://github.com/hanjinliu/napari-filaments\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-filaments\"}],\"description\":\"# napari-filaments\\n\\n[![License BSD-3](https://img.shields.io/pypi/l/napari-filaments.svg?color=green)](https://github.com/hanjinliu/napari-filaments/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-filaments.svg?color=green)](https://pypi.org/project/napari-filaments)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-filaments.svg?color=green)](https://python.org)\\n[![tests](https://github.com/hanjinliu/napari-filaments/workflows/tests/badge.svg)](https://github.com/hanjinliu/napari-filaments/actions)\\n[![codecov](https://codecov.io/gh/hanjinliu/napari-filaments/branch/main/graph/badge.svg)](https://codecov.io/gh/hanjinliu/napari-filaments)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-filaments)](https://napari-hub.org/plugins/napari-filaments)\\n\\nA napari plugin for filament analysis.\\n\\nThis plugin helps you to manually track filaments using path shapes of `Shapes` layer.\\n\\n![](https://github.com/hanjinliu/napari-filaments/raw/main/resources/fit.gif)\\n\\nCurrently implemented functions\\n\\n- Fit paths to filaments in an image as a 2-D spline curve.\\n- Clip/extend paths.\\n- Measurement of filament length at sub-pixel precision.\\n- Basic quantification (mean, std, etc.) along paths.\\n- Import paths from ImageJ ROI file.\\n\\nBasic Usage\\n-----------\\n\\nClick `Layers > open image` to open a tif file. You'll find the image you chose and a shapes layer are added to the layer list.\\n\\n![](https://github.com/hanjinliu/napari-filaments/raw/main/resources/fig.png)\\n\\n- The \\\"target filaments\\\" box shows the working shapes layer.\\n- The \\\"target image\\\" box shows the image layer on which fitting and quantification will be conducted.\\n- The \\\"filament\\\" box shows currently selected shape (initially this box is empty).\\n\\nAdd path shapes and push key `F1` to fit the shape to the filament in the target image layer.\\n\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n\\n## Installation\\n\\nYou can install `napari-filaments` via [pip]:\\n\\n    pip install napari-filaments\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/hanjinliu/napari-filaments.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-filaments\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/hanjinliu/napari-filaments/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-filaments\\n\\n\\n\\n\\n\\n\\nA napari plugin for filament analysis.\\nThis plugin helps you to manually track filaments using path shapes of Shapes layer.\\n\\nCurrently implemented functions\\n\\nFit paths to filaments in an image as a 2-D spline curve.\\nClip/extend paths.\\nMeasurement of filament length at sub-pixel precision.\\nBasic quantification (mean, std, etc.) along paths.\\nImport paths from ImageJ ROI file.\\n\\nBasic Usage\\nClick Layers > open image to open a tif file. You'll find the image you chose and a shapes layer are added to the layer list.\\n\\n\\nThe \\\"target filaments\\\" box shows the working shapes layer.\\nThe \\\"target image\\\" box shows the image layer on which fitting and quantification will be conducted.\\nThe \\\"filament\\\" box shows currently selected shape (initially this box is empty).\\n\\nAdd path shapes and push key F1 to fit the shape to the filament in the target image layer.\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\nInstallation\\nYou can install napari-filaments via pip:\\npip install napari-filaments\\n\\nTo install latest development version :\\npip install git+https://github.com/hanjinliu/napari-filaments.git\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-filaments\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari filaments\",\"documentation\":\"https://github.com/hanjinliu/napari-filaments#README.md\",\"first_released\":\"2022-07-01T08:02:15.100050Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-filaments\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/hanjinliu/napari-filaments\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-07-16T06:24:33.490837Z\",\"report_issues\":\"https://github.com/hanjinliu/napari-filaments/issues\",\"requirements\":[\"magic-class (>=0.6.7)\",\"magicgui\",\"matplotlib\",\"numpy\",\"qtpy\",\"scipy\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"tox ; extra == 'testing'\"],\"summary\":\"A napari plugin for filament analysis\",\"support\":\"https://github.com/hanjinliu/napari-filaments/issues\",\"twitter\":\"\",\"version\":\"0.2.1\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"v.o.van_der_valk@lumc.nl\",\"name\":\"Viktor van der Valk\"}],\"code_repository\":\"https://github.com/ViktorvdValk/napari-checkerboard\",\"conda\":[],\"description\":\"# napari-checkerboard\\n\\n[![License](https://img.shields.io/pypi/l/napari-checkerboard.svg?color=green)](https://github.com/ViktorvdValk/napari-checkerboard/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-checkerboard.svg?color=green)](https://pypi.org/project/napari-checkerboard)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-checkerboard.svg?color=green)](https://python.org)\\n[![tests](https://github.com/ViktorvdValk/napari-checkerboard/workflows/tests/badge.svg)](https://github.com/ViktorvdValk/napari-checkerboard/actions)\\n[![codecov](https://codecov.io/gh/ViktorvdValk/napari-checkerboard/branch/master/graph/badge.svg)](https://codecov.io/gh/ViktorvdValk/napari-checkerboard)\\n\\nCompare two images with the itk checkerboard filter\\n\\n\\n<img width=\\\"1430\\\" alt=\\\"Screenshot 2021-05-12 at 15 03 17\\\" src=\\\"https://user-images.githubusercontent.com/33719474/117979519-48bd4680-b333-11eb-874c-d9ec09681d93.png\\\">\\n\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-checkerboard` via [pip]:\\n\\n    pip install napari-checkerboard\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [Apache Software License 2.0] license,\\n\\\"napari-checkerboard\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/ViktorvdValk/napari-checkerboard/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-checkerboard\\n\\n\\n\\n\\n\\nCompare two images with the itk checkerboard filter\\n\\n\\nThis napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-checkerboard via pip:\\npip install napari-checkerboard\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the Apache Software License 2.0 license,\\n\\\"napari-checkerboard\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"napari-checkerboard\",\"documentation\":\"\",\"first_released\":\"2021-05-11T14:38:31.343521Z\",\"license\":\"Apache-2.0\",\"name\":\"napari-checkerboard\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/ViktorvdValk/napari-checkerboard\",\"python_version\":\">=3.6\",\"reader_file_extensions\":[],\"release_date\":\"2021-05-31T15:45:25.395347Z\",\"report_issues\":\"\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy (>=1.19.0)\",\"napari (>=0.4.6)\",\"magicgui (>=0.2.6)\",\"itk-elastix (>=0.11.1)\",\"itk-napari-conversion (>=0.3.1)\",\"napari-itk-io (>=0.1.0)\"],\"summary\":\"Compare two images with the itk checkerboard filter\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.0.3\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Marc Boucsein\"},{\"name\":\"Robin Koch\"}],\"code_repository\":\"https://github.com/DKFZ-TMTRR/Partial-Aligner\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"partial-aligner\"}],\"description\":\"# Partial-Aligner\\n\\n[![License](https://img.shields.io/pypi/l/napari-medical-image-formats.svg?color=green)](https://github.com/MBPhys/Partial-Aligner/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/Partial-Aligner.svg?color=green)](https://pypi.org/project/Partial-Aligner)\\n[![Python Version](https://img.shields.io/pypi/pyversions/Partial-Aligner.svg?color=green)](https://python.org)\\n\\n\\nA napari plugin to affine transform images and parts of images in 2D and 3D. It was developed in the context of brain slice registration and solves multiple, related problems when working with histology slices.\\n\\n----------------------------------\\n\\n## Installation\\n\\nYou can install `Partial-Aligner` via [pip]:\\n\\n    pip install Partial-Aligner\\n    \\nTo make full use of this plugin, please also install the sister plugins:\\n\\n    pip install Label-Creator\\n    pip install Layer-Data-Replace\\n    pip install World2Data\\n\\n## Usage\\n\\nIt is important to note that this plugin is part of a group of plugins ([Label-Creator](https://github.com/DKFZ-TMTRR/Label-Creator, \\\"Creates Labels\\\"), [Layer-Data-Replace](https://github.com/DKFZ-TMTRR/Layer-Data-Replace, \\\"Replaces the data of a layer with other data\\\"), [World2Data](https://github.com/DKFZ-TMTRR/World2Data, \\\"Applies a transformation to an image\\\")) which are intended to be used together. \\n\\nThe principle workflow with this plugin is as follows:\\n\\n1. Load an image of interest (ioi) using standard napari.\\n2. Find out meaningful transformation parameters for the ioi (or part of it) based on what you see in the viewer.\\n3. (optional) Save the affine transformation matrix (can later be applied to other modalities)\\n4. Apply the transformation to create a new, altered version of the ioi (use plugin [World2Data](https://github.com/DKFZ-TMTRR/World2Data, \\\"Applies a transformation to an image\\\"))\\n\\nDecisions on the parameters (step 2) are made based on the problem at hand:\\n\\n- Registration: You have a second (fixed) image and you want to align your ioi to that image? Transform your whole ioi! Just play with the transformation parameters until you are happy with the alignment of ioi and fixed image.\\n\\n<p align=\\\"center\\\">\\n    <img src=\\\"https://user-images.githubusercontent.com/36212786/149524198-9a25b6dc-4169-4546-85b3-7c2f57fccc97.png\\\" width=\\\"50%\\\" height=\\\"50%\\\">  <br /> \\n     <i>DAPI staining (red) before (left) and after (right) manual registration on an MRI image (green).</i> \\n</p>\\n\\n- Histology artifact repair: Parts of your histology slice are misplaced? Transform the misplaced parts! Label them and change the transformation parameters for the misplaced parts until you are happy with their alignment with the rest of the image.\\n\\n<p align=\\\"center\\\">\\n<img src=\\\"https://user-images.githubusercontent.com/36212786/149526385-09aeebe2-d03e-4dd4-a424-d0f3af207529.png\\\" width=\\\"50%\\\" height=\\\"50%\\\">  <br /> \\n     <i> Original slice with misplaced region (left), marked using the label function (middle) and after manual adjustment (right), where the misplaced region (green) was cut and newly positioned.</i> \\n</p>\\n\\nTo make this plugin run reasonably fast, the affine transformations are not applied to the image data in real time. Instead, the internal napari viewing parameters are changed according to the transformation parameters. Therefore, to save transformed image data, the [World2Data](https://github.com/DKFZ-TMTRR/World2Data, \\\"Applies a transformation to an image\\\") plugin is used, which calculates and saves the resulting image based on the internal napari viewing parameters.\\n\\n\\nHere we showcase a resulting multimodal 3D alignment of a whole mouse brain. The modalities are CT, MRI, simulated radiation dose distributions, DAPI staining and DNA-damage repair foci, with a Nissl-staining mouse atlas as template.\\n\\nhttps://user-images.githubusercontent.com/36212786/149530462-51a53631-bf74-459b-ab4e-572c52cf2692.mov\\n\\n\\n\\n\\n\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox].\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"Partial-Aligner\\\" is free and open source software.\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/MBPhys/Partial-Aligner/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"Partial-Aligner\\n\\n\\n\\nA napari plugin to affine transform images and parts of images in 2D and 3D. It was developed in the context of brain slice registration and solves multiple, related problems when working with histology slices.\\n\\nInstallation\\nYou can install Partial-Aligner via pip:\\npip install Partial-Aligner\\n\\nTo make full use of this plugin, please also install the sister plugins:\\npip install Label-Creator\\npip install Layer-Data-Replace\\npip install World2Data\\n\\nUsage\\nIt is important to note that this plugin is part of a group of plugins (Label-Creator, Layer-Data-Replace, World2Data) which are intended to be used together. \\nThe principle workflow with this plugin is as follows:\\n\\nLoad an image of interest (ioi) using standard napari.\\nFind out meaningful transformation parameters for the ioi (or part of it) based on what you see in the viewer.\\n(optional) Save the affine transformation matrix (can later be applied to other modalities)\\nApply the transformation to create a new, altered version of the ioi (use plugin World2Data)\\n\\nDecisions on the parameters (step 2) are made based on the problem at hand:\\n\\nRegistration: You have a second (fixed) image and you want to align your ioi to that image? Transform your whole ioi! Just play with the transformation parameters until you are happy with the alignment of ioi and fixed image.\\n\\n\\n \\nDAPI staining (red) before (left) and after (right) manual registration on an MRI image (green).\\n\\n\\nHistology artifact repair: Parts of your histology slice are misplaced? Transform the misplaced parts! Label them and change the transformation parameters for the misplaced parts until you are happy with their alignment with the rest of the image.\\n\\n\\n \\n Original slice with misplaced region (left), marked using the label function (middle) and after manual adjustment (right), where the misplaced region (green) was cut and newly positioned.\\n\\nTo make this plugin run reasonably fast, the affine transformations are not applied to the image data in real time. Instead, the internal napari viewing parameters are changed according to the transformation parameters. Therefore, to save transformed image data, the World2Data plugin is used, which calculates and saves the resulting image based on the internal napari viewing parameters.\\nHere we showcase a resulting multimodal 3D alignment of a whole mouse brain. The modalities are CT, MRI, simulated radiation dose distributions, DAPI staining and DNA-damage repair foci, with a Nissl-staining mouse atlas as template.\\nhttps://user-images.githubusercontent.com/36212786/149530462-51a53631-bf74-459b-ab4e-572c52cf2692.mov\\nContributing\\nContributions are very welcome. Tests can be run with tox.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"Partial-Aligner\\\" is free and open source software.\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"Partial-Aligner\",\"documentation\":\"\",\"first_released\":\"2022-01-14T15:10:00.208187Z\",\"license\":\"BSD-3-Clause\",\"name\":\"Partial-Aligner\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/DKFZ-TMTRR/Partial-Aligner\",\"python_version\":\">=3.9\",\"reader_file_extensions\":[],\"release_date\":\"2022-01-14T15:10:00.208187Z\",\"report_issues\":\"\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"packaging\",\"dask\"],\"summary\":\"A napari plugin for manual registration of (a part of) an image\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.0.1\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Chi-Li Chiu\"}],\"code_repository\":\"https://github.com/chili-chiu/napari-bio-sample-data\",\"conda\":[],\"description\":\"# napari-bio-sample-data\\n\\n[![License](https://img.shields.io/pypi/l/napari-bio-sample-data.svg?color=green)](https://github.com/chili-chiu/napari-bio-sample-data/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-bio-sample-data.svg?color=green)](https://pypi.org/project/napari-bio-sample-data)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-bio-sample-data.svg?color=green)](https://python.org)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-bio-sample-data)](https://napari-hub.org/plugins/napari-bio-sample-data)\\n\\na sample data plugin for bio-related demos\\n\\n----------------------------------\\nThis plugin contains 5 sample datasets with additional napari layer types:\\n\\n(1) 3D EM dataset (image + points + vectors)  \\nImage credit: Alister Burt  \\nThe [original data](https://github.com/alisterburt/napari-cryo-et-demo) is down-sampled to have smaller file size.  \\n<img width=\\\"300\\\" alt=\\\"image\\\" src=\\\"https://user-images.githubusercontent.com/89602983/178569428-7daa2eb8-a3ff-4c0e-8e5f-4f615a55684f.png\\\">\\n\\n(2) 2D skin RGB dataset (image + shape)  \\nImage credit: skimage.data.skin  \\n<img width=\\\"300\\\" alt=\\\"image\\\" src=\\\"https://user-images.githubusercontent.com/89602983/178569580-bf77e55c-71cc-4883-9fe5-ed94e05f2a29.png\\\">\\n  \\n(3) 3D nuclei dataset (image + label + surface)  \\nImage credit: skimage.data.cells3d  \\n<img width=\\\"300\\\" alt=\\\"image\\\" src=\\\"https://user-images.githubusercontent.com/89602983/178569701-7c9b1cc3-c1c3-4e54-8ca0-fb2b530f858e.png\\\">\\n\\n(4) 2D timelapse dataset (image + points + tracks)  \\nImage credit: [Cell Tracking Challenge](http://celltrackingchallenge.net/2d-datasets/)  \\nThe original data is cropped to have smaller file size.  \\n<img width=\\\"300\\\" alt=\\\"image\\\" src=\\\"https://user-images.githubusercontent.com/89602983/178569846-b995d1cb-c1ec-4363-ba1a-71243ffea4e0.png\\\">\\n\\n(5) large multi-resolution 3D EM dataset  \\nImage credit: [Janelia Open Organelle](https://openorganelle.janelia.org/datasets/jrc_hela-1)   \\nThis plugin only accesses 2 lower resolution levels.  \\n<img width=\\\"300\\\" alt=\\\"image\\\" src=\\\"https://user-images.githubusercontent.com/89602983/178570136-6f59ba3c-d687-446c-9f5e-1df567a62948.png\\\">\\n\\nDatasets (1)-(4) are stored locally.   \\nDataset (5) is downloaded and temporarily stored on RAM when accessed.    \\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-bio-sample-data` via [pip]:\\n\\n    pip install napari-bio-sample-data\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/chili-chiu/napari-bio-sample-data.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-bio-sample-data\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/chili-chiu/napari-bio-sample-data/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-bio-sample-data\\n\\n\\n\\n\\na sample data plugin for bio-related demos\\n\\nThis plugin contains 5 sample datasets with additional napari layer types:\\n(1) 3D EM dataset (image + points + vectors)\\nImage credit: Alister Burt\\nThe original data is down-sampled to have smaller file size.\\n\\n(2) 2D skin RGB dataset (image + shape)\\nImage credit: skimage.data.skin\\n\\n(3) 3D nuclei dataset (image + label + surface)\\nImage credit: skimage.data.cells3d\\n\\n(4) 2D timelapse dataset (image + points + tracks)\\nImage credit: Cell Tracking Challenge\\nThe original data is cropped to have smaller file size.\\n\\n(5) large multi-resolution 3D EM dataset\\nImage credit: Janelia Open Organelle \\nThis plugin only accesses 2 lower resolution levels.\\n\\nDatasets (1)-(4) are stored locally. \\nDataset (5) is downloaded and temporarily stored on RAM when accessed.    \\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-bio-sample-data via pip:\\npip install napari-bio-sample-data\\n\\nTo install latest development version :\\npip install git+https://github.com/chili-chiu/napari-bio-sample-data.git\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-bio-sample-data\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari bio sample data\",\"documentation\":\"https://github.com/chili-chiu/napari-bio-sample-data#README.md\",\"first_released\":\"2022-07-12T19:19:01.431082Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-bio-sample-data\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"sample_data\"],\"project_site\":\"https://github.com/chili-chiu/napari-bio-sample-data\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-07-13T21:13:29.844275Z\",\"report_issues\":\"https://github.com/chili-chiu/napari-bio-sample-data/issues\",\"requirements\":[\"numpy\",\"fsspec\",\"zarr (>=2.12.0)\",\"dask\",\"s3fs\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\"],\"summary\":\"a sample data plugin for bio-related demos\",\"support\":\"https://github.com/chili-chiu/napari-bio-sample-data/issues\",\"twitter\":\"\",\"version\":\"0.0.4\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"alan.watson@pitt.edu\",\"name\":\"Alan M Watson\"}],\"code_repository\":\"https://github.com/AlanMWatson/napari-imaris-loader\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-imaris-loader\"}],\"description\":\"\\n\\n# Description\\n\\nThis plugin enables viewing of Bitplane Imaris files, including very large datasets.  The GIFs below demonstrate rendering of a ~2TB IMS file containing a 2 color whole mouse brain.  The plugin has been tested on datasets as large as 20TB.\\n\\n**NOTE: For this plugin to work \\\"File/Preferences/Experimental/Render Images Asynchronously\\\" must be selected.**\\n\\n### Open IMS file:\\n\\n![Open IMS file GIF](https://i.imgur.com/ByHb0wI.gif \\\"Open IMS file\\\")\\n\\n\\n\\n### Render in 3D:\\n\\nA plugin is provided to dynamically reload the data after selecting the lowest resolution level to be included in the viewer.  Since napari only renders the lowest resolution, the user can use this plugin to control the quality of 3D rendering.  See features and limitations for tips on suggested usage.\\n\\n![3D Rendering and Quality Adjustment GIF](https://i.imgur.com/MZNlWtM.gif \\\"3D Rendering and Quality Adjustment\\\")\\n\\n### Features\\n\\n* Multiscale Rendering\\n  * Image pyramids which are present in the native IMS format are automatically added to napari during file loading.\\n* Chunks are implemented by dask and matched to the chunk sizes stored in each dataset.  (Napari appears to only ask for 2D chunks - unclear how helpful this feature is currently)\\n* Successfully handles multi-terabyte multi-timepoint multi-channel datasets.\\n* Tested with all sample files provided by Bitplane.\\n* Higher 3D rendering quality is enabled by a widget that reloads data after specifying the lowest resolution level (higher number = lower resolution) to be included in the multiscale series.\\n\\n### Known Issues / limitations\\n\\n* Currently, this is **only an image loader**, and there are no features for loading or viewing objects\\n* Napari sometimes throws errors indicating that it expected a 3D or 5D array but receives the other.\\n  * This sometimes *but relatively rarely* causes napari to crash\\n  * Would like to enable Asynchronous Tiling of Images, but this results in more instability and causes crashes.\\n\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-imaris-loader` via [pip]:\\n\\n    pip install napari-imaris-loader\\n\\n## Change Log:\\n\\n##### <u>v0.1.2:</u>\\n\\n**Fixed:** Issue where .ims files containing a single color 2D image would not open.\\n\\n**Fixed:** Issue where using the widget to change resolutions while in 3D rendering would cause a crash.  Now the viewer is automatically forced into 2D rendering mode when the widget is used.\\n\\n**Dependency change:** The loader is now dependent in a separate package for loading IMS files.  https://pypi.org/project/imaris-ims-file-reader/\\n\\n**v0.1.3:**\\n\\nDocumentation\\n\\n**v0.1.4:**\\n\\nAdd napari to install requirements for plugin compatibility\\n\\n**v0.1.5:**\\n\\nChanges to napari:\\n\\n- now requires napari[all] upon install.\\n- requires >=v0.1.5 of imaris-ims-file-reader\\n\\n**v0.1.6:**\\n\\n- Fix issue #7 where contrastLimits possibly unbound in reader\\n\\n**v0.1.7:**\\n\\n- For squeeze_output=False when opening .ims file for Napari compatibility\\n\\n**v0.1.8:**\\n\\n- Add automatic determination of contrast_limits\\n- Fix bug in squeeze_output\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-imaris-loader\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/AlanMWatson/napari-imaris-loader/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"Description\\nThis plugin enables viewing of Bitplane Imaris files, including very large datasets.  The GIFs below demonstrate rendering of a ~2TB IMS file containing a 2 color whole mouse brain.  The plugin has been tested on datasets as large as 20TB.\\nNOTE: For this plugin to work \\\"File/Preferences/Experimental/Render Images Asynchronously\\\" must be selected.\\nOpen IMS file:\\n\\nRender in 3D:\\nA plugin is provided to dynamically reload the data after selecting the lowest resolution level to be included in the viewer.  Since napari only renders the lowest resolution, the user can use this plugin to control the quality of 3D rendering.  See features and limitations for tips on suggested usage.\\n\\nFeatures\\n\\nMultiscale Rendering\\nImage pyramids which are present in the native IMS format are automatically added to napari during file loading.\\nChunks are implemented by dask and matched to the chunk sizes stored in each dataset.  (Napari appears to only ask for 2D chunks - unclear how helpful this feature is currently)\\nSuccessfully handles multi-terabyte multi-timepoint multi-channel datasets.\\nTested with all sample files provided by Bitplane.\\nHigher 3D rendering quality is enabled by a widget that reloads data after specifying the lowest resolution level (higher number = lower resolution) to be included in the multiscale series.\\n\\nKnown Issues / limitations\\n\\nCurrently, this is only an image loader, and there are no features for loading or viewing objects\\nNapari sometimes throws errors indicating that it expected a 3D or 5D array but receives the other.\\nThis sometimes but relatively rarely causes napari to crash\\nWould like to enable Asynchronous Tiling of Images, but this results in more instability and causes crashes.\\n\\n\\nThis napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-imaris-loader via pip:\\npip install napari-imaris-loader\\n\\nChange Log:\\nv0.1.2:\\nFixed: Issue where .ims files containing a single color 2D image would not open.\\nFixed: Issue where using the widget to change resolutions while in 3D rendering would cause a crash.  Now the viewer is automatically forced into 2D rendering mode when the widget is used.\\nDependency change: The loader is now dependent in a separate package for loading IMS files.  https://pypi.org/project/imaris-ims-file-reader/\\nv0.1.3:\\nDocumentation\\nv0.1.4:\\nAdd napari to install requirements for plugin compatibility\\nv0.1.5:\\nChanges to napari:\\n\\nnow requires napari[all] upon install.\\nrequires >=v0.1.5 of imaris-ims-file-reader\\n\\nv0.1.6:\\n\\nFix issue #7 where contrastLimits possibly unbound in reader\\n\\nv0.1.7:\\n\\nFor squeeze_output=False when opening .ims file for Napari compatibility\\n\\nv0.1.8:\\n\\nAdd automatic determination of contrast_limits\\nFix bug in squeeze_output\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-imaris-loader\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari-imaris-loader\",\"documentation\":\"https://github.com/AlanMWatson/napari-imaris-loader#README.md\",\"first_released\":\"2021-10-21T20:10:04.792830Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-imaris-loader\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\",\"widget\"],\"project_site\":\"https://github.com/AlanMWatson/napari-imaris-loader\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[\"*\"],\"release_date\":\"2022-04-13T18:03:21.139205Z\",\"report_issues\":\"https://github.com/AlanMWatson/napari-imaris-loader/issues\",\"requirements\":[\"napari[all]\",\"napari-plugin-engine (>=0.1.4)\",\"imaris-ims-file-reader (>=0.1.5)\",\"numpy\",\"h5py\",\"dask\"],\"summary\":\"Napari plugin for loading Bitplane imaris files '.ims'\",\"support\":\"https://github.com/AlanMWatson/napari-imaris-loader/issues\",\"twitter\":\"\",\"version\":\"0.1.8\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"jhnnsrs@gmail.com\",\"name\":\"jhnnsrs\"}],\"code_repository\":null,\"conda\":[],\"description\":\"\",\"description_content_type\":\"None\",\"description_text\":\"\",\"development_status\":[],\"display_name\":\"faser\",\"documentation\":\"\",\"first_released\":\"2022-05-23T13:06:03.257630Z\",\"license\":\"\",\"name\":\"faser\",\"npe2\":true,\"operating_system\":[],\"plugin_types\":[\"widget\"],\"project_site\":\"\",\"python_version\":\">=3.8,<3.11\",\"reader_file_extensions\":[],\"release_date\":\"2022-06-01T13:42:15.985064Z\",\"report_issues\":\"\",\"requirements\":[\"magicgui (>=0.4.0,<0.5.0)\",\"napari-plugin_engine (>=0.1.4,<0.2.0)\",\"numpy (>=1.22.4,<2.0.0)\",\"pydantic (>=1.9.1,<2.0.0)\"],\"summary\":\"\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.2.9\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Robert Haase\"}],\"code_repository\":\"https://github.com/haesleinhuepf/napari-brightness-contrast\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-brightness-contrast\"}],\"description\":\"# napari-brightness-contrast\\n\\n[![License](https://img.shields.io/pypi/l/napari-brightness-contrast.svg?color=green)](https://github.com/haesleinhuepf/napari-brightness-contrast/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-brightness-contrast.svg?color=green)](https://pypi.org/project/napari-brightness-contrast)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-brightness-contrast.svg?color=green)](https://python.org)\\n[![tests](https://github.com/haesleinhuepf/napari-brightness-contrast/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-brightness-contrast/actions)\\n[![codecov](https://codecov.io/gh/haesleinhuepf/napari-brightness-contrast/branch/master/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-brightness-contrast)\\n[![Development Status](https://img.shields.io/pypi/status/napari-brightness-contrast.svg)](https://en.wikipedia.org/wiki/Software_release_life_cycle#Alpha)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-brightness-contrast)](https://napari-hub.org/plugins/napari-brightness-contrast)\\n\\nAdvanced layer histogram visualization options, e.g. for brightness / contrast\\n![](https://github.com/haesleinhuepf/napari-brightness-contrast/blob/main/docs/images/napari-brightness-contrast3.gif?raw=true)\\n\\nNote: This will not work for big image data at the moment. \\nIf the user interface feels slow, consider installing [pyclesperanto](https://github.com/clEsperanto/pyclesperanto_prototype) to speed it up.\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-brightness-contrast` via [pip]:\\n\\n    pip install napari-brightness-contrast\\n\\n## Contributing\\n\\nContributions are very welcome.  \\nAfter cloning the repo, install using `pip install -e .[tests]` to enable testing via `pytest`.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-brightness-contrast\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [open a thread on image.sc](https://image.sc) along with a detailed description and tag [@haesleinhuepf](https://github.com/haesleinhuepf).\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/haesleinhuepf/napari-brightness-contrast/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-brightness-contrast\\n\\n\\n\\n\\n\\n\\n\\nAdvanced layer histogram visualization options, e.g. for brightness / contrast\\n\\nNote: This will not work for big image data at the moment. \\nIf the user interface feels slow, consider installing pyclesperanto to speed it up.\\n\\nThis napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-brightness-contrast via pip:\\npip install napari-brightness-contrast\\n\\nContributing\\nContributions are very welcome.\\nAfter cloning the repo, install using pip install -e .[tests] to enable testing via pytest.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-brightness-contrast\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please open a thread on image.sc along with a detailed description and tag @haesleinhuepf.\",\"development_status\":[\"Development Status :: 3 - Alpha\"],\"display_name\":\"napari-brightness-contrast\",\"documentation\":\"https://github.com/haesleinhuepf/napari-brightness-contrast#README.md\",\"first_released\":\"2021-08-07T07:55:39.881737Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-brightness-contrast\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/haesleinhuepf/napari-brightness-contrast\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-09-25T14:18:56.879055Z\",\"report_issues\":\"https://github.com/haesleinhuepf/napari-brightness-contrast/issues\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"napari\",\"numpy\",\"pyqtgraph\",\"superqt\",\"napari-tools-menu\",\"pytest ; extra == 'tests'\",\"pytest-qt ; extra == 'tests'\"],\"summary\":\"Advanced layer visualization options\",\"support\":\"https://github.com/haesleinhuepf/napari-brightness-contrast/issues\",\"twitter\":\"\",\"version\":\"0.1.8\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"trevor.j.manz@gmail.com\",\"name\":\"Trevor Manz\"}],\"category\":{\"Workflow step\":[\"Visualization\"]},\"category_hierarchy\":{\"Workflow step\":[[\"Visualization\"]]},\"code_repository\":\"https://github.com/manzt/napari-dzi-zarr\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-dzi-zarr\"}],\"description\":\"# napari-dzi-zarr\\n\\n[![License](https://img.shields.io/pypi/l/napari-dzi-zarr.svg?color=green)](https://github.com/napari/napari-dzi-zarr/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-dzi-zarr.svg?color=green)](https://pypi.org/project/napari-dzi-zarr)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-dzi-zarr.svg?color=green)](https://python.org)\\n[![tests](https://github.com/manzt/napari-dzi-zarr/workflows/tests/badge.svg)](https://github.com/manzt/napari-dzi-zarr/actions)\\n\\nAn experimental plugin for viewing Deep Zoom Images (DZI) with napari + zarr + dask.\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n## Description \\n\\nThe [DZI File Format](https://github.com/openseadragon/openseadragon/wiki/The-DZI-File-Format) \\nis a pyramidal tile source specification where individual tiles are RGB/RGBA JPEG/PNG images. \\nDZI is a very popular tile source for zoomable web-viewers like \\n[OpenSeadragon](https://openseadragon.github.io/), and as a result many tile sources are available over \\nHTTP. This plugin wraps a DZI tile source (local or remote) as a multiscale Zarr, where each pyramidal level is a `zarr.Array` of shape `(level_height, level_width, 3/4)`, allowing the same images to be viewed \\nin `napari` + `dask`.\\n\\n## Installation\\n\\nYou can install `napari-dzi-zarr` via [pip]:\\n\\n    pip install napari-dzi-zarr\\n\\n\\n## Usage\\n\\nThis high-resolution scan of Rembrandt's Night Watch is available thanks to [R.G Erdmann](https://twitter.com/erdmann). More examples can be found on [boschproject.org](http://boschproject.org).\\n\\n    $ napari http://hyper-resolution.org/dzi/Rijksmuseum/SK-C-5/SK-C-5_VIS_20-um_2019-12-21.dzi\\n\\n![Rembrandt's Night Watch in napari](./night_watch_napari.png)\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox].\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-dzi-zarr\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/manzt/napari-dzi-zarr/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-dzi-zarr\\n\\n\\n\\n\\nAn experimental plugin for viewing Deep Zoom Images (DZI) with napari + zarr + dask.\\n\\nThis napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.\\nDescription\\nThe DZI File Format \\nis a pyramidal tile source specification where individual tiles are RGB/RGBA JPEG/PNG images. \\nDZI is a very popular tile source for zoomable web-viewers like \\nOpenSeadragon, and as a result many tile sources are available over \\nHTTP. This plugin wraps a DZI tile source (local or remote) as a multiscale Zarr, where each pyramidal level is a zarr.Array of shape (level_height, level_width, 3/4), allowing the same images to be viewed \\nin napari + dask.\\nInstallation\\nYou can install napari-dzi-zarr via pip:\\npip install napari-dzi-zarr\\n\\nUsage\\nThis high-resolution scan of Rembrandt's Night Watch is available thanks to R.G Erdmann. More examples can be found on boschproject.org.\\n$ napari http://hyper-resolution.org/dzi/Rijksmuseum/SK-C-5/SK-C-5_VIS_20-um_2019-12-21.dzi\\n\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-dzi-zarr\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"napari-dzi-zarr\",\"documentation\":\"\",\"first_released\":\"2020-08-20T17:18:30.022784Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-dzi-zarr\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\"],\"project_site\":\"https://github.com/manzt/napari-dzi-zarr\",\"python_version\":\">=3.6\",\"reader_file_extensions\":[\"*\"],\"release_date\":\"2021-04-06T14:13:16.654713Z\",\"report_issues\":\"\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy (>=0.1.19)\",\"zarr (>=0.2.4)\",\"dask[array] (>=2.23.0)\",\"fsspec (>=0.8.0)\",\"requests (>=2.24.0)\",\"aiohttp (>=3.6.2)\",\"imageio (>=2.9.0)\"],\"summary\":\"An experimental plugin for viewing Deep Zoom Images (DZI) with napari and zarr.\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.1.2\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"dkrentzel@pm.me\",\"name\":\"Daniel Krentzel\"}],\"category\":{\"Image modality\":[\"Electron microscopy\",\"Multimodal imaging\"],\"Workflow step\":[\"Image registration\"]},\"category_hierarchy\":{\"Image modality\":[[\"Electron microscopy\",\"Correlative light and electron microscopy\"],[\"Multimodal imaging\",\"Correlative light and electron microscopy\"]],\"Workflow step\":[[\"Image registration\"]]},\"code_repository\":\"https://github.com/krentzd/napari-clemreg\",\"conda\":[],\"description\":\"# napari-clemreg\\n\\n<!-- [![License](https://img.shields.io/pypi/l/napari-clemreg.svg?color=green)](https://github.com/krentzd/napari-clemreg/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-clemreg.svg?color=green)](https://pypi.org/project/napari-clemreg)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-clemreg.svg?color=green)](https://python.org)\\n[![tests](https://github.com/krentzd/napari-clemreg/workflows/tests/badge.svg)](https://github.com/krentzd/napari-clemreg/actions)\\n[![codecov](https://codecov.io/gh/krentzd/napari-clemreg/branch/master/graph/badge.svg)](https://codecov.io/gh/krentzd/napari-clemreg) -->\\n\\nAn automated point-set based registration algorithm for correlative light and electron microscopy (CLEM) \\n----------------------------------\\n## Installation\\n\\nTo install `napari-clemreg` it is recommended to create a fresh [conda] enviornment with Python 3.8:\\n\\n```\\nconda create -n clemreg_env python=3.8\\n```\\nNext, install `napari` with the following command via [pip]: \\n\\n```\\npip install \\\"napari[all]\\\"\\n```\\n\\nFinally, `napari-clemreg` can be installed with:\\n```\\npip install napari-clemreg\\n```\\n\\n\\nWhen installing `napari-clemreg` on a Windows machine, the following error might appear:\\n```\\nerror Microsoft Visual C++ 14.0 is required\\n```\\nEnsure that [Visual Studios C++ 14.00](https://visualstudio.microsoft.com/thank-you-downloading-visual-studio/?sku=BuildTools&rel=16) is installed\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [MIT] license,\\n\\\"napari-clemreg\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/krentzd/napari-clemreg/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n[conda]: https://docs.conda.io/en/latest/\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-clemreg\\n\\nAn automated point-set based registration algorithm for correlative light and electron microscopy (CLEM)\\nInstallation\\nTo install napari-clemreg it is recommended to create a fresh conda enviornment with Python 3.8:\\nconda create -n clemreg_env python=3.8\\nNext, install napari with the following command via pip: \\npip install \\\"napari[all]\\\"\\nFinally, napari-clemreg can be installed with:\\npip install napari-clemreg\\nWhen installing napari-clemreg on a Windows machine, the following error might appear:\\nerror Microsoft Visual C++ 14.0 is required\\nEnsure that Visual Studios C++ 14.00 is installed\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the MIT license,\\n\\\"napari-clemreg\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari-clemreg\",\"documentation\":\"https://github.com/krentzd/napari-clemreg#README.md\",\"first_released\":\"2021-06-01T13:21:09.681333Z\",\"license\":\"MIT\",\"name\":\"napari-clemreg\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\",\"widget\"],\"project_site\":\"https://github.com/krentzd/napari-clemreg\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[\"*\"],\"release_date\":\"2022-02-06T23:32:42.086726Z\",\"report_issues\":\"https://github.com/krentzd/napari-clemreg/issues\",\"requirements\":null,\"summary\":\"A plugin for registering multimodal image volumes based on common segmented structures of interest with point-clouds.\",\"support\":\"https://github.com/krentzd/napari-clemreg/issues\",\"twitter\":\"\",\"version\":\"0.0.1a4\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Andy Sweet\"},{\"name\":\"Chi-Li Chiu\"}],\"code_repository\":\"https://github.com/andy-sweet/napari-blob-detection\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-blob-detection\"}],\"description\":\"# napari-blob-detection\\n\\n[![License](https://img.shields.io/pypi/l/napari-blob-detection.svg?color=green)](https://github.com/andy-sweet/napari-blob-detection/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-blob-detection.svg?color=green)](https://pypi.org/project/napari-blob-detection)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-blob-detection.svg?color=green)](https://python.org)\\n[![tests](https://github.com/andy-sweet/napari-blob-detection/workflows/tests/badge.svg)](https://github.com/andy-sweet/napari-blob-detection/actions)\\n[![codecov](https://codecov.io/gh/andy-sweet/napari-blob-detection/branch/main/graph/badge.svg)](https://codecov.io/gh/andy-sweet/napari-blob-detection)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-blob-detection)](https://napari-hub.org/plugins/napari-blob-detection)\\n\\nDetects blobs in images\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/plugins/stable/index.html\\n-->\\n\\nThis plugin consists of two widgets:\\n\\n1. Detects blobs on images\\n2. Convert points layer to labels layer\\n\\n----------------------------------\\n\\n### Detects blobs on images\\n\\nThis widget uses [scikit-image's blob detection algorithms](https://scikit-image.org/docs/stable/auto_examples/features_detection/plot_blob.html) to detect bright blobs on dark backgrounds.\\n\\nParameters\\n\\n- method: Laplacian of Gaussian (most accurate) or Difference of Gaussian (faster approximation) \\n- image: Image layer for blob detection. Can be a 2D, 3D, or higher dimensionality image.\\n- dimensionality: users can specify if the image is 2D(+t) or 3D(+t).\\n- min sigma: the smallest blob size to detect\\n- max sigma: the largest blob size to detect\\n- threshold: the lower the threshold, the more low intensity blobs are detected. \\n\\nOutput\\n\\nBlobs are represented by the Points layer.\\nThe size of each blob is proportional to `Points.feature['sigma']`,\\nwhich signifies the scale at which the feature point was found.\\n\\n### Convert points layer to labels layer\\n\\nThis widget takes a points layer and converts it into a labels layer, with the image dimension matching the selected image layer.\\nBy converting points to labels, users can leverage feature extraction functions that are available to labels to the detected points.\\n\\n----------------------------------\\n\\n## Installation\\n\\nYou can install `napari-blob-detection` via [pip]:\\n\\n    pip install napari-blob-detection\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/andy-sweet/napari-blob-detection.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-blob-detection\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/andy-sweet/napari-blob-detection/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-blob-detection\\n\\n\\n\\n\\n\\n\\nDetects blobs in images\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nThis plugin consists of two widgets:\\n\\nDetects blobs on images\\nConvert points layer to labels layer\\n\\n\\nDetects blobs on images\\nThis widget uses scikit-image's blob detection algorithms to detect bright blobs on dark backgrounds.\\nParameters\\n\\nmethod: Laplacian of Gaussian (most accurate) or Difference of Gaussian (faster approximation) \\nimage: Image layer for blob detection. Can be a 2D, 3D, or higher dimensionality image.\\ndimensionality: users can specify if the image is 2D(+t) or 3D(+t).\\nmin sigma: the smallest blob size to detect\\nmax sigma: the largest blob size to detect\\nthreshold: the lower the threshold, the more low intensity blobs are detected. \\n\\nOutput\\nBlobs are represented by the Points layer.\\nThe size of each blob is proportional to Points.feature['sigma'],\\nwhich signifies the scale at which the feature point was found.\\nConvert points layer to labels layer\\nThis widget takes a points layer and converts it into a labels layer, with the image dimension matching the selected image layer.\\nBy converting points to labels, users can leverage feature extraction functions that are available to labels to the detected points.\\n\\nInstallation\\nYou can install napari-blob-detection via pip:\\npip install napari-blob-detection\\n\\nTo install latest development version :\\npip install git+https://github.com/andy-sweet/napari-blob-detection.git\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-blob-detection\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari blob detection\",\"documentation\":\"https://github.com/andy-sweet/napari-blob-detection#README.md\",\"first_released\":\"2022-04-22T20:36:05.579776Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-blob-detection\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/andy-sweet/napari-blob-detection\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-04-22T21:55:45.936883Z\",\"report_issues\":\"https://github.com/andy-sweet/napari-blob-detection/issues\",\"requirements\":[\"napari (>=0.4.13)\",\"numpy\",\"scikit-image\",\"magicgui\",\"pytest ; extra == 'test'\"],\"summary\":\"Detects blobs in images\",\"support\":\"https://github.com/andy-sweet/napari-blob-detection/issues\",\"twitter\":\"\",\"version\":\"0.0.2\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"jacopo.abramo@gmail.com\",\"name\":\"Jacopo Abramo\"}],\"code_repository\":\"https://github.com/jacopoabramo/napari-live-recording\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-live-recording\"}],\"description\":\"# napari-live-recording\\n\\n[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://github.com/jacopoabramo/napari-live-recording/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-live-recording.svg?color=green)](https://pypi.org/project/napari-live-recording)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-live-recording.svg?color=green)](https://python.org)\\n[![tests](https://github.com/jethro33/napari-live-recording/workflows/tests/badge.svg)](https://github.com/jacopoabramo/napari-live-recording/actions)\\n[![codecov](https://codecov.io/gh/jethro33/napari-live-recording/branch/master/graph/badge.svg)](https://codecov.io/gh/jacopoabramo/napari-live-recording)\\n\\nA napari plugin for live video recording with a generic camera device and generate a stack of TIFF images from said device.\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-live-recording` via [pip]:\\n\\n    pip install napari-live-recording\\n\\n## Documentation\\n\\nYou can review the documentation of this plugin [here](./docs/README.md)\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [MIT] license,\\n\\\"napari-live-recording\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/jacopoabramo/napari-live-recording/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-live-recording\\n\\n\\n\\n\\n\\nA napari plugin for live video recording with a generic camera device and generate a stack of TIFF images from said device.\\n\\nThis napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-live-recording via pip:\\npip install napari-live-recording\\n\\nDocumentation\\nYou can review the documentation of this plugin here\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the MIT license,\\n\\\"napari-live-recording\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari-live-recording\",\"documentation\":\"https://github.com/jacopoabramo/napari-live-recording#README.md\",\"first_released\":\"2021-10-05T19:20:55.265189Z\",\"license\":\"MIT\",\"name\":\"napari-live-recording\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/jethro33/napari-live-recording\",\"python_version\":\">=3.6\",\"reader_file_extensions\":[],\"release_date\":\"2022-05-02T11:24:37.421410Z\",\"report_issues\":\"https://github.com/jacopoabramo/napari-live-recording/issues\",\"requirements\":[\"superqt\",\"numpy\",\"opencv-python\",\"opencv-contrib-python\",\"dask-image\",\"napari\",\"qtpy\"],\"summary\":\"A napari plugin for live video recording with a generic camera device.\",\"support\":\"https://github.com/jacopoabramo/napari-live-recording/issues\",\"twitter\":\"\",\"version\":\"0.2.1\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Herearii Metuarea\"}],\"code_repository\":\"https://github.com/hereariim/napari-apple\",\"description\":\"# napari-apple\\r\\n\\r\\n[![License BSD-3](https://img.shields.io/pypi/l/napari-apple.svg?color=green)](https://github.com/hereariim/napari-apple/raw/main/LICENSE)\\r\\n[![PyPI](https://img.shields.io/pypi/v/napari-apple.svg?color=green)](https://pypi.org/project/napari-apple)\\r\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-apple.svg?color=green)](https://python.org)\\r\\n[![tests](https://github.com/hereariim/napari-apple/workflows/tests/badge.svg)](https://github.com/hereariim/napari-apple/actions)\\r\\n[![codecov](https://codecov.io/gh/hereariim/napari-apple/branch/main/graph/badge.svg)](https://codecov.io/gh/hereariim/napari-apple)\\r\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-apple)](https://napari-hub.org/plugins/napari-apple)\\r\\n\\r\\nDetection of apple based on YOLOv4 model\\r\\n\\r\\n----------------------------------\\r\\n\\r\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\r\\n\\r\\n<!--\\r\\nDon't miss the full getting started guide to set up your new package:\\r\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\r\\n\\r\\nand review the napari docs for plugin developers:\\r\\nhttps://napari.org/plugins/index.html\\r\\n-->\\r\\n\\r\\n## Installation\\r\\n\\r\\nFirst, please note that this module **only works** on a Linux Ubuntu system. Indeed, the launch of the YOLO module is a command that is executed on a Linux Ubuntu system.\\r\\n\\r\\nBefore you can operate the module, you must install the `napari-apple` module and Darknet on your machine.\\r\\n\\r\\n### Instruction for napari-module\\r\\n\\r\\nYou can install `napari-apple` via [pip]:\\r\\n\\r\\n    pip install napari-apple\\r\\n\\r\\nTo install latest development version :\\r\\n\\r\\n    pip install git+https://github.com/hereariim/napari-apple.git\\r\\n\\r\\n### Instruction Darknet\\r\\n\\r\\nDarknet is the module where the pre-trained YOLO model is located. You can install Darknet by running this command:\\r\\n\\r\\n    git clone https://github.com/pjreddie/darknet\\r\\n    cd darknet\\r\\n    make\\r\\n    \\r\\nWhen Darknet is installed, you have to put the weights of the apple detection model in the cfg subfolder. You find the weights in the weight-darknet folder. \\r\\n\\r\\n## Contributing\\r\\n\\r\\nContributions are very welcome. Tests can be run with [tox], please ensure\\r\\nthe coverage at least stays the same before you submit a pull request.\\r\\n\\r\\n## License\\r\\n\\r\\nDistributed under the terms of the [BSD-3] license,\\r\\n\\\"napari-apple\\\" is free and open source software\\r\\n\\r\\n## Issues\\r\\n\\r\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\r\\n\\r\\n[napari]: https://github.com/napari/napari\\r\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\r\\n[@napari]: https://github.com/napari\\r\\n[MIT]: http://opensource.org/licenses/MIT\\r\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\r\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\r\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\r\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\r\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\r\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\r\\n\\r\\n[file an issue]: https://github.com/hereariim/napari-apple/issues\\r\\n\\r\\n[napari]: https://github.com/napari/napari\\r\\n[tox]: https://tox.readthedocs.io/en/latest/\\r\\n[pip]: https://pypi.org/project/pip/\\r\\n[PyPI]: https://pypi.org/\\r\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-apple\\n\\n\\n\\n\\n\\n\\nDetection of apple based on YOLOv4 model\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nFirst, please note that this module only works on a Linux Ubuntu system. Indeed, the launch of the YOLO module is a command that is executed on a Linux Ubuntu system.\\nBefore you can operate the module, you must install the napari-apple module and Darknet on your machine.\\nInstruction for napari-module\\nYou can install napari-apple via pip:\\npip install napari-apple\\n\\nTo install latest development version :\\npip install git+https://github.com/hereariim/napari-apple.git\\n\\nInstruction Darknet\\nDarknet is the module where the pre-trained YOLO model is located. You can install Darknet by running this command:\\ngit clone https://github.com/pjreddie/darknet\\ncd darknet\\nmake\\n\\nWhen Darknet is installed, you have to put the weights of the apple detection model in the cfg subfolder. You find the weights in the weight-darknet folder. \\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-apple\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"Apple\",\"documentation\":\"https://github.com/hereariim/napari-apple#README.md\",\"first_released\":\"2022-06-23T21:15:48.770489Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-apple\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\",\"writer\",\"widget\",\"sample_data\"],\"project_site\":\"https://github.com/hereariim/napari-apple\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[\"*.npy\"],\"release_date\":\"2022-12-08T16:36:22.328263Z\",\"report_issues\":\"https://github.com/hereariim/napari-apple/issues\",\"requirements\":[\"numpy\",\"magicgui\",\"qtpy\",\"opencv-python-headless\",\"scikit-image\",\"napari\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\"],\"summary\":\"Detection of apple based on YOLOv4 model\",\"support\":\"https://github.com/hereariim/napari-apple/issues\",\"twitter\":\"\",\"version\":\"0.0.2\",\"visibility\":\"public\",\"writer_file_extensions\":[\".npy\"],\"writer_save_layers\":[\"image\",\"labels\"]}",
  "{\"authors\":[{\"email\":\"benkantor@gmail.com\",\"name\":\"Ben Kantor\"}],\"code_repository\":\"https://github.com/bkntr/napari-bigwarp\",\"conda\":[],\"description\":\"# napari-bigwarp\\n\\n[![License](https://img.shields.io/pypi/l/napari-bigwarp.svg?color=green)](https://github.com/bkntr/napari-bigwarp/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-bigwarp.svg?color=green)](https://pypi.org/project/napari-bigwarp)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-bigwarp.svg?color=green)](https://python.org)\\n[![tests](https://github.com/bkntr/napari-bigwarp/workflows/tests/badge.svg)](https://github.com/bkntr/napari-bigwarp/actions)\\n[![codecov](https://codecov.io/gh/bkntr/napari-bigwarp/branch/main/graph/badge.svg)](https://codecov.io/gh/bkntr/napari-bigwarp)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-bigwarp)](https://napari-hub.org/plugins/napari-bigwarp)\\n\\nBigWarp-like interface for napari\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/plugins/stable/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-bigwarp` via [pip]:\\n\\n    pip install napari-bigwarp\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/bkntr/napari-bigwarp.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-bigwarp\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/bkntr/napari-bigwarp/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-bigwarp\\n\\n\\n\\n\\n\\n\\nBigWarp-like interface for napari\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-bigwarp via pip:\\npip install napari-bigwarp\\n\\nTo install latest development version :\\npip install git+https://github.com/bkntr/napari-bigwarp.git\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-bigwarp\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari-bigwarp\",\"documentation\":\"https://github.com/bkntr/napari-bigwarp#README.md\",\"first_released\":\"2022-01-26T09:04:00.005701Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-bigwarp\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/bkntr/napari-bigwarp\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-01-26T09:04:00.005701Z\",\"report_issues\":\"https://github.com/bkntr/napari-bigwarp/issues\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"opencv-contrib-python\",\"opencv-python\"],\"summary\":\"BigWarp-like interface for napari\",\"support\":\"https://github.com/bkntr/napari-bigwarp/issues\",\"twitter\":\"\",\"version\":\"0.0.1\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Meizhu Liang\"}],\"code_repository\":\"https://github.com/Meizhu-Liang/napari-generic-SIMulator\",\"description\":\"# napari-generic-SIMulator\\n\\n[![License BSD-3](https://img.shields.io/pypi/l/napari-generic-SIMulator.svg?color=green)](https://github.com/Meizhu-Liang/napari-generic-SIMulator/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-generic-SIMulator.svg?color=green)](https://pypi.org/project/napari-generic-SIMulator)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-generic-SIMulator.svg?color=green)](https://python.org)\\n[![tests](https://github.com/Meizhu-Liang/napari-generic-SIMulator/workflows/tests/badge.svg)](https://github.com/Meizhu-Liang/napari-generic-SIMulator/actions)\\n[![codecov](https://codecov.io/gh/Meizhu-Liang/napari-generic-SIMulator/branch/main/graph/badge.svg)](https://codecov.io/gh/Meizhu-Liang/napari-generic-SIMulator)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-generic-SIMulator)](https://napari-hub.org/plugins/napari-generic-SIMulator)\\n\\nA napari plugin to simulate raw-image stacks of Structured illumination microscopy (SIM) with light sheet. \\n\\nThe simulation is originally based on the paper <strong>GPU-accelerated real-time reconstruction in Python of three-dimensional datasets from structured illumination microscopy with hexagonal patterns</strong> by\\nHai Gong, Wenjun Guo and Mark A. A. Neil (https://doi.org/10.1098/rsta.2020.0162). \\n\\nThe calculation can be GPU-accelerated if the CUPY (tested with cupy 8.3.0) is installed. In addition, the TORCH package can complete the acceleration both on CPU if TORCH is installed, and on GPU if TORCH is compiled with the CUDA (tested with torch v1.12.0+cu116) enabled.\\n\\nCurrently applies to:\\n- conventional 2-beam SIM data with 3 angles and 3 phases\\n- 3-beam hexagonal SIM data with 7 phases, as described in the paper\\n- 3-beam hexagonal SIM data with 7 phases at right-angles\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-generic-SIMulator` via [pip]:\\n\\n    pip install napari-generic-SIMulator\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/Meizhu-Liang/napari-generic-SIMulator.git\\n\\n## Usage\\n\\n1) Open napari and create the viewer.\\n\\n\\n2) Launch the widget in ***Plugin***\\n    ![raw](https://github.com/Meizhu-Liang/napari-generic-SIMulator/raw/main/images/img.png)\\n    ![raw](https://github.com/Meizhu-Liang/napari-generic-SIMulator/raw/main/images/img_1.png)\\n\\n\\n3) Adjust the parameters in the widget and calculate the raw-image stack.\\n    ![raw](https://github.com/Meizhu-Liang/napari-generic-SIMulator/raw/main/images/img_2.png)\\n\\n\\n4) The sum, psf and otf can be showed. Note the all of these correspond the generated raw-image stack, so keep the parameters the same before showing the sum (or psf and otf).\\n    ![raw](https://github.com/Meizhu-Liang/napari-generic-SIMulator/raw/main/images/img_3.png)\\n\\n\\n5) The raw image stacks can be then processed by napari-sim-processor (https://www.napari-hub.org/plugins/napari-sim-processor).\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-generic-SIMulator\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/Meizhu-Liang/napari-generic-SIMulator/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-generic-SIMulator\\n\\n\\n\\n\\n\\n\\nA napari plugin to simulate raw-image stacks of Structured illumination microscopy (SIM) with light sheet. \\nThe simulation is originally based on the paper GPU-accelerated real-time reconstruction in Python of three-dimensional datasets from structured illumination microscopy with hexagonal patterns by\\nHai Gong, Wenjun Guo and Mark A. A. Neil (https://doi.org/10.1098/rsta.2020.0162). \\nThe calculation can be GPU-accelerated if the CUPY (tested with cupy 8.3.0) is installed. In addition, the TORCH package can complete the acceleration both on CPU if TORCH is installed, and on GPU if TORCH is compiled with the CUDA (tested with torch v1.12.0+cu116) enabled.\\nCurrently applies to:\\n- conventional 2-beam SIM data with 3 angles and 3 phases\\n- 3-beam hexagonal SIM data with 7 phases, as described in the paper\\n- 3-beam hexagonal SIM data with 7 phases at right-angles\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-generic-SIMulator via pip:\\npip install napari-generic-SIMulator\\n\\nTo install latest development version :\\npip install git+https://github.com/Meizhu-Liang/napari-generic-SIMulator.git\\n\\nUsage\\n1) Open napari and create the viewer.\\n2) Launch the widget in Plugin\\n\\n\\n3) Adjust the parameters in the widget and calculate the raw-image stack.\\n    \\n4) The sum, psf and otf can be showed. Note the all of these correspond the generated raw-image stack, so keep the parameters the same before showing the sum (or psf and otf).\\n    \\n5) The raw image stacks can be then processed by napari-sim-processor (https://www.napari-hub.org/plugins/napari-sim-processor).\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-generic-SIMulator\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari generic SIMulator\",\"documentation\":\"https://github.com/Meizhu-Liang/napari-generic-SIMulator#README.md\",\"first_released\":\"2022-06-30T18:20:27.340885Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-generic-SIMulator\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/Meizhu-Liang/napari-generic-SIMulator\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2023-01-17T15:36:21.304425Z\",\"report_issues\":\"https://github.com/Meizhu-Liang/napari-generic-SIMulator/issues\",\"requirements\":[\"numpy\",\"magicgui\",\"qtpy\",\"tifffile\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\",\"tifffile ; extra == 'testing'\"],\"summary\":\"A simple plugin to use with napari to simulate raw image stacks in Structured illumination microscopy (SIM) with napari.\",\"support\":\"https://github.com/Meizhu-Liang/napari-generic-SIMulator/issues\",\"twitter\":\"\",\"version\":\"0.0.19\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"jonsson@mpi-cbg.de\",\"name\":\"Joel Jonsson\"}],\"code_repository\":null,\"conda\":[],\"description\":\"# napari-apr-viewer\\n\\n[![License](https://img.shields.io/pypi/l/napari-apr-viewer.svg?color=green)](https://github.com/AdaptiveParticles/napari-apr-viewer/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-apr-viewer.svg?color=green)](https://pypi.org/project/napari-apr-viewer)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-apr-viewer.svg?color=green)](https://python.org)\\n[![tests](https://github.com/AdaptiveParticles/napari-apr-viewer/workflows/tests/badge.svg)](https://github.com/AdaptiveParticles/napari-apr-viewer/actions)\\n[![codecov](https://codecov.io/gh/AdaptiveParticles/napari-apr-viewer/branch/main/graph/badge.svg)](https://codecov.io/gh/AdaptiveParticles/napari-apr-viewer)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-apr-viewer)](https://napari-hub.org/plugins/napari-apr-viewer)\\n\\nA simple plugin to view APR images in napari\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n\\n## Installation\\n\\nYou can install `napari-apr-viewer` via [pip]:\\n\\n    pip install napari-apr-viewer\\n\\n\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [Apache Software License 2.0] license,\\n\\\"napari-apr-viewer\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n[file an issue]: https://github.com/AdaptiveParticles/napari-apr-viewer/issues\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-apr-viewer\\n\\n\\n\\n\\n\\n\\nA simple plugin to view APR images in napari\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\nInstallation\\nYou can install napari-apr-viewer via pip:\\npip install napari-apr-viewer\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the Apache Software License 2.0 license,\\n\\\"napari-apr-viewer\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari-apr-viewer\",\"documentation\":\"\",\"first_released\":\"2021-11-30T23:22:09.686972Z\",\"license\":\"Apache-2.0\",\"name\":\"napari-apr-viewer\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\",\"writer\",\"widget\"],\"project_site\":\"\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[\"*\"],\"release_date\":\"2022-05-25T11:16:04.274323Z\",\"report_issues\":\"\",\"requirements\":[\"numpy\",\"pyapr (>=1.0.0rc1)\",\"napari\",\"napari-plugin-engine (>=0.1.4)\",\"qtpy\",\"magicgui\"],\"summary\":\"A simple plugin to view APR images in napari\",\"support\":\"\",\"twitter\":\"\",\"version\":\"1.0.0\",\"writer_file_extensions\":[],\"writer_save_layers\":[\"image\"]}",
  "{\"authors\":[{\"name\":\"Allen Goodman\",\"orcid\":\"0000-0002-6434-2320\"}],\"code_repository\":\"https://github.com/0x00b1/napari-features\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-features\"}],\"description\":\"# napari-features\\n\\n[![License](https://img.shields.io/pypi/l/napari-features.svg?color=green)](https://github.com/0x00b1/napari-features/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-features.svg?color=green)](https://pypi.org/project/napari-features)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-features.svg?color=green)](https://python.org)\\n[![tests](https://github.com/0x00b1/napari-features/workflows/tests/badge.svg)](https://github.com/0x00b1/napari-features/actions)\\n[![codecov](https://codecov.io/gh/0x00b1/napari-features/branch/master/graph/badge.svg)](https://codecov.io/gh/0x00b1/napari-features)\\n\\nAn extensible, general-purpose feature extraction plug-in for the [Napari](https://napari.org) image viewer.\\n\\n## Features\\n\\n### Color\\n\\n#### Image\\n\\n    color_image_integrated_intensity\\n    color_image_maximum_intensity\\n    color_image_mean_intensity\\n    color_image_median_absolute_deviation_intensity\\n    color_image_median_intensity\\n    color_image_minimum_intensity\\n    color_image_quantile_1_intensity\\n    color_image_quantile_3_intensity\\n    color_image_standard_deviation_intensity\\n\\n#### Object\\n\\n    color_object_center_mass_intensity_x\\n    color_object_center_mass_intensity_y\\n    color_object_integrated_intensity\\n    color_object_integrated_intensity_edge\\n    color_object_mass_displacement\\n    color_object_maximum_intensity\\n    color_object_maximum_intensity_edge\\n    color_object_maximum_intensity_x\\n    color_object_maximum_intensity_y\\n    color_object_mean_intensity\\n    color_object_mean_intensity_edge\\n    color_object_median_absolute_deviation_intensity\\n    color_object_median_intensity\\n    color_object_median_intensity_edge\\n    color_object_minimum_intensity\\n    color_object_minimum_intensity_edge\\n    color_object_quantile_1_intensity\\n    color_object_quantile_1_intensity_edge   \\n    color_object_quantile_3_intensity\\n    color_object_quantile_3_intensity_edge\\n    color_object_standard_deviation_intensity\\n    color_object_standard_deviation_intensity_edge\\n    Object distribution\\n    color_object_distribution_coefficient_of_variation_intensity\\n    color_object_distribution_integrated_intensity\\n    Color_object_distribution_mean_intensity\\n\\n### Location\\n\\n#### Object neighborhood\\n\\n    location_object_neighborhood_angle\\n    location_object_neighborhood_closest_0_distance\\n    location_object_neighborhood_closest_0_object_index\\n    location_object_neighborhood_closest_1_distance\\n    location_object_neighborhood_closest_1_object_index\\n    location_object_neighborhood_closest_2_distance\\n    location_object_neighborhood_closest_2_object_index\\n    location_object_neighborhood_neighbors\\n    location_object_neighborhood_touching\\n\\n### Metadata\\n\\n#### Image\\n\\n    metadata_image_checksum\\n    metadata_image_filename\\n\\n#### Layer\\n\\n    metadata_layer_name\\n    metadata_layer_type\\n\\n#### Object\\n\\n    metadata_object_index\\n\\n### Shape\\n\\n#### Image\\n\\n    shape_image_area\\n\\n#### Image skeleton\\n\\n    shape_image_skeleton_branches\\n    shape_image_skeleton_endpoints\\n    shape_image_skeleton_length\\n    shape_image_skeleton_trunks\\n\\n#### Object\\n\\n    shape_object_area\\n    shape_object_bounding_box_area\\n    shape_object_bounding_box_maximum_x\\n    shape_object_bounding_box_maximum_y\\n    shape_object_bounding_box_maximum_z\\n    shape_object_bounding_box_minimum_x\\n    shape_object_bounding_box_minimum_y\\n    shape_object_bounding_box_minimum_z\\n    shape_object_bounding_box_volume\\n    shape_object_central_moment_0_0_0\\n    shape_object_central_moment_0_0_1\\n    shape_object_central_moment_0_1_2\\n    shape_object_central_moment_0_1_3\\n    shape_object_central_moment_1_2_0\\n    shape_object_central_moment_1_2_1\\n    shape_object_central_moment_1_3_2\\n    shape_object_central_moment_1_3_3\\n    shape_object_central_moment_2_0_0\\n    shape_object_central_moment_2_0_1\\n    shape_object_central_moment_2_1_2\\n    shape_object_central_moment_2_1_3\\n    shape_object_central_moment_3_2_0\\n    shape_object_central_moment_3_2_1\\n    shape_object_central_moment_3_3_2\\n    shape_object_central_moment_3_3_3\\n    shape_object_centroid_x\\n    shape_object_centroid_y\\n    shape_object_centroid_z\\n    shape_object_compactness\\n    shape_object_eccentricity\\n    shape_object_equivalent_diameter\\n    shape_object_euler_number\\n    shape_object_extent\\n    shape_object_form_factor\\n    shape_object_hu_moment_0\\n    shape_object_hu_moment_1\\n    shape_object_hu_moment_2\\n    shape_object_hu_moment_3\\n    shape_object_hu_moment_4\\n    shape_object_hu_moment_5\\n    shape_object_hu_moment_6\\n    shape_object_inertia_tensor_eigenvalues_x\\n    shape_object_inertia_tensor_eigenvalues_y\\n    shape_object_inertia_tensor_eigenvalues_z\\n    shape_object_inertia_tensor_x_x\\n    shape_object_inertia_tensor_x_y\\n    Shape_object_inertia_tensor_x_z\\n    shape_object_inertia_tensor_y_x\\n    shape_object_inertia_tensor_y_y\\n    shape_object_inertia_tensor_y_z\\n    shape_object_inertia_tensor_z_x\\n    shape_object_inertia_tensor_z_y\\n    shape_object_inertia_tensor_z_z\\n    shape_object_major_axis_length\\n    shape_object_maximum_feret_diameter\\n    shape_object_maximum_radius\\n    shape_object_mean_radius\\n    shape_object_median_radius\\n    shape_object_minimum_feret_diameter\\n    shape_object_minor_axis_length\\n    shape_object_normalized_moment_x_y\\n    shape_object_orientation\\n    shape_object_perimeter\\n    shape_object_solidity\\n    shape_object_spatial_moment_0_0_0\\n    shape_object_spatial_moment_0_0_1\\n    shape_object_spatial_moment_0_1_2\\n    shape_object_spatial_moment_0_1_3\\n    shape_object_spatial_moment_1_2_0\\n    shape_object_spatial_moment_1_2_1\\n    shape_object_spatial_moment_1_3_2\\n    shape_object_spatial_moment_1_3_3\\n    shape_object_spatial_moment_2_0_0\\n    shape_object_spatial_moment_2_0_1\\n    shape_object_spatial_moment_2_1_2\\n    shape_object_spatial_moment_2_1_3\\n    shape_object_spatial_moment_3_2_0\\n    shape_object_spatial_moment_3_2_1\\n    shape_object_spatial_moment_3_3_2\\n    shape_object_spatial_moment_3_3_3\\n    shape_object_surface_area\\n    shape_object_volume\\n    shape_object_zernike shape features\\n    Object skeleton\\n    shape_object_skeleton_endpoints\\n    shape_object_skeleton_branches\\n    shape_object_skeleton_length\\n    shape_object_skeleton_trunks\\n\\n### Texture\\n\\n#### Object\\n\\n    texture_object_haralick_angular_second_moment\\n    texture_object_haralick_contrast\\n    texture_object_haralick_coorelation\\n    texture_object_haralick_sum_of_squares_variance\\n    texture_object_haralick_inverse_difference_moment\\n    texture_object_haralick_sum_average\\n    texture_object_haralick_sum_variance\\n    texture_object_haralick_sum_entropy\\n    texture_object_haralick_entropy\\n    texture_object_haralick_difference_variance\\n    texture_object_haralick_measure_of_correlation_0\\n    texture_object_haralick_measure_of_correlation_1\\n    texture_object_haralick_maximum_correlation_coefficient\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-features\\n\\n\\n\\n\\n\\nAn extensible, general-purpose feature extraction plug-in for the Napari image viewer.\\nFeatures\\nColor\\nImage\\ncolor_image_integrated_intensity\\ncolor_image_maximum_intensity\\ncolor_image_mean_intensity\\ncolor_image_median_absolute_deviation_intensity\\ncolor_image_median_intensity\\ncolor_image_minimum_intensity\\ncolor_image_quantile_1_intensity\\ncolor_image_quantile_3_intensity\\ncolor_image_standard_deviation_intensity\\n\\nObject\\ncolor_object_center_mass_intensity_x\\ncolor_object_center_mass_intensity_y\\ncolor_object_integrated_intensity\\ncolor_object_integrated_intensity_edge\\ncolor_object_mass_displacement\\ncolor_object_maximum_intensity\\ncolor_object_maximum_intensity_edge\\ncolor_object_maximum_intensity_x\\ncolor_object_maximum_intensity_y\\ncolor_object_mean_intensity\\ncolor_object_mean_intensity_edge\\ncolor_object_median_absolute_deviation_intensity\\ncolor_object_median_intensity\\ncolor_object_median_intensity_edge\\ncolor_object_minimum_intensity\\ncolor_object_minimum_intensity_edge\\ncolor_object_quantile_1_intensity\\ncolor_object_quantile_1_intensity_edge   \\ncolor_object_quantile_3_intensity\\ncolor_object_quantile_3_intensity_edge\\ncolor_object_standard_deviation_intensity\\ncolor_object_standard_deviation_intensity_edge\\nObject distribution\\ncolor_object_distribution_coefficient_of_variation_intensity\\ncolor_object_distribution_integrated_intensity\\nColor_object_distribution_mean_intensity\\n\\nLocation\\nObject neighborhood\\nlocation_object_neighborhood_angle\\nlocation_object_neighborhood_closest_0_distance\\nlocation_object_neighborhood_closest_0_object_index\\nlocation_object_neighborhood_closest_1_distance\\nlocation_object_neighborhood_closest_1_object_index\\nlocation_object_neighborhood_closest_2_distance\\nlocation_object_neighborhood_closest_2_object_index\\nlocation_object_neighborhood_neighbors\\nlocation_object_neighborhood_touching\\n\\nMetadata\\nImage\\nmetadata_image_checksum\\nmetadata_image_filename\\n\\nLayer\\nmetadata_layer_name\\nmetadata_layer_type\\n\\nObject\\nmetadata_object_index\\n\\nShape\\nImage\\nshape_image_area\\n\\nImage skeleton\\nshape_image_skeleton_branches\\nshape_image_skeleton_endpoints\\nshape_image_skeleton_length\\nshape_image_skeleton_trunks\\n\\nObject\\nshape_object_area\\nshape_object_bounding_box_area\\nshape_object_bounding_box_maximum_x\\nshape_object_bounding_box_maximum_y\\nshape_object_bounding_box_maximum_z\\nshape_object_bounding_box_minimum_x\\nshape_object_bounding_box_minimum_y\\nshape_object_bounding_box_minimum_z\\nshape_object_bounding_box_volume\\nshape_object_central_moment_0_0_0\\nshape_object_central_moment_0_0_1\\nshape_object_central_moment_0_1_2\\nshape_object_central_moment_0_1_3\\nshape_object_central_moment_1_2_0\\nshape_object_central_moment_1_2_1\\nshape_object_central_moment_1_3_2\\nshape_object_central_moment_1_3_3\\nshape_object_central_moment_2_0_0\\nshape_object_central_moment_2_0_1\\nshape_object_central_moment_2_1_2\\nshape_object_central_moment_2_1_3\\nshape_object_central_moment_3_2_0\\nshape_object_central_moment_3_2_1\\nshape_object_central_moment_3_3_2\\nshape_object_central_moment_3_3_3\\nshape_object_centroid_x\\nshape_object_centroid_y\\nshape_object_centroid_z\\nshape_object_compactness\\nshape_object_eccentricity\\nshape_object_equivalent_diameter\\nshape_object_euler_number\\nshape_object_extent\\nshape_object_form_factor\\nshape_object_hu_moment_0\\nshape_object_hu_moment_1\\nshape_object_hu_moment_2\\nshape_object_hu_moment_3\\nshape_object_hu_moment_4\\nshape_object_hu_moment_5\\nshape_object_hu_moment_6\\nshape_object_inertia_tensor_eigenvalues_x\\nshape_object_inertia_tensor_eigenvalues_y\\nshape_object_inertia_tensor_eigenvalues_z\\nshape_object_inertia_tensor_x_x\\nshape_object_inertia_tensor_x_y\\nShape_object_inertia_tensor_x_z\\nshape_object_inertia_tensor_y_x\\nshape_object_inertia_tensor_y_y\\nshape_object_inertia_tensor_y_z\\nshape_object_inertia_tensor_z_x\\nshape_object_inertia_tensor_z_y\\nshape_object_inertia_tensor_z_z\\nshape_object_major_axis_length\\nshape_object_maximum_feret_diameter\\nshape_object_maximum_radius\\nshape_object_mean_radius\\nshape_object_median_radius\\nshape_object_minimum_feret_diameter\\nshape_object_minor_axis_length\\nshape_object_normalized_moment_x_y\\nshape_object_orientation\\nshape_object_perimeter\\nshape_object_solidity\\nshape_object_spatial_moment_0_0_0\\nshape_object_spatial_moment_0_0_1\\nshape_object_spatial_moment_0_1_2\\nshape_object_spatial_moment_0_1_3\\nshape_object_spatial_moment_1_2_0\\nshape_object_spatial_moment_1_2_1\\nshape_object_spatial_moment_1_3_2\\nshape_object_spatial_moment_1_3_3\\nshape_object_spatial_moment_2_0_0\\nshape_object_spatial_moment_2_0_1\\nshape_object_spatial_moment_2_1_2\\nshape_object_spatial_moment_2_1_3\\nshape_object_spatial_moment_3_2_0\\nshape_object_spatial_moment_3_2_1\\nshape_object_spatial_moment_3_3_2\\nshape_object_spatial_moment_3_3_3\\nshape_object_surface_area\\nshape_object_volume\\nshape_object_zernike shape features\\nObject skeleton\\nshape_object_skeleton_endpoints\\nshape_object_skeleton_branches\\nshape_object_skeleton_length\\nshape_object_skeleton_trunks\\n\\nTexture\\nObject\\ntexture_object_haralick_angular_second_moment\\ntexture_object_haralick_contrast\\ntexture_object_haralick_coorelation\\ntexture_object_haralick_sum_of_squares_variance\\ntexture_object_haralick_inverse_difference_moment\\ntexture_object_haralick_sum_average\\ntexture_object_haralick_sum_variance\\ntexture_object_haralick_sum_entropy\\ntexture_object_haralick_entropy\\ntexture_object_haralick_difference_variance\\ntexture_object_haralick_measure_of_correlation_0\\ntexture_object_haralick_measure_of_correlation_1\\ntexture_object_haralick_maximum_correlation_coefficient\\n\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari-features\",\"documentation\":\"https://github.com/0x00b1/napari-features#README.md\",\"first_released\":\"2021-06-17T23:20:42.709211Z\",\"license\":\"MIT\",\"name\":\"napari-features\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[],\"project_site\":\"https://github.com/0x00b1/napari-features\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2021-08-24T17:16:42.406784Z\",\"report_issues\":\"https://github.com/0x00b1/napari-features/issues\",\"requirements\":[\"magicgui (>=0.2.9)\",\"napari (>=0.4.10)\",\"napari-plugin-engine (>=0.1.4)\",\"numpy (>=1.19.5)\",\"pandas (>=1.2.4)\",\"qtpy (>=1.9.0)\",\"scikit-image (>=0.18.1)\",\"scipy (>=1.4.1)\"],\"summary\":\"extensible, general-purpose feature extraction\",\"support\":\"https://github.com/0x00b1/napari-features/issues\",\"twitter\":\"\",\"version\":\"0.1.4\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"p.schoennenbeck@fz-juelich.de\",\"name\":\"Philipp Schoennenbeck\"}],\"code_repository\":\"https://github.com/Croxa/napari-brushsettings\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-brushsettings\"}],\"description\":\"# napari-brushsettings\\n\\n[![License](https://img.shields.io/pypi/l/napari-brushsettings.svg?color=green)](https://github.com/Croxa/napari-brushsettings/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-brushsettings.svg?color=green)](https://pypi.org/project/napari-brushsettings)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-brushsettings.svg?color=green)](https://python.org)\\n[![tests](https://github.com/Croxa/napari-brushsettings/workflows/tests/badge.svg)](https://github.com/Croxa/napari-brushsettings/actions)\\n[![codecov](https://codecov.io/gh/Croxa/napari-brushsettings/branch/master/graph/badge.svg)](https://codecov.io/gh/Croxa/napari-brushsettings)\\n\\nA simple plugin to set the brush settings for segmentation in napari\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-brushsettings` via [pip]:\\n\\n    pip install napari-brushsettings\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-brushsettings\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/Croxa/napari-brushsettings/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-brushsettings\\n\\n\\n\\n\\n\\nA simple plugin to set the brush settings for segmentation in napari\\n\\nThis napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-brushsettings via pip:\\npip install napari-brushsettings\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-brushsettings\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari-brushsettings\",\"documentation\":\"https://github.com/Croxa/napari-brushsettings#README.md\",\"first_released\":\"2021-08-30T14:03:28.700519Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-brushsettings\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/Croxa/napari-brushsettings\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2021-08-30T14:36:18.687181Z\",\"report_issues\":\"https://github.com/Croxa/napari-brushsettings/issues\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\"],\"summary\":\"A simple plugin to set the brush settings for segmentation in napari\",\"support\":\"https://github.com/Croxa/napari-brushsettings/issues\",\"twitter\":\"\",\"version\":\"0.0.2\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Marc Boucsein\"},{\"name\":\"Robin Koch\"}],\"code_repository\":\"https://github.com/MBPhys/Offset-Subtraction\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"offset-subtraction\"}],\"description\":\"# Offset-Subtraction\\n\\n[![License](https://img.shields.io/pypi/l/napari-medical-image-formats.svg?color=green)](https://github.com/MBPhys/Offset-Subtraction/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/Offset-Subtraction.svg?color=green)](https://pypi.org/project/Offset-Subtraction)\\n[![Python Version](https://img.shields.io/pypi/pyversions/Offset-Subtraction.svg?color=green)](https://python.org)\\n\\n\\nA napari plugin in oder to subtract an intensity offset such as autofluorescence\\n\\n----------------------------------\\n\\n## Installation\\n\\nYou can install `Offset-Subtraction` via [pip]:\\n\\n    pip install Offset-Subtraction\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"Offset-Subtraction\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/MBPhys/Offset-Subtraction/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"Offset-Subtraction\\n\\n\\n\\nA napari plugin in oder to subtract an intensity offset such as autofluorescence\\n\\nInstallation\\nYou can install Offset-Subtraction via pip:\\npip install Offset-Subtraction\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"Offset-Subtraction\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"Offset-Subtraction\",\"documentation\":\"\",\"first_released\":\"2022-01-13T11:52:17.512835Z\",\"license\":\"BSD-3-Clause\",\"name\":\"Offset-Subtraction\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/MBPhys/Offset-Subtraction\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-01-13T11:52:17.512835Z\",\"report_issues\":\"\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"dask\"],\"summary\":\"A napari plugin in oder to subtract an intensity offset such as autofluorescence\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.0.5\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Marc Boucsein\"},{\"name\":\"Robin Koch\"}],\"code_repository\":\"https://github.com/MBPhys/napari-elementary-numpy-operations\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-elementary-numpy-operations\"}],\"description\":\"# napari-elementary-numpy-operations\\n\\n[![License](https://img.shields.io/pypi/l/napari-medical-image-formats.svg?color=green)](https://github.com/MBPhys/napari-elementary-numpy-operations/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-elementary-numpy-operations.svg?color=green)](https://pypi.org/project/napari-elementary-numpy-operations)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-elementary-numpy-operations.svg?color=green)](https://python.org)\\n\\n\\nA napari plugin covers elementary numpy operations like swap axes, flip, sqeeze an array or rotate an arrays by 90° steps.\\n\\n----------------------------------\\n\\n## Installation\\n\\nYou can install `napari-elementary-numpy-operations` via [pip]:\\n\\n    napari-elementary-numpy-operations\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-elementary-numpy-operations\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/MBPhys/napari-elementary-numpy-operations/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-elementary-numpy-operations\\n\\n\\n\\nA napari plugin covers elementary numpy operations like swap axes, flip, sqeeze an array or rotate an arrays by 90° steps.\\n\\nInstallation\\nYou can install napari-elementary-numpy-operations via pip:\\nnapari-elementary-numpy-operations\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-elementary-numpy-operations\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"napari-elementary-numpy-operations\",\"documentation\":\"\",\"first_released\":\"2022-01-12T12:15:11.182134Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-elementary-numpy-operations\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/MBPhys/napari-elementary-numpy-operations\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-01-21T16:42:21.676529Z\",\"report_issues\":\"\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"qtpy\",\"superqt\"],\"summary\":\"A napari plugin covers elementary numpy operations like swap axes, flip, sqeeze an array or rotate an arrays by 90° steps\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.0.5\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"talley.lambert@gmail.com\",\"name\":\"Talley Lambert\"}],\"code_repository\":\"https://github.com/tlambert03/napari-dv\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-dv\"}],\"description\":\"# napari-dv\\n\\n[![License](https://img.shields.io/pypi/l/napari-dv.svg?color=green)](https://github.com/tlambert03/napari-dv/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-dv.svg?color=green)](https://pypi.org/project/napari-dv)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-dv.svg?color=green)](https://python.org)\\n[![tests](https://github.com/tlambert03/napari-dv/workflows/tests/badge.svg)](https://github.com/tlambert03/napari-dv/actions)\\n[![codecov](https://codecov.io/gh/tlambert03/napari-dv/branch/master/graph/badge.svg)](https://codecov.io/gh/tlambert03/napari-dv)\\n\\nDeltavision/MRC file reader for napari.\\n\\nThis wraps the [mrc](https://github.com/tlambert03/mrc) library.\\n\\nSee also [napari-aicsimageio](https://github.com/AllenCellModeling/napari-aicsimageio), which also uses the [mrc](https://github.com/tlambert03/mrc) to provide dv file support,\\nalong with many other common file formats.\\n\\n## Installation\\n\\nYou can install `napari-dv` via [pip]:\\n\\n    pip install napari-dv\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [MIT] license,\\n\\\"napari-dv\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[file an issue]: https://github.com/tlambert03/napari-dv/issues\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-dv\\n\\n\\n\\n\\n\\nDeltavision/MRC file reader for napari.\\nThis wraps the mrc library.\\nSee also napari-aicsimageio, which also uses the mrc to provide dv file support,\\nalong with many other common file formats.\\nInstallation\\nYou can install napari-dv via pip:\\npip install napari-dv\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the MIT license,\\n\\\"napari-dv\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"napari-dv\",\"documentation\":\"https://github.com/tlambert03/napari-dv#README.md\",\"first_released\":\"2020-02-02T21:12:45.927865Z\",\"license\":\"MIT\",\"name\":\"napari-dv\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\",\"writer\"],\"project_site\":\"https://github.com/tlambert03/napari-dv\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[\"*.dv\",\"*.mrc\"],\"release_date\":\"2022-03-19T15:58:33.601081Z\",\"report_issues\":\"https://github.com/tlambert03/napari-dv/issues\",\"requirements\":[\"mrc (>=0.2.0)\",\"napari-plugin-engine (>=0.1.4)\",\"numpy ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\"],\"summary\":\"Deltavision/MRC file reader for napari\",\"support\":\"https://github.com/tlambert03/napari-dv/issues\",\"twitter\":\"\",\"version\":\"0.3.0\",\"visibility\":\"public\",\"writer_file_extensions\":[\".dv\",\".mrc\"],\"writer_save_layers\":[\"image\"]}",
  "{\"authors\":[{\"name\":\"Robert Haase\"}],\"category\":{\"Workflow step\":[\"Pixel classification\",\"Object classification\",\"Image annotation\",\"Image feature detection\"]},\"category_hierarchy\":{\"Workflow step\":[[\"Pixel classification\"],[\"Object classification\"],[\"Image annotation\"],[\"Image feature detection\"]]},\"code_repository\":\"https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification\",\"description\":\"# napari-accelerated-pixel-and-object-classification (APOC)\\r\\n\\r\\n[![License](https://img.shields.io/pypi/l/napari-accelerated-pixel-and-object-classification.svg?color=green)](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/LICENSE)\\r\\n[![PyPI](https://img.shields.io/pypi/v/napari-accelerated-pixel-and-object-classification.svg?color=green)](https://pypi.org/project/napari-accelerated-pixel-and-object-classification)\\r\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-accelerated-pixel-and-object-classification.svg?color=green)](https://python.org)\\r\\n[![tests](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/actions)\\r\\n[![codecov](https://codecov.io/gh/haesleinhuepf/napari-accelerated-pixel-and-object-classification/branch/main/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-accelerated-pixel-and-object-classification)\\r\\n[![Development Status](https://img.shields.io/pypi/status/napari-accelerated-pixel-and-object-classification.svg)](https://en.wikipedia.org/wiki/Software_release_life_cycle#Alpha)\\r\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-accelerated-pixel-and-object-classification)](https://napari-hub.org/plugins/napari-accelerated-pixel-and-object-classification)\\r\\n[![DOI](https://zenodo.org/badge/412525441.svg)](https://zenodo.org/badge/latestdoi/412525441)\\r\\n \\r\\n[clesperanto](https://github.com/clEsperanto/pyclesperanto_prototype) meets [scikit-learn](https://scikit-learn.org/stable/) to classify pixels and objects in images, on a [GPU](https://en.wikipedia.org/wiki/Graphics_processing_unit) using [OpenCL](https://www.khronos.org/opencl/) in [napari].\\r\\n\\r\\n![](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/screencast.gif)\\r\\nThe processed example image was kindly acquired by Daniela Vorkel, Myers lab, MPI-CBG / CSBD ([Download full video](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/demo_lund.mp4))\\r\\n\\r\\nFor using the accelerated pixel and object classifiers in python, check out [apoc](https://github.com/haesleinhuepf/apoc).\\r\\nTraining classifiers from pairs of image and label-mask folders is explained in \\r\\n[this notebook](https://github.com/haesleinhuepf/apoc/blob/main/demo/train_on_folders.ipynb).\\r\\nFor executing APOC's pixel and object classifiers in [Fiji](https://fiji.sc) using [clij2](https://clij.github.io) please read the documentation of the [corresponding Fiji plugin](https://github.com/clij/clijx-accelerated-pixel-and-object-classification). Table classifiers and object mergers are not compatible with Fiji yet.\\r\\n\\r\\n![](https://github.com/clij/clijx-accelerated-pixel-and-object-classification/raw/main/docs/screenshot.png)\\r\\n\\r\\n\\r\\n\\r\\n## Usage\\r\\n\\r\\n### Object and Semantic Segmentation\\r\\n\\r\\nStarting point is napari with at least one image layer and one labels layer (your annotation).\\r\\n\\r\\n![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/object_segmentation_starting_point.png)\\r\\n\\r\\nYou find Object and Semantic Segmentation in the `Tools > Segmentation / labeling`. When starting those, the following graphical user interface will show up.\\r\\n\\r\\n![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/object_and_semantic_segmentation.png)\\r\\n\\r\\n1. Choose one or multiple images to train on. These images will be considered as multiple channels. Thus, they need to be spatially correlated. \\r\\n   Training from multiple images showing different scenes is not (yet) supported from the graphical user interface. Check out [this notebook](https://github.com/haesleinhuepf/apoc/blob/main/demo/demp_pixel_classifier_continue_training.ipynb) if you want to train from multiple image-annotation pairs.\\r\\n2. Select a file where the classifier should be saved. If the file exists already, it will be overwritten.\\r\\n3. Select the ground-truth annotation labels layer. \\r\\n4. Select which label corresponds to foreground (not available in Semantic Segmentation)\\r\\n5. Select the feature images that should be considered for segmentation. If segmentation appears pixelated, try increasing the selected sigma values and untick `Consider original image`.\\r\\n6. Tree depth and number of trees allow you to fine-tune how to deal with manifold regions of different characteristics. The higher these numbers, the longer segmentation will take. In case you use many images and many features, high depth and number of trees might be necessary. (See also `max_depth` and `n_estimators` in the [scikit-learn documentation of the Random Forest Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).\\r\\n7. The estimation of memory consumption allows you to tune the configuration to your GPU-hardware. Also consider the GPU-hardware of others who want to use your classifier.\\r\\n8. Click on Run when you're done with configuring. If the segmentation doesn't fit after the first execution, consider fine-tuning the ground-truth annotation and try again.\\r\\n\\r\\nA successful segmentation can for example look like this:\\r\\n\\r\\n![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/object_segmentation_result.png)\\r\\n\\r\\nAfter your classifier has been trained successfully, click on the \\\"Application / Prediction\\\" tab. If you apply the classifier again, python code will be generated. \\r\\nYou can use this code for example to apply the same classifier to a folder of images. If you're new to this, check out [this notebook](https://github.com/BiAPoL/Bio-image_Analysis_with_Python/blob/main/image_processing/12_process_folders.ipynb).\\r\\n\\r\\n![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/code_generation.png)\\r\\n\\r\\nA pre-trained classifier can be [applied from scripts as shown in the example notebook](https://github.com/haesleinhuepf/apoc/blob/main/demo/demo_object_segmenter.ipynb) or from the `Tools > Segmentation / labeling > Object segmentation (apply pretrained, APOC)`.\\r\\n\\r\\n### Integration with the napari-assistant\\r\\n\\r\\nPre-trained models can also be assembled to workflows using the [napari-assistant](https://www.napari-hub.org/plugins/napari-assistant). You find APOC-operations in the categories `Filter`, `Label` and `Label Filters`:\\r\\n\\r\\n![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/assistant.png)\\r\\n\\r\\n### Semantic segmentation\\r\\n\\r\\nUsers can also generate semantic segmentation label images where the label identifier corresponds to a class the pixel has been allocated to. \\r\\nThe tool can be found in the menu `Tools > Segmentation / labeling > Semantic segmentation (APOC)`.\\r\\nIt works analogously like the Object Segmenter, just without the need to specify the class identifier that objects correspond to.\\r\\n\\r\\n![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/semantic_segmentation.png)\\r\\n\\r\\n### Probability maps\\r\\n\\r\\nThe tool for generating probability maps (`Tools > Filtering > Probability Mapper (APOC)` menu) works analogously to the Object Segmenter as well. \\r\\nThe only difference is that the result image is not a label image but an intensity image where the intensity represents the probability (between 0 and 1)\\r\\nthat a pixel belongs to a given class. In this example: The raw image (grey) has been annotated with three classes: background (black, label 1), foreground (white, label 2) and edges (grey, label 3).\\r\\nThe probability mapper was configured to create probability image (shown in green) for edges (label 3):\\r\\n\\r\\n![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/probability_mapper.png)\\r\\n\\r\\n### Classifier statistics\\r\\n\\r\\nWhile training, you can also activate the `Show classifier statistics` checkbox. \\r\\nWhen doing so, it is recommended to increase the number of trees so that the measurements are more reliable, especially when selecting many features.\\r\\nThis will open a small table after training where you can see how large the share of decision trees are for each analysed feature image.\\r\\n\\r\\n![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/classifier_statistics.png)\\r\\n\\r\\nIt is recommended to turn on/off the features that hold a very large share (green) or a very small share (magenta) of trees in the random forest. \\r\\nRetrain the classifier to see how the features influence the decision making.\\r\\n\\r\\nNote: Multiple of these parameters may be correlated. \\r\\nIf you select 11 feature images, which all allow to make the pixel classification similarly, but 10 of those are correlated, these 10 may appear with a share of about 0.05 while the 11th parameter has a share of 0.5. \\r\\nThus, study these values with care.\\r\\n\\r\\n### Merging objects\\r\\n\\r\\nAfter segmentation, you can merge labeled objects using the `Tools > Segmentation post-processing > Merge objects (APOC)` menu. \\r\\nAnnotate label edges that should be merged with intensity 1 and those which should be kept with intensity 2 in a blank label image.\\r\\nSelect which features should be considered for merging:\\r\\n* `touch_portion`: The relative amount an object touches another. E.g. in a symmetric, honey-comb like tissue, neighboring cells have a touch-portion of `1/6` to each other.\\r\\n* `touch_count`: The number of pixels where object touch. When using this parameter, make sure that images used for training and prediction have the same voxel size.\\r\\n* `mean_touch_intensity`: The mean average intensity between touching objects. When using this parameter, make sure images used for training and prediction are normalized the same way.\\r\\n* `centroid_distance`: The distance (in pixels or voxels) between centroids of labeled objects. \\r\\n* `mean_intensity_difference`: The absolute difference between the mean intensity of the two objects. This measurement allows differentiating bright and dark object and [not] merging them.\\r\\n* `standard_deviation_intensity_difference`: The absolute difference between the standard deviation of the two objects. This measurement allows to differentiate [in]homogeneous objects and [not] merge them.\\r\\n* `area_difference`: The difference in area/volume/pixel-count allows differentiating small and large objects and [not] merging them.\\r\\n* `mean_max_distance_to_centroid_ratio_difference`: This parameter is a shape descriptor, similar to elongation, allowing to differentiate roundish and elongate object and [not] merging them.\\r\\n\\r\\nNote: most features are recommended to be used in isotropic images only.\\r\\n\\r\\n![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/merge_objects.gif)\\r\\n\\r\\nFor training, use an image with equivalized intensity (1), an over-segmented label image (2) and annotations (3). When drawing annotations in a new labels layer, make sure to misguide the algorithm draw on edges of touching objects a 1 if those should be merged and a 2 if they should be kept. Make sure there are no 1/2 annotation circles on both: labels which should be merged and kept.\\r\\n\\r\\n![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/merge_objects2.png)\\r\\n\\r\\n### Object classification\\r\\n\\r\\nClick the menu `Tools > Segmentation post-processing > Object classification (APOC)`. \\r\\n\\r\\n![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/menu.png)\\r\\n\\r\\nThis user interface will be shown:\\r\\n\\r\\n![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/object_classifier_gui.png)\\r\\n\\r\\n1. The image layer will be used for intensity based feature extraction (see below).\\r\\n2. The labels layer should be contain the segmentation of objects that should be classified. \\r\\n   You can use the Object Segmenter explained above to create this layer.\\r\\n3. The annotation layer should contain manual annotations of object classes. \\r\\n   You can draw lines crossing single and multiple objects of the same kind. \\r\\n   For example draw a line through some elongated objects with label \\\"1\\\" and another line through some rather roundish objects with label \\\"2\\\".\\r\\n   If these lines touch the background, that will be ignored.\\r\\n4. Tree depth and number of trees allow you to fine-tune how to deal with manifold objects of different characteristics. The higher these numbers, the longer classification will take. In case you use many features, high depth and number of trees might be necessary. (See also `max_depth` and `n_estimators` in the [scikit-learn documentation of the Random Forest Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).\\r\\n5. Select the right features for training. For example, for differentiating objects according to their shape as suggested above, select \\\"shape\\\".\\r\\n   The features are extracted using clEsperanto and are shown by example in [this notebook](https://github.com/clEsperanto/pyclesperanto_prototype/blob/master/demo/tissues/parametric_maps.ipynb).\\r\\n6. Click on the `Run` button. If classification doesn't perform well in the first attempt, try changing selected features.  \\r\\n\\r\\nIf classification worked well, it may for example look like this. Note the two thick lines which were drawn to annotate elongated and roundish objects with brown and cyan:\\r\\n\\r\\n![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/object_classification_result.png)\\r\\n\\r\\nA pre-trained model can later be applied [from scripts as shown in the example notebook](https://github.com/haesleinhuepf/apoc/blob/main/demo/cell_classification.ipynb) or using the menu `Tools > Segmentation post-processing > Object classification (apply pretrained, APOC)`.\\r\\n\\r\\n### Object selection\\r\\n\\r\\nAnalogously to object classification, the object selector removes all objects from a label image that do not belong to a specified class.\\r\\nIt can be found in the menu `Tools > Segmentation post-processing > Object selection (APOC)`. \\r\\n\\r\\n![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/select_objects.gif)\\r\\n\\r\\n\\r\\n### Feature correlation matrix\\r\\n\\r\\nWhen training object classifiers it is crucial to investigate to which degree features are correlated and select the right, ideally uncorrelated features to classify objects robustly.\\r\\nAfter measuring features with any compatible napari plugin listed below, you can visualize the feature correlation matrix using the menu `Tools > Measurement tables > Show feature correlation matrix (pandas, APOC)` and by selecting the labels layer which has been analyzed.\\r\\nBefore computing the correlation matrix, all rows containing [NaN](https://en.wikipedia.org/wiki/NaN) values are removed.\\r\\nFor further details, please refer to the [documentation of the underlying function in pandas](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html).\\r\\n\\r\\n![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/feature_correlation_matrix.png)\\r\\n\\r\\n### Surface Vertex Classification (SVeC)\\r\\n\\r\\nWhen using napari-APOC in combination with [napari-process-points-and-surfaces>=0.3.3](https://github.com/haesleinhuepf/napari-process-points-and-surfaces), \\r\\none can also classify vertices. Therefore, use for example the menu `Measurement > Surface quality table (vedo, nppas)` to determine quantitative measurements\\r\\nand the menu `Surfaces > Annotate surface manually (nppas)` for manual annotations. It is recommended to annotate the entire surface with value 1 as background, and specific regions of interest with integer numbers > 1.\\r\\nAfter measurements have been extracted and annotations were made, start SVeC from the `Surfaces > Surface vertex classification (custom properties, APOC)` menu. It can be used like the Object Classifier explained above.\\r\\n\\r\\n![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/demo_vertex_classification.gif)\\r\\n\\r\\n[Download full video](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/demo_vertex_classification.mp4)\\r\\n\\r\\n### Classifier statistics\\r\\nAfter classifier training, you can study the share of the individual features/measurements and how they are correlated by activating the checkboxes `Show classifier statistics` and `Show feature correlation matrix`.\\r\\n\\r\\n![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/correlation_matrix2.png)\\r\\n\\r\\nThis can help understanding how the classifier works. Furthermore, you can accelerate the classifier by reducing the number of correlated features.\\r\\n\\r\\n### Object classification from custom measurements\\r\\n\\r\\nYou can also classify labeled objects according to custom measurements. For deriving those measurements, you can use these napari plugins:\\r\\n\\r\\n* [morphometrics](https://www.napari-hub.org/plugins/morphometrics)\\r\\n* [PartSeg](https://www.napari-hub.org/plugins/PartSeg)\\r\\n* [napari-simpleitk-image-processing](https://www.napari-hub.org/plugins/napari-simpleitk-image-processing)\\r\\n* [napari-cupy-image-processing](https://www.napari-hub.org/plugins/napari-cupy-image-processing)\\r\\n* [napari-pyclesperanto-assistant](https://www.napari-hub.org/plugins/napari-pyclesperanto-assistant)\\r\\n* [napari-skimage-regionprops](https://www.napari-hub.org/plugins/napari-skimage-regionprops)\\r\\n\\r\\nFurthermore, if you use napari from Python, you can also create a dictionary or pandas DataFrame with measurements and store it in the `labels_layer.features` to make them available in the object classifier.\\r\\n\\r\\nAfter labels have been measured, you can start the `Object Classifier (custom properties, APOC)` from the `Tools > Segmentation post-processing` menu:\\r\\n\\r\\n![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/table_row_classifier_gui.png)\\r\\n\\r\\n1. Select the labels layers that has been measured.\\r\\n2. The annotation layer should contain manual annotations of object classes. \\r\\n   You can draw lines crossing single and multiple objects of the same kind. \\r\\n   For example draw a line through some elongated objects with label \\\"1\\\" and another line through some rather roundish objects with label \\\"2\\\".\\r\\n   If these lines touch the background, that will be ignored.\\r\\n3. Select the measurements / features that should be used for object classification.\\r\\n4. Use the `Update Measurements` button in case you did new measurements after Object classifier dialog was opened.\\r\\n5. Enter the filename of the classifier to be trained here. This file will be overwritten in case it existed already.\\r\\n6. Tree depth and number of trees allow you to fine-tune how to deal with manifold objects of different characteristics. The higher these numbers, the longer classification will take. In case you use many features, high depth and number of trees might be necessary. (See also `max_depth` and `n_estimators` in the [scikit-learn documentation of the Random Forest Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).\\r\\n7. The classification result will be stored under this name in the labels-layer's properties.\\r\\n8. Choose if the results table should be shown. Choose if classifier statistics should be shown. [Read more about classifier statistics](https://haesleinhuepf.github.io/BioImageAnalysisNotebooks/27_cell_classification/forest_statistics.html).\\r\\n9. Click on `Run` to start training and prediction.\\r\\n\\r\\nYou can also train those classifiers from Python and reuse them: [Read more about using the TableRowClassifier from python](https://haesleinhuepf.github.io/BioImageAnalysisNotebooks/27_cell_classification/apoc_simpleitk_object_classification.html)\\r\\n\\r\\n### Classifier statistics and correlation matrix\\r\\nAfter classifier training, you can study the share of the individual features/measurements and how they are correlated by activating the checkboxes `Show classifier statistics` and `Show correlation matrix`.\\r\\n![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/correlation_matrix.png)\\r\\n\\r\\nThis can help understanding how the classifier works. Furthermore, you can accelerate the classifier by reducing the number of correlated features.\\r\\n\\r\\n----------------------------------\\r\\n\\r\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\\r\\n\\r\\n## Installation\\r\\n\\r\\nIt is recommended to install the plugin in a conda environment. Therefore install conda first, e.g. [mini-conda](https://docs.conda.io/en/latest/miniconda.html).\\r\\nIf you never worked with conda before, reading this [short introduction](https://github.com/BiAPoL/Bio-image_Analysis_with_Python/blob/main/conda_basics/01_conda_environments.md) might be helpful.\\r\\n\\r\\nOptional: Setup a fresh conda environment, activate it and install napari:\\r\\n\\r\\n```\\r\\nconda create --name napari_apoc python=3.9\\r\\nconda activate napari_apoc\\r\\nconda install napari\\r\\n```\\r\\n\\r\\nIf your conda environment is set up, you can install `napari-accelerated-pixel-and-object-classification` using [pip]. Note: you need [pyopencl](https://documen.tician.de/pyopencl/) first.\\r\\n\\r\\n```\\r\\nconda install -c conda-forge pyopencl\\r\\npip install napari-accelerated-pixel-and-object-classification\\r\\n```\\r\\n\\r\\nMac-users please also install this:\\r\\n\\r\\n    conda install -c conda-forge ocl_icd_wrapper_apple\\r\\n    \\r\\nLinux users please also install this:\\r\\n    \\r\\n    conda install -c conda-forge ocl-icd-system\\r\\n\\r\\n\\r\\n## Contributing\\r\\n \\r\\nContributions, feedback and suggestions are very welcome. Tests can be run with [tox], please ensure\\r\\nthe coverage at least stays the same before you submit a pull request.\\r\\n\\r\\n## Similar software\\r\\nThere are other napari plugins and other software with similar functionality for interactive classification of pixels and objects.\\r\\n\\r\\n* [napari-feature-classifier](https://github.com/fractal-napari-plugins-collection/napari-feature-classifier)\\r\\n* [napari-buds](https://www.napari-hub.org/plugins/napari-buds)\\r\\n* [ilastik](https://www.ilastik.org/)\\r\\n* [Fiji's Trainable Weka Segmentation](https://imagej.net/plugins/tws/)\\r\\n* [scikit-learn](https://scikit-learn.org/stable/)\\r\\n\\r\\n## License\\r\\n\\r\\nDistributed under the terms of the [BSD-3] license,\\r\\n\\\"napari-accelerated-pixel-and-object-classification\\\" is free and open source software\\r\\n\\r\\n## Issues\\r\\n\\r\\nIf you encounter any problems, please [open a thread on image.sc](https://image.sc) along with a detailed description and tag [@haesleinhuepf](https://github.com/haesleinhuepf).\\r\\n\\r\\n[napari]: https://github.com/napari/napari\\r\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\r\\n[@napari]: https://github.com/napari\\r\\n[MIT]: http://opensource.org/licenses/MIT\\r\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\r\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\r\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\r\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\r\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\r\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\r\\n[file an issue]: https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/issues\\r\\n[napari]: https://github.com/napari/napari\\r\\n[tox]: https://tox.readthedocs.io/en/latest/\\r\\n[pip]: https://pypi.org/project/pip/\\r\\n[PyPI]: https://pypi.org/\\r\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-accelerated-pixel-and-object-classification (APOC)\\n\\n\\n\\n\\n\\n\\n\\n\\nclesperanto meets scikit-learn to classify pixels and objects in images, on a GPU using OpenCL in napari.\\n\\nThe processed example image was kindly acquired by Daniela Vorkel, Myers lab, MPI-CBG / CSBD (Download full video)\\nFor using the accelerated pixel and object classifiers in python, check out apoc.\\nTraining classifiers from pairs of image and label-mask folders is explained in \\nthis notebook.\\nFor executing APOC's pixel and object classifiers in Fiji using clij2 please read the documentation of the corresponding Fiji plugin. Table classifiers and object mergers are not compatible with Fiji yet.\\n\\nUsage\\nObject and Semantic Segmentation\\nStarting point is napari with at least one image layer and one labels layer (your annotation).\\n\\nYou find Object and Semantic Segmentation in the Tools > Segmentation / labeling. When starting those, the following graphical user interface will show up.\\n\\n\\nChoose one or multiple images to train on. These images will be considered as multiple channels. Thus, they need to be spatially correlated. \\n   Training from multiple images showing different scenes is not (yet) supported from the graphical user interface. Check out this notebook if you want to train from multiple image-annotation pairs.\\nSelect a file where the classifier should be saved. If the file exists already, it will be overwritten.\\nSelect the ground-truth annotation labels layer. \\nSelect which label corresponds to foreground (not available in Semantic Segmentation)\\nSelect the feature images that should be considered for segmentation. If segmentation appears pixelated, try increasing the selected sigma values and untick Consider original image.\\nTree depth and number of trees allow you to fine-tune how to deal with manifold regions of different characteristics. The higher these numbers, the longer segmentation will take. In case you use many images and many features, high depth and number of trees might be necessary. (See also max_depth and n_estimators in the scikit-learn documentation of the Random Forest Classifier.\\nThe estimation of memory consumption allows you to tune the configuration to your GPU-hardware. Also consider the GPU-hardware of others who want to use your classifier.\\nClick on Run when you're done with configuring. If the segmentation doesn't fit after the first execution, consider fine-tuning the ground-truth annotation and try again.\\n\\nA successful segmentation can for example look like this:\\n\\nAfter your classifier has been trained successfully, click on the \\\"Application / Prediction\\\" tab. If you apply the classifier again, python code will be generated. \\nYou can use this code for example to apply the same classifier to a folder of images. If you're new to this, check out this notebook.\\n\\nA pre-trained classifier can be applied from scripts as shown in the example notebook or from the Tools > Segmentation / labeling > Object segmentation (apply pretrained, APOC).\\nIntegration with the napari-assistant\\nPre-trained models can also be assembled to workflows using the napari-assistant. You find APOC-operations in the categories Filter, Label and Label Filters:\\n\\nSemantic segmentation\\nUsers can also generate semantic segmentation label images where the label identifier corresponds to a class the pixel has been allocated to. \\nThe tool can be found in the menu Tools > Segmentation / labeling > Semantic segmentation (APOC).\\nIt works analogously like the Object Segmenter, just without the need to specify the class identifier that objects correspond to.\\n\\nProbability maps\\nThe tool for generating probability maps (Tools > Filtering > Probability Mapper (APOC) menu) works analogously to the Object Segmenter as well. \\nThe only difference is that the result image is not a label image but an intensity image where the intensity represents the probability (between 0 and 1)\\nthat a pixel belongs to a given class. In this example: The raw image (grey) has been annotated with three classes: background (black, label 1), foreground (white, label 2) and edges (grey, label 3).\\nThe probability mapper was configured to create probability image (shown in green) for edges (label 3):\\n\\nClassifier statistics\\nWhile training, you can also activate the Show classifier statistics checkbox. \\nWhen doing so, it is recommended to increase the number of trees so that the measurements are more reliable, especially when selecting many features.\\nThis will open a small table after training where you can see how large the share of decision trees are for each analysed feature image.\\n\\nIt is recommended to turn on/off the features that hold a very large share (green) or a very small share (magenta) of trees in the random forest. \\nRetrain the classifier to see how the features influence the decision making.\\nNote: Multiple of these parameters may be correlated. \\nIf you select 11 feature images, which all allow to make the pixel classification similarly, but 10 of those are correlated, these 10 may appear with a share of about 0.05 while the 11th parameter has a share of 0.5. \\nThus, study these values with care.\\nMerging objects\\nAfter segmentation, you can merge labeled objects using the Tools > Segmentation post-processing > Merge objects (APOC) menu. \\nAnnotate label edges that should be merged with intensity 1 and those which should be kept with intensity 2 in a blank label image.\\nSelect which features should be considered for merging:\\n* touch_portion: The relative amount an object touches another. E.g. in a symmetric, honey-comb like tissue, neighboring cells have a touch-portion of 1/6 to each other.\\n* touch_count: The number of pixels where object touch. When using this parameter, make sure that images used for training and prediction have the same voxel size.\\n* mean_touch_intensity: The mean average intensity between touching objects. When using this parameter, make sure images used for training and prediction are normalized the same way.\\n* centroid_distance: The distance (in pixels or voxels) between centroids of labeled objects. \\n* mean_intensity_difference: The absolute difference between the mean intensity of the two objects. This measurement allows differentiating bright and dark object and [not] merging them.\\n* standard_deviation_intensity_difference: The absolute difference between the standard deviation of the two objects. This measurement allows to differentiate [in]homogeneous objects and [not] merge them.\\n* area_difference: The difference in area/volume/pixel-count allows differentiating small and large objects and [not] merging them.\\n* mean_max_distance_to_centroid_ratio_difference: This parameter is a shape descriptor, similar to elongation, allowing to differentiate roundish and elongate object and [not] merging them.\\nNote: most features are recommended to be used in isotropic images only.\\n\\nFor training, use an image with equivalized intensity (1), an over-segmented label image (2) and annotations (3). When drawing annotations in a new labels layer, make sure to misguide the algorithm draw on edges of touching objects a 1 if those should be merged and a 2 if they should be kept. Make sure there are no 1/2 annotation circles on both: labels which should be merged and kept.\\n\\nObject classification\\nClick the menu Tools > Segmentation post-processing > Object classification (APOC). \\n\\nThis user interface will be shown:\\n\\n\\nThe image layer will be used for intensity based feature extraction (see below).\\nThe labels layer should be contain the segmentation of objects that should be classified. \\n   You can use the Object Segmenter explained above to create this layer.\\nThe annotation layer should contain manual annotations of object classes. \\n   You can draw lines crossing single and multiple objects of the same kind. \\n   For example draw a line through some elongated objects with label \\\"1\\\" and another line through some rather roundish objects with label \\\"2\\\".\\n   If these lines touch the background, that will be ignored.\\nTree depth and number of trees allow you to fine-tune how to deal with manifold objects of different characteristics. The higher these numbers, the longer classification will take. In case you use many features, high depth and number of trees might be necessary. (See also max_depth and n_estimators in the scikit-learn documentation of the Random Forest Classifier.\\nSelect the right features for training. For example, for differentiating objects according to their shape as suggested above, select \\\"shape\\\".\\n   The features are extracted using clEsperanto and are shown by example in this notebook.\\nClick on the Run button. If classification doesn't perform well in the first attempt, try changing selected features.  \\n\\nIf classification worked well, it may for example look like this. Note the two thick lines which were drawn to annotate elongated and roundish objects with brown and cyan:\\n\\nA pre-trained model can later be applied from scripts as shown in the example notebook or using the menu Tools > Segmentation post-processing > Object classification (apply pretrained, APOC).\\nObject selection\\nAnalogously to object classification, the object selector removes all objects from a label image that do not belong to a specified class.\\nIt can be found in the menu Tools > Segmentation post-processing > Object selection (APOC). \\n\\nFeature correlation matrix\\nWhen training object classifiers it is crucial to investigate to which degree features are correlated and select the right, ideally uncorrelated features to classify objects robustly.\\nAfter measuring features with any compatible napari plugin listed below, you can visualize the feature correlation matrix using the menu Tools > Measurement tables > Show feature correlation matrix (pandas, APOC) and by selecting the labels layer which has been analyzed.\\nBefore computing the correlation matrix, all rows containing NaN values are removed.\\nFor further details, please refer to the documentation of the underlying function in pandas.\\n\\nSurface Vertex Classification (SVeC)\\nWhen using napari-APOC in combination with napari-process-points-and-surfaces>=0.3.3, \\none can also classify vertices. Therefore, use for example the menu Measurement > Surface quality table (vedo, nppas) to determine quantitative measurements\\nand the menu Surfaces > Annotate surface manually (nppas) for manual annotations. It is recommended to annotate the entire surface with value 1 as background, and specific regions of interest with integer numbers > 1.\\nAfter measurements have been extracted and annotations were made, start SVeC from the Surfaces > Surface vertex classification (custom properties, APOC) menu. It can be used like the Object Classifier explained above.\\n\\nDownload full video\\nClassifier statistics\\nAfter classifier training, you can study the share of the individual features/measurements and how they are correlated by activating the checkboxes Show classifier statistics and Show feature correlation matrix.\\n\\nThis can help understanding how the classifier works. Furthermore, you can accelerate the classifier by reducing the number of correlated features.\\nObject classification from custom measurements\\nYou can also classify labeled objects according to custom measurements. For deriving those measurements, you can use these napari plugins:\\n\\nmorphometrics\\nPartSeg\\nnapari-simpleitk-image-processing\\nnapari-cupy-image-processing\\nnapari-pyclesperanto-assistant\\nnapari-skimage-regionprops\\n\\nFurthermore, if you use napari from Python, you can also create a dictionary or pandas DataFrame with measurements and store it in the labels_layer.features to make them available in the object classifier.\\nAfter labels have been measured, you can start the Object Classifier (custom properties, APOC) from the Tools > Segmentation post-processing menu:\\n\\n\\nSelect the labels layers that has been measured.\\nThe annotation layer should contain manual annotations of object classes. \\n   You can draw lines crossing single and multiple objects of the same kind. \\n   For example draw a line through some elongated objects with label \\\"1\\\" and another line through some rather roundish objects with label \\\"2\\\".\\n   If these lines touch the background, that will be ignored.\\nSelect the measurements / features that should be used for object classification.\\nUse the Update Measurements button in case you did new measurements after Object classifier dialog was opened.\\nEnter the filename of the classifier to be trained here. This file will be overwritten in case it existed already.\\nTree depth and number of trees allow you to fine-tune how to deal with manifold objects of different characteristics. The higher these numbers, the longer classification will take. In case you use many features, high depth and number of trees might be necessary. (See also max_depth and n_estimators in the scikit-learn documentation of the Random Forest Classifier.\\nThe classification result will be stored under this name in the labels-layer's properties.\\nChoose if the results table should be shown. Choose if classifier statistics should be shown. Read more about classifier statistics.\\nClick on Run to start training and prediction.\\n\\nYou can also train those classifiers from Python and reuse them: Read more about using the TableRowClassifier from python\\nClassifier statistics and correlation matrix\\nAfter classifier training, you can study the share of the individual features/measurements and how they are correlated by activating the checkboxes Show classifier statistics and Show correlation matrix.\\n\\nThis can help understanding how the classifier works. Furthermore, you can accelerate the classifier by reducing the number of correlated features.\\n\\nThis napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.\\nInstallation\\nIt is recommended to install the plugin in a conda environment. Therefore install conda first, e.g. mini-conda.\\nIf you never worked with conda before, reading this short introduction might be helpful.\\nOptional: Setup a fresh conda environment, activate it and install napari:\\nconda create --name napari_apoc python=3.9\\nconda activate napari_apoc\\nconda install napari\\nIf your conda environment is set up, you can install napari-accelerated-pixel-and-object-classification using pip. Note: you need pyopencl first.\\nconda install -c conda-forge pyopencl\\npip install napari-accelerated-pixel-and-object-classification\\nMac-users please also install this:\\nconda install -c conda-forge ocl_icd_wrapper_apple\\n\\nLinux users please also install this:\\nconda install -c conda-forge ocl-icd-system\\n\\nContributing\\nContributions, feedback and suggestions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nSimilar software\\nThere are other napari plugins and other software with similar functionality for interactive classification of pixels and objects.\\n\\nnapari-feature-classifier\\nnapari-buds\\nilastik\\nFiji's Trainable Weka Segmentation\\nscikit-learn\\n\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-accelerated-pixel-and-object-classification\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please open a thread on image.sc along with a detailed description and tag @haesleinhuepf.\",\"development_status\":[\"Development Status :: 3 - Alpha\"],\"display_name\":\"napari-accelerated-pixel-and-object-classification\",\"documentation\":\"https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification\",\"first_released\":\"2021-10-02T19:44:17.676429Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-accelerated-pixel-and-object-classification\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2023-01-28T17:47:12.041679Z\",\"report_issues\":\"https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/issues\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"apoc (>=0.12.0)\",\"napari-tools-menu (>=0.1.17)\",\"napari-time-slicer\",\"superqt\",\"imageio (!=2.22.1)\",\"vispy (<0.12.0)\"],\"summary\":\"Pixel and label classification using OpenCL-based Random Forest Classifiers\",\"support\":\"https://forum.image.sc/tag/clij\",\"twitter\":\"\",\"version\":\"0.12.3\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Anna Medyukhina\",\"orcid\":\"0000-0001-5268-713X\"}],\"code_repository\":\"https://github.com/amedyukhina/napari-filament-annotator\",\"conda\":[],\"description\":\"# 3D Filament Annotator\\n\\n## Summary\\n\\n3D Filament Annotator is a tool for annotating filaments and other curvilinear structures in 3D. \\nThe 3D annotation is done by annotating the filament in two different projections, \\ncalculating intersection, and refining the filament position with active contours.\\n\\n![demo](https://raw.githubusercontent.com/amedyukhina/napari-filament-annotator/main/docs/demo_09.gif)\\n\\n## Intended Audience & Supported Data\\n\\nThe plugin is intended for annotation of filamentous structures from a 3D view. \\nThe main use-case are structures that are not visible in a single-slice image due to being\\ntoo thin or too low intensity. \\n\\nNo expertise in image analysis is required to use this plugin, though a basic knowledge of \\nactive contours would be helpful to set the parameters.\\n\\nThe plugin expects single-channel 3D images as input. Time-series data are not yet supported.\\n\\n## Quickstart\\n\\n**1. Open example image**\\n\\n![Open example image](https://raw.githubusercontent.com/amedyukhina/napari-filament-annotator/main/docs/demo_01.png)\\n\\n**2. Start the 3D annotator plugin**\\n\\n![Start 3D annotator plugin](https://raw.githubusercontent.com/amedyukhina/napari-filament-annotator/main/docs/demo_02.png)\\n\\n**3. Adjust image parameters**\\n\\nAdjust the voxel size of the image and the Gaussian-smoothing sigma that will be used to smooth the image for active\\ncontour refinement of filament position.\\n\\n- Voxel size in xy and z\\n- Sigma um: smoothing sigma, microns (or the same units as used for the voxel size)\\n\\n![Adjust image parameters](https://raw.githubusercontent.com/amedyukhina/napari-filament-annotator/main/docs/demo_03.png)\\n\\n**4. Add annotation layer**\\n\\nClick the \\\"Add annotation layer\\\" button to add a new Shapes layer for annotation.\\n\\nThis step might take several seconds, depending on the image size, due to some filtering \\nthat is performed behind the scenes.\\n\\n![Add annotation layer](https://raw.githubusercontent.com/amedyukhina/napari-filament-annotator/main/docs/demo_05.png)\\n\\n**5. Annotate filaments**\\n\\n1. Rotate the image to find a position, where the filament is clearly visible\\n2. Draw a line over the filament, by holding \\\"Control\\\" (or \\\"Command\\\" on macOS) and clicking with the mouse:\\n   this will draw a polygon with potential filament locations\\n3. Rotate the image to view the filament from another angle and repeat step 2\\n4. Rotate the image again: this will calculate the filament position from the intersection of the two polygons\\n5. Repeat steps 1-4 for other filaments\\n\\nHot keys to edit the annotations:\\n\\n- `p`: delete the last added point (during the polygon drawing)\\n- `d`: delete the last added shape (polygon or filament)\\n- `f`: delete the first point of the last added filament\\n- `l`: delete the last point of the last added filament\\n\\n![Annotate](https://raw.githubusercontent.com/amedyukhina/napari-filament-annotator/main/docs/demo_09.gif)\\n\\n**6. Save annotations**\\n\\nSave final or intermediate annotations to a csv file.\\n\\nThere is an option to load previously annotated filaments and continue the annotation.\\n\\n![Save annotations](https://raw.githubusercontent.com/amedyukhina/napari-filament-annotator/main/docs/demo_10.png)\\n\\n## Documentation \\n\\nFor more details on the plugin functionality and tips for adjusting parameters,\\nplease refer to the [Filamenter Annotator Tutorial](https://github.com/amedyukhina/napari-filament-annotator/blob/main/docs/tutorial.md)\\n\\n## Getting Help\\n\\nIf you encounter any problems, please \\n[file an issue](https://github.com/amedyukhina/napari-filament-annotator/issues) \\nalong with a detailed description.\\n\\nIf you have a question, you can ask it in the \\n[Discussion tab](https://github.com/amedyukhina/napari-filament-annotator/discussions) \\nof the project.\\n\\n## How to Cite\\n\\nAnna Medyukhina. (2022). amedyukhina/napari-filament-annotator: Version 0.1 (v0.1). Zenodo. \\nhttps://doi.org/10.5281/zenodo.7145278\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"3D Filament Annotator\\nSummary\\n3D Filament Annotator is a tool for annotating filaments and other curvilinear structures in 3D. \\nThe 3D annotation is done by annotating the filament in two different projections, \\ncalculating intersection, and refining the filament position with active contours.\\n\\nIntended Audience & Supported Data\\nThe plugin is intended for annotation of filamentous structures from a 3D view. \\nThe main use-case are structures that are not visible in a single-slice image due to being\\ntoo thin or too low intensity. \\nNo expertise in image analysis is required to use this plugin, though a basic knowledge of \\nactive contours would be helpful to set the parameters.\\nThe plugin expects single-channel 3D images as input. Time-series data are not yet supported.\\nQuickstart\\n1. Open example image\\n\\n2. Start the 3D annotator plugin\\n\\n3. Adjust image parameters\\nAdjust the voxel size of the image and the Gaussian-smoothing sigma that will be used to smooth the image for active\\ncontour refinement of filament position.\\n\\nVoxel size in xy and z\\nSigma um: smoothing sigma, microns (or the same units as used for the voxel size)\\n\\n\\n4. Add annotation layer\\nClick the \\\"Add annotation layer\\\" button to add a new Shapes layer for annotation.\\nThis step might take several seconds, depending on the image size, due to some filtering \\nthat is performed behind the scenes.\\n\\n5. Annotate filaments\\n\\nRotate the image to find a position, where the filament is clearly visible\\nDraw a line over the filament, by holding \\\"Control\\\" (or \\\"Command\\\" on macOS) and clicking with the mouse:\\n   this will draw a polygon with potential filament locations\\nRotate the image to view the filament from another angle and repeat step 2\\nRotate the image again: this will calculate the filament position from the intersection of the two polygons\\nRepeat steps 1-4 for other filaments\\n\\nHot keys to edit the annotations:\\n\\np: delete the last added point (during the polygon drawing)\\nd: delete the last added shape (polygon or filament)\\nf: delete the first point of the last added filament\\nl: delete the last point of the last added filament\\n\\n\\n6. Save annotations\\nSave final or intermediate annotations to a csv file.\\nThere is an option to load previously annotated filaments and continue the annotation.\\n\\nDocumentation\\nFor more details on the plugin functionality and tips for adjusting parameters,\\nplease refer to the Filamenter Annotator Tutorial\\nGetting Help\\nIf you encounter any problems, please \\nfile an issue \\nalong with a detailed description.\\nIf you have a question, you can ask it in the \\nDiscussion tab \\nof the project.\\nHow to Cite\\nAnna Medyukhina. (2022). amedyukhina/napari-filament-annotator: Version 0.1 (v0.1). Zenodo. \\nhttps://doi.org/10.5281/zenodo.7145278\",\"development_status\":[\"Development Status :: 3 - Alpha\"],\"display_name\":\"napari 3D filament annotator\",\"documentation\":\"https://github.com/amedyukhina/napari-filament-annotator#README.md\",\"first_released\":\"2022-10-04T21:47:29.936695Z\",\"license\":\"Apache-2.0\",\"name\":\"napari-filament-annotator\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\",\"sample_data\"],\"project_site\":\"https://github.com/amedyukhina/napari-filament-annotator\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-10-06T19:55:40.774807Z\",\"report_issues\":\"https://github.com/amedyukhina/napari-filament-annotator/issues\",\"requirements\":[\"Geometry3D\",\"networkx\",\"numpy\",\"magicgui\",\"pandas\",\"qtpy\",\"scipy\",\"sklearn\",\"imageio (!=2.22.1)\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\"],\"summary\":\"Annotation of filaments / curvilinear structures in 3D\",\"support\":\"https://github.com/amedyukhina/napari-filament-annotator/issues\",\"twitter\":\"\",\"version\":\"0.1.2\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Markus Stabrin\"}],\"code_repository\":null,\"description\":\"# napari-boxmanager\\n\\n[![License Mozilla Public License 2.0](https://img.shields.io/pypi/l/napari-boxmanager.svg?color=green)](https://github.com/mstabrin/napari-boxmanager/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-boxmanager.svg?color=green)](https://pypi.org/project/napari-boxmanager)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-boxmanager.svg?color=green)](https://python.org)\\n[![tests](https://github.com/mstabrin/napari-boxmanager/workflows/tests/badge.svg)](https://github.com/mstabrin/napari-boxmanager/actions)\\n[![codecov](https://codecov.io/gh/mstabrin/napari-boxmanager/branch/main/graph/badge.svg)](https://codecov.io/gh/mstabrin/napari-boxmanager)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-boxmanager)](https://napari-hub.org/plugins/napari-boxmanager)\\n\\nParticle selection tool for cryo-em\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/plugins/index.html\\n-->\\n\\n## Installation\\n\\n**!!!** The installation of the napari-boxmanager requires at least napari version `0.4.16.rc8`! **!!!**\\nIf the version is not yet available use:\\n\\n    conda create -y -n napari-env -c conda-forge python=3.10\\n    conda activate napari-env\\n    pip install 'napari[all]'\\n    pip uninstall napari\\n    pip install git+https://github.com/napari/napari\\n\\nYou can install `napari-boxmanager` via [pip]:\\n\\n    pip install napari-boxmanager\\n\\n\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [Mozilla Public License 2.0] license,\\n\\\"napari-boxmanager\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-boxmanager\\n\\n\\n\\n\\n\\n\\nParticle selection tool for cryo-em\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\n!!! The installation of the napari-boxmanager requires at least napari version 0.4.16.rc8! !!!\\nIf the version is not yet available use:\\nconda create -y -n napari-env -c conda-forge python=3.10\\nconda activate napari-env\\npip install 'napari[all]'\\npip uninstall napari\\npip install git+https://github.com/napari/napari\\n\\nYou can install napari-boxmanager via pip:\\npip install napari-boxmanager\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the Mozilla Public License 2.0 license,\\n\\\"napari-boxmanager\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please [file an issue] along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"Box Manager\",\"documentation\":\"\",\"first_released\":\"2022-09-26T11:24:43.663677Z\",\"license\":\"MPL-2.0\",\"name\":\"napari-boxmanager\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\",\"writer\",\"widget\"],\"project_site\":\"\",\"python_version\":\">=3.10\",\"reader_file_extensions\":[\"*.cbox\",\"*.box\",\"*.rec\",\"*.tloc\",\"*.coords\",\"*.star\",\"*.mrcs\",\"*.temb\",\"*.tmap\",\"*.st\",\"*.mrc\"],\"release_date\":\"2022-11-09T07:29:47.055950Z\",\"report_issues\":\"\",\"requirements\":[\"matplotlib\",\"mrcfile\",\"numpy\",\"pandas\",\"pystardb\",\"mrcfile ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"tox ; extra == 'testing'\"],\"summary\":\"Particle selection tool for cryo-em\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.2.10\",\"writer_file_extensions\":[\".st\",\".rec\",\".tloc\",\".mrcs\",\".cbox\",\".tmap\",\".mrc\",\".box\",\".star\",\".temb\",\".coords\"],\"writer_save_layers\":[\"points\",\"image\"]}",
  "{\"authors\":[{\"name\":\"Tim Morello\"}],\"code_repository\":\"https://github.com/tdmorello/napari-geojson\",\"description\":\"# napari-geojson\\n\\n[![License](https://img.shields.io/pypi/l/napari-geojson.svg?color=green)](https://github.com/tdmorello/napari-geojson/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-geojson.svg?color=green)](https://pypi.org/project/napari-geojson)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-geojson.svg?color=green)](https://python.org)\\n[![tests](https://github.com/tdmorello/napari-geojson/workflows/tests/badge.svg)](https://github.com/tdmorello/napari-geojson/actions)\\n[![codecov](https://codecov.io/gh/tdmorello/napari-geojson/branch/main/graph/badge.svg)](https://codecov.io/gh/tdmorello/napari-geojson)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-geojson)](https://napari-hub.org/plugins/napari-geojson)\\n\\nRead and write geojson files in napari.\\n\\n![](https://github.com/tdmorello/napari-geojson/raw/main/resources/output.gif)\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/plugins/stable/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-geojson` via [pip]:\\n\\n    pip install napari-geojson\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/tdmorello/napari-geojson.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-geojson\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/tdmorello/napari-geojson/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-geojson\\n\\n\\n\\n\\n\\n\\nRead and write geojson files in napari.\\n\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-geojson via pip:\\npip install napari-geojson\\n\\nTo install latest development version :\\npip install git+https://github.com/tdmorello/napari-geojson.git\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-geojson\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 3 - Alpha\"],\"display_name\":\"napari-geojson\",\"documentation\":\"https://github.com/tdmorello/napari-geojson#README.md\",\"first_released\":\"2021-12-27T22:49:46.771804Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-geojson\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\",\"writer\"],\"project_site\":\"https://github.com/tdmorello/napari-geojson\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[\"*.geojson\"],\"release_date\":\"2022-11-10T15:27:48.589387Z\",\"report_issues\":\"https://github.com/tdmorello/napari-geojson/issues\",\"requirements\":[\"geojson\",\"numpy\",\"black ; extra == 'dev'\",\"flake8 ; extra == 'dev'\",\"flake8-black ; extra == 'dev'\",\"flake8-docstrings ; extra == 'dev'\",\"flake8-isort ; extra == 'dev'\",\"isort ; extra == 'dev'\",\"mypy ; extra == 'dev'\",\"pre-commit ; extra == 'dev'\",\"pytest ; extra == 'dev'\",\"pytest-cov ; extra == 'dev'\",\"pytest-qt ; extra == 'dev'\",\"tox ; extra == 'dev'\",\"napari[all] ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"tox ; extra == 'testing'\"],\"summary\":\"Read and write geojson files in napari\",\"support\":\"https://github.com/tdmorello/napari-geojson/issues\",\"twitter\":\"\",\"version\":\"0.1.3\",\"visibility\":\"public\",\"writer_file_extensions\":[\".geojson\"],\"writer_save_layers\":[\"shapes\",\"points\"]}",
  "{\"authors\":[{\"name\":\"Mara Lampert\"}],\"code_repository\":\"https://github.com/biapol/guanine-crystal-analysis\",\"conda\":[],\"description\":\"# guanine-crystal-analysis\\n\\n[![License BSD-3](https://img.shields.io/pypi/l/guanine-crystal-analysis.svg?color=green)](https://github.com/biopo/guanine-crystal-analysis/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/guanine-crystal-analysis.svg?color=green)](https://pypi.org/project/guanine-crystal-analysis)\\n[![Python Version](https://img.shields.io/pypi/pyversions/guanine-crystal-analysis.svg?color=green)](https://python.org)\\n[![tests](https://github.com/biopo/guanine-crystal-analysis/workflows/tests/badge.svg)](https://github.com/biopo/guanine-crystal-analysis/actions)\\n[![codecov](https://codecov.io/gh/biopo/guanine-crystal-analysis/branch/main/graph/badge.svg)](https://codecov.io/gh/biopo/guanine-crystal-analysis)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/guanine-crystal-analysis)](https://napari-hub.org/plugins/guanine-crystal-analysis)\\n\\nA plugin for the guanine crystal segmentation, classification and characterization in the zebrafish eye\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `guanine-crystal-analysis` via [pip]:\\n\\n    pip install guanine-crystal-analysis\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/biopo/guanine-crystal-analysis.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"guanine-crystal-analysis\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/biopo/guanine-crystal-analysis/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"guanine-crystal-analysis\\n\\n\\n\\n\\n\\n\\nA plugin for the guanine crystal segmentation, classification and characterization in the zebrafish eye\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install guanine-crystal-analysis via pip:\\npip install guanine-crystal-analysis\\n\\nTo install latest development version :\\npip install git+https://github.com/biopo/guanine-crystal-analysis.git\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"guanine-crystal-analysis\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"Guanine Crystal Analysis\",\"documentation\":\"https://github.com/biapol/guanine-crystal-analysis#README.md\",\"first_released\":\"2022-07-26T15:15:12.639399Z\",\"license\":\"BSD-3-Clause\",\"name\":\"guanine-crystal-analysis\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\",\"sample_data\"],\"project_site\":\"https://github.com/biapol/guanine-crystal-analysis\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-07-26T15:15:12.639399Z\",\"report_issues\":\"https://github.com/biapol/guanine-crystal-analysis/issues\",\"requirements\":[\"numpy\",\"magicgui\",\"qtpy\",\"apoc\",\"scikit-image\",\"pandas\",\"napari-simpleitk-image-processing\",\"napari-skimage-regionprops\",\"pyclesperanto-prototype\",\"scikit-learn\",\"napari-workflows\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\"],\"summary\":\"A plugin for the guanine crystal segmentation, classification and characterization in the zebrafish eye\",\"support\":\"https://github.com/biapol/guanine-crystal-analysis/issues\",\"twitter\":\"\",\"version\":\"0.0.1\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Xavier Casas Moreno\"}],\"code_repository\":\"https://github.com/kasasxav/napari-file-watcher\",\"description\":\"# File watcher plugin for napari (napari-file-watcher)\\r\\n\\r\\n\\r\\nThis plugin contains two widgets: file watcher and script editor.\\r\\n\\r\\n\\r\\n## Usage: file watcher\\r\\n\\r\\nThe file watcher monitors a folder and displays its images (tiff, ome-zarr or hdf5) as napari layers, watch the following video for a demo:\\r\\n\\r\\n[![IMAGE ALT TEXT](http://img.youtube.com/vi/lFRVwlHgJ-Y/0.jpg)](https://www.youtube.com/watch?v=lFRVwlHgJ-Y \\\"Demo napari-file-watcher\\\")\\r\\n\\r\\nInstructions:\\r\\n\\r\\n1. Select the folder you want to monitor by pressing \\\"Browse\\\".\\r\\n2. Select the extension of the files: \\\"zarr\\\", \\\"hdf5\\\" or \\\"tiff\\\".\\r\\n3. Click \\\"Watch and run\\\" to display the current items & the newly arrived as napari layers.\\r\\n4. If you click in one of the files of the list, the metadata will show (for hdf5 and zarr)\\r\\n\\r\\n## Usage: scripting editor\\r\\n\\r\\nThe script editor is for developing scripts and saving them in the filesystem. \\r\\nWe have used this widget in the context of developing scripts for microscopy control software that implements another file watcher.\\r\\n\\r\\nInstructions:\\r\\n\\r\\n1. Select the folder where you want to save your scripts in \\\"Browse\\\".\\r\\n2. Type the name of the script in the edit box below.\\r\\n3. Click \\\"Add\\\" for saving it into the folder after typing, or \\\"Open\\\" to display an existing file.\\r\\n\\r\\n## Installation\\r\\n\\r\\nYou can install `napari-file-watcher` via [pip]:\\r\\n\\r\\n    pip install napari-file-watcher\\r\\n\\r\\nOr if you plan to develop it:\\r\\n\\r\\n    git clone https://github.com/kasasxav/napari-file-watcher\\r\\n    cd napari-file-watcher\\r\\n    pip install -e .\\r\\n\\r\\nIf there is an error message suggesting that git is not installed, run `conda install git`.\\r\\n\\r\\n## Contributing\\r\\n\\r\\nContributions are welcome, tests are run with pytest.\\r\\n\\r\\n## Issues\\r\\n\\r\\nIssues can be reported at: https://github.com/kasasxav/napari-file-watcher/issues\\r\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"File watcher plugin for napari (napari-file-watcher)\\nThis plugin contains two widgets: file watcher and script editor.\\nUsage: file watcher\\nThe file watcher monitors a folder and displays its images (tiff, ome-zarr or hdf5) as napari layers, watch the following video for a demo:\\n\\nInstructions:\\n\\nSelect the folder you want to monitor by pressing \\\"Browse\\\".\\nSelect the extension of the files: \\\"zarr\\\", \\\"hdf5\\\" or \\\"tiff\\\".\\nClick \\\"Watch and run\\\" to display the current items & the newly arrived as napari layers.\\nIf you click in one of the files of the list, the metadata will show (for hdf5 and zarr)\\n\\nUsage: scripting editor\\nThe script editor is for developing scripts and saving them in the filesystem. \\nWe have used this widget in the context of developing scripts for microscopy control software that implements another file watcher.\\nInstructions:\\n\\nSelect the folder where you want to save your scripts in \\\"Browse\\\".\\nType the name of the script in the edit box below.\\nClick \\\"Add\\\" for saving it into the folder after typing, or \\\"Open\\\" to display an existing file.\\n\\nInstallation\\nYou can install napari-file-watcher via [pip]:\\npip install napari-file-watcher\\n\\nOr if you plan to develop it:\\ngit clone https://github.com/kasasxav/napari-file-watcher\\ncd napari-file-watcher\\npip install -e .\\n\\nIf there is an error message suggesting that git is not installed, run conda install git.\\nContributing\\nContributions are welcome, tests are run with pytest.\\nIssues\\nIssues can be reported at: https://github.com/kasasxav/napari-file-watcher/issues\",\"development_status\":[],\"display_name\":\"napari-file-watcher\",\"documentation\":\"https://github.com/kasasxav/napari-file-watcher/blob/main/README.md\",\"first_released\":\"2023-01-16T10:34:53.282663Z\",\"license\":\"GPL-3.0\",\"name\":\"napari-file-watcher\",\"npe2\":true,\"operating_system\":[],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/kasasxav/napari-file-watcher\",\"python_version\":\"\",\"reader_file_extensions\":[],\"release_date\":\"2023-01-16T16:31:04.623176Z\",\"report_issues\":\"https://github.com/kasasxav/napari-file-watcher\",\"requirements\":[\"napari\",\"ome-zarr\",\"zarr\",\"h5py\",\"PyQt5\",\"qtpy\",\"QScintilla\"],\"summary\":\"A napari plugin for file watching\",\"support\":\"https://github.com/kasasxav/napari-file-watcher/issues\",\"twitter\":\"\",\"version\":\"0.1.1\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Kevin Yamauchi\"}],\"code_repository\":\"https://github.com/morphometrics/morphometrics-engine\",\"conda\":[],\"description\":\"# morphometrics-engine\\n\\n[![License BSD-3](https://img.shields.io/pypi/l/morphometrics-engine.svg?color=green)](https://github.com/morphometrics/morphometrics-engine/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/morphometrics-engine.svg?color=green)](https://pypi.org/project/morphometrics-engine)\\n[![Python Version](https://img.shields.io/pypi/pyversions/morphometrics-engine.svg?color=green)](https://python.org)\\n[![tests](https://github.com/morphometrics/morphometrics-engine/workflows/tests/badge.svg)](https://github.com/morphometrics/morphometrics-engine/actions)\\n[![codecov](https://codecov.io/gh/morphometrics/morphometrics-engine/branch/main/graph/badge.svg)](https://codecov.io/gh/morphometrics/morphometrics-engine)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/morphometrics-engine)](https://napari-hub.org/plugins/morphometrics-engine)\\n\\nA morphometrics measurement engine.\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/stable/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `morphometrics-engine` via [pip]:\\n\\n    pip install morphometrics-engine\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/morphometrics/morphometrics-engine.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"morphometrics-engine\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/morphometrics/morphometrics-engine/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"morphometrics-engine\\n\\n\\n\\n\\n\\n\\nA morphometrics measurement engine.\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install morphometrics-engine via pip:\\npip install morphometrics-engine\\n\\nTo install latest development version :\\npip install git+https://github.com/morphometrics/morphometrics-engine.git\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"morphometrics-engine\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"morphometrics engine\",\"documentation\":\"https://github.com/morphometrics/morphometrics-engine#README.md\",\"first_released\":\"2022-10-23T08:44:53.733804Z\",\"license\":\"BSD-3-Clause\",\"name\":\"morphometrics-engine\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/morphometrics/morphometrics-engine\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-10-23T08:44:53.733804Z\",\"report_issues\":\"https://github.com/morphometrics/morphometrics-engine/issues\",\"requirements\":[\"napari\",\"napari-skimage-regionprops\",\"numpy\",\"magicgui\",\"pandas\",\"qtpy\",\"superqt\",\"tqdm\",\"toolz\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\"],\"summary\":\"A morphometrics measurement engine.\",\"support\":\"https://github.com/morphometrics/morphometrics-engine/issues\",\"twitter\":\"\",\"version\":\"0.0.1\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"robert.haase@tu-dresden.de\",\"name\":\"Robert Haase\"}],\"code_repository\":\"https://github.com/haesleinhuepf/napari-curtain\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-curtain\"}],\"description\":\"# napari-curtain\\n\\n[![License](https://img.shields.io/pypi/l/napari-curtain.svg?color=green)](https://github.com/haesleinhuepf/napari-curtain/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-curtain.svg?color=green)](https://pypi.org/project/napari-curtain)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-curtain.svg?color=green)](https://python.org)\\n[![tests](https://github.com/haesleinhuepf/napari-curtain/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-curtain/actions)\\n[![codecov](https://codecov.io/gh/haesleinhuepf/napari-curtain/branch/main/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-curtain)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-curtain)](https://napari-hub.org/plugins/napari-curtain)\\n\\nView one image over another as curtain\\n\\n![](https://github.com/haesleinhuepf/napari-curtain/raw/main/docs/curtain_screencast.gif)\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n## Usage\\n\\nYou find the `Curtain` plugin in the menu `Tools > Visualization > Curtain`. Move the position of the slider left/right \\nas shown in the video above. In case one image is much bright than the other, you can modify the `factors` above the \\nslider until visualization pleases.\\n\\n## Installation\\n\\nYou can install `napari-curtain` via [pip]:\\n\\n    pip install napari-curtain\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-curtain\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/haesleinhuepf/napari-curtain/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-curtain\\n\\n\\n\\n\\n\\n\\nView one image over another as curtain\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\nUsage\\nYou find the Curtain plugin in the menu Tools > Visualization > Curtain. Move the position of the slider left/right \\nas shown in the video above. In case one image is much bright than the other, you can modify the factors above the \\nslider until visualization pleases.\\nInstallation\\nYou can install napari-curtain via pip:\\npip install napari-curtain\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-curtain\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 3 - Alpha\"],\"display_name\":\"napari-curtain\",\"documentation\":\"https://github.com/haesleinhuepf/napari-curtain#README.md\",\"first_released\":\"2021-11-07T16:51:53.631261Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-curtain\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/haesleinhuepf/napari-curtain\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-01-01T16:24:39.640036Z\",\"report_issues\":\"https://github.com/haesleinhuepf/napari-curtain/issues\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"napari-tools-menu\"],\"summary\":\"View one image over another as curtain\",\"support\":\"https://github.com/haesleinhuepf/napari-curtain/issues\",\"twitter\":\"\",\"version\":\"0.1.1\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"brisvag@gmail.com\",\"name\":\"Lorenzo Gaifas\"}],\"code_repository\":\"https://github.com/brisvag/napari-em-reader\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-em-reader\"}],\"description\":\"# napari-em-reader\\n\\n[![License](https://img.shields.io/pypi/l/napari-em-reader.svg?color=green)](https://github.com/brisvag/napari-em-reader/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-em-reader.svg?color=green)](https://pypi.org/project/napari-em-reader)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-em-reader.svg?color=green)](https://python.org)\\n[![tests](https://github.com/brisvag/napari-em-reader/workflows/tests/badge.svg)](https://github.com/brisvag/napari-em-reader/actions)\\n[![codecov](https://codecov.io/gh/brisvag/napari-em-reader/branch/master/graph/badge.svg)](https://codecov.io/gh/brisvag/napari-em-reader)\\n\\nA napari plugin to read .em files\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-em-reader` via [pip]:\\n\\n    pip install napari-em-reader\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-em-reader\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/brisvag/napari-em-reader/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-em-reader\\n\\n\\n\\n\\n\\nA napari plugin to read .em files\\n\\nThis napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-em-reader via pip:\\npip install napari-em-reader\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-em-reader\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"napari-em-reader\",\"documentation\":\"\",\"first_released\":\"2021-02-23T14:51:15.118105Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-em-reader\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\"],\"project_site\":\"https://github.com/brisvag/napari-em-reader\",\"python_version\":\">=3.6\",\"reader_file_extensions\":[\"*\"],\"release_date\":\"2021-02-23T14:51:15.118105Z\",\"report_issues\":\"\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"emfile (>=0.2)\"],\"summary\":\"A napari plugin to read .em files\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.1.0\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Léo Guignard\"}],\"code_repository\":\"https://github.com/leoguignard/napari-boids\",\"conda\":[],\"description\":\"# napari-boids\\n\\n[![License BSD-3](https://img.shields.io/pypi/l/napari-boids.svg?color=green)](https://github.com/leoguignard/napari-boids/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-boids.svg?color=green)](https://pypi.org/project/napari-boids)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-boids.svg?color=green)](https://python.org)\\n[![tests](https://github.com/leoguignard/napari-boids/workflows/tests/badge.svg)](https://github.com/leoguignard/napari-boids/actions)\\n[![codecov](https://codecov.io/gh/leoguignard/napari-boids/branch/main/graph/badge.svg)](https://codecov.io/gh/leoguignard/napari-boids)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-boids)](https://napari-hub.org/plugins/napari-boids)\\n\\nA plugin to look at boids\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-boids` via [pip]:\\n\\n    pip install napari-boids\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/leoguignard/napari-boids.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-boids\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/leoguignard/napari-boids/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-boids\\n\\n\\n\\n\\n\\n\\nA plugin to look at boids\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-boids via pip:\\npip install napari-boids\\n\\nTo install latest development version :\\npip install git+https://github.com/leoguignard/napari-boids.git\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-boids\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"Boids\",\"documentation\":\"https://github.com/leoguignard/napari-boids#README.md\",\"first_released\":\"2022-08-08T17:10:01.488885Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-boids\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/leoguignard/napari-boids\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-08-08T17:10:01.488885Z\",\"report_issues\":\"https://github.com/leoguignard/napari-boids/issues\",\"requirements\":null,\"summary\":\"A plugin to look at boids\",\"support\":\"https://github.com/leoguignard/napari-boids/issues\",\"twitter\":\"\",\"version\":\"0.0.1\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"lalit@mpi-cbg.de\",\"name\":\"Manan Lalit\"}],\"category\":{\"Image modality\":[\"Fluorescence microscopy\",\"Confocal microscopy\"],\"Supported data\":[\"3D\"],\"Workflow step\":[\"Image registration\",\"Object feature extraction\"]},\"category_hierarchy\":{\"Image modality\":[[\"Fluorescence microscopy\",\"Light-sheet microscopy\"],[\"Confocal microscopy\"]],\"Supported data\":[[\"3D\"]],\"Workflow step\":[[\"Image registration\"],[\"Object feature extraction\",\"Shape features extraction\"]]},\"code_repository\":\"https://github.com/juglab/PlatyMatch\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"platymatch\"}],\"description\":\"[![DOI:10.1007/978-3-030-66415-2_30](https://zenodo.org/badge/DOI/10.1007/978-3-030-66415-2_30.svg)](https://link.springer.com/chapter/10.1007/978-3-030-66415-2_30)\\n[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)\\n[![PyPI](https://img.shields.io/pypi/v/PlatyMatch.svg?color=green)](https://pypi.org/project/PlatyMatch)\\n[![Python Version](https://img.shields.io/pypi/pyversions/PlatyMatch.svg?color=green)](https://python.org)\\n[![tests](https://github.com/juglab/PlatyMatch/workflows/tests/badge.svg)](https://github.com/juglab/PlatyMatch/actions)\\n[![codecov](https://codecov.io/gh/juglab/PlatyMatch/branch/master/graph/badge.svg)](https://codecov.io/gh/juglab/PlatyMatch)\\n\\n\\n<p align=\\\"center\\\">\\n  <img src=\\\"https://user-images.githubusercontent.com/34229641/117537510-b26ee500-b001-11eb-9642-3baa461bfc94.png\\\" width=400 />\\n</p>\\n<h2 align=\\\"center\\\">Registration of Multi-modal Volumetric Images by Establishing Cell Correspondence</h2>\\n\\n## Table of Contents\\n\\n- **[Introduction](#introduction)**\\n- **[Dependencies](#dependencies)**\\n- **[Getting Started](#getting-started)**\\n- **[Datasets](#datasets)**\\n- **[Registering your data](#registering-your-data)**\\n- **[Contributing](#contributing)**\\n- **[Issues](#issues)**\\n- **[Citation](#citation)**\\n\\n### Introduction\\nThis repository hosts the version of the code used for the **[publication](https://link.springer.com/chapter/10.1007/978-3-030-66415-2_30)** **Registration of Multi-modal Volumetric Images by Establishing Cell Correspondence**. \\n\\nWe refer to the techniques elaborated in the publication, here as **PlatyMatch**. `PlatyMatch` performs a linear registration of volumetric, microscopy images of embryos by establishing correspondences between cells. \\n\\n`PlatyMatch` first detects nuclei in the two images being considered, next calculates unique `shape context` features for each nucleus detection which encapsulates the neighborhood as seen by that nucleus, and finally identifies pairs of matching nuclei through maximum bipartite matching applied to the pairwise distance matrix generated from these features. \\n\\n### Dependencies \\n\\nYou can install `PlatyMatch` via **[pip]**:\\n\\n```\\nconda create -y -n PlatyMatchEnv python==3.8\\nconda activate PlatyMatchEnv\\npython3 -m pip install PlatyMatch\\n```\\n\\n### Getting Started\\n\\nType in the following commands in a new terminal window.\\n\\n```\\nconda activate PlatyMatchEnv\\nnapari\\n```\\n\\nNext, select `PlatyMatch` from `Plugins> Add Dock Widget`.\\n\\n### Datasets\\n\\nDatasets are available in **`bic_eccv_data.zip`** as release assets **[here](https://github.com/juglab/PlatyMatch/releases/tag/v0.0.1)**.\\nThese comprise of images, nuclei detections and keypoint locations for confocal images of 12 individual specimens under the `01-insitus` directory and static snapshots of a live embryo imaged through Light Sheet Microscopy under the `02-live` directory. \\nFolders with the same name in these two directories correspond in their developmental age, for example, `01-insitus/02` corresponds to `02-live/02`, `01-insitus/03` corresponds to `02-live/03` and so on.   \\n\\n\\n### Registering your data\\n\\n- **Detect Nuclei** \\n\\t- Drag and drop your images in the viewer \\n\\t- Click on `Sync with Viewer` button to refresh the drop-down menus \\n\\t- Select the appropriate image in the drop down menu (for which nuclei detections are desired)\\n\\t- Select **`Detect Nuclei`** from the drop-down menu\\n\\t- Specify the anisotropy factor (`Anisotropy (Z)`) (i.e. the ratio of the size of the z pixel with respect to the x or y pixel. This factor is typically more than 1.0 because the z dimension is often undersampled)\\n\\t- Ideally min scales and max scales should be estimated from your data (`min_scale` should be set as `min_radius/sqrt(3)` and `max_scale` should be set as `max_radius/sqrt(3)`. The default values of `min_scale=5` and `max_scale=9` generally works well).  \\n\\t- Click `Run Scale Space Log` button. Please note that this step takes a few minutes.\\n\\t- Wait until a confirmation message suggesting that nuclei detection is over shows up on the terminal\\n\\t- Export the nuclei locations (`Export detections to csv`) to a csv file\\n\\t- Repeat this step for all images which need to be matched\\n\\n\\n\\n\\nhttps://user-images.githubusercontent.com/34229641/120660618-cd5d3980-c487-11eb-8996-326264a4df87.mp4\\n\\n\\n- **Estimate Transform**\\n\\t- In case, nuclei were exported to a csv in the `Detect Nuclei` panel, tick `csv` checkbox\\n\\t- If the nuclei detected were specified in the order id, z, y and x in the csv file, then tick `IZYXR` checkbox\\n\\t- Additionally if there is a header in the csv file, tick `Header` checkbox\\n\\t- Load the detections for the `Moving Image`, which is defined as the image which will be transformed to later match another `fixed` image\\n\\t- Load the detections for the `Fixed Image`\\n\\t- Click on `Run` pushbutton. Once the calculation is complete, a confirmation message shows up in the terminal. Export the transform matrix to a csv (Note that this step can take a few minutes)\\n\\t- It is also possible to estimate the transform in a `supervised` fashion. For this, upload the locations of a few matching keypoints in both images. These locations serve to provide a good starting point for the transform calculation. Once the keypoint files have been uploaded for both the images, then click `Run` and then export the transform matrix to a csv file \\n\\n\\nhttps://user-images.githubusercontent.com/34229641/120685628-53857a00-c4a0-11eb-8f92-7ffac730e28a.mp4\\n\\n\\n\\n- **Evaluate Metrics**\\n\\t- Drag images which need to be transformed, in the viewer\\n\\t- Click on `Sync with Viewer` button to refresh the drop-down menus\\n\\t- Specify the anisotropy factor (`Moving Image Anisotropy (Z)` and `Fixed Image Anisotropy (Z)`) (i.e. the ratio of the size of the z pixel with respect to the x or y pixel. This factor is typically more than 1.0 because the z dimension is often undersampled)\\n\\t- Load the transform which was calculated in the previous steps\\n\\t- If you simply wish to export a transformed version of the moving image, click on `Export Transformed Image`\\n\\t- Additionally, one could quantify metrics such as average registration error evaluated on a few keypoints. To do so, tick the `csv` checkbox, if keypoints and detections are available as a csv file. Then load the keypoints for the moving image (`Moving Kepoints`) and the fixed image (`Fixed Keypoints`)\\n\\t- Also, upload the detections calculated in the previous steps (`Detect Nuclei`)  by uploading the `Moving Detections` and the `Fixed Detections`\\n\\t- Click on the `Run` push button\\n\\t- The text fields such as `Matching Accuracy`(0 to 1, with 1 being the best) and `Average Registration Error` (the lower the better) should become populated once the results are available\\n\\n\\n\\nhttps://user-images.githubusercontent.com/34229641/120685654-5b451e80-c4a0-11eb-8d7d-de58b8b8304d.mp4\\n\\n\\n### Contributing\\n\\nContributions are very welcome. Tests can be run with **[tox]**.\\n\\n### Issues\\n\\nIf you encounter any problems, please **[file an issue]** along with a detailed description.\\n\\n[file an issue]: https://github.com/juglab/PlatyMatch/issues\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/EmbedSeg/\\n\\n\\n### Citation\\nIf you find our work useful in your research, please consider citing:\\n\\n```bibtex\\n@InProceedings{10.1007/978-3-030-66415-2_30,\\nauthor=\\\"Lalit, Manan and Handberg-Thorsager, Mette and Hsieh, Yu-Wen and Jug, Florian and Tomancak, Pavel\\\",\\neditor=\\\"Bartoli, Adrien\\nand Fusiello, Andrea\\\",\\ntitle=\\\"Registration of Multi-modal Volumetric Images by Establishing Cell Correspondence\\\",\\nbooktitle=\\\"Computer Vision -- ECCV 2020 Workshops\\\",\\nyear=\\\"2020\\\",\\npublisher=\\\"Springer International Publishing\\\",\\naddress=\\\"Cham\\\",\\npages=\\\"458--473\\\",\\nisbn=\\\"978-3-030-66415-2\\\"\\n}\\n```\\n\\n`PlatyMatch` plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/juglab/PlatyMatch/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"\\n\\n\\n\\n\\n\\n\\n\\n\\nRegistration of Multi-modal Volumetric Images by Establishing Cell Correspondence\\nTable of Contents\\n\\nIntroduction\\nDependencies\\nGetting Started\\nDatasets\\nRegistering your data\\nContributing\\nIssues\\nCitation\\n\\nIntroduction\\nThis repository hosts the version of the code used for the publication Registration of Multi-modal Volumetric Images by Establishing Cell Correspondence. \\nWe refer to the techniques elaborated in the publication, here as PlatyMatch. PlatyMatch performs a linear registration of volumetric, microscopy images of embryos by establishing correspondences between cells. \\nPlatyMatch first detects nuclei in the two images being considered, next calculates unique shape context features for each nucleus detection which encapsulates the neighborhood as seen by that nucleus, and finally identifies pairs of matching nuclei through maximum bipartite matching applied to the pairwise distance matrix generated from these features. \\nDependencies\\nYou can install PlatyMatch via pip:\\nconda create -y -n PlatyMatchEnv python==3.8\\nconda activate PlatyMatchEnv\\npython3 -m pip install PlatyMatch\\nGetting Started\\nType in the following commands in a new terminal window.\\nconda activate PlatyMatchEnv\\nnapari\\nNext, select PlatyMatch from Plugins> Add Dock Widget.\\nDatasets\\nDatasets are available in bic_eccv_data.zip as release assets here.\\nThese comprise of images, nuclei detections and keypoint locations for confocal images of 12 individual specimens under the 01-insitus directory and static snapshots of a live embryo imaged through Light Sheet Microscopy under the 02-live directory. \\nFolders with the same name in these two directories correspond in their developmental age, for example, 01-insitus/02 corresponds to 02-live/02, 01-insitus/03 corresponds to 02-live/03 and so on.   \\nRegistering your data\\n\\nDetect Nuclei \\nDrag and drop your images in the viewer \\nClick on Sync with Viewer button to refresh the drop-down menus \\nSelect the appropriate image in the drop down menu (for which nuclei detections are desired)\\nSelect Detect Nuclei from the drop-down menu\\nSpecify the anisotropy factor (Anisotropy (Z)) (i.e. the ratio of the size of the z pixel with respect to the x or y pixel. This factor is typically more than 1.0 because the z dimension is often undersampled)\\nIdeally min scales and max scales should be estimated from your data (min_scale should be set as min_radius/sqrt(3) and max_scale should be set as max_radius/sqrt(3). The default values of min_scale=5 and max_scale=9 generally works well).  \\nClick Run Scale Space Log button. Please note that this step takes a few minutes.\\nWait until a confirmation message suggesting that nuclei detection is over shows up on the terminal\\nExport the nuclei locations (Export detections to csv) to a csv file\\nRepeat this step for all images which need to be matched\\n\\n\\n\\nhttps://user-images.githubusercontent.com/34229641/120660618-cd5d3980-c487-11eb-8996-326264a4df87.mp4\\n\\nEstimate Transform\\nIn case, nuclei were exported to a csv in the Detect Nuclei panel, tick csv checkbox\\nIf the nuclei detected were specified in the order id, z, y and x in the csv file, then tick IZYXR checkbox\\nAdditionally if there is a header in the csv file, tick Header checkbox\\nLoad the detections for the Moving Image, which is defined as the image which will be transformed to later match another fixed image\\nLoad the detections for the Fixed Image\\nClick on Run pushbutton. Once the calculation is complete, a confirmation message shows up in the terminal. Export the transform matrix to a csv (Note that this step can take a few minutes)\\nIt is also possible to estimate the transform in a supervised fashion. For this, upload the locations of a few matching keypoints in both images. These locations serve to provide a good starting point for the transform calculation. Once the keypoint files have been uploaded for both the images, then click Run and then export the transform matrix to a csv file \\n\\n\\n\\nhttps://user-images.githubusercontent.com/34229641/120685628-53857a00-c4a0-11eb-8f92-7ffac730e28a.mp4\\n\\nEvaluate Metrics\\nDrag images which need to be transformed, in the viewer\\nClick on Sync with Viewer button to refresh the drop-down menus\\nSpecify the anisotropy factor (Moving Image Anisotropy (Z) and Fixed Image Anisotropy (Z)) (i.e. the ratio of the size of the z pixel with respect to the x or y pixel. This factor is typically more than 1.0 because the z dimension is often undersampled)\\nLoad the transform which was calculated in the previous steps\\nIf you simply wish to export a transformed version of the moving image, click on Export Transformed Image\\nAdditionally, one could quantify metrics such as average registration error evaluated on a few keypoints. To do so, tick the csv checkbox, if keypoints and detections are available as a csv file. Then load the keypoints for the moving image (Moving Kepoints) and the fixed image (Fixed Keypoints)\\nAlso, upload the detections calculated in the previous steps (Detect Nuclei)  by uploading the Moving Detections and the Fixed Detections\\nClick on the Run push button\\nThe text fields such as Matching Accuracy(0 to 1, with 1 being the best) and Average Registration Error (the lower the better) should become populated once the results are available\\n\\n\\n\\nhttps://user-images.githubusercontent.com/34229641/120685654-5b451e80-c4a0-11eb-8d7d-de58b8b8304d.mp4\\nContributing\\nContributions are very welcome. Tests can be run with tox.\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\\nCitation\\nIf you find our work useful in your research, please consider citing:\\nbibtex\\n@InProceedings{10.1007/978-3-030-66415-2_30,\\nauthor=\\\"Lalit, Manan and Handberg-Thorsager, Mette and Hsieh, Yu-Wen and Jug, Florian and Tomancak, Pavel\\\",\\neditor=\\\"Bartoli, Adrien\\nand Fusiello, Andrea\\\",\\ntitle=\\\"Registration of Multi-modal Volumetric Images by Establishing Cell Correspondence\\\",\\nbooktitle=\\\"Computer Vision -- ECCV 2020 Workshops\\\",\\nyear=\\\"2020\\\",\\npublisher=\\\"Springer International Publishing\\\",\\naddress=\\\"Cham\\\",\\npages=\\\"458--473\\\",\\nisbn=\\\"978-3-030-66415-2\\\"\\n}\\nPlatyMatch plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"PlatyMatch\",\"documentation\":\"\",\"first_released\":\"2021-05-28T20:21:08.816113Z\",\"license\":\"BSD-3\",\"name\":\"PlatyMatch\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/juglab/PlatyMatch\",\"python_version\":\">=3.6\",\"reader_file_extensions\":[],\"release_date\":\"2021-06-08T12:34:42.356971Z\",\"report_issues\":\"\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"scikit-image\",\"scikit-learn\",\"tqdm\",\"simpleitk\",\"napari[all]\",\"pandas\",\"pytest\"],\"summary\":\"PlatyMatch allows registration of volumetric images of embryos by establishing correspondences between cells\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.0.3\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Robert Haase\"}],\"code_repository\":\"https://github.com/haesleinhuepf/napari-assistant-plugin-generator\",\"description\":\"# napari-assistant-plugin-generator\\r\\n[![License](https://img.shields.io/pypi/l/napari-assistant-plugin-generator.svg?color=green)](https://github.com/haesleinhuepf/napari-assistant-plugin-generator/raw/master/LICENSE)\\r\\n[![PyPI](https://img.shields.io/pypi/v/napari-assistant-plugin-generator.svg?color=green)](https://pypi.org/project/napari-assistant-plugin-generator)\\r\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-assistant-plugin-generator.svg?color=green)](https://python.org)\\r\\n[![tests](https://github.com/haesleinhuepf/napari-assistant-plugin-generator/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-assistant-plugin-generator/actions)\\r\\n[![codecov](https://codecov.io/gh/haesleinhuepf/napari-assistant-plugin-generator/branch/master/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-assistant-plugin-generator)\\r\\n[![Development Status](https://img.shields.io/pypi/status/napari-assistant-plugin-generator.svg)](https://en.wikipedia.org/wiki/Software_release_life_cycle#Alpha)\\r\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-assistant-plugin-generator)](https://napari-hub.org/plugins/napari-assistant-plugin-generator)\\r\\n[![DOI](https://zenodo.org/badge/322312181.svg)](https://zenodo.org/badge/latestdoi/322312181)\\r\\n\\r\\nThe napari-assistant-plugin-generator is a [napari](https://github.com/napari/napari) plugin to generate python code which can be pip-installed and serve as napari-plugin compatible with the [napari-assistant](https://www.napari-hub.org/plugins/napari-assistant).\\r\\n\\r\\n## Usage\\r\\n\\r\\nFor demonstrating how one can generate a Napari plugin from an existing workflow, we demonstrate the procedure by reusing function from [napari-segment-blobs-and-things-with-membranes](https://www.napari-hub.org/plugins/napari-segment-blobs-and-things-with-membranes) (nsbatwm) for generating a plugin.\\r\\n\\r\\n* After installing nsbatwm you can start the assistant from the menu `Tools > Utilities > Assistant (na)` or run `naparia` from the command line. \\r\\n\\r\\n* Open the blobs example image `Blobs`, e.g. after downloading it from [here](https://github.com/clEsperanto/napari_pyclesperanto_assistant/blob/master/napari_pyclesperanto_assistant/data/blobs.tif).\\r\\n\\r\\n* In the Assistant, click on the `Remove noise` button and select `Gaussian (scikit-image, nsbatwm)` from the operation pulldown.\\r\\n* Click the `Binarize` button and select `Threshold (Otsu 1979, scikit-image, nsbatwm)` operation.\\r\\n* Click the `Label` button and select 'Connected component labeling (scikit-image, nsbatwm)' from the operation pulldown.\\r\\n\\r\\nAfterwards, your Napari with the configured workflow should look like this:\\r\\n\\r\\n![img.png](https://github.com/haesleinhuepf/napari-assistant-plugin-generator/raw/main/docs/screenshot_workflow.png)\\r\\n\\r\\n### Plugin generation\\r\\n\\r\\nBefore running the plugin-generator, make sure you are connected to the internet, \\r\\nbecause a [plugin template](https://github.com/haesleinhuepf/cookiecutter-napari-assistant-plugin-generator-plugin) will be downloaded. \\r\\nThe plugin generator can be found in the menu `Tools > Utilities > Generate Napari plugin from workflow (na)` and also \\r\\nin the `Generate code...` of the Assistant:\\r\\n\\r\\n![img.png](https://github.com/haesleinhuepf/napari-assistant-plugin-generator/raw/main/docs/generate_napari_plugin1.png)\\r\\n\\r\\nIn the plugin generator dialog, please enter this information:\\r\\n* **output dir:** Folder where the Napari plugin code should be saved. If not specified, the plugin will be stored in the current directory Napari was started from.\\r\\n* **plugin name:** Name of the plugin. A folder with this name will be generated in the folder specified above. Plugin names must not contain special characters or spaces. Use `_` instead.\\r\\n* **developer name:** Your name as it will be displayed later on the [napari-hub](https://napari-hub.org) in case you decide to publish your plugin.\\r\\n* **developer email:** Your email as it will be stored in the configuration of your plugin. This email-address is visible to the public.\\r\\n* **github username:** Your username on [github](https://github.com). URLs in the plugin documentation will point to your profile on github.\\r\\n* **short description:** Please write one sentence explaining what the plugin is doing.\\r\\n* **license:** Choose the open-source license your plugin code will be licensed. If you are not sure which one to use, consult [choosealicense.com](https://choosealicense.com).\\r\\n* **tools menu** The menu under `Tools` where your plugin will be found after installing it.\\r\\n* **menu name** The menu entry will have this title.\\r\\n\\r\\n![img.png](https://github.com/haesleinhuepf/napari-assistant-plugin-generator/raw/main/docs/generate_napari_plugin2.png)\\r\\n\\r\\nAfter the Napari plugin code has been generated, open it in the integrated development environment (IDE) of your choice. \\r\\nGo through the files in the directory and search for `TODO` entries. Start with the `readme.md` and the `requirements.txt` files:\\r\\n\\r\\n![img.png](https://github.com/haesleinhuepf/napari-assistant-plugin-generator/raw/main/docs/generate_napari_plugin4.png)\\r\\n\\r\\nHighly relevant might be the python file `my_napari_assistant_plugin/_function.py`. It contains a function with python code representing the workflow we designed by clicking above:\\r\\n\\r\\n```python\\r\\nfrom napari_plugin_engine import napari_hook_implementation\\r\\nfrom napari_tools_menu import register_function\\r\\nfrom napari_time_slicer import time_slicer\\r\\n\\r\\n@napari_hook_implementation\\r\\ndef napari_experimental_provide_function():\\r\\n    return [process_image]\\r\\n\\r\\n@register_function(menu=\\\"Segmentation / labeling > Segment image\\\")\\r\\n@time_slicer\\r\\ndef process_image(image0_b: \\\"napari.types.ImageData\\\", gaussian_blur_sigma_2: float = 1.0, connected_component_labeling_exclude_on_edges_3: bool = False) -> \\\"napari.types.LabelsData\\\":\\r\\n    \\\"\\\"\\\"\\r\\n    Short plugin description\\r\\n    \\r\\n    # TODO: Provide more detailed documentation here. E.g. specify the parameters and what values users should enter.\\r\\n    \\\"\\\"\\\"\\r\\n    # TODO: Check the list of parameters of the function definition above. \\r\\n    # If there are parameters that should not be editable by the end user, move their definition and values here instead.\\r\\n    \\r\\n    import napari_segment_blobs_and_things_with_membranes as nsbatwm  # version 0.3.3\\r\\n    \\r\\n    \\r\\n    # gaussian blur\\r\\n    image1_G = nsbatwm.gaussian_blur(image0_b, sigma=gaussian_blur_sigma_2)\\r\\n    \\r\\n    # threshold otsu\\r\\n    image2_T = nsbatwm.threshold_otsu(image1_G)\\r\\n    \\r\\n    # connected component labeling\\r\\n    image3_C = nsbatwm.connected_component_labeling(\\r\\n        image2_T, exclude_on_edges=connected_component_labeling_exclude_on_edges_3)\\r\\n    return image3_C\\r\\n```\\r\\n\\r\\nIt is recommended to inspect the generated code and rename variables to be more meaningful.\\r\\nFor renaming variables, make use of your IDE's tools. \\r\\nFor example variables can be renamed conveniently in [pycharm](https://www.jetbrains.com/pycharm/) using the right-click menu:\\r\\n\\r\\n![img.png](https://github.com/haesleinhuepf/napari-assistant-plugin-generator/raw/main/docs/generate_napari_plugin3.png)\\r\\n\\r\\nThe `readme.md` file also contains instructions for how to install and distribute your plugin. \\r\\nTL:DR: As a plugin developer you typically execute this command from the terminal within your plugin's root directory to install your plugin in an `editable`. \\r\\nThis command allows you to modify the code and test it without the need for re-installing your plugin.\\r\\n\\r\\n```\\r\\npip install -e .\\r\\n```\\r\\n\\r\\nIf installation was successful, you will find your plugin in the menu you specified and a dialog will open requesting the parameters of the generated Python function in `<your_plugin_folder>/<your_plugin_folder>/_function.py`:\\r\\n\\r\\n![img.png](https://github.com/haesleinhuepf/napari-assistant-plugin-generator/raw/main/docs/generate_napari_plugin5.png)\\r\\n\\r\\n![img.png](https://github.com/haesleinhuepf/napari-assistant-plugin-generator/raw/main/docs/generate_napari_plugin6.png)\\r\\n\\r\\n\\r\\n## Installation\\r\\n\\r\\nIt is recommended to install [devbio-napari](https://www.napari-hub.org/plugins/devbio-napari#installation) first. It comes with many image processing functions that can be combined in workflows and where it is easy to generate plugins from.\\r\\n\\r\\nAfterwards, the plugin generator can be installed using `pip`:\\r\\n```\\r\\npip install napari-assistant-plugin-generator\\r\\n```\\r\\n\\r\\nAlso make sure you have `git` installed. E.g. using `mamba`/`conda`:\\r\\n\\r\\n```\\r\\nmamba install git\\r\\n```\\r\\n\\r\\n## Feedback welcome!\\r\\n\\r\\nThe napari-assistant is developed in the open because we believe in the open source community. Feel free to drop feedback as [github issue](https://github.com/haesleinhuepf/napari-assistant-plugin-generator/issues) or via [image.sc](https://image.sc)\\r\\n\\r\\n## Contributing\\r\\n\\r\\nContributions are very welcome. Please ensure\\r\\nthe test coverage at least stays the same before you submit a pull request.\\r\\n\\r\\n## License\\r\\n\\r\\nDistributed under the terms of the [BSD-3] license,\\r\\n\\\"napari-assistant-plugin-generator\\\" is free and open source software\\r\\n\\r\\n## Acknowledgements\\r\\nThis project was supported by the Deutsche Forschungsgemeinschaft under Germany’s Excellence Strategy – EXC2068 - Cluster of Excellence \\\"Physics of Life\\\" of TU Dresden. \\r\\nThis project has been made possible in part by grant number [2021-240341 (Napari plugin accelerator grant)](https://chanzuckerberg.com/science/programs-resources/imaging/napari/improving-image-processing/) from the Chan Zuckerberg Initiative DAF, an advised fund of the Silicon Valley Community Foundation.\\r\\n\\r\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\r\\n\\r\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-assistant-plugin-generator\\n\\n\\n\\n\\n\\n\\n\\n\\nThe napari-assistant-plugin-generator is a napari plugin to generate python code which can be pip-installed and serve as napari-plugin compatible with the napari-assistant.\\nUsage\\nFor demonstrating how one can generate a Napari plugin from an existing workflow, we demonstrate the procedure by reusing function from napari-segment-blobs-and-things-with-membranes (nsbatwm) for generating a plugin.\\n\\n\\nAfter installing nsbatwm you can start the assistant from the menu Tools > Utilities > Assistant (na) or run naparia from the command line. \\n\\n\\nOpen the blobs example image Blobs, e.g. after downloading it from here.\\n\\n\\nIn the Assistant, click on the Remove noise button and select Gaussian (scikit-image, nsbatwm) from the operation pulldown.\\n\\nClick the Binarize button and select Threshold (Otsu 1979, scikit-image, nsbatwm) operation.\\nClick the Label button and select 'Connected component labeling (scikit-image, nsbatwm)' from the operation pulldown.\\n\\nAfterwards, your Napari with the configured workflow should look like this:\\n\\nPlugin generation\\nBefore running the plugin-generator, make sure you are connected to the internet, \\nbecause a plugin template will be downloaded. \\nThe plugin generator can be found in the menu Tools > Utilities > Generate Napari plugin from workflow (na) and also \\nin the Generate code... of the Assistant:\\n\\nIn the plugin generator dialog, please enter this information:\\n* output dir: Folder where the Napari plugin code should be saved. If not specified, the plugin will be stored in the current directory Napari was started from.\\n* plugin name: Name of the plugin. A folder with this name will be generated in the folder specified above. Plugin names must not contain special characters or spaces. Use _ instead.\\n* developer name: Your name as it will be displayed later on the napari-hub in case you decide to publish your plugin.\\n* developer email: Your email as it will be stored in the configuration of your plugin. This email-address is visible to the public.\\n* github username: Your username on github. URLs in the plugin documentation will point to your profile on github.\\n* short description: Please write one sentence explaining what the plugin is doing.\\n* license: Choose the open-source license your plugin code will be licensed. If you are not sure which one to use, consult choosealicense.com.\\n* tools menu The menu under Tools where your plugin will be found after installing it.\\n* menu name The menu entry will have this title.\\n\\nAfter the Napari plugin code has been generated, open it in the integrated development environment (IDE) of your choice. \\nGo through the files in the directory and search for TODO entries. Start with the readme.md and the requirements.txt files:\\n\\nHighly relevant might be the python file my_napari_assistant_plugin/_function.py. It contains a function with python code representing the workflow we designed by clicking above:\\n```python\\nfrom napari_plugin_engine import napari_hook_implementation\\nfrom napari_tools_menu import register_function\\nfrom napari_time_slicer import time_slicer\\n@napari_hook_implementation\\ndef napari_experimental_provide_function():\\n    return [process_image]\\n@register_function(menu=\\\"Segmentation / labeling > Segment image\\\")\\n@time_slicer\\ndef process_image(image0_b: \\\"napari.types.ImageData\\\", gaussian_blur_sigma_2: float = 1.0, connected_component_labeling_exclude_on_edges_3: bool = False) -> \\\"napari.types.LabelsData\\\":\\n    \\\"\\\"\\\"\\n    Short plugin description\\n# TODO: Provide more detailed documentation here. E.g. specify the parameters and what values users should enter.\\n\\\"\\\"\\\"\\n# TODO: Check the list of parameters of the function definition above. \\n# If there are parameters that should not be editable by the end user, move their definition and values here instead.\\n\\nimport napari_segment_blobs_and_things_with_membranes as nsbatwm  # version 0.3.3\\n\\n\\n# gaussian blur\\nimage1_G = nsbatwm.gaussian_blur(image0_b, sigma=gaussian_blur_sigma_2)\\n\\n# threshold otsu\\nimage2_T = nsbatwm.threshold_otsu(image1_G)\\n\\n# connected component labeling\\nimage3_C = nsbatwm.connected_component_labeling(\\n    image2_T, exclude_on_edges=connected_component_labeling_exclude_on_edges_3)\\nreturn image3_C\\n\\n```\\nIt is recommended to inspect the generated code and rename variables to be more meaningful.\\nFor renaming variables, make use of your IDE's tools. \\nFor example variables can be renamed conveniently in pycharm using the right-click menu:\\n\\nThe readme.md file also contains instructions for how to install and distribute your plugin. \\nTL:DR: As a plugin developer you typically execute this command from the terminal within your plugin's root directory to install your plugin in an editable. \\nThis command allows you to modify the code and test it without the need for re-installing your plugin.\\npip install -e .\\nIf installation was successful, you will find your plugin in the menu you specified and a dialog will open requesting the parameters of the generated Python function in <your_plugin_folder>/<your_plugin_folder>/_function.py:\\n\\n\\nInstallation\\nIt is recommended to install devbio-napari first. It comes with many image processing functions that can be combined in workflows and where it is easy to generate plugins from.\\nAfterwards, the plugin generator can be installed using pip:\\npip install napari-assistant-plugin-generator\\nAlso make sure you have git installed. E.g. using mamba/conda:\\nmamba install git\\nFeedback welcome!\\nThe napari-assistant is developed in the open because we believe in the open source community. Feel free to drop feedback as github issue or via image.sc\\nContributing\\nContributions are very welcome. Please ensure\\nthe test coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-assistant-plugin-generator\\\" is free and open source software\\nAcknowledgements\\nThis project was supported by the Deutsche Forschungsgemeinschaft under Germany’s Excellence Strategy – EXC2068 - Cluster of Excellence \\\"Physics of Life\\\" of TU Dresden. \\nThis project has been made possible in part by grant number 2021-240341 (Napari plugin accelerator grant) from the Chan Zuckerberg Initiative DAF, an advised fund of the Silicon Valley Community Foundation.\",\"development_status\":[],\"display_name\":\"\",\"documentation\":\"https://github.com/haesleinhuepf/napari-assistant-plugin-generator/\",\"first_released\":\"2022-11-09T14:50:52.822849Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-assistant-plugin-generator\",\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[],\"project_site\":\"https://github.com/haesleinhuepf/napari-assistant-plugin-generator/\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-11-09T14:50:52.822849Z\",\"report_issues\":\"https://github.com/haesleinhuepf/napari-assistant-plugin-generator/issues\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"magicgui\",\"numpy (!=1.19.4)\",\"jupytext\",\"jupyter\",\"dask\",\"autopep8\",\"cookiecutter\",\"napari-tools-menu\",\"napari-assistant (>=0.4.1)\"],\"summary\":\"Auto-generate python code from within napari to make napari-assistant compatible plugins\",\"support\":\"https://forum.image.sc/\",\"twitter\":\"\",\"version\":\"0.1.0\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"alan.watson@pitt.edu\",\"name\":\"Alan M Watson\"}],\"code_repository\":\"https://github.com/brain-image-library/napari-bil-data-viewer\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-bil-data-viewer\"}],\"description\":\"<p href=\\\"https://www.brainimagelibrary.org/\\\">\\n    <align=\\\"center\\\" width=\\\"100%\\\">\\n    <img width=\\\"100%\\\" src=\\\"https://i.imgur.com/ljZKq8h.png\\\">\\n</p>\\n\\n\\n# Description\\n\\nView datasets archived at the **[Brain Image Library](https://www.brainimagelibrary.org/)**.\\n\\n**NOTE: This plugin is under early development.  Currently, only a subset of single-channel, fMOST datasets which include projections are available to view.  An example can be found [here]( https://download.brainimagelibrary.org/2b/da/2bdaf9e66a246844/mouseID_405429-182725/).\\n\\n\\n\\n![Plugin Demo GIF](https://imgur.com/gkDCsMd.gif \\\"Plugin Demo GIF\\\")\\n\\n\\n\\n### Features\\n\\n* Multiscale Rendering\\n  * In datasets that include multiple resolution representations of the data, each resolution can be combined to improve the speed of browsing and user experience.  An example of a dataset with multiple resolution projections can be found [here](https://download.brainimagelibrary.org/2b/da/2bdaf9e66a246844/mouseID_405429-182725/).\\n  * All datasets included in the current release of napari-bil-data-viewer use multi-resolution datasets.\\n* 3D rendering of whole datasets.  The lowest resolution is used for rendering.  Currently, this is a limitation imposed by napari.\\n* The plugin does NOT require a BIL account as datasets are already accessible via https.\\n\\n### Known Issues / limitations\\n* Currently the only datasets that are available are those which have been manually selected by the developers.  If you would like a specific dataset to be included please consider adding the dataset(s) to the [dataset_info.py](https://github.com/brain-image-library/napari-bil-data-viewer/blob/main/napari_bil_data_viewer/dataset_info.py) file and submitting a pull request.\\n* To inquire about this plugin please contact Brain Image Library support:  bil-support@psc.edu\\n* The plugin is still under development.  We appreciate all [reports of issues / errors](https://github.com/brain-image-library/napari-bil-data-viewer/issues) which occur during use.\\n\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Installation\\n\\nOption #1: Install plugin via the napari plugin menu\\n\\n1. Menu: Plugins >> Install/Uninstall Plugins\\n2. Search: napari-bil-data-viewer\\n3. Select install\\n\\nOption #2:  Install a fresh python virtual environment\\n\\n```bash\\n# Example of venv creation using conda\\nconda create -y -n bil-viewer python=3.8\\nconda activate bil-viewer\\n\\n# Install napari-bil-data-viewer\\npip install napari-bil-data-viewer\\n\\n# Run Napari\\nnapari\\n```\\n\\n## Contributing\\n\\nPlease consider contributing to this project!  Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-bil-data-viewer\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/brain-image-library/napari-bil-data-viewer/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n## Change Log:\\n\\n##### <u>v0.1.0:</u>\\n\\nInitial release.\\n\\n<u>**v0.1.1 & v0.1.2:**</u>\\n\\nChanges to documentation\",\"description_content_type\":\"text/markdown\",\"description_text\":\"\\n\\n\\n\\nDescription\\nView datasets archived at the Brain Image Library.\\n**NOTE: This plugin is under early development.  Currently, only a subset of single-channel, fMOST datasets which include projections are available to view.  An example can be found here.\\n\\nFeatures\\n\\nMultiscale Rendering\\nIn datasets that include multiple resolution representations of the data, each resolution can be combined to improve the speed of browsing and user experience.  An example of a dataset with multiple resolution projections can be found here.\\nAll datasets included in the current release of napari-bil-data-viewer use multi-resolution datasets.\\n3D rendering of whole datasets.  The lowest resolution is used for rendering.  Currently, this is a limitation imposed by napari.\\nThe plugin does NOT require a BIL account as datasets are already accessible via https.\\n\\nKnown Issues / limitations\\n\\nCurrently the only datasets that are available are those which have been manually selected by the developers.  If you would like a specific dataset to be included please consider adding the dataset(s) to the dataset_info.py file and submitting a pull request.\\nTo inquire about this plugin please contact Brain Image Library support:  bil-support@psc.edu\\nThe plugin is still under development.  We appreciate all reports of issues / errors which occur during use.\\n\\n\\nThis napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nOption #1: Install plugin via the napari plugin menu\\n\\nMenu: Plugins >> Install/Uninstall Plugins\\nSearch: napari-bil-data-viewer\\nSelect install\\n\\nOption #2:  Install a fresh python virtual environment\\n```bash\\nExample of venv creation using conda\\nconda create -y -n bil-viewer python=3.8\\nconda activate bil-viewer\\nInstall napari-bil-data-viewer\\npip install napari-bil-data-viewer\\nRun Napari\\nnapari\\n```\\nContributing\\nPlease consider contributing to this project!  Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-bil-data-viewer\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\\nChange Log:\\nv0.1.0:\\nInitial release.\\nv0.1.1 & v0.1.2:\\nChanges to documentation\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari-bil-data-viewer\",\"documentation\":\"https://github.com/brain-image-library/napari-bil-data-viewer#README.md\",\"first_released\":\"2022-01-24T23:17:23.459374Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-bil-data-viewer\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/brain-image-library/napari-bil-data-viewer\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-01-25T16:30:08.480658Z\",\"report_issues\":\"https://github.com/brain-image-library/napari-bil-data-viewer/issues\",\"requirements\":[\"napari[all]\",\"napari-plugin-engine (>=0.1.4)\",\"scikit-image\",\"fsspec\",\"requests\",\"aiohttp\",\"imagecodecs\",\"beautifulsoup4\",\"dask\"],\"summary\":\"Napari plugin for viewing Brain Image Library datasets\",\"support\":\"https://github.com/brain-image-library/napari-bil-data-viewer\",\"twitter\":\"\",\"version\":\"0.1.2\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Marc Boucsein\"},{\"name\":\"Robin Koch\"}],\"code_repository\":\"https://github.com/MBPhys/Image-Composer\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"image-composer\"}],\"description\":\"# Image-Composer\\n\\n[![License](https://img.shields.io/pypi/l/napari-medical-image-formats.svg?color=green)](https://github.com/MBPhys/Image-Composer/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/Image-Composer.svg?color=green)](https://pypi.org/project/Image-Composer)\\n[![Python Version](https://img.shields.io/pypi/pyversions/Image-Composer.svg?color=green)](https://python.org)\\n\\n\\nA napari plugin in order to compose a background image with a foreground image.\\n\\n----------------------------------\\n\\n## Installation\\n\\nYou can install `Image-Composer` via [pip]:\\n\\n    pip install Image-Composer\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"Image-Composer\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/MBPhys/Image-Composer/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"Image-Composer\\n\\n\\n\\nA napari plugin in order to compose a background image with a foreground image.\\n\\nInstallation\\nYou can install Image-Composer via pip:\\npip install Image-Composer\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"Image-Composer\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"Image-Composer\",\"documentation\":\"\",\"first_released\":\"2022-01-10T19:11:03.062277Z\",\"license\":\"BSD-3-Clause\",\"name\":\"Image-Composer\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/MBPhys/Image-Composer\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-01-12T12:56:51.937813Z\",\"report_issues\":\"\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\"],\"summary\":\"A napari plugin in order to compose a background image with a foreground image\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.0.19\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"S. Panigrahi\"},{\"name\":\"L. Espinosa\"},{\"name\":\"IAM\"},{\"name\":\"LCB\"}],\"code_repository\":\"https://github.com/pswap/misic\",\"description\":\"# misic-napari\\n\\n<!-- [![License](https://img.shields.io/pypi/l/misic-napari-plugin.svg?color=green)](https://github.com/pswap/misic-napari-plugin/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/misic-napari-plugin.svg?color=green)](https://pypi.org/project/misic-napari-plugin)\\n[![Python Version](https://img.shields.io/pypi/pyversions/misic-napari-plugin.svg?color=green)](https://python.org)\\n[![tests](https://github.com/pswap/misic-napari-plugin/workflows/tests/badge.svg)](https://github.com/pswap/misic-napari-plugin/actions)\\n[![codecov](https://codecov.io/gh/pswap/misic-napari-plugin/branch/master/graph/badge.svg)](https://codecov.io/gh/pswap/misic-napari-plugin) -->\\n\\n----------------------------------\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\nA napari plugin for [MiSiC](https://elifesciences.org/articles/65151). Segmentation of bacteria in dense colonies. \\nThe plugin provides acces to preprocessing of the image like scaling, gamma correction, sharpness and noise variance that can improve the segmentation of bacteria irrespective of the imaging modality.\\n\\n## Install Napari\\nInstall napari either the bundled app or through [pip/conda]\\nhttps://napari.org/#installation\\n\\n## Installation\\n\\nInstall `misic-napari` through plugin manager in napari.\\n\\nOr\\n\\nYou can install `misic-napari` via [pip] in the napari console:\\n\\n    pip install misic-napari\\n\\n## Tutorial\\nNote: \\nThe image should be in the format [n,row,col] or [row,col], i.e., a single image or a stack. Hyper-stacks are not supported yet. \\n\\n#### get_width\\n\\n\\nCreates a Shapes layer with name 'cell-width' where the cell width can be hand drawn using line drawing tools in the shapes layer. This need not be precise and can be adjusted later. Click `get_cell_width` to obtain the desired mean cell width. This will be used to scale the image accordingly before segmentation.\\n \\n#### segment\\n\\nThis can be used to quickly set the parameters that can be later used to segment the whole stack.\\n\\n```\\nuse_roi\\n```\\nA square ROI of side 256 is created by default for quickly checking adjusting the segmentation parameters. The roi can be resized or moved in the `roi` shapes layer.\\n\\n```\\nlight_background\\n```\\nTrue; for phase-contrast images.\\n\\nFalse; for bright-field and fluorescence images.\\n\\n```\\nuse_local_noise\\n```\\nIf checked, this adds noise to image with local variance. In this case, a noise_var of around 0.1 works well. If unchecked, this adds noise with global variance of noise_var/100. Adding may help in removing false positives.\\n\\n```\\ngaussian_laplace\\n```\\nUseful when segmenting fluorescence images. \\n\\n```\\nadjust_scale\\n```\\nFine-tuning the scale around ([0.8,1.2]) the scale obtained from cell-width determined in `get_cell_width`.\\n\\n```\\nnoise_var\\n```\\nAmount of noise to be added to the image at the preprocessing step. This helps reduce the False Positives and, in many cases, to separate cells effectively. \\n```\\ngamma\\n```\\ngamma correction \\n\\n```\\nsharpness_scale and sharpness_amount\\n```\\nUnsharp mask based sharpness with sigma = sharpness_scale and amount = sharpness_amount\\n\\n\\n\\n### segment_stack\\nSegments the entire stack using the parameters that were obtained in \\\"segment\\\".\\n\\n\\n### save\\nThe parameters can be saved in a json file. \\n\\n## License\\n\\nDistributed under the terms of the [MIT] license,\\n\\\"misic-napari\\\" is free and open source software\\n\\n## Cite\\n```\\n@article {10.7554/eLife.65151,\\narticle_type = {journal},\\ntitle = {Misic, a general deep learning-based method for the high-throughput cell segmentation of complex bacterial communities},\\nauthor = {Panigrahi, Swapnesh and Murat, Dorothée and Le Gall, Antoine and Martineau, Eugénie and Goldlust, Kelly and Fiche, Jean-Bernard and Rombouts, Sara and Nöllmann, Marcelo and Espinosa, Leon and Mignot, Tâm},\\neditor = {Xiao, Jie and Storz, Gisela and Hensel, Zach},\\nvolume = 10,\\nyear = 2021,\\nmonth = {sep},\\npub_date = {2021-09-09},\\npages = {e65151},\\ncitation = {eLife 2021;10:e65151},\\ndoi = {10.7554/eLife.65151},\\nurl = {https://doi.org/10.7554/eLife.65151},\\nabstract = {Studies of bacterial communities, biofilms and microbiomes, are multiplying due to their impact on health and ecology. Live imaging of microbial communities requires new tools for the robust identification of bacterial cells in dense and often inter-species populations, sometimes over very large scales. Here, we developed MiSiC, a general deep-learning-based 2D segmentation method that automatically segments single bacteria in complex images of interacting bacterial communities with very little parameter adjustment, independent of the microscopy settings and imaging modality. Using a bacterial predator-prey interaction model, we demonstrate that MiSiC enables the analysis of interspecies interactions, resolving processes at subcellular scales and discriminating between species in millimeter size datasets. The simple implementation of MiSiC and the relatively low need in computing power make its use broadly accessible to fields interested in bacterial interactions and cell biology.},\\nkeywords = {Deep learning, image analysis, microscopy, myxococcus xanthus, biofilms},\\njournal = {eLife},\\nissn = {2050-084X},\\npublisher = {eLife Sciences Publications, Ltd},\\n}\\n```\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"misic-napari\\n\\n\\n\\nA napari plugin for MiSiC. Segmentation of bacteria in dense colonies. \\nThe plugin provides acces to preprocessing of the image like scaling, gamma correction, sharpness and noise variance that can improve the segmentation of bacteria irrespective of the imaging modality.\\nInstall Napari\\nInstall napari either the bundled app or through [pip/conda]\\nhttps://napari.org/#installation\\nInstallation\\nInstall misic-napari through plugin manager in napari.\\nOr\\nYou can install misic-napari via [pip] in the napari console:\\npip install misic-napari\\n\\nTutorial\\nNote: \\nThe image should be in the format [n,row,col] or [row,col], i.e., a single image or a stack. Hyper-stacks are not supported yet. \\nget_width\\nCreates a Shapes layer with name 'cell-width' where the cell width can be hand drawn using line drawing tools in the shapes layer. This need not be precise and can be adjusted later. Click get_cell_width to obtain the desired mean cell width. This will be used to scale the image accordingly before segmentation.\\nsegment\\nThis can be used to quickly set the parameters that can be later used to segment the whole stack.\\nuse_roi\\nA square ROI of side 256 is created by default for quickly checking adjusting the segmentation parameters. The roi can be resized or moved in the roi shapes layer.\\nlight_background\\nTrue; for phase-contrast images.\\nFalse; for bright-field and fluorescence images.\\nuse_local_noise\\nIf checked, this adds noise to image with local variance. In this case, a noise_var of around 0.1 works well. If unchecked, this adds noise with global variance of noise_var/100. Adding may help in removing false positives.\\ngaussian_laplace\\nUseful when segmenting fluorescence images. \\nadjust_scale\\nFine-tuning the scale around ([0.8,1.2]) the scale obtained from cell-width determined in get_cell_width.\\nnoise_var\\nAmount of noise to be added to the image at the preprocessing step. This helps reduce the False Positives and, in many cases, to separate cells effectively. \\ngamma\\ngamma correction \\nsharpness_scale and sharpness_amount\\nUnsharp mask based sharpness with sigma = sharpness_scale and amount = sharpness_amount\\nsegment_stack\\nSegments the entire stack using the parameters that were obtained in \\\"segment\\\".\\nsave\\nThe parameters can be saved in a json file. \\nLicense\\nDistributed under the terms of the [MIT] license,\\n\\\"misic-napari\\\" is free and open source software\\nCite\\n@article {10.7554/eLife.65151,\\narticle_type = {journal},\\ntitle = {Misic, a general deep learning-based method for the high-throughput cell segmentation of complex bacterial communities},\\nauthor = {Panigrahi, Swapnesh and Murat, Dorothée and Le Gall, Antoine and Martineau, Eugénie and Goldlust, Kelly and Fiche, Jean-Bernard and Rombouts, Sara and Nöllmann, Marcelo and Espinosa, Leon and Mignot, Tâm},\\neditor = {Xiao, Jie and Storz, Gisela and Hensel, Zach},\\nvolume = 10,\\nyear = 2021,\\nmonth = {sep},\\npub_date = {2021-09-09},\\npages = {e65151},\\ncitation = {eLife 2021;10:e65151},\\ndoi = {10.7554/eLife.65151},\\nurl = {https://doi.org/10.7554/eLife.65151},\\nabstract = {Studies of bacterial communities, biofilms and microbiomes, are multiplying due to their impact on health and ecology. Live imaging of microbial communities requires new tools for the robust identification of bacterial cells in dense and often inter-species populations, sometimes over very large scales. Here, we developed MiSiC, a general deep-learning-based 2D segmentation method that automatically segments single bacteria in complex images of interacting bacterial communities with very little parameter adjustment, independent of the microscopy settings and imaging modality. Using a bacterial predator-prey interaction model, we demonstrate that MiSiC enables the analysis of interspecies interactions, resolving processes at subcellular scales and discriminating between species in millimeter size datasets. The simple implementation of MiSiC and the relatively low need in computing power make its use broadly accessible to fields interested in bacterial interactions and cell biology.},\\nkeywords = {Deep learning, image analysis, microscopy, myxococcus xanthus, biofilms},\\njournal = {eLife},\\nissn = {2050-084X},\\npublisher = {eLife Sciences Publications, Ltd},\\n}\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"misic-napari\",\"documentation\":\"\",\"first_released\":\"2021-12-07T20:17:57.756112Z\",\"license\":\"MIT\",\"name\":\"misic-napari\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/pswap/misic\",\"python_version\":\">=3.6\",\"reader_file_extensions\":[],\"release_date\":\"2022-12-15T15:36:30.601282Z\",\"report_issues\":\"\",\"requirements\":[\"tensorflow\",\"termcolor\"],\"summary\":\"Segmentation of bacteria agnostic to imaging modality\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.2.3\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"robert.haase@tu-dresden.de\",\"name\":\"Robert Haase\"}],\"code_repository\":\"https://github.com/haesleinhuepf/beetlesafari\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"beetlesafari\"}],\"description\":\"A library for working with light sheet imaging data of developing embryos acquired using [ClearControl](https://github.com/ClearControl) at the [Center for Systems Biology Dresden](https://www.csbdresden.de/), e.g. _Tribolium castaneum_.\\n\\n# Installation\\n```\\nconda install -c conda-forge pyopencl\\npip install beetlesafari\\n```\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"A library for working with light sheet imaging data of developing embryos acquired using ClearControl at the Center for Systems Biology Dresden, e.g. Tribolium castaneum.\\nInstallation\\nconda install -c conda-forge pyopencl\\npip install beetlesafari\",\"development_status\":[\"Development Status :: 3 - Alpha\"],\"display_name\":\"beetlesafari\",\"documentation\":\"\",\"first_released\":\"2021-06-04T17:04:34.199080Z\",\"license\":\"BSD-3-Clause\",\"name\":\"beetlesafari\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[],\"project_site\":\"https://github.com/haesleinhuepf/beetlesafari\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-06-21T03:20:03.238550Z\",\"report_issues\":\"\",\"requirements\":[\"numpy\",\"pyopencl\",\"toolz\",\"scikit-image\",\"requests\",\"pyclesperanto-prototype\",\"napari\",\"magicgui\",\"dask\",\"cachetools\",\"napari-tools-menu\"],\"summary\":\"A napari plugin for loading and working with light sheet imaging data of developing embryos acquired using ClearControl, e.g. _Tribolium castaneum_.\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.4.0\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"jordao.bragantini@gmail.com\",\"name\":\"Jordão Bragantini\"}],\"code_repository\":null,\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"grabber-ift\"}],\"description\":\"# Grabber: A Tool to Improve Convergence in Interactive Image Segmentation\\n\\n[![License](https://img.shields.io/pypi/l/grabber.svg?color=green)](https://github.com/LIDS-UNICAMP/grabber/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/grabber.svg?color=green)](https://pypi.org/project/grabber)\\n[![Python Version](https://img.shields.io/pypi/pyversions/grabber.svg?color=green)](https://python.org)\\n[![tests](https://github.com/LIDS-UNICAMP/grabber/workflows/tests/badge.svg)](https://github.com/LIDS-UNICAMP/grabber/actions)\\n[![codecov](https://codecov.io/gh/LIDS-UNICAMP/grabber/branch/master/graph/badge.svg)](https://codecov.io/gh/LIDS-UNICAMP/grabber)\\n\\nA tool for contour-based segmentation correction (2D only).\\n\\nThis repository provides a demo code of the paper:\\n> **Grabber: A Tool to Improve Convergence in Interactive Image Segmentation**\\n> [Jordão Bragantini](https://jookuma.github.io/), Bruno Moura, [Alexandre X. Falcão](http://lids.ic.unicamp.br/), [Fábio A. M. Cappabianco](https://scholar.google.com/citations?user=qmH9VEEAAAAJ&hl=en&oi=ao)\\n\\nhttps://user-images.githubusercontent.com/21022743/145699960-57da06a5-668f-4e81-82b5-7f3d3ddf8ee3.mp4\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `grabber-ift` via [pip]:\\n\\n    pip install grabber-ift\\n\\n\\n## Known Limitations\\n\\nThis implementation doesn't support the items below, feel free to open a PR to add them.\\n\\n- It only support 2D image, supporting 3D images isn't trivial, but it could be applied per slice with minor changes.\\n\\n## Citation\\n\\nIf this work was useful for your research, please cite our paper:\\n\\n```\\n@article{bragantini2020grabber,\\n  title={Grabber: A Tool to Improve Convergence in Interactive Image Segmentation,\\n  author={Bragantini, Jord{\\\\~a}o and Bruno Moura, Falc{\\\\~a}o, Alexandre Xavier and Cappabianco, F{\\\\'a}bio AM,\\n  journal={Pattern Recognition Letters},\\n  year={2020}\\n}\\n```\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [MIT] license,\\n\\\"grabber\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"Grabber: A Tool to Improve Convergence in Interactive Image Segmentation\\n\\n\\n\\n\\n\\nA tool for contour-based segmentation correction (2D only).\\nThis repository provides a demo code of the paper:\\n\\nGrabber: A Tool to Improve Convergence in Interactive Image Segmentation\\nJordão Bragantini, Bruno Moura, Alexandre X. Falcão, Fábio A. M. Cappabianco\\n\\nhttps://user-images.githubusercontent.com/21022743/145699960-57da06a5-668f-4e81-82b5-7f3d3ddf8ee3.mp4\\n\\nThis napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install grabber-ift via pip:\\npip install grabber-ift\\n\\nKnown Limitations\\nThis implementation doesn't support the items below, feel free to open a PR to add them.\\n\\nIt only support 2D image, supporting 3D images isn't trivial, but it could be applied per slice with minor changes.\\n\\nCitation\\nIf this work was useful for your research, please cite our paper:\\n@article{bragantini2020grabber,\\n  title={Grabber: A Tool to Improve Convergence in Interactive Image Segmentation,\\n  author={Bragantini, Jord{\\\\~a}o and Bruno Moura, Falc{\\\\~a}o, Alexandre Xavier and Cappabianco, F{\\\\'a}bio AM,\\n  journal={Pattern Recognition Letters},\\n  year={2020}\\n}\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the MIT license,\\n\\\"grabber\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please [file an issue] along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"grabber-ift\",\"documentation\":\"\",\"first_released\":\"2021-12-12T06:26:35.907956Z\",\"license\":\"MIT\",\"name\":\"grabber-ift\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2021-12-14T03:26:55.818612Z\",\"report_issues\":\"\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"pyift (>=0.0.4)\",\"opencv-python-headless (>=4.4.0)\",\"scipy (>=1.7.2)\"],\"summary\":\"A tool for contour-based segmentation correction (2D only).\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.2.2\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Eduardo Gonzalez Solares\"}],\"code_repository\":null,\"description\":\"# IMAXT multiscale napari plugin\\n\\n[![License GNU LGPL v3.0](https://img.shields.io/pypi/l/imaxt-multiscale-plugin.svg?color=green)](https://github.com/eg266/imaxt-multiscale-plugin/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/imaxt-multiscale-plugin.svg?color=green)](https://pypi.org/project/imaxt-multiscale-plugin)\\n[![Python Version](https://img.shields.io/pypi/pyversions/imaxt-multiscale-plugin.svg?color=green)](https://python.org)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/imaxt-multiscale-plugin)](https://napari-hub.org/plugins/imaxt-multiscale-plugin)\\n\\nA napari plugin to visualize multi-resolution images created with the IMAXT mosaic pipeline.\\n\\n----------------------------------------------------\\n\\n## Installation\\n\\nYou can install `imaxt-multiscale-plugin` via [pip]:\\n\\n    pip install imaxt-multiscale-plugin\\n\\n\\n## Usage\\n\\nRun [napari] with the name of the sample to visualize either a local path:\\n\\n    napari /storage/imaxt/eglez/processed/stpt/20220606_PDX_AB559_GFP_005503_100x15um\\n\\nor a sample in S3 storage:\\n\\n    napari s3://imaxtgw/stpt/20220608_DI_PDX_SA535_Tum_5223_04280_100x15um\\n    \\n## Screenshots\\n\\n![Alt text](https://gitlab.developers.cam.ac.uk/astronomy/camcead/imaxt/imaxt-multiscale-plugin/-/raw/main/assets/napari1.png \\\"a title\\\")\\n![Alt text](https://gitlab.developers.cam.ac.uk/astronomy/camcead/imaxt/imaxt-multiscale-plugin/-/raw/main/assets/napari2.png \\\"a title\\\")\\n![Alt text](https://gitlab.developers.cam.ac.uk/astronomy/camcead/imaxt/imaxt-multiscale-plugin/-/raw/main/assets/napari3.png \\\"a title\\\")\\n![Alt text](https://gitlab.developers.cam.ac.uk/astronomy/camcead/imaxt/imaxt-multiscale-plugin/-/raw/main/assets/napari4.png \\\"a title\\\")\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [GNU LGPL v3.0] license,\\n\\\"imaxt-multiscale-plugin\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"IMAXT multiscale napari plugin\\n\\n\\n\\n\\nA napari plugin to visualize multi-resolution images created with the IMAXT mosaic pipeline.\\n\\nInstallation\\nYou can install imaxt-multiscale-plugin via pip:\\npip install imaxt-multiscale-plugin\\n\\nUsage\\nRun napari with the name of the sample to visualize either a local path:\\nnapari /storage/imaxt/eglez/processed/stpt/20220606_PDX_AB559_GFP_005503_100x15um\\n\\nor a sample in S3 storage:\\nnapari s3://imaxtgw/stpt/20220608_DI_PDX_SA535_Tum_5223_04280_100x15um\\n\\nScreenshots\\n\\n\\n\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the GNU LGPL v3.0 license,\\n\\\"imaxt-multiscale-plugin\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please [file an issue] along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"IMAXT Multiscale Image Napari Plugin\",\"documentation\":\"\",\"first_released\":\"2022-07-27T15:03:33.793891Z\",\"license\":\"LGPL-3.0-only\",\"name\":\"imaxt-multiscale-plugin\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\",\"writer\",\"widget\"],\"project_site\":\"https://gitlab.developers.cam.ac.uk/astronomy/camcead/imaxt/imaxt-multiscale-plugin\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[\"*\"],\"release_date\":\"2022-12-11T09:43:26.452179Z\",\"report_issues\":\"\",\"requirements\":[\"numpy\",\"magicgui\",\"qtpy\",\"imaxt-image\",\"xarray\",\"dask\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\"],\"summary\":\"A simple plugin to use with napari\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.2.2\",\"writer_file_extensions\":[\".npy\",\".tif\",\".tiff\"],\"writer_save_layers\":[\"labels\",\"image\"]}",
  "{\"authors\":[{\"name\":\"Grzegorz Bokota\"},{\"name\":\"Jacek Sroka\"},{\"name\":\"Subhadip Basu\"},{\"name\":\"Nirmal Das\"},{\"name\":\"Paweł Trzaskoma\"},{\"name\":\"Yana Yushkevich\"},{\"name\":\"Agnieszka Grabowska\"},{\"name\":\"Adriana Magalska\"},{\"name\":\"Dariusz Plewczyński\"}],\"category\":{\"Supported data\":[\"3D\"],\"Workflow step\":[\"Image Segmentation\",\"Image reconstruction\",\"Image feature detection\",\"Object feature extraction\"]},\"category_hierarchy\":{\"Supported data\":[[\"3D\"]],\"Workflow step\":[[\"Image Segmentation\",\"Cell segmentation\"],[\"Image Segmentation\",\"Connected-component analysis\"],[\"Image reconstruction\"],[\"Image feature detection\"],[\"Object feature extraction\"]]},\"citations\":{\"APA\":\"Bokota G., Sroka J., Basu S., Das N., Trzaskoma P., Yushkevich Y., Grabowska A., Magalska A., Plewczyński D. PartSeg: a tool for quantitative feature extraction from 3D microscopy images for dummies DOI: doi.org/10.1186/s12859-021-03984-1\\n\",\"BibTex\":\"@misc{YourReferenceHere,\\nauthor = {Bokota, Grzegorz and Sroka, Jacek and Basu, Subhadip and Das, Nirmal and Trzaskoma, Paweł and Yushkevich, Yana and Grabowska, Agnieszka and Magalska, Adriana and Plewczyński, Dariusz},\\ndoi = {doi.org/10.1186/s12859-021-03984-1},\\ntitle = {PartSeg: a tool for quantitative feature extraction from 3D microscopy images for dummies}\\n}\\n\",\"RIS\":\"TY  - GEN\\nAU  - Bokota, Grzegorz\\nAU  - Sroka, Jacek\\nAU  - Basu, Subhadip\\nAU  - Das, Nirmal\\nAU  - Trzaskoma, Paweł\\nAU  - Yushkevich, Yana\\nAU  - Grabowska, Agnieszka\\nAU  - Magalska, Adriana\\nAU  - Plewczyński, Dariusz\\nDO  - doi.org/10.1186/s12859-021-03984-1\\nTI  - PartSeg: a tool for quantitative feature extraction from 3D microscopy images for dummies\\nER\\n\",\"citation\":\"# This CITATION.cff file was generated with cffinit.\\n# Visit https://bit.ly/cffinit to generate yours today!\\n\\ncff-version: 1.2.0\\ntitle: >-\\n  PartSeg: a tool for quantitative feature extraction\\n  from 3D microscopy images for dummies\\nmessage: Cite as\\ntype: software\\nauthors:\\n  - given-names: Grzegorz\\n    family-names: Bokota\\n    email: g.bokota@mimuw.edu.pl\\n    orcid: 'https://orcid.org/0000-0002-5470-1676'\\n  - given-names: Jacek\\n    family-names: Sroka\\n    orcid: 'https://orcid.org/0000-0002-1714-9667'\\n  - orcid: 'https://orcid.org/0000-0003-1780-0461'\\n    given-names: Subhadip\\n    family-names: Basu\\n  - given-names: Nirmal\\n    family-names: Das\\n  - given-names: Paweł\\n    family-names: Trzaskoma\\n    orcid: 'https://orcid.org/0000-0002-5823-6572'\\n  - given-names: Yana\\n    family-names: Yushkevich\\n    orcid: 'https://orcid.org/0000-0002-5475-8613'\\n  - given-names: Agnieszka\\n    family-names: Grabowska\\n    orcid: 'https://orcid.org/0000-0002-8804-6817'\\n  - given-names: Adriana\\n    family-names: Magalska\\n    orcid: 'https://orcid.org/0000-0002-0517-7134'\\n  - given-names: Dariusz\\n    family-names: Plewczyński\\n    orcid: 'https://orcid.org/0000-0002-3840-7610'\\ndoi: doi.org/10.1186/s12859-021-03984-1\\n\"},\"code_repository\":\"https://github.com/4DNucleome/PartSeg\",\"description\":\"# Description\\n\\nPartSeg is a specialized GUI for feature extraction from multichannel light microscopy images, but also most of its features are available as napari Widgets.\\nInformation about PartSeg as standalone program or library are [here](https://github.com/4DNucleome/PartSeg)\\n\\n## Who is This For?\\n\\nThis plugin is for 2D and 3D segmentation of distinct objects from images and measuring various parameters of these objects.\\n\\nThis plugin process everything in memory, so it may not work with images stored in dask arrays. Currently, this plugin does not support the processing of Time data.\\n\\n## How-to Guide\\n\\nThe example data is stored [here](https://4dnucleome.cent.uw.edu.pl/PartSeg/Downloads/test_data.tbz2).\\n\\nhttps://user-images.githubusercontent.com/3826210/149146100-973498c7-7d2e-4298-a3c8-3051912e3183.mp4\\n\\nThe above video presents simple segmentation and measurement of various parameters of this segmentation (ROI).\\n\\nAs bellow described, algorithms are the result of porting PartSeg utilities to napari\\nthen detailed description could be found in PartSeg documentation/\\n\\n### ROI Extraction (Segmentation, pixel labeling)\\n\\nThe PartSeg is focused on the reproducible ROI Extraction process and offers two groups of algorithms:\\n\\n*   __ROI Mask Extraction__ set of algorithms (from PartSeg ROI Analysis) to work on a whole stack and mainly used for extracting nucleus or cell from a stack.\\n*   __ROI Analysis Extraction__ set of algorithms (from PartSeg ROI Mask) for detailed segmentation on the level of a single nucleus.\\n    If possible, they use an inner caching mechanism to improve performance while adjusting parameters.\\n\\nAlgorithms from both groups should support masking.\\n(perform ROI extraction only on the mask layer's area has non-zero values).\\n\\nParameters of ROI Extraction could be saved for later reuse (in the program) or exported to JSON and imported in another instance.\\nWith an accuracy of up to channel selection, they are identical to PartSeg,\\nso importing should work both ways, but the channel selection step needs to be repeated.\\n\\nThe list of available algorithms could be extensible using the PartSeg plugin mechanism.\\n\\n### Measurement widgets\\n\\nPartSeg offers two measurement widgets:\\n\\n#### Measurement\\n\\nInterface to PartSeg measurement engine.\\nIn this widget, there are two tabs. **Measurement settings* that allow\\nto define, delete, import, and export set of measurements\\n\\n![Measurement Settings](https://i.imgur.com/cfuXRRD.png)\\n\\nand **Measurement** for performing measures using an already defined set.\\n\\n![Measurement](https://i.imgur.com/4LzvqRp.png)\\n\\nThe list of available measurements could be extensible using the PartSeg plugin mechanism.\\n\\n#### Simple Measurement\\n\\n![Simple Measurement](https://i.imgur.com/Rnq6lF5.png)\\n\\nSet of measurements that could be calculated per component respecting data voxel size.\\nIn comparison to  *Measurement* list of available measures is limited to ones that do not need\\n*Mask* information and could be calculated per component.\\n\\nThis widget is equivalent to the PartSeg ROI Mask Simple Measurement window.\\n\\n### Search label\\n\\nWidget to find the layer with the given number By highlighting it or zooming on it. The highlight widget uses white color, so the highlight may not be visible if the label has a bright color.\\n\\nhttps://user-images.githubusercontent.com/3826210/154669409-cdac9be8-3dbf-4a0e-a66f-af8a44aed0fb.mp4\\n\\n### Mask create\\n\\nTransform labels layer into another labels layer with the possibility to dilate, and filling small holes\\n\\n![Mask create widget](https://i.imgur.com/FIJGLjb.png)\\n\\n## Reader plugins\\n\\nIn this plugin, there are also all PartSeg readers and writers.\\nThe most important readers are this, which allows loading PartSeg projects to napari.\\nThe one which could impact a user workflow is tiff reader.\\nIn comparison to the napari default one, there are two essential differences.\\nNapari's built-in plugin loads data as they are in a file.\\nPartSeg plugin read file metadata and return data in TZYX order.\\nPartSeg reader returns each channel as a separate layer.\\nPartSeg reader also tries to parse voxel size metadata and set scale parameters to nanometers' size.\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"Description\\nPartSeg is a specialized GUI for feature extraction from multichannel light microscopy images, but also most of its features are available as napari Widgets.\\nInformation about PartSeg as standalone program or library are here\\nWho is This For?\\nThis plugin is for 2D and 3D segmentation of distinct objects from images and measuring various parameters of these objects.\\nThis plugin process everything in memory, so it may not work with images stored in dask arrays. Currently, this plugin does not support the processing of Time data.\\nHow-to Guide\\nThe example data is stored here.\\nhttps://user-images.githubusercontent.com/3826210/149146100-973498c7-7d2e-4298-a3c8-3051912e3183.mp4\\nThe above video presents simple segmentation and measurement of various parameters of this segmentation (ROI).\\nAs bellow described, algorithms are the result of porting PartSeg utilities to napari\\nthen detailed description could be found in PartSeg documentation/\\nROI Extraction (Segmentation, pixel labeling)\\nThe PartSeg is focused on the reproducible ROI Extraction process and offers two groups of algorithms:\\n\\nROI Mask Extraction set of algorithms (from PartSeg ROI Analysis) to work on a whole stack and mainly used for extracting nucleus or cell from a stack.\\nROI Analysis Extraction set of algorithms (from PartSeg ROI Mask) for detailed segmentation on the level of a single nucleus.\\n    If possible, they use an inner caching mechanism to improve performance while adjusting parameters.\\n\\nAlgorithms from both groups should support masking.\\n(perform ROI extraction only on the mask layer's area has non-zero values).\\nParameters of ROI Extraction could be saved for later reuse (in the program) or exported to JSON and imported in another instance.\\nWith an accuracy of up to channel selection, they are identical to PartSeg,\\nso importing should work both ways, but the channel selection step needs to be repeated.\\nThe list of available algorithms could be extensible using the PartSeg plugin mechanism.\\nMeasurement widgets\\nPartSeg offers two measurement widgets:\\nMeasurement\\nInterface to PartSeg measurement engine.\\nIn this widget, there are two tabs. *Measurement settings that allow\\nto define, delete, import, and export set of measurements\\n\\nand Measurement for performing measures using an already defined set.\\n\\nThe list of available measurements could be extensible using the PartSeg plugin mechanism.\\nSimple Measurement\\n\\nSet of measurements that could be calculated per component respecting data voxel size.\\nIn comparison to  Measurement list of available measures is limited to ones that do not need\\nMask information and could be calculated per component.\\nThis widget is equivalent to the PartSeg ROI Mask Simple Measurement window.\\nSearch label\\nWidget to find the layer with the given number By highlighting it or zooming on it. The highlight widget uses white color, so the highlight may not be visible if the label has a bright color.\\nhttps://user-images.githubusercontent.com/3826210/154669409-cdac9be8-3dbf-4a0e-a66f-af8a44aed0fb.mp4\\nMask create\\nTransform labels layer into another labels layer with the possibility to dilate, and filling small holes\\n\\nReader plugins\\nIn this plugin, there are also all PartSeg readers and writers.\\nThe most important readers are this, which allows loading PartSeg projects to napari.\\nThe one which could impact a user workflow is tiff reader.\\nIn comparison to the napari default one, there are two essential differences.\\nNapari's built-in plugin loads data as they are in a file.\\nPartSeg plugin read file metadata and return data in TZYX order.\\nPartSeg reader returns each channel as a separate layer.\\nPartSeg reader also tries to parse voxel size metadata and set scale parameters to nanometers' size.\",\"development_status\":[\"Development Status :: 3 - Alpha\"],\"display_name\":\"PartSeg\",\"documentation\":\"https://partseg.readthedocs.io/en/stable/\",\"first_released\":\"2019-01-14T23:29:35.670697Z\",\"license\":\"BSD-3-Clause\",\"name\":\"PartSeg\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\",\"writer\",\"widget\"],\"project_site\":\"https://4dnucleome.cent.uw.edu.pl/PartSeg/\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[\"*\"],\"release_date\":\"2022-11-13T16:23:50.506449Z\",\"report_issues\":\"https://github.com/4DNucleome/PartSeg/issues\",\"requirements\":[\"IPython (>=7.7.0)\",\"PartSegCore-compiled-backend (>=0.13.11)\",\"PartSegData (==0.10.0)\",\"QtAwesome (!=1.2.0,>=1.0.3)\",\"QtPy (>=1.7.0)\",\"SimpleITK (>=1.1.0)\",\"appdirs (>=1.4.4)\",\"czifile (>=2019.5.22)\",\"defusedxml (>=0.6.0)\",\"h5py (>=2.8.0)\",\"imagecodecs (>=2020.5.30)\",\"imageio (>=2.5.0)\",\"ipykernel (>=5.2.0)\",\"magicgui (!=0.5.0,>=0.4.0)\",\"mahotas (>=1.4.9)\",\"napari (>=0.4.12)\",\"nme (>=0.1.6)\",\"numpy (>=1.18.5)\",\"oiffile (>=2020.1.18)\",\"openpyxl (>=2.4.9)\",\"packaging (>=20.0)\",\"pandas (>=0.24.0)\",\"psygnal (>=0.3.1)\",\"pydantic (>=1.8.1)\",\"pygments (>=2.4.0)\",\"qtconsole (>=4.7.7)\",\"requests (>=2.18.0)\",\"scipy (>=1.2.0)\",\"sentry-sdk (>=0.14.3)\",\"six (>=1.11.0)\",\"superqt (>=0.2.4)\",\"sympy (>=1.1.1)\",\"tifffile (>=2020.9.30)\",\"traceback-with-variables (>=2.0.4)\",\"vispy (>=0.6.4)\",\"xlrd (>=1.1.0)\",\"xlsxwriter\",\"importlib-metadata (<4.12.0) ; python_version < \\\"3.8\\\"\",\"typing-extensions (>=3.7.4.3) ; python_version < \\\"3.8\\\"\",\"vispy (<0.10.0) ; python_version < \\\"3.8\\\"\",\"PyOpenGL-accelerate (>=3.1.5) ; extra == 'accelerate'\",\"PyOpenGL-accelerate (>=3.1.5) ; extra == 'all'\",\"PyQt5 (!=5.15.0,>=5.12.3) ; extra == 'all'\",\"autodoc-pydantic (==1.7.2) ; extra == 'docs'\",\"sphinx (!=3.0.0,!=3.5.0) ; extra == 'docs'\",\"sphinx-autodoc-typehints (==1.18.3) ; extra == 'docs'\",\"sphinx-qt-documentation (==0.4) ; extra == 'docs'\",\"PyQt5 (!=5.15.0,>=5.12.3) ; extra == 'pyqt'\",\"PyQt5 (!=5.15.0,>=5.12.3) ; extra == 'pyqt5'\",\"PySide2 (!=5.15.0,>=5.12.3) ; extra == 'pyside'\",\"PySide2 (!=5.15.0,>=5.12.3) ; extra == 'pyside2'\",\"codecov ; extra == 'test'\",\"lxml ; extra == 'test'\",\"pytest (>=7.0.0) ; extra == 'test'\",\"pytest-cov ; extra == 'test'\",\"pytest-qt ; extra == 'test'\",\"pytest-timeout ; extra == 'test'\",\"scikit-image ; extra == 'test'\",\"tox ; extra == 'test'\"],\"summary\":\"PartSeg is python GUI and set of napari plugins for bio imaging analysis especially nucleus analysis,\",\"support\":\"https://github.com/4DNucleome/PartSeg/issues\",\"twitter\":\"\",\"version\":\"0.14.6\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[\"labels\",\"image\"]}",
  "{\"authors\":[{\"name\":\"Marc Boucsein\"},{\"name\":\"Robin Koch\"}],\"code_repository\":\"https://github.com/MBPhys/Layer-Data-Replace\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"layer-data-replace\"}],\"description\":\"# Layer-Data-Replace\\n\\n[![License](https://img.shields.io/pypi/l/napari-medical-image-formats.svg?color=green)](https://github.com/MBPhys/Layer-Data-Replace/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/Layer-Data-Replace.svg?color=green)](https://pypi.org/project/Layer-Data-Replace)\\n[![Python Version](https://img.shields.io/pypi/pyversions/Layer-Data-Replace.svg?color=green)](https://python.org)\\n\\n\\nA napari plugin in order to replace parts of the data of a layer by another one.\\n\\n----------------------------------\\n\\n## Installation\\n\\nYou can install `Layer-Data-Replace` via [pip]:\\n\\n    pip install Layer-Data-Replace\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"Layer-Data-Replace\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/MBPhys/Layer-Data-Replace/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"Layer-Data-Replace\\n\\n\\n\\nA napari plugin in order to replace parts of the data of a layer by another one.\\n\\nInstallation\\nYou can install Layer-Data-Replace via pip:\\npip install Layer-Data-Replace\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"Layer-Data-Replace\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"Layer-Data-Replace\",\"documentation\":\"\",\"first_released\":\"2022-01-13T15:35:24.841872Z\",\"license\":\"BSD-3-Clause\",\"name\":\"Layer-Data-Replace\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/MBPhys/Layer-Data-Replace\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-01-13T15:35:24.841872Z\",\"report_issues\":\"\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"dask\"],\"summary\":\"A napari plugin in order to replace parts of the data of a layer by another one\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.0.5\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Kevin Yamauchi\"}],\"code_repository\":\"https://github.com/morphometrics/morphospace\",\"description\":\"# morphospace\\n\\n[![License BSD-3](https://img.shields.io/pypi/l/morphospace.svg?color=green)](https://github.com/morphometrics/morphospace/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/morphospace.svg?color=green)](https://pypi.org/project/morphospace)\\n[![Python Version](https://img.shields.io/pypi/pyversions/morphospace.svg?color=green)](https://python.org)\\n[![tests](https://github.com/morphometrics/morphospace/workflows/tests/badge.svg)](https://github.com/morphometrics/morphospace/actions)\\n[![codecov](https://codecov.io/gh/morphometrics/morphospace/branch/main/graph/badge.svg)](https://codecov.io/gh/morphometrics/morphospace)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/morphospace)](https://napari-hub.org/plugins/morphospace)\\n\\na library for creating  and exploring morphospaces.\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `morphospace` via [pip]:\\n\\n    pip install morphospace\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/morphometrics/morphospace.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"morphospace\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/morphometrics/morphospace/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"morphospace\\n\\n\\n\\n\\n\\n\\na library for creating  and exploring morphospaces.\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install morphospace via pip:\\npip install morphospace\\n\\nTo install latest development version :\\npip install git+https://github.com/morphometrics/morphospace.git\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"morphospace\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"morphospace\",\"documentation\":\"https://github.com/morphometrics/morphospace#README.md\",\"first_released\":\"2022-12-10T13:52:40.629551Z\",\"license\":\"BSD-3-Clause\",\"name\":\"morphospaces\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\",\"sample_data\"],\"project_site\":\"https://github.com/morphometrics/morphospaces\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-12-10T13:52:40.629551Z\",\"report_issues\":\"https://github.com/morphometrics/morphospace/issues\",\"requirements\":[\"h5py\",\"magicgui\",\"numpy\",\"pytorch-lightning\",\"qtpy\",\"scikit-image\",\"scipy\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"tox ; extra == 'testing'\"],\"summary\":\"a library for creating  and exploring morphospaces.\",\"support\":\"https://github.com/morphometrics/morphospace/issues\",\"twitter\":\"\",\"version\":\"0.0.3\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Robert Haase\"},{\"name\":\"Ryan Savill\"}],\"code_repository\":\"https://github.com/haesleinhuepf/napari-assistant\",\"description\":\"# napari-assistant\\r\\n[![License](https://img.shields.io/pypi/l/napari-assistant.svg?color=green)](https://github.com/haesleinhuepf/napari-assistant/raw/master/LICENSE)\\r\\n[![PyPI](https://img.shields.io/pypi/v/napari-assistant.svg?color=green)](https://pypi.org/project/napari-assistant)\\r\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-assistant.svg?color=green)](https://python.org)\\r\\n[![tests](https://github.com/haesleinhuepf/napari-assistant/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-assistant/actions)\\r\\n[![codecov](https://codecov.io/gh/haesleinhuepf/napari-assistant/branch/master/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-assistant)\\r\\n[![Development Status](https://img.shields.io/pypi/status/napari-assistant.svg)](https://en.wikipedia.org/wiki/Software_release_life_cycle#Alpha)\\r\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-assistant)](https://napari-hub.org/plugins/napari-assistant)\\r\\n[![DOI](https://zenodo.org/badge/463875112.svg)](https://zenodo.org/badge/latestdoi/463875112)\\r\\n\\r\\n\\r\\nThe napari-assistant is a [napari](https://github.com/napari/napari) meta-plugin for building image processing workflows. \\r\\n\\r\\n## Usage\\r\\n\\r\\nAfter installing one or more napari plugins that use the napari-assistant as user interface, you can start it from the \\r\\nmenu `Tools > Utilities > Assistant (na)` or run `naparia` from the command line. \\r\\n\\r\\nBy clicking on the buttons in the assistant, you can setup a workflow for processing the images.\\r\\n\\r\\n![img.png](https://github.com/haesleinhuepf/napari-assistant/raw/main/docs/napari-assistant-screenshot.png)\\r\\n\\r\\nWhile setting up your workflow, you can at any point select a layer from the layer list (1) and change the parameters of\\r\\nthe corresponding operation (2). The layer will update when you change parameters and also all subsequent operations. \\r\\nYou can also vary which operation is applied to the image (3). Also make sure the right input image layer is selected (4).\\r\\n\\r\\n![img.png](https://github.com/haesleinhuepf/napari-assistant/raw/main/docs/design_workflows.png)\\r\\n\\r\\n### Saving and loading workflows\\r\\n\\r\\nYou can also save and load workflows to disk. \\r\\n\\r\\n![img.png](https://github.com/haesleinhuepf/napari-assistant/raw/main/docs/save_and_load.png)\\r\\n\\r\\nAfter loading a workflow, make sure that the right input images are selected.\\r\\n\\r\\n### Code generation\\r\\n\\r\\nThe napari-assistant allows exporting the given workflow as Python script and Jupyter Notebook. \\r\\n\\r\\n![img.png](https://github.com/haesleinhuepf/napari-assistant/raw/main/docs/code_generator.png)\\r\\n\\r\\nFurthermore, if you have the [napari-script-editor](https://www.napari-hub.org/plugins/napari-script-editor) installed,\\r\\nyou can also send the current workflow as code to the script editor from the same menu.\\r\\n\\r\\n![img.png](https://github.com/haesleinhuepf/napari-assistant/raw/main/docs/napari_script_editor.png)\\r\\n\\r\\n### Plugin generation\\r\\n\\r\\nThere is also a Napari plugin generator available. Check out [its documentation](https://github.com/haesleinhuepf/napari-assistant-plugin-generator) to learn how napari-assistant compatible plugins can be generated directly from within the assistant.\\r\\n\\r\\n## Installation\\r\\n\\r\\nIt is recommended to install the napari-assistant via one of the plugins that use it as graphical user interface.\\r\\nYou find a complete list of plugins that use the assistant [on the napari-hub](https://www.napari-hub.org/?search=napari-assistant&sort=relevance).\\r\\nMultiple of these plugins come bundled when installing [devbio-napari](https://www.napari-hub.org/plugins/devbio-napari).\\r\\n\\r\\n## For developers\\r\\n\\r\\nIf you want to make your napari-plugin accessible from the napari-assistant, consider programming functions with a simple \\r\\ninterface that consume images, labels, integers, floats and strings. Annotate input and return types, e.g. like this:\\r\\n```python\\r\\ndef example_function_widget(image: \\\"napari.types.ImageData\\\") -> \\\"napari.types.LabelsData\\\":\\r\\n    from skimage.filters import threshold_otsu\\r\\n    binary_image = image > threshold_otsu(image)\\r\\n\\r\\n    from skimage.measure import label\\r\\n    return label(binary_image)\\r\\n```\\r\\n\\r\\nFurthermore, please add your function to the napari.yaml which uses [npe2](https://github.com/napari/npe2):\\r\\n```\\r\\nname: napari-npe2-test\\r\\ndisplay_name: napari-npe2-test\\r\\ncontributions:\\r\\n  commands: \\r\\n    - id: napari-npe2-test.make_magic_widget\\r\\n      python_name: napari_npe2_test._widget:example_magic_widget\\r\\n      title: Make example magic widget\\r\\n  widgets:\\r\\n    - command: napari-npe2-test.make_magic_widget\\r\\n      display_name: Segmentation / labeling > Otsu Labeling (nnpe2t)\\r\\n```\\r\\n\\r\\nTo put it in the right button within the napari-assistant, please use one of the following prefixes for the `display_name`:\\r\\n* `Filtering / noise removal > `\\r\\n* `Filtering / background removal > `\\r\\n* `Filtering > `\\r\\n* `Image math > `\\r\\n* `Transform > `\\r\\n* `Projection > `\\r\\n* `Segmentation / binarization > `\\r\\n* `Segmentation / labeling > `\\r\\n* `Segmentation post-processing > `\\r\\n* `Measurement > `\\r\\n* `Label neighbor filters > `\\r\\n* `Label filters > `\\r\\n* `Visualization > `\\r\\n\\r\\nYou find a fully functional example [here](https://github.com/haesleinhuepf/napari-npe2-test).\\r\\n\\r\\nLast but not least, to make your napari-plugin is listed in the napari-hub when searching for \\\"napari-assistant\\\", make sure\\r\\nyou mention it in your `readme`.\\r\\n\\r\\n## Feedback welcome!\\r\\n\\r\\nThe napari-assistant is developed in the open because we believe in the open source community. Feel free to drop feedback as [github issue](https://github.com/haesleinhuepf/napari-assistant/issues) or via [image.sc](https://image.sc)\\r\\n\\r\\n## Contributing\\r\\n\\r\\nContributions are very welcome. Please ensure\\r\\nthe test coverage at least stays the same before you submit a pull request.\\r\\n\\r\\n## License\\r\\n\\r\\nDistributed under the terms of the [BSD-3] license,\\r\\n\\\"napari-assistant\\\" is free and open source software\\r\\n\\r\\n## Acknowledgements\\r\\nThis project was supported by the Deutsche Forschungsgemeinschaft under Germany’s Excellence Strategy – EXC2068 - Cluster of Excellence \\\"Physics of Life\\\" of TU Dresden. \\r\\nThis project has been made possible in part by grant number [2021-240341 (Napari plugin accelerator grant)](https://chanzuckerberg.com/science/programs-resources/imaging/napari/improving-image-processing/) from the Chan Zuckerberg Initiative DAF, an advised fund of the Silicon Valley Community Foundation.\\r\\n\\r\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\r\\n\\r\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-assistant\\n\\n\\n\\n\\n\\n\\n\\n\\nThe napari-assistant is a napari meta-plugin for building image processing workflows. \\nUsage\\nAfter installing one or more napari plugins that use the napari-assistant as user interface, you can start it from the \\nmenu Tools > Utilities > Assistant (na) or run naparia from the command line. \\nBy clicking on the buttons in the assistant, you can setup a workflow for processing the images.\\n\\nWhile setting up your workflow, you can at any point select a layer from the layer list (1) and change the parameters of\\nthe corresponding operation (2). The layer will update when you change parameters and also all subsequent operations. \\nYou can also vary which operation is applied to the image (3). Also make sure the right input image layer is selected (4).\\n\\nSaving and loading workflows\\nYou can also save and load workflows to disk. \\n\\nAfter loading a workflow, make sure that the right input images are selected.\\nCode generation\\nThe napari-assistant allows exporting the given workflow as Python script and Jupyter Notebook. \\n\\nFurthermore, if you have the napari-script-editor installed,\\nyou can also send the current workflow as code to the script editor from the same menu.\\n\\nPlugin generation\\nThere is also a Napari plugin generator available. Check out its documentation to learn how napari-assistant compatible plugins can be generated directly from within the assistant.\\nInstallation\\nIt is recommended to install the napari-assistant via one of the plugins that use it as graphical user interface.\\nYou find a complete list of plugins that use the assistant on the napari-hub.\\nMultiple of these plugins come bundled when installing devbio-napari.\\nFor developers\\nIf you want to make your napari-plugin accessible from the napari-assistant, consider programming functions with a simple \\ninterface that consume images, labels, integers, floats and strings. Annotate input and return types, e.g. like this:\\n```python\\ndef example_function_widget(image: \\\"napari.types.ImageData\\\") -> \\\"napari.types.LabelsData\\\":\\n    from skimage.filters import threshold_otsu\\n    binary_image = image > threshold_otsu(image)\\nfrom skimage.measure import label\\nreturn label(binary_image)\\n\\n```\\nFurthermore, please add your function to the napari.yaml which uses npe2:\\nname: napari-npe2-test\\ndisplay_name: napari-npe2-test\\ncontributions:\\n  commands: \\n    - id: napari-npe2-test.make_magic_widget\\n      python_name: napari_npe2_test._widget:example_magic_widget\\n      title: Make example magic widget\\n  widgets:\\n    - command: napari-npe2-test.make_magic_widget\\n      display_name: Segmentation / labeling > Otsu Labeling (nnpe2t)\\nTo put it in the right button within the napari-assistant, please use one of the following prefixes for the display_name:\\n* Filtering / noise removal >\\n* Filtering / background removal >\\n* Filtering >\\n* Image math >\\n* Transform >\\n* Projection >\\n* Segmentation / binarization >\\n* Segmentation / labeling >\\n* Segmentation post-processing >\\n* Measurement >\\n* Label neighbor filters >\\n* Label filters >\\n* Visualization >\\nYou find a fully functional example here.\\nLast but not least, to make your napari-plugin is listed in the napari-hub when searching for \\\"napari-assistant\\\", make sure\\nyou mention it in your readme.\\nFeedback welcome!\\nThe napari-assistant is developed in the open because we believe in the open source community. Feel free to drop feedback as github issue or via image.sc\\nContributing\\nContributions are very welcome. Please ensure\\nthe test coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-assistant\\\" is free and open source software\\nAcknowledgements\\nThis project was supported by the Deutsche Forschungsgemeinschaft under Germany’s Excellence Strategy – EXC2068 - Cluster of Excellence \\\"Physics of Life\\\" of TU Dresden. \\nThis project has been made possible in part by grant number 2021-240341 (Napari plugin accelerator grant) from the Chan Zuckerberg Initiative DAF, an advised fund of the Silicon Valley Community Foundation.\",\"development_status\":[],\"display_name\":\"napari-assistant\",\"documentation\":\"https://github.com/haesleinhuepf/napari-assistant/\",\"first_released\":\"2022-03-05T08:51:21.134774Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-assistant\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/haesleinhuepf/napari-assistant/\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-12-24T21:08:37.108920Z\",\"report_issues\":\"https://github.com/haesleinhuepf/napari-assistant/issues\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"toolz\",\"napari (>=0.4.14)\",\"magicgui\",\"numpy (!=1.19.4)\",\"pyperclip\",\"loguru\",\"jupytext\",\"jupyter\",\"pandas\",\"napari-time-slicer (>=0.4.8)\",\"napari-workflows (>=0.2.4)\"],\"summary\":\"A pocket calculator like interface to image processing in napari\",\"support\":\"https://forum.image.sc/\",\"twitter\":\"\",\"version\":\"0.4.4\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"jru@stowers.org\",\"name\":\"Jay Unruh\"}],\"code_repository\":\"https://github.com/jayunruh/napari-IP-workflow\",\"conda\":[],\"description\":\"# napari-IP-workflow\\n\\n[![License](https://img.shields.io/pypi/l/napari-IP-workflow.svg?color=green)](https://github.com/jayunruh/napari-IP-workflow/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-IP-workflow.svg?color=green)](https://pypi.org/project/napari-IP-workflow)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-IP-workflow.svg?color=green)](https://python.org)\\n[![tests](https://github.com/jayunruh/napari-IP-workflow/workflows/tests/badge.svg)](https://github.com/jayunruh/napari-IP-workflow/actions)\\n[![codecov](https://codecov.io/gh/jayunruh/napari-IP-workflow/branch/main/graph/badge.svg)](https://codecov.io/gh/jayunruh/napari-IP-workflow)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-IP-workflow)](https://napari-hub.org/plugins/napari-IP-workflow)\\n\\nA plugin to do image preprocessing, segmentation, and measurements on other images.  The typical workflow is background subtraction followed by smoothing, thresholding, and size filtering.  This is typically done on nuclear stained images.  Segmentation can optionally be followed by circular label expansion to find cytoplasmic signals. The labeled signals are then measured on background subtracted images.\\n\\n##General organization\\n\\nThe code is separated into non-interactive processing functions (ipfunctions module) and an interactive widget (segwidget module).  Please look at the code on github for examples: [Github](https://github.com/jayunruh/napari-ip-workflow). The expected workflow is from jupyter notebooks with an interactive workflow shown in src/napari-ip-workflow/_tests/standard_segementation_widget.ipynb and a non-interactive workflow shown in src/napari-ip-workflow/_tests/standard_segmentation.ipynb.  The expectation is to find the best parameters in an interactive way (ideally testing on several images) and then use the non-interactive workflow to batch through more data sets.  All image processing algorithms are in the ipfunctions module and the segwidget module has the Napari widget code.  Below I describe the strategies that are utilized in the workflow.\\n\\n## Background subtraction strategy\\n\\nAutomated background subtraction (e.g. as in Fiji) is often accomplished with a low pass filter-style approach like rolling ball background subtraction.  This approach fails as feature sizes grow larger or measurements approach the background.  Manual selection of the background is more robust but introduces human variability and isn't compatible with high throughput analyses.  Our approach is to attempt to automate regional selection of background as follows.  First the image is smoothed with a Gaussian filter to eliminate background noise.  Next, minimum values are subtracted from each channel and the resulting images are summed.   Next, a uniform 2D boxcar smoothing is applied to the image--background level regions in the resulting image are at least the boxcar size distance away from foreground objects.  The minimum pixel in that resulting image is a good approximation for the background region of the image.  A thick border is specified to avoid lower intensity regions at the border of the image.  This algorithm is implemented in the ipfunctions module as findBackground.  Once the background region is found, it can be measured with measureCirc.\\n\\n## Segmentation and thresholding strategy\\n\\nThere are many automated thresholding algorithms available via python and, by extension, Napari.  This program uses a very simplistic but powerful method.  Most segmentable images consist of foreground and background components.  In imaging, the foreground is more noisy than the background.  Ideally a smoothed background subtracted image will have a maximum intensity that represents the foreground well and a background intensity of 0.  In that case, the threshold level can be easily defined as a fraction of that smoothed maximum intensity.  A threshold fraction of 0.25 tends to work well but lower values may be more robust if background is fairly smooth and the foreground is noisier.  In some cases the foreground has anomalous high values that will skew the estimation.  In that case it may be better to estimate the foreground as e.g. the 99th percentile of the intensity.  In some cases it may be useful to use the average intensity as a reference point instead or use the raw intensity value (statistic is Identity).  Those last options tend to be less robust and it may be desired in those cases to use some of the more complex autothresholding methods.  After thresholding, objects on the image edge are eliminated and objects are filtered according to size.  The minimum area can easy remove small debris that can contaminate a measurement.  The maximum area can be used for large contaminants or poorly segmented clusters of cells that might not be desired in the analysis.\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-IP-workflow` via [pip]:\\n\\n    pip install napari-IP-workflow\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/jayunruh/napari-IP-workflow.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [GNU GPL v3.0] license,\\n\\\"napari-IP-workflow\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/jayunruh/napari-IP-workflow/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-IP-workflow\\n\\n\\n\\n\\n\\n\\nA plugin to do image preprocessing, segmentation, and measurements on other images.  The typical workflow is background subtraction followed by smoothing, thresholding, and size filtering.  This is typically done on nuclear stained images.  Segmentation can optionally be followed by circular label expansion to find cytoplasmic signals. The labeled signals are then measured on background subtracted images.\\nGeneral organization\\nThe code is separated into non-interactive processing functions (ipfunctions module) and an interactive widget (segwidget module).  Please look at the code on github for examples: Github. The expected workflow is from jupyter notebooks with an interactive workflow shown in src/napari-ip-workflow/_tests/standard_segementation_widget.ipynb and a non-interactive workflow shown in src/napari-ip-workflow/_tests/standard_segmentation.ipynb.  The expectation is to find the best parameters in an interactive way (ideally testing on several images) and then use the non-interactive workflow to batch through more data sets.  All image processing algorithms are in the ipfunctions module and the segwidget module has the Napari widget code.  Below I describe the strategies that are utilized in the workflow.\\nBackground subtraction strategy\\nAutomated background subtraction (e.g. as in Fiji) is often accomplished with a low pass filter-style approach like rolling ball background subtraction.  This approach fails as feature sizes grow larger or measurements approach the background.  Manual selection of the background is more robust but introduces human variability and isn't compatible with high throughput analyses.  Our approach is to attempt to automate regional selection of background as follows.  First the image is smoothed with a Gaussian filter to eliminate background noise.  Next, minimum values are subtracted from each channel and the resulting images are summed.   Next, a uniform 2D boxcar smoothing is applied to the image--background level regions in the resulting image are at least the boxcar size distance away from foreground objects.  The minimum pixel in that resulting image is a good approximation for the background region of the image.  A thick border is specified to avoid lower intensity regions at the border of the image.  This algorithm is implemented in the ipfunctions module as findBackground.  Once the background region is found, it can be measured with measureCirc.\\nSegmentation and thresholding strategy\\nThere are many automated thresholding algorithms available via python and, by extension, Napari.  This program uses a very simplistic but powerful method.  Most segmentable images consist of foreground and background components.  In imaging, the foreground is more noisy than the background.  Ideally a smoothed background subtracted image will have a maximum intensity that represents the foreground well and a background intensity of 0.  In that case, the threshold level can be easily defined as a fraction of that smoothed maximum intensity.  A threshold fraction of 0.25 tends to work well but lower values may be more robust if background is fairly smooth and the foreground is noisier.  In some cases the foreground has anomalous high values that will skew the estimation.  In that case it may be better to estimate the foreground as e.g. the 99th percentile of the intensity.  In some cases it may be useful to use the average intensity as a reference point instead or use the raw intensity value (statistic is Identity).  Those last options tend to be less robust and it may be desired in those cases to use some of the more complex autothresholding methods.  After thresholding, objects on the image edge are eliminated and objects are filtered according to size.  The minimum area can easy remove small debris that can contaminate a measurement.  The maximum area can be used for large contaminants or poorly segmented clusters of cells that might not be desired in the analysis.\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-IP-workflow via pip:\\npip install napari-IP-workflow\\n\\nTo install latest development version :\\npip install git+https://github.com/jayunruh/napari-IP-workflow.git\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the GNU GPL v3.0 license,\\n\\\"napari-IP-workflow\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"Image Processing Workflow\",\"documentation\":\"https://github.com/jayunruh/napari-IP-workflow#README.md\",\"first_released\":\"2022-05-17T20:25:56.447707Z\",\"license\":\"GPL-3.0\",\"name\":\"napari-IP-workflow\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/jayunruh/napari-IP-workflow\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-05-18T16:41:45.901404Z\",\"report_issues\":\"https://github.com/jayunruh/napari-IP-workflow/issues\",\"requirements\":[\"numpy\",\"magicgui\",\"qtpy\",\"pandas\",\"skimage\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\"],\"summary\":\"A plugin to do image preprocessing, segmentation, and measurements on other images.\",\"support\":\"https://github.com/jayunruh/napari-IP-workflow/issues\",\"twitter\":\"\",\"version\":\"0.0.3\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Robert Haase\"}],\"code_repository\":\"https://github.com/haesleinhuepf/devbio-napari\",\"description\":\"# devbio-napari\\r\\n\\r\\n[![License](https://img.shields.io/pypi/l/devbio-napari.svg?color=green)](https://github.com/haesleinhuepf/devbio-napari/raw/master/LICENSE)\\r\\n[![PyPI](https://img.shields.io/pypi/v/devbio-napari.svg?color=green)](https://pypi.org/project/devbio-napari)\\r\\n[![Python Version](https://img.shields.io/pypi/pyversions/devbio-napari.svg?color=green)](https://python.org)\\r\\n[![tests](https://github.com/haesleinhuepf/devbio-napari/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-plot-profile/actions)\\r\\n[![codecov](https://codecov.io/gh/haesleinhuepf/devbio-napari/branch/master/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/devbio-napari)\\r\\n[![Development Status](https://img.shields.io/pypi/status/devbio-napari.svg)](https://en.wikipedia.org/wiki/Software_release_life_cycle#Alpha)\\r\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/devbio-napari)](https://napari-hub.org/plugins/devbio-napari)\\r\\n\\r\\n\\r\\nA bundle of napari plugins useful for 3D+t image processing and analysis for studying developmental biology.\\r\\n\\r\\n* [accelerated-pixel-and-object-classification](https://www.napari-hub.org/plugins/napari-accelerated-pixel-and-object-classification)\\r\\n  * Instance segmentation\\r\\n  * Semantic segmentation\\r\\n  * Object classification\\r\\n  * Random Forest Classifier training\\r\\n* [animation](https://www.napari-hub.org/plugins/napari-animation) \\r\\n  * Visualization\\r\\n* [blob-detection](https://www.napari-hub.org/plugins/napari-blob-detection)\\r\\n  * Detection\\r\\n* [brightness-contrast](https://www.napari-hub.org/plugins/napari-brightness-contrast)\\r\\n  * Visualization\\r\\n* [clusters-plotter](https://www.napari-hub.org/plugins/napari-clusters-plotter)\\r\\n  * Visualization\\r\\n  * Plotting\\r\\n  * Semantic object segmentation\\r\\n  * Dimensionality reduction\\r\\n  * Unsupervised machine learning\\r\\n* [crop](https://www.napari-hub.org/plugins/napari-crop)\\r\\n  * Transformation\\r\\n* [curtain](https://www.napari-hub.org/plugins/napari-curtain)\\r\\n  * Visualization \\r\\n* [czifile2](https://www.napari-hub.org/plugins/napari-czifile2)\\r\\n  * File input/output\\r\\n* [folder-browser](https://www.napari-hub.org/plugins/napari-folder-browser)\\r\\n  * File input/output\\r\\n* [layer-details-display](https://www.napari-hub.org/plugins/napari-layer-details-display)\\r\\n  * Visualization\\r\\n* [mouse-controls](https://www.napari-hub.org/plugins/napari-mouse-controls)\\r\\n  * Interaction\\r\\n* [PlatyMatch](https://www.napari-hub.org/plugins/PlatyMatch)\\r\\n  * Image registration\\r\\n* [plot-profile](https://www.napari-hub.org/plugins/napari-plot-profile)\\r\\n  * Visualization\\r\\n  * Quantification\\r\\n* [plugin-search](https://www.napari-hub.org/plugins/napari-plugin-search)\\r\\n  * Interaction\\r\\n* [pyclesperanto-assistant](https://www.napari-hub.org/plugins/napari-pyclesperanto-assistant)\\r\\n  * Filtering\\r\\n  * Instance segmentation\\r\\n  * Semantic segmentation\\r\\n  * Quantification\\r\\n* [pystackreg](https://www.napari-hub.org/plugins/napari-pystackreg)\\r\\n  * Image registration\\r\\n  * Motion correction\\r\\n* [RedLionfish](https://www.napari-hub.org/plugins/RedLionfish)\\r\\n  * Deconvolution\\r\\n  * Processing\\r\\n* [roi](https://www.napari-hub.org/plugins/napari-roi)\\r\\n  * Manual segmentation\\r\\n* [segment-blobs-and-things-with-membranes](https://www.napari-hub.org/plugins/napari-segment-blobs-and-things-with-membranes)\\r\\n  * Filtering\\r\\n  * Instance segmentation\\r\\n  * Semantic segmentation\\r\\n* [simpleitk-image-processing](https://www.napari-hub.org/plugins/napari-simpleitk-image-processing)\\r\\n  * Filtering\\r\\n  * Instance segmentation\\r\\n  * Semantic segmentation\\r\\n  * Quantification\\r\\n* [skimage-regionprops](https://www.napari-hub.org/plugins/napari-skimage-regionprops)\\r\\n  * Quantification\\r\\n* [tabu](https://www.napari-hub.org/plugins/napari-tabu)\\r\\n  * Interaction\\r\\n* [the-segmentation-game](https://www.napari-hub.org/plugins/the-segmentation-game)\\r\\n  * Quantification\\r\\n  * Segmentation quality assurance\\r\\n* [workflow-inspector](https://www.napari-hub.org/plugins/napari-workflow-inspector)\\r\\n  * Visualization\\r\\n* [workflow-optimizer](https://www.napari-hub.org/plugins/napari-workflow-optimizer)\\r\\n  * Interaction\\r\\n  * Optimization\\r\\n\\r\\n----------------------------------\\r\\n\\r\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\\r\\n\\r\\n## Installation\\r\\n\\r\\nYou can install `devbio-napari` via conda/mamba. If you have never used conda before, please [read this guide first](https://biapol.github.io/blog/johannes_mueller/anaconda_getting_started/).  \\r\\nStart by installing mamba in your base environment:\\r\\n\\r\\n```\\r\\nconda install mamba -c conda-forge\\r\\n```\\r\\n\\r\\nAfterwards, create an environment using mamba.\\r\\n\\r\\n```\\r\\nmamba create --name devbio-napari-env python=3.9 devbio-napari -c conda-forge\\r\\n```\\r\\n\\r\\nAfterwards, activate the environment like this:\\r\\n    \\r\\n    conda activate devbio-napari-env\\r\\n\\r\\nAfterwards, run this command from the command line\\r\\n\\r\\n```\\r\\nnaparia\\r\\n```\\r\\n\\r\\nThis window should open. It shows the [Assistant](https://www.napari-hub.org/plugins/napari-assistant) graphical user interface. \\r\\nRead more about how to use it in its [documentation](https://www.napari-hub.org/plugins/napari-assistant).\\r\\n\\r\\n![img.png](https://github.com/haesleinhuepf/devbio-napari/raw/master/docs/screenshot.png)\\r\\n\\r\\n## Troubleshooting: Graphics cards drivers\\r\\n\\r\\nIn case error messages contains \\\"ImportError: DLL load failed while importing cl: The specified procedure could not be found\\\" [see also](https://github.com/clEsperanto/pyclesperanto_prototype/issues/55) or \\\"\\\"clGetPlatformIDs failed: PLATFORM_NOT_FOUND_KHR\\\", please install recent drivers for your graphics card and/or OpenCL device. Select the right driver source depending on your hardware from this list:\\r\\n\\r\\n* [AMD drivers](https://www.amd.com/en/support)\\r\\n* [NVidia drivers](https://www.nvidia.com/download/index.aspx)\\r\\n* [Intel CPU OpenCL drivers](https://www.intel.com/content/www/us/en/developer/articles/tool/opencl-drivers.html#latest_CPU_runtime)\\r\\n* [Microsoft Windows OpenCL support](https://www.microsoft.com/en-us/p/opencl-and-opengl-compatibility-pack/9nqpsl29bfff)\\r\\n\\r\\nSometimes, mac-users need to install this:\\r\\n\\r\\n    conda install -c conda-forge ocl_icd_wrapper_apple\\r\\n\\r\\nSometimes, linux users need to install this:\\r\\n\\r\\n    conda install -c conda-forge ocl-icd-system\\r\\n\\r\\n\\r\\nIn case installation didn't work in the first attempt, you may have to call this command line to reset the napari configuration:\\r\\n\\r\\n```\\r\\nnapari --reset\\r\\n```\\r\\n\\r\\n## Contributing\\r\\n\\r\\nContributions are very welcome. \\r\\nIf you want to [suggest a new napari plugin](https://github.com/haesleinhuepf/devbio-napari/pulls) to become part of this distribution, please make sure it interoperates nicely with the other plugins. \\r\\nFor example, if the plugin you suggest provided cell segmentation algorithms, please check if the resulting segmented cells can be analysed using napari-skimage-regionprops.\\r\\nFurthermore, please make sure the README of the plugin you are proposing comes with user documentation, e.g. a step-by-step guide with screenshots explaining what users can do with the plugin and how to use it. \\r\\nIt is recommended to provide example data as well so that end-users can try out the plugin under conditions it was developed for.\\r\\n\\r\\n## License\\r\\n\\r\\nDistributed under the terms of the [BSD-3] license,\\r\\n\\\"devbio-napari\\\" is free and open source software\\r\\n\\r\\n## Issues\\r\\n\\r\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\r\\n\\r\\n[napari]: https://github.com/napari/napari\\r\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\r\\n[@napari]: https://github.com/napari\\r\\n[MIT]: http://opensource.org/licenses/MIT\\r\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\r\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\r\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\r\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\r\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\r\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\r\\n[file an issue]: https://github.com/haesleinhuepf/devbio/issues\\r\\n[napari]: https://github.com/napari/napari\\r\\n[tox]: https://tox.readthedocs.io/en/latest/\\r\\n[pip]: https://pypi.org/project/pip/\\r\\n[PyPI]: https://pypi.org/\\r\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"devbio-napari\\n\\n\\n\\n\\n\\n\\n\\nA bundle of napari plugins useful for 3D+t image processing and analysis for studying developmental biology.\\n\\naccelerated-pixel-and-object-classification\\nInstance segmentation\\nSemantic segmentation\\nObject classification\\nRandom Forest Classifier training\\nanimation \\nVisualization\\nblob-detection\\nDetection\\nbrightness-contrast\\nVisualization\\nclusters-plotter\\nVisualization\\nPlotting\\nSemantic object segmentation\\nDimensionality reduction\\nUnsupervised machine learning\\ncrop\\nTransformation\\ncurtain\\nVisualization \\nczifile2\\nFile input/output\\nfolder-browser\\nFile input/output\\nlayer-details-display\\nVisualization\\nmouse-controls\\nInteraction\\nPlatyMatch\\nImage registration\\nplot-profile\\nVisualization\\nQuantification\\nplugin-search\\nInteraction\\npyclesperanto-assistant\\nFiltering\\nInstance segmentation\\nSemantic segmentation\\nQuantification\\npystackreg\\nImage registration\\nMotion correction\\nRedLionfish\\nDeconvolution\\nProcessing\\nroi\\nManual segmentation\\nsegment-blobs-and-things-with-membranes\\nFiltering\\nInstance segmentation\\nSemantic segmentation\\nsimpleitk-image-processing\\nFiltering\\nInstance segmentation\\nSemantic segmentation\\nQuantification\\nskimage-regionprops\\nQuantification\\ntabu\\nInteraction\\nthe-segmentation-game\\nQuantification\\nSegmentation quality assurance\\nworkflow-inspector\\nVisualization\\nworkflow-optimizer\\nInteraction\\nOptimization\\n\\n\\nThis napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.\\nInstallation\\nYou can install devbio-napari via conda/mamba. If you have never used conda before, please read this guide first.\\nStart by installing mamba in your base environment:\\nconda install mamba -c conda-forge\\nAfterwards, create an environment using mamba.\\nmamba create --name devbio-napari-env python=3.9 devbio-napari -c conda-forge\\nAfterwards, activate the environment like this:\\nconda activate devbio-napari-env\\n\\nAfterwards, run this command from the command line\\nnaparia\\nThis window should open. It shows the Assistant graphical user interface. \\nRead more about how to use it in its documentation.\\n\\nTroubleshooting: Graphics cards drivers\\nIn case error messages contains \\\"ImportError: DLL load failed while importing cl: The specified procedure could not be found\\\" see also or \\\"\\\"clGetPlatformIDs failed: PLATFORM_NOT_FOUND_KHR\\\", please install recent drivers for your graphics card and/or OpenCL device. Select the right driver source depending on your hardware from this list:\\n\\nAMD drivers\\nNVidia drivers\\nIntel CPU OpenCL drivers\\nMicrosoft Windows OpenCL support\\n\\nSometimes, mac-users need to install this:\\nconda install -c conda-forge ocl_icd_wrapper_apple\\n\\nSometimes, linux users need to install this:\\nconda install -c conda-forge ocl-icd-system\\n\\nIn case installation didn't work in the first attempt, you may have to call this command line to reset the napari configuration:\\nnapari --reset\\nContributing\\nContributions are very welcome. \\nIf you want to suggest a new napari plugin to become part of this distribution, please make sure it interoperates nicely with the other plugins. \\nFor example, if the plugin you suggest provided cell segmentation algorithms, please check if the resulting segmented cells can be analysed using napari-skimage-regionprops.\\nFurthermore, please make sure the README of the plugin you are proposing comes with user documentation, e.g. a step-by-step guide with screenshots explaining what users can do with the plugin and how to use it. \\nIt is recommended to provide example data as well so that end-users can try out the plugin under conditions it was developed for.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"devbio-napari\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 3 - Alpha\"],\"display_name\":\"devbio-napari\",\"documentation\":\"https://github.com/haesleinhuepf/devbio-napari#README.md\",\"first_released\":\"2021-06-08T07:39:43.455373Z\",\"license\":\"BSD-3-Clause\",\"name\":\"devbio-napari\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[],\"project_site\":\"https://github.com/haesleinhuepf/devbio-napari\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-12-11T11:01:07.549032Z\",\"report_issues\":\"https://github.com/haesleinhuepf/devbio-napari/issues\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy (>=1.21.4)\",\"napari-pyclesperanto-assistant\",\"napari-skimage-regionprops\",\"napari-animation\",\"PlatyMatch\",\"napari-plot-profile\",\"napari-accelerated-pixel-and-object-classification\",\"napari-brightness-contrast\",\"napari-plugin-search\",\"napari-segment-blobs-and-things-with-membranes\",\"napari-simpleitk-image-processing\",\"napari-folder-browser\",\"napari-crop\",\"napari-clusters-plotter\",\"napari-tabu\",\"napari-workflow-optimizer\",\"napari-workflow-inspector\",\"napari-curtain\",\"napari-layer-details-display\",\"napari\",\"vispy\",\"napari-mouse-controls\",\"the-segmentation-game\",\"napari-blob-detection\",\"jupyterlab\",\"napari-czifile2\",\"napari-roi\",\"pydantic (!=1.10.0)\",\"napari-pystackreg\",\"imageio (!=2.22.1)\",\"redlionfish\",\"jupyter-server (<2.0.0)\"],\"summary\":\"A bundle of napari plugins useful for 3D+t image processing and analysis for studying developmental biology.\",\"support\":\"https://github.com/haesleinhuepf/devbio-napari/issues\",\"twitter\":\"\",\"version\":\"0.8.1\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"lefevreedg@gmail.com\",\"name\":\"Edgar Lefevre\"}],\"code_repository\":\"https://github.com/EdgarLefevre/napari-deepmeta\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-deepmeta\"}],\"description\":\"# napari-deepmeta\\n\\n[![License MIT](https://img.shields.io/pypi/l/napari-deepmeta.svg?color=green)](https://github.com/EdgarLefevre/napari-deepmeta/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-deepmeta.svg?color=green)](https://pypi.org/project/napari-deepmeta)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-deepmeta.svg?color=green)](https://python.org)\\n[![tests](https://github.com/EdgarLefevre/napari-deepmeta/workflows/tests/badge.svg)](https://github.com/EdgarLefevre/napari-deepmeta/actions)\\n[![codecov](https://codecov.io/gh/EdgarLefevre/napari-deepmeta/branch/main/graph/badge.svg)](https://codecov.io/gh/EdgarLefevre/napari-deepmeta)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-deepmeta)](https://napari-hub.org/plugins/napari-deepmeta)\\n\\nMice lungs and metastases segmentation tool.\\nThis tool is a demo tool for DeepMeta network.\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n## Installation\\n\\nYou can install `napari-deepmeta` via [pip]:\\n\\n    pip install napari-deepmeta\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/EdgarLefevre/napari-deepmeta.git\\n\\n\\n## Usage\\n\\nThis plugin is designed to process your mouse MRI images with our dataset. It comes with a demo, including one of our\\ntest images.\\n\\nBy opening the deepmeta demo plugin, you will see an interface with one unique button, by clicking on it, it will load an image,\\nrun prediction and then draw the masks contours on each slice.\\n\\nIf you open the deepmeta plugin, you will see an interface with one button and 3 checkboxes.\\nBy checking the checkboxes, you add steps to the pipeline (enhance contrast, do postprocessing, segment metastases).\\nOnce everything is setup, just click on the button and wait (the waiting time depends on your computer performance.)\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [MIT] license,\\n\\\"napari-deepmeta\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/EdgarLefevre/napari-deepmeta/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-deepmeta\\n\\n\\n\\n\\n\\n\\nMice lungs and metastases segmentation tool.\\nThis tool is a demo tool for DeepMeta network.\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\nInstallation\\nYou can install napari-deepmeta via pip:\\npip install napari-deepmeta\\n\\nTo install latest development version :\\npip install git+https://github.com/EdgarLefevre/napari-deepmeta.git\\n\\nUsage\\nThis plugin is designed to process your mouse MRI images with our dataset. It comes with a demo, including one of our\\ntest images.\\nBy opening the deepmeta demo plugin, you will see an interface with one unique button, by clicking on it, it will load an image,\\nrun prediction and then draw the masks contours on each slice.\\nIf you open the deepmeta plugin, you will see an interface with one button and 3 checkboxes.\\nBy checking the checkboxes, you add steps to the pipeline (enhance contrast, do postprocessing, segment metastases).\\nOnce everything is setup, just click on the button and wait (the waiting time depends on your computer performance.)\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the MIT license,\\n\\\"napari-deepmeta\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari DeepMeta\",\"documentation\":\"https://github.com/EdgarLefevre/napari-deepmeta#README.md\",\"first_released\":\"2021-06-02T14:41:03.386781Z\",\"license\":\"MIT\",\"name\":\"napari-deepmeta\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/EdgarLefevre/napari-deepmeta\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-07-07T08:55:23.291200Z\",\"report_issues\":\"https://github.com/EdgarLefevre/napari-deepmeta/issues\",\"requirements\":[\"connected-components-3d\",\"magicgui\",\"napari\",\"numpy\",\"opencv-python\",\"qtpy\",\"scikit-image\",\"torch\",\"connected-components-3d ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"opencv-python ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"tox ; extra == 'testing'\"],\"summary\":\"Mice lungs and metastases segmentation tool.\",\"support\":\"https://github.com/EdgarLefevre/napari-deepmeta/issues\",\"twitter\":\"\",\"version\":\"2.1\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Volker Baecker\"}],\"code_repository\":\"https://github.com/MontpellierRessourcesImagerie/napari-J\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-j\"}],\"description\":\"# napari-J\\n\\n[![License](https://img.shields.io/pypi/l/napari-J.svg?color=green)](https://github.com/MontpellierRessourcesImagerie/napari-J/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-J.svg?color=green)](https://pypi.org/project/napari-J)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-J.svg?color=green)](https://python.org)\\n[![tests](https://github.com/MontpellierRessourcesImagerie/napari-J/workflows/tests/badge.svg)](https://github.com/MontpellierRessourcesImagerie/napari-J/actions)\\n[![codecov](https://codecov.io/gh/MontpellierRessourcesImagerie/napari-J/branch/master/graph/badge.svg)](https://codecov.io/gh/MontpellierRessourcesImagerie/napari-J)\\n\\nA plugin to exchange data with FIJI and to use FIJI image analysis from napari.\\nCurrent features are:\\n\\n * get the active image from FIJI\\n * send a screenshot to FIJI\\n * get a set of points from the FIJI results table\\n * filter the points in napari\\n * send the filtered points back to FIJI\\n \\nKnown problems:\\n\\n* Crashes on linux  when the file-dialog is opened. Workaround: Set the option ``Use JFileChooser to open/save`` from the ``Edit>Options>Input/Output`` menu.\\n* 03.05.2022 - For now please use it with napari 0.4.12, there is a vispy bug in 0.4.15 concerning labelled masks\\n* 03.05.2022 - Currently you need to have the range of the quality values for point between 0 and 255, in the new version they can have any range, but we are waiting for the bug in napari 0.4.15 to be fixed to release this. \\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-J` via [pip]:\\n\\n    pip install napari-J\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [MIT] license,\\n\\\"napari-J\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue](https://github.com/MontpellierRessourcesImagerie/napari-J/issues) along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/MontpellierRessourcesImagerie/napari-J/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-J\\n\\n\\n\\n\\n\\nA plugin to exchange data with FIJI and to use FIJI image analysis from napari.\\nCurrent features are:\\n\\nget the active image from FIJI\\nsend a screenshot to FIJI\\nget a set of points from the FIJI results table\\nfilter the points in napari\\nsend the filtered points back to FIJI\\n\\nKnown problems:\\n\\nCrashes on linux  when the file-dialog is opened. Workaround: Set the option Use JFileChooser to open/save from the Edit>Options>Input/Output menu.\\n03.05.2022 - For now please use it with napari 0.4.12, there is a vispy bug in 0.4.15 concerning labelled masks\\n03.05.2022 - Currently you need to have the range of the quality values for point between 0 and 255, in the new version they can have any range, but we are waiting for the bug in napari 0.4.15 to be fixed to release this. \\n\\n\\nThis napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-J via pip:\\npip install napari-J\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the MIT license,\\n\\\"napari-J\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"napari-J\",\"documentation\":\"https://github.com/MontpellierRessourcesImagerie/napari-J#README.md\",\"first_released\":\"2022-01-07T13:39:21.434595Z\",\"license\":\"MIT\",\"name\":\"napari-J\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/MontpellierRessourcesImagerie/napari-J\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-10-16T21:53:27.380502Z\",\"report_issues\":\"https://github.com/MontpellierRessourcesImagerie/napari-J/issues\",\"requirements\":[\"JPype1 (>=1.2.1)\",\"matplotlib\",\"imageio-ffmpeg\",\"matplotlib ; extra == 'testing'\",\"imageio-ffmpeg ; extra == 'testing'\",\"python-matplotlib-qt5 ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\"],\"summary\":\"A plugin to exchange data with FIJI and to use FIJI image analysis from napari\",\"support\":\"https://github.com/MontpellierRessourcesImagerie/napari-J/issues\",\"twitter\":\"\",\"version\":\"0.2.4\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Graham Dellaire\"},{\"name\":\"Robert Haase\"}],\"code_repository\":\"https://github.com/gdellaire/bbii-decon\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"bbii-decon\"}],\"description\":\"# BBii-Decon\\n\\n[![License](https://img.shields.io/pypi/l/bbii-decon.svg?color=green)](https://github.com/gdellaire/bbii-decon/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/bbii-decon.svg?color=green)](https://pypi.org/project/bbii-decon)\\n[![Python Version](https://img.shields.io/pypi/pyversions/bbii-decon.svg?color=green)](https://python.org)\\n[![tests](https://github.com/gdellaire/bbii-decon/workflows/tests/badge.svg)](https://github.com/gdellaire/bbii-decon/actions)\\n[![codecov](https://codecov.io/gh/gdellaire/bbii-decon/branch/main/graph/badge.svg)](https://codecov.io/gh/gdellaire/bbii-decon)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/bbii-decon)](https://napari-hub.org/plugins/bbii-decon)\\n\\nProjected Barzilai-Borwein Image Deconvolution with Infeasible Iterates (BBii-Decon)\\n\\n\\nThe projected Barzilai-Borwein method of image deconvolution utilizing infeasible iterates (BBii-Decon), utilizes Barzilai-Borwein (BB) or projected BB (PBB) method and enforces a nonnegativity constraint, but allows for infeasible iterates between projections. This algorithm (BBii) results in faster convergence than the basic PBB method, while achieving better quality images, with reduced background than the unconstrained BB method (1). \\n\\nThe code represented is based on the original BBii algorithm written in MatLab by Kathleen Fraser and Dirk Arnold, which was ported to python 3.8 by Graham Dellaire, Dirk Arnold and Kathleen Fraser for non-commercial use.\\n\\nThe first implementation shown here is for 2D deconvolution using a known 2D PSF of 256 X 256 pixels, and images of at least 256 pixels in one dimension. One file implements just the deconvolution of a blurred image, while the second file contains a modification of the BBii-Decon algorithm that has a built in heuristic for measuring image reconstruction error relative to a ground truth image. For general 2D deconvolution, either a theoretical 2D PSF (if you know the optical properties of your system) or the central in focus image of a fluorescent bead taken with the same imaging setup (lens, magnification, camera) can produce a suitable PSF.\\n\\n### GPU-acceleration\\n\\nFor most 2D deconvolution, optimal results are obtained with 10 iterations of the algorithm. However, if processing takes too long, acceleration using graphics processing units (GPUs) may make sense, especially for processing larger images with >10 iterations or 3D images. (Note: At this time BBii-Decon is optimized for 2D deconvolution, with a 3D implementation planned in future). \\n\\nThis plugin supports accelerated processing using the [cupy](https://cupy.dev) library. To make use of it, please follow \\n[the instructions](https://docs.cupy.dev/en/stable/install.html#installing-cupy-from-conda-forge) to install cupy. \\nInstallation may look like this:\\n```\\nconda create --name cupy_p38 python=3.8\\nconda activate cupy_p38\\nconda install -c conda-forge cupy cudatoolkit=10.2\\n```\\n\\nIf cupy installation worked out, you will find another checkbox in the user interface. By activating it, processing \\nshould become faster by factor 5-10, depending on processed image data and use GPU hardware.\\n\\n![img.png](https://github.com/gdellaire/BBii-Decon/raw/main/demo/use_GPU_checkbox.png)\\n\\n## Usage - napari\\n\\nYou can use the BBii deconvolution from within napari by clicking the menu `Plugins > bbii-decon > bbii deconvolution`. \\nIn the dialog, select the PSF, the image to process (a) and click on `Run`. After a moment, the deconvolved image (b) \\nwill show up.\\n\\n![img.png](https://github.com/gdellaire/BBii-Decon/raw/main/demo/screenshot_napari.png)\\n\\n## Usage from python\\n\\nYou can also call the function from python. There is a full working example in [this notebook](demo/BBii_Decon_2D_2021.ipynb).\\n\\n```\\nfrom bbii_decon import bbii\\n\\nbbii(PSF, image, number_of_iterations = 15, tau = 1.0e-08, rho = 0.98)\\n```\\n\\n\\n## Citation\\n1) [Kathleen Fraser, Dirk V. Arnold, and Graham Dellaire (2014). Projected Barzilai-Borwein\\nmethod with infeasible iterates for nonnegative least-squares image deblurring. In Proceedings\\nof the Eleventh Conference on Computer and Robot Vision (CRV 2014), Montreal, Canada, pp.\\n189--194.](https://ieeexplore.ieee.org/abstract/document/6816842)\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/plugins/stable/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `bbii-decon` via [pip]:\\n\\n    pip install bbii-decon\\n\\n\\n## Installation for developers\\n\\nClone the github repository:\\n\\n```\\nconda install git\\n\\ngit clone https://github.com/gdellaire/BBii-Decon.git\\n\\ncd BBii-Decon\\n\\npip install -e .\\n```\\n\\n## Deployment to pypi\\n\\nFor deploying the plugin to the python package index (pypi), one needs a [pypi user account](https://pypi.org/account/register/) \\nfirst. For deploying the plugin to pypi, one needs to install some tools:\\n\\n```\\npython -m pip install --user --upgrade setuptools wheel\\npython -m pip install --user --upgrade twine\\n```\\n\\nThe following command allows us to package the souce code as a python wheel. Make sure that the 'dist' and 'build' folders are deleted before doing this:\\n\\n```\\npython setup.py sdist bdist_wheel\\n```\\n\\nThis command ships the just generated to pypi:\\n\\n```\\npython -m twine upload --repository pypi dist/*\\n```\\n\\n[Read more about distributing your python package via pypi](https://realpython.com/pypi-publish-python-package/#publishing-to-pypi).\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"bbii-decon\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/gdellaire/bbii-decon/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"BBii-Decon\\n\\n\\n\\n\\n\\n\\nProjected Barzilai-Borwein Image Deconvolution with Infeasible Iterates (BBii-Decon)\\nThe projected Barzilai-Borwein method of image deconvolution utilizing infeasible iterates (BBii-Decon), utilizes Barzilai-Borwein (BB) or projected BB (PBB) method and enforces a nonnegativity constraint, but allows for infeasible iterates between projections. This algorithm (BBii) results in faster convergence than the basic PBB method, while achieving better quality images, with reduced background than the unconstrained BB method (1). \\nThe code represented is based on the original BBii algorithm written in MatLab by Kathleen Fraser and Dirk Arnold, which was ported to python 3.8 by Graham Dellaire, Dirk Arnold and Kathleen Fraser for non-commercial use.\\nThe first implementation shown here is for 2D deconvolution using a known 2D PSF of 256 X 256 pixels, and images of at least 256 pixels in one dimension. One file implements just the deconvolution of a blurred image, while the second file contains a modification of the BBii-Decon algorithm that has a built in heuristic for measuring image reconstruction error relative to a ground truth image. For general 2D deconvolution, either a theoretical 2D PSF (if you know the optical properties of your system) or the central in focus image of a fluorescent bead taken with the same imaging setup (lens, magnification, camera) can produce a suitable PSF.\\nGPU-acceleration\\nFor most 2D deconvolution, optimal results are obtained with 10 iterations of the algorithm. However, if processing takes too long, acceleration using graphics processing units (GPUs) may make sense, especially for processing larger images with >10 iterations or 3D images. (Note: At this time BBii-Decon is optimized for 2D deconvolution, with a 3D implementation planned in future). \\nThis plugin supports accelerated processing using the cupy library. To make use of it, please follow \\nthe instructions to install cupy. \\nInstallation may look like this:\\nconda create --name cupy_p38 python=3.8\\nconda activate cupy_p38\\nconda install -c conda-forge cupy cudatoolkit=10.2\\nIf cupy installation worked out, you will find another checkbox in the user interface. By activating it, processing \\nshould become faster by factor 5-10, depending on processed image data and use GPU hardware.\\n\\nUsage - napari\\nYou can use the BBii deconvolution from within napari by clicking the menu Plugins > bbii-decon > bbii deconvolution. \\nIn the dialog, select the PSF, the image to process (a) and click on Run. After a moment, the deconvolved image (b) \\nwill show up.\\n\\nUsage from python\\nYou can also call the function from python. There is a full working example in this notebook.\\n```\\nfrom bbii_decon import bbii\\nbbii(PSF, image, number_of_iterations = 15, tau = 1.0e-08, rho = 0.98)\\n```\\nCitation\\n1) Kathleen Fraser, Dirk V. Arnold, and Graham Dellaire (2014). Projected Barzilai-Borwein\\nmethod with infeasible iterates for nonnegative least-squares image deblurring. In Proceedings\\nof the Eleventh Conference on Computer and Robot Vision (CRV 2014), Montreal, Canada, pp.\\n189--194.\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install bbii-decon via pip:\\npip install bbii-decon\\n\\nInstallation for developers\\nClone the github repository:\\n```\\nconda install git\\ngit clone https://github.com/gdellaire/BBii-Decon.git\\ncd BBii-Decon\\npip install -e .\\n```\\nDeployment to pypi\\nFor deploying the plugin to the python package index (pypi), one needs a pypi user account \\nfirst. For deploying the plugin to pypi, one needs to install some tools:\\npython -m pip install --user --upgrade setuptools wheel\\npython -m pip install --user --upgrade twine\\nThe following command allows us to package the souce code as a python wheel. Make sure that the 'dist' and 'build' folders are deleted before doing this:\\npython setup.py sdist bdist_wheel\\nThis command ships the just generated to pypi:\\npython -m twine upload --repository pypi dist/*\\nRead more about distributing your python package via pypi.\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"bbii-decon\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"bbii-decon\",\"documentation\":\"https://github.com/gdellaire/bbii-decon#README.md\",\"first_released\":\"2021-12-13T21:08:13.613383Z\",\"license\":\"BSD-3-Clause\",\"name\":\"bbii-decon\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/gdellaire/bbii-decon\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2021-12-13T21:08:13.613383Z\",\"report_issues\":\"https://github.com/gdellaire/bbii-decon/issues\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"pypher\"],\"summary\":\"Projected Barzilai-Borwein Image Deconvolution with Infeasible Iterates (BBii-Decon)\",\"support\":\"https://github.com/gdellaire/bbii-decon/issues\",\"twitter\":\"\",\"version\":\"0.0.1\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"niknett@gmail.com\",\"name\":\"Niklas Netter\"}],\"code_repository\":\"https://github.com/gatoniel/napari-3d-ortho-viewer\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-3d-ortho-viewer\"}],\"description\":\"# napari-3d-ortho-viewer\\n\\n[![License](https://img.shields.io/pypi/l/napari-3d-ortho-viewer.svg?color=green)](https://github.com/gatoniel/napari-3d-ortho-viewer/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-3d-ortho-viewer.svg?color=green)](https://pypi.org/project/napari-3d-ortho-viewer)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-3d-ortho-viewer.svg?color=green)](https://python.org)\\n[![tests](https://github.com/gatoniel/napari-3d-ortho-viewer/workflows/tests/badge.svg)](https://github.com/gatoniel/napari-3d-ortho-viewer/actions)\\n[![codecov](https://codecov.io/gh/gatoniel/napari-3d-ortho-viewer/branch/main/graph/badge.svg)](https://codecov.io/gh/gatoniel/napari-3d-ortho-viewer)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-3d-ortho-viewer)](https://napari-hub.org/plugins/napari-3d-ortho-viewer)\\n\\nNapari 3D Ortho Viewer - an ortho viewer for napari for 3D images\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-3d-ortho-viewer` via [pip]:\\n\\n    pip install napari-3d-ortho-viewer\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/gatoniel/napari-3d-ortho-viewer.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [MIT] license,\\n\\\"napari-3d-ortho-viewer\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/gatoniel/napari-3d-ortho-viewer/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-3d-ortho-viewer\\n\\n\\n\\n\\n\\n\\nNapari 3D Ortho Viewer - an ortho viewer for napari for 3D images\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-3d-ortho-viewer via pip:\\npip install napari-3d-ortho-viewer\\n\\nTo install latest development version :\\npip install git+https://github.com/gatoniel/napari-3d-ortho-viewer.git\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the MIT license,\\n\\\"napari-3d-ortho-viewer\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari-3d-ortho-viewer\",\"documentation\":\"https://github.com/gatoniel/napari-3d-ortho-viewer#README.md\",\"first_released\":\"2021-12-03T01:07:40.403247Z\",\"license\":\"MIT\",\"name\":\"napari-3d-ortho-viewer\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/gatoniel/napari-3d-ortho-viewer\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2021-12-03T01:07:40.403247Z\",\"report_issues\":\"https://github.com/gatoniel/napari-3d-ortho-viewer/issues\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\"],\"summary\":\"Napari 3D Ortho Viewer - an ortho viewer for napari for 3D images\",\"support\":\"https://github.com/gatoniel/napari-3d-ortho-viewer/issues\",\"twitter\":\"\",\"version\":\"0.0.1\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Hanjin Liu\"}],\"code_repository\":\"https://github.com/hanjinliu/napari-macrokit\",\"description\":\"# napari-macrokit\\n\\n[![License BSD-3](https://img.shields.io/pypi/l/napari-macrokit.svg?color=green)](https://github.com/hanjinliu/napari-macrokit/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-macrokit.svg?color=green)](https://pypi.org/project/napari-macrokit)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-macrokit.svg?color=green)](https://python.org)\\n[![tests](https://github.com/hanjinliu/napari-macrokit/workflows/tests/badge.svg)](https://github.com/hanjinliu/napari-macrokit/actions)\\n[![codecov](https://codecov.io/gh/hanjinliu/napari-macrokit/branch/main/graph/badge.svg)](https://codecov.io/gh/hanjinliu/napari-macrokit)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-macrokit)](https://napari-hub.org/plugins/napari-macrokit)\\n\\nExecutable script generation for napari plugins.\\n\\n![](https://github.com/hanjinliu/napari-macrokit/blob/main/images/example.gif)\\n&uarr; [Example](https://github.com/hanjinliu/napari-macrokit/blob/main/examples/regionprops.py) showing the real-time recording of GUI operation.\\n\\nThis napari plugin aims at making image analysis reproducible with arbitrary input/output types.\\n\\n## Usage\\n\\nCreate a macro object, decorate functions with `record` method and run!\\n\\n```python\\nfrom napari_macrokit import get_macro\\n\\nmacro = get_macro(\\\"my-plugin-specifier\\\")  # get macro object\\n\\n# define a function\\n@macro.record\\ndef add(a: float, b: float) -> float:\\n    return a + b\\n\\n# run\\nresult = add(3.2, 5.4)\\nadd(result, 1.0)\\n\\nmacro\\n\\n# Out:\\n# >>> float0 = add(3.2, 5.4)\\n# >>> float1 = add(float0, 1.0)\\n```\\n\\n## Record GUI Operations\\n\\nYou can use recordable functions in your widgets to keep tracks of GUI operations.\\nMore simply, you can double-decorate functions with `record` and `magicgui`.\\n\\n```python\\nimport numpy as np\\nfrom magicgui import magicgui\\nimport napari\\nfrom napari.types import ImageData\\nfrom napari_macrokit import get_macro\\n\\nmacro = get_macro(\\\"my-plugin-specifier\\\")  # get macro object\\n\\n# define recordable magicgui\\n@magicgui\\n@macro.record\\ndef add(image: ImageData, b: float) -> ImageData:\\n    return image + b\\n\\nviewer = napari.Viewer()  # launch a viewer\\nviewer.add_image(np.random.random((100, 100)))  # image data\\nviewer.window.add_dock_widget(add)  # add magicgui to the viewer\\n```\\n\\nRunning add twice in GUI and you'll find macro updated like below.\\n\\n```python\\nmacro\\n# Out\\n# >>> image0 = add(viewer.layers['Image'].data, 0.06)\\n# >>> image1 = add(image0, 0.12)\\n```\\n\\n## Combining Plugins\\n\\nSuppose you have two modules that use `napari-macrokit`.\\n\\n```python\\n# napari_module_0.py\\n\\nfrom napari.types import ImageData\\nfrom scipy import ndimage as ndi\\nfrom napari_macrokit import get_macro\\n\\nmacro = get_macro(\\\"napari-module-0\\\")\\n\\n@macro.record\\ndef gaussian_filter(image: ImageData, sigma: float) -> ImageData:\\n    return ndi.gaussian_filter(image, sigma=sigma)\\n\\n@macro.record\\ndef threshold(image: ImageData, value: float) -> ImageData:\\n    return image > value\\n```\\n\\n```python\\n# napari_module_1.py\\n\\nfrom napari.types import ImageData\\nimport numpy as np\\nfrom napari_macrokit import get_macro\\nmacro = get_macro(\\\"napari-module-1\\\")\\n\\n@macro.record\\ndef estimate_background(image: ImageData) -> float:\\n    return np.percentile(image, 10.0)\\n\\n```\\n\\nYou can use functions from both modules to build an analysis workflow by collecting existing macro objects with `collect_macro` function. All the recordable actions in the modules will also be recorded to the returned macro object.\\n\\n```python\\nimport numpy as np\\nfrom napari_macrokit import collect_macro\\nfrom napari_module_0 import gaussian_filter, threshold\\nfrom napari_module_1 import estimate_background\\n\\n# global_macro will record all the macro available at this point\\nglobal_macro = collect_macro()\\n\\n# start image analysis!\\nimage = np.random.random((100, 100))\\n\\nout = gaussian_filter(image, 2.0)\\nthresh = estimate_background(out)\\nbinary = threshold(out, thresh)\\n\\nmacro\\n# Out\\n# >>> image0 = gaussian_filter(arr0, 2.0)\\n# >>> float0 = estimate_background(image0)\\n# >>> image1 = threshold(image1, float0)\\n```\\n\\n---------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n## Installation\\n\\nYou can install `napari-macrokit` via [pip]:\\n\\n    pip install napari-macrokit\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/hanjinliu/napari-macrokit.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-macrokit\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/hanjinliu/napari-macrokit/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-macrokit\\n\\n\\n\\n\\n\\n\\nExecutable script generation for napari plugins.\\n\\n↑ Example showing the real-time recording of GUI operation.\\nThis napari plugin aims at making image analysis reproducible with arbitrary input/output types.\\nUsage\\nCreate a macro object, decorate functions with record method and run!\\n```python\\nfrom napari_macrokit import get_macro\\nmacro = get_macro(\\\"my-plugin-specifier\\\")  # get macro object\\ndefine a function\\n@macro.record\\ndef add(a: float, b: float) -> float:\\n    return a + b\\nrun\\nresult = add(3.2, 5.4)\\nadd(result, 1.0)\\nmacro\\nOut:\\n>>> float0 = add(3.2, 5.4)\\n>>> float1 = add(float0, 1.0)\\n```\\nRecord GUI Operations\\nYou can use recordable functions in your widgets to keep tracks of GUI operations.\\nMore simply, you can double-decorate functions with record and magicgui.\\n```python\\nimport numpy as np\\nfrom magicgui import magicgui\\nimport napari\\nfrom napari.types import ImageData\\nfrom napari_macrokit import get_macro\\nmacro = get_macro(\\\"my-plugin-specifier\\\")  # get macro object\\ndefine recordable magicgui\\n@magicgui\\n@macro.record\\ndef add(image: ImageData, b: float) -> ImageData:\\n    return image + b\\nviewer = napari.Viewer()  # launch a viewer\\nviewer.add_image(np.random.random((100, 100)))  # image data\\nviewer.window.add_dock_widget(add)  # add magicgui to the viewer\\n```\\nRunning add twice in GUI and you'll find macro updated like below.\\n```python\\nmacro\\nOut\\n>>> image0 = add(viewer.layers['Image'].data, 0.06)\\n>>> image1 = add(image0, 0.12)\\n```\\nCombining Plugins\\nSuppose you have two modules that use napari-macrokit.\\n```python\\nnapari_module_0.py\\nfrom napari.types import ImageData\\nfrom scipy import ndimage as ndi\\nfrom napari_macrokit import get_macro\\nmacro = get_macro(\\\"napari-module-0\\\")\\n@macro.record\\ndef gaussian_filter(image: ImageData, sigma: float) -> ImageData:\\n    return ndi.gaussian_filter(image, sigma=sigma)\\n@macro.record\\ndef threshold(image: ImageData, value: float) -> ImageData:\\n    return image > value\\n```\\n```python\\nnapari_module_1.py\\nfrom napari.types import ImageData\\nimport numpy as np\\nfrom napari_macrokit import get_macro\\nmacro = get_macro(\\\"napari-module-1\\\")\\n@macro.record\\ndef estimate_background(image: ImageData) -> float:\\n    return np.percentile(image, 10.0)\\n```\\nYou can use functions from both modules to build an analysis workflow by collecting existing macro objects with collect_macro function. All the recordable actions in the modules will also be recorded to the returned macro object.\\n```python\\nimport numpy as np\\nfrom napari_macrokit import collect_macro\\nfrom napari_module_0 import gaussian_filter, threshold\\nfrom napari_module_1 import estimate_background\\nglobal_macro will record all the macro available at this point\\nglobal_macro = collect_macro()\\nstart image analysis!\\nimage = np.random.random((100, 100))\\nout = gaussian_filter(image, 2.0)\\nthresh = estimate_background(out)\\nbinary = threshold(out, thresh)\\nmacro\\nOut\\n>>> image0 = gaussian_filter(arr0, 2.0)\\n>>> float0 = estimate_background(image0)\\n>>> image1 = threshold(image1, float0)\\n```\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\nInstallation\\nYou can install napari-macrokit via pip:\\npip install napari-macrokit\\n\\nTo install latest development version :\\npip install git+https://github.com/hanjinliu/napari-macrokit.git\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-macrokit\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari macro-kit\",\"documentation\":\"https://github.com/hanjinliu/napari-macrokit#README.md\",\"first_released\":\"2023-01-27T12:24:15.215078Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-macrokit\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/hanjinliu/napari-macrokit\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2023-01-27T12:24:15.215078Z\",\"report_issues\":\"https://github.com/hanjinliu/napari-macrokit/issues\",\"requirements\":[\"numpy\",\"magicgui\",\"qtpy\",\"macro-kit (>=0.4.0)\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\"],\"summary\":\"Executable script generation for napari plugins\",\"support\":\"https://github.com/hanjinliu/napari-macrokit/issues\",\"twitter\":\"\",\"version\":\"0.0.1\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"cqzhang@g.ecc.u-tokyo.ac.jp\",\"name\":\"Chenqi Zhang\"}],\"code_repository\":\"https://github.com/zcqwh/iacs_ipac_reader\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"iacs-ipac-reader\"}],\"description\":\"# iacs_ipac_reader\\n\\n[![License](https://img.shields.io/pypi/l/iacs_ipac_reader.svg?color=green)](https://github.com/zcqwh/iacs_ipac_reader/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/iacs_ipac_reader.svg?color=green)](https://pypi.org/project/iacs_ipac_reader)\\n[![Python Version](https://img.shields.io/pypi/pyversions/iacs_ipac_reader.svg?color=green)](https://python.org)\\n[![tests](https://github.com/zcqwh/iacs_ipac_reader/workflows/tests/badge.svg)](https://github.com/zcqwh/iacs_ipac_reader/actions)\\n[![codecov](https://codecov.io/gh/zcqwh/iacs_ipac_reader/branch/main/graph/badge.svg)](https://codecov.io/gh/zcqwh/iacs_ipac_reader)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/iacs_ipac_reader)](https://napari-hub.org/plugins/iacs_ipac_reader)\\n\\nA plugin used a convolutional neural network (CNN) to distinguish single platelets, platelet clusters, and white blood cells and performed classical image analysis for each subpopulation individually. Based on the derived single-cell features for each population, a Random Forest (RF) model was trained and used to classify COVID-19 associated thrombosis and non-COVID-19 associated thrombosis.\\n\\nMore information about IACS/iPAC.  \\n__IACS__: DOI: [10.1016/j.cell.2018.08.028](https://www.sciencedirect.com/science/article/pii/S0092867418310444)   \\n__iPAC__: DOI: [10.7554/eLife.52938](https://elifesciences.org/articles/52938)\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/plugins/stable/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `iacs_ipac_reader` via [pip]:\\n\\n    pip install iacs_ipac_reader\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/zcqwh/iacs_ipac_reader.git\\n\\n\\n## Introduction\\n\\nThe iacs-ipac-reader plugin mainly include 3 functional tabs:\\n\\n* iPAC\\n* IACS\\n* AID classif.\\n\\n\\n\\n### iPAC image contour tracker\\n<center>Interface of iPAC contour tracker</center>    \\n\\n![ipac.](https://github.com/zcqwh/iacs_ipac_reader/blob/main/Tutorial/pictures/ipac.png?raw=true \\\"iPAC\\\")\\n\\n### IACS image contour tracker\\n<center>Interface of IACS contour tracker</center>    \\n\\n![iacs.](https://github.com/zcqwh/iacs_ipac_reader/blob/main/Tutorial/pictures/iacs.png?raw=true \\\"IACS\\\")\\n\\n### AID classif.\\n<center>Interface of AID classif.</center>     \\n \\n![AID_classif.](https://github.com/zcqwh/iacs_ipac_reader/blob/main/Tutorial/pictures/classifier.jpg?raw=true \\\"AID classif\\\")\\n\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"iacs_ipac_reader\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/zcqwh/iacs_ipac_reader/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"iacs_ipac_reader\\n\\n\\n\\n\\n\\n\\nA plugin used a convolutional neural network (CNN) to distinguish single platelets, platelet clusters, and white blood cells and performed classical image analysis for each subpopulation individually. Based on the derived single-cell features for each population, a Random Forest (RF) model was trained and used to classify COVID-19 associated thrombosis and non-COVID-19 associated thrombosis.\\nMore information about IACS/iPAC.\\nIACS: DOI: 10.1016/j.cell.2018.08.028 \\niPAC: DOI: 10.7554/eLife.52938\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install iacs_ipac_reader via pip:\\npip install iacs_ipac_reader\\n\\nTo install latest development version :\\npip install git+https://github.com/zcqwh/iacs_ipac_reader.git\\n\\nIntroduction\\nThe iacs-ipac-reader plugin mainly include 3 functional tabs:\\n\\niPAC\\nIACS\\nAID classif.\\n\\niPAC image contour tracker\\nInterface of iPAC contour tracker \\n\\nIACS image contour tracker\\nInterface of IACS contour tracker \\n\\nAID classif.\\nInterface of AID classif. \\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"iacs_ipac_reader\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"iacs-ipac-reader\",\"documentation\":\"https://github.com/zcqwh/iacs_ipac_reader#README.md\",\"first_released\":\"2022-01-21T08:07:34.145777Z\",\"license\":\"BSD-3-Clause\",\"name\":\"iacs-ipac-reader\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/zcqwh/iacs_ipac_reader\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-04-12T06:17:48.661473Z\",\"report_issues\":\"https://github.com/zcqwh/iacs_ipac_reader/issues\",\"requirements\":[\"h5py (>=3.5.0)\",\"napari (>=0.4.12)\",\"napari-plugin-engine (>=0.2.0)\",\"numpy (>=1.21.4)\",\"opencv-contrib-python-headless (>=4.4.0.46)\",\"openpyxl (>=3.0.9)\",\"sklearn (>=0.0)\",\"PyQt5 (==5.12.3)\",\"pandas (>=1.4.0)\"],\"summary\":\"A reader plugin for read iacs/ipac images and export .rtdc files.\",\"support\":\"https://github.com/zcqwh/iacs_ipac_reader/issues\",\"twitter\":\"\",\"version\":\"0.0.13\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Kevin Tan\"}],\"code_repository\":null,\"description\":\"# napari-live-flim\\n\\n[![License GNU GPL v3.0](https://img.shields.io/pypi/l/napari-live-flim.svg?color=green)](https://github.com/facetorched/napari-live-flim/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-live-flim.svg?color=green)](https://pypi.org/project/napari-live-flim)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-live-flim.svg?color=green)](https://python.org)\\n[![tests](https://github.com/facetorched/napari-live-flim/workflows/tests/badge.svg)](https://github.com/facetorched/napari-live-flim/actions)\\n[![codecov](https://codecov.io/gh/facetorched/napari-live-flim/branch/main/graph/badge.svg)](https://codecov.io/gh/facetorched/napari-live-flim)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-live-flim)](https://napari-hub.org/plugins/napari-live-flim)\\n\\nA plugin for real-time FLIM analysis\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-live-flim` via [pip]:\\n\\n    pip install napari-live-flim\\n\\n\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [GNU GPL v3.0] license,\\n\\\"napari-live-flim\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-live-flim\\n\\n\\n\\n\\n\\n\\nA plugin for real-time FLIM analysis\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-live-flim via pip:\\npip install napari-live-flim\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the GNU GPL v3.0 license,\\n\\\"napari-live-flim\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please [file an issue] along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"Napari Live Flim\",\"documentation\":\"\",\"first_released\":\"2022-11-09T21:14:47.769823Z\",\"license\":\"GPL-3.0-only\",\"name\":\"napari-live-flim\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-11-09T21:14:47.769823Z\",\"report_issues\":\"\",\"requirements\":[\"dataclasses-json\",\"flimlib\",\"magicgui\",\"matplotlib\",\"numpy\",\"qtpy\",\"scipy\",\"superqt\",\"vispy\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\"],\"summary\":\"A plugin for real-time FLIM analysis\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.1.0\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Cyril Achard\"},{\"name\":\"Maxime Vidal\"},{\"name\":\"Jessy Lauer\"},{\"name\":\"Mackenzie Mathis\"}],\"category\":{\"Image modality\":[\"Fluorescence microscopy\"],\"Supported data\":[\"3D\"],\"Workflow step\":[\"Image Segmentation\",\"Visualization\"]},\"category_hierarchy\":{\"Image modality\":[[\"Fluorescence microscopy\",\"Light-sheet microscopy\"]],\"Supported data\":[[\"3D\"]],\"Workflow step\":[[\"Image Segmentation\",\"Cell segmentation\"],[\"Visualization\",\"Image visualisation\"],[\"Image Segmentation\",\"Region growing\",\"Watershed segmentation\"],[\"Visualization\",\"Plotting\"]]},\"code_repository\":\"https://github.com/AdaptiveMotorControlLab/CellSeg3d\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-cellseg3d\"}],\"description\":\"# napari-cellseg3D: a napari plug-in for direct 3D cell segmentation with deep learning\\n\\n\\n<img src=\\\"https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/838605d0-9723-4e43-83cd-6dbfe4adf36b/cellseg-logo.png?format=1500w\\\" title=\\\"cellseg3d\\\" alt=\\\"cellseg3d logo\\\" width=\\\"350\\\" align=\\\"right\\\" vspace = \\\"80\\\"/>\\n\\n<a href=\\\"https://github.com/psf/black\\\"><img alt=\\\"Code style: black\\\" src=\\\"https://img.shields.io/badge/code%20style-black-000000.svg\\\"></a>\\n[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://github.com/AdaptiveMotorControlLab/CellSeg3d/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-cellseg3d.svg?color=green)](https://pypi.org/project/napari-cellseg3d)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-cellseg-annotator.svg?color=green)](https://python.org)\\n[![codecov](https://codecov.io/gh/AdaptiveMotorControlLab/CellSeg3d/branch/main/graph/badge.svg?token=hzUcn3XN8F)](https://codecov.io/gh/AdaptiveMotorControlLab/CellSeg3d)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-cellseg3d)](https://www.napari-hub.org/plugins/napari-cellseg3d)\\n\\n\\nA napari plugin for 3D cell segmentation: training, inference, and data review. In particular, this project was developed for analysis of mesoSPIM-acquired (cleared tissue + lightsheet) datasets.\\n\\n----------------------------------\\n\\n## News\\n\\n**June 2022: This is an alpha version, please expect bugs and issues, and help us make the code better by reporting them as an issue!**\\n\\n\\n\\n## Installation\\n\\nNote : we recommend using conda to create a new environment for the plugin.\\n\\n    conda create --name python=3.8 napari-cellseg3d\\n    conda activate napari-cellseg3d\\n\\nYou can install `napari-cellseg3d` via [pip]:  \\n\\n    pip install napari-cellseg3d\\n\\nOR directly via [napari-hub]:\\n\\n- Install napari from pip with `pip install \\\"napari[all]\\\"`,\\nthen from the “Plugins” menu within the napari application, select “Install/Uninstall Package(s)...”\\n- Copy `napari-cellseg3d` and paste it where it says “Install by name/url…”\\n- Click “Install”\\n\\n## Documentation\\n\\nAvailable at https://AdaptiveMotorControlLab.github.io/CellSeg3d\\n\\nYou can also generate docs by running ``make html`` in the docs folder.\\n\\n## Usage\\n\\nTo use the plugin, please run:\\n```\\nnapari\\n```\\nThen go into Plugins > napari-cellseg3d, and choose which tool to use.\\n\\n- **Review**: This module allows you to review your labels, from predictions or manual labeling, and correct them if needed. It then saves the status of each file in a csv, for easier monitoring.\\n- **Inference**: This module allows you to use pre-trained segmentation algorithms on volumes to automatically label cells and compute statistics.\\n- **Train**:  This module allows you to train segmentation algorithms from labeled volumes.\\n- **Utilities**: This module allows you to perform several actions like cropping your volumes and labels dynamically, by selecting a fixed size volume and moving it around the image; computing prediction scores from ground truth and predicition labels; or converting labels from instance to segmentation and the opposite.\\n\\n\\n## Requirements\\n**Python >= 3.8 required**\\n\\nRequires **pytorch** and **MONAI**.\\nFor PyTorch, please see [PyTorch's website for installation instructions].\\nA CUDA-capable GPU is not needed but very strongly recommended, especially for training.\\nIf you get errors from MONAI regarding missing readers, please see [MONAI's optional dependencies] page for instructions on getting the readers required by your images.\\n\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n\\n## Testing\\n\\nTo run tests locally:\\n\\n- Locally : run ``pytest`` in the plugin folder\\n- Locally with coverage : In the plugin folder, run ``coverage run --source=napari_cellseg3d -m pytest`` then ``coverage xml`` to generate a .xml coverage file.\\n- With tox : run ``tox`` in the plugin folder (will simulate tests with several python and OS configs, requires substantial storage space)\\n\\n## Contributing\\n\\nContributions are very welcome.\\n\\nPlease ensure the coverage at least stays the same before you submit a pull request.\\n\\nFor local installation from Github cloning, please run:\\n\\n```\\npip install -e .\\n```\\n\\n## License\\n\\nDistributed under the terms of the [MIT] license.\\n\\n\\\"napari-cellseg3d\\\" is free and open source software.\\n\\n[napari-hub]: https://www.napari-hub.org/plugins/napari-cellseg3d\\n\\n[file an issue]: https://github.com/AdaptiveMotorControlLab/CellSeg3d/issues\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n[PyTorch's website for installation instructions]: https://pytorch.org/get-started/locally/\\n[MONAI's optional dependencies]: https://docs.monai.io/en/stable/installation.html#installing-the-recommended-dependencies\\n\\n## Acknowledgements\\n\\nThis plugin was developed by Cyril Achard, Maxime Vidal, Mackenzie Mathis. This work was funded, in part, from the Wyss Center to the [Mathis Laboratory of Adaptive Motor Control](https://www.mackenziemathislab.org/).\\n\\n\\n## Plugin base\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/plugins/stable/index.html\\n-->\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-cellseg3D: a napari plug-in for direct 3D cell segmentation with deep learning\\n\\n\\n\\n\\n\\n\\n\\nA napari plugin for 3D cell segmentation: training, inference, and data review. In particular, this project was developed for analysis of mesoSPIM-acquired (cleared tissue + lightsheet) datasets.\\n\\nNews\\nJune 2022: This is an alpha version, please expect bugs and issues, and help us make the code better by reporting them as an issue!\\nInstallation\\nNote : we recommend using conda to create a new environment for the plugin.\\nconda create --name python=3.8 napari-cellseg3d\\nconda activate napari-cellseg3d\\n\\nYou can install napari-cellseg3d via pip:  \\npip install napari-cellseg3d\\n\\nOR directly via napari-hub:\\n\\nInstall napari from pip with pip install \\\"napari[all]\\\",\\nthen from the “Plugins” menu within the napari application, select “Install/Uninstall Package(s)...”\\nCopy napari-cellseg3d and paste it where it says “Install by name/url…”\\nClick “Install”\\n\\nDocumentation\\nAvailable at https://AdaptiveMotorControlLab.github.io/CellSeg3d\\nYou can also generate docs by running make html in the docs folder.\\nUsage\\nTo use the plugin, please run:\\nnapari\\nThen go into Plugins > napari-cellseg3d, and choose which tool to use.\\n\\nReview: This module allows you to review your labels, from predictions or manual labeling, and correct them if needed. It then saves the status of each file in a csv, for easier monitoring.\\nInference: This module allows you to use pre-trained segmentation algorithms on volumes to automatically label cells and compute statistics.\\nTrain:  This module allows you to train segmentation algorithms from labeled volumes.\\nUtilities: This module allows you to perform several actions like cropping your volumes and labels dynamically, by selecting a fixed size volume and moving it around the image; computing prediction scores from ground truth and predicition labels; or converting labels from instance to segmentation and the opposite.\\n\\nRequirements\\nPython >= 3.8 required\\nRequires pytorch and MONAI.\\nFor PyTorch, please see PyTorch's website for installation instructions.\\nA CUDA-capable GPU is not needed but very strongly recommended, especially for training.\\nIf you get errors from MONAI regarding missing readers, please see MONAI's optional dependencies page for instructions on getting the readers required by your images.\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\\nTesting\\nTo run tests locally:\\n\\nLocally : run pytest in the plugin folder\\nLocally with coverage : In the plugin folder, run coverage run --source=napari_cellseg3d -m pytest then coverage xml to generate a .xml coverage file.\\nWith tox : run tox in the plugin folder (will simulate tests with several python and OS configs, requires substantial storage space)\\n\\nContributing\\nContributions are very welcome.\\nPlease ensure the coverage at least stays the same before you submit a pull request.\\nFor local installation from Github cloning, please run:\\npip install -e .\\nLicense\\nDistributed under the terms of the MIT license.\\n\\\"napari-cellseg3d\\\" is free and open source software.\\nAcknowledgements\\nThis plugin was developed by Cyril Achard, Maxime Vidal, Mackenzie Mathis. This work was funded, in part, from the Wyss Center to the Mathis Laboratory of Adaptive Motor Control.\\nPlugin base\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"Cell Segmentation Annotator\",\"documentation\":\"https://adaptivemotorcontrollab.github.io/CellSeg3d/res/welcome.html\",\"first_released\":\"2022-06-25T17:39:48.308233Z\",\"license\":\"MIT\",\"name\":\"napari-cellseg3d\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/AdaptiveMotorControlLab/CellSeg3d\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-08-19T16:11:07.693839Z\",\"report_issues\":\"https://github.com/AdaptiveMotorControlLab/CellSeg3d/issues\",\"requirements\":[\"numpy\",\"napari[all] (>=0.4.14)\",\"QtPy\",\"opencv-python (>=4.5.5)\",\"dask-image (>=0.6.0)\",\"scikit-image (>=0.19.2)\",\"matplotlib (>=3.4.1)\",\"tifffile (>=2022.2.9)\",\"imageio-ffmpeg (>=0.4.5)\",\"torch (>=1.11)\",\"monai[einops,itk,nibabel,scikit-image] (>=0.9.0)\",\"tqdm\",\"monai (>=0.9.0)\",\"nibabel\",\"scikit-image\",\"pillow\",\"matplotlib\",\"vispy (>=0.9.6)\"],\"summary\":\"plugin for cell segmentation\",\"support\":\"https://github.com/AdaptiveMotorControlLab/CellSeg3d/issues\",\"twitter\":\"\",\"version\":\"0.0.1rc4\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Grzegorz Bokota\"}],\"code_repository\":\"https://github.com/4DNucleome/PartSeg-smfish\",\"description\":\"# PartSeg-smfish\\n\\n[![License BSD-3](https://img.shields.io/pypi/l/PartSeg-smfish.svg?color=green)](https://github.com/4DNucleome/PartSeg-smfish/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/PartSeg-smfish.svg?color=green)](https://pypi.org/project/PartSeg-smfish)\\n[![Python Version](https://img.shields.io/pypi/pyversions/PartSeg-smfish.svg?color=green)](https://python.org)\\n[![tests](https://github.com/4DNucleome/PartSeg-smfish/workflows/tests/badge.svg)](https://github.com/4DNucleome/PartSeg-smfish/actions)\\n[![codecov](https://codecov.io/gh/4DNucleome/PartSeg-smfish/branch/main/graph/badge.svg)](https://codecov.io/gh/4DNucleome/PartSeg-smfish)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/PartSeg-smfish)](https://napari-hub.org/plugins/PartSeg-smfish)\\n\\nPartSeg and napari plugin for smfish data\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/stable/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `PartSeg-smfish` via [pip]:\\n\\n    pip install PartSeg-smfish\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/4DNucleome/PartSeg-smfish.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"PartSeg-smfish\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/4DNucleome/PartSeg-smfish/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"PartSeg-smfish\\n\\n\\n\\n\\n\\n\\nPartSeg and napari plugin for smfish data\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install PartSeg-smfish via pip:\\npip install PartSeg-smfish\\n\\nTo install latest development version :\\npip install git+https://github.com/4DNucleome/PartSeg-smfish.git\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"PartSeg-smfish\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"PartSeg-smfish\",\"documentation\":\"https://github.com/4DNucleome/PartSeg-smfish#README.md\",\"first_released\":\"2022-10-24T11:04:32.268909Z\",\"license\":\"BSD-3-Clause\",\"name\":\"PartSeg-smfish\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/4DNucleome/PartSeg-smfish\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-12-06T10:08:29.060888Z\",\"report_issues\":\"https://github.com/4DNucleome/PartSeg-smfish/issues\",\"requirements\":[\"PartSeg (>=0.13.0)\",\"numpy\",\"napari\",\"qtpy\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\"],\"summary\":\"PartSeg and napari plugin for smfish data\",\"support\":\"https://github.com/4DNucleome/PartSeg-smfish/issues\",\"twitter\":\"\",\"version\":\"0.1.3\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"v.o.van_der_valk@lumc.nl\",\"name\":\"Viktor van der Valk\"}],\"category\":{\"Image modality\":[\"Medical imaging\"],\"Supported data\":[\"2D\",\"3D\"],\"Workflow step\":[\"Image registration\",\"Image Segmentation\"]},\"category_hierarchy\":{\"Image modality\":[[\"Medical imaging\"]],\"Supported data\":[[\"2D\"],[\"3D\"]],\"Workflow step\":[[\"Image registration\",\"Affine registration\",\"Rigid registration\"],[\"Image registration\",\"Deformable registration\"],[\"Image registration\",\"Affine registration\"],[\"Image Segmentation\"]]},\"code_repository\":\"https://github.com/SuperElastix/elastix_napari\",\"conda\":[],\"description\":\"# Description\\n\\nThis plugin makes the elastix toolbox for rigid and nonrigid registration of images available in napari.\\nelastix is open source software, based on the well-known Insight Segmentation and Registration Toolkit (ITK). The software consists of a collection of algorithms that are commonly used to solve (medical) image registration problems. The modular design of elastix allows the user to quickly configure, test, and compare different registration methods for a specific application.\\n\\n# Who is This For?\\n\\nWith this plugin both 2D and 3D images in all file formats available in ITK can be registered.\\nThe plugin supports various transformations including rigid, affine and bspline.\\n\\nRegistration within the plugin is done based on user defined parameters, but for novice users\\ndefaults for each transformation model are available.\\n\\n# How to Guide\\n\\nLoad the images you want to register into napari and select them in the fixed (or reference) and moving image dropdowns of plugin interface.\\n\\nFor fast and easy registration only the preferred transformation (rigid, affine or bspline) has to be selected (see Transformations section for explanation).\\n\\nFor more advanced registrations the following adjustments can be made in the plugin:\\n\\n- Masks for both the fixed and the moving images can be selected to let elastix only include certain areas in the registration. These masks have to be loaded into napari and selected in the correct mask dropdown menus, which appear when the masks box is ticked.\\n- Point sets for both the fixed and the moving images can de selected to use certain points to aid registration. These point set files have to be .txt files in the following format:\\n\\n  index/point\\\\\\n  #points\\\\\\n  point1 x point1 y [point1 z]\\\\\\n  point2 x point2 y [point2 z]\\n\\n  The first line indicates whether the points are given as “indices” (of the fixed image), or as “points” (in\\n  physical coordinates). The second line stores the number of points that will be specified. After that the\\n  point data is given. For example:\\n\\n  point\\\\\\n  3\\\\\\n  2.32 5.34 -4.12\\\\\\n  -1.56 0.12 9.23\\\\\\n  1.00 7.34 -0.23\\n\\n- An initial transform file that specifies a transform that is applied before the registration is done, can be uploaded as a .txt file. For the latest file and transform formats that are supported, see the [elastix manual](https://elastix.lumc.nl/doxygen/index.html)\\n\\n- For the most common registration parameters adjustments can be made in the plugin GUI\\n\\n- Other, less common registration parameters can be adjusted by uploading custom transform parameter file(s). (Select 'custom' in the preset dropdown).\\n\\n\\n<img width=\\\"1438\\\" alt=\\\"Screenshot 2021-05-12 at 15 07 24\\\" src=\\\"https://user-images.githubusercontent.com/33719474/117980045-d6009b00-b333-11eb-9976-f64d34f4f7cc.png\\\">\\n\\n# Transformations\\n\\nIn the plugin 3 common transformations are available as presets, other transformations can be done with the 'custom' option in the preset dropdown. The plugin then has the ability to upload custom parameter files in which other transformations can be specified.\\n\\nThe three common transformations are:\\n\\n- [Rigid Transform](https://en.wikipedia.org/wiki/Rigid_transformation):\\nAlso known as a Euclidean transformation, this transform preserves the Euclidean\\ndistance between each pair of points on the image. This includes rotation,\\ntranslation and reflection but not scaling or shearing.\\n\\n\\n- [Affine Transform](https://en.wikipedia.org/wiki/Affine_transformation):\\nThis transfrom preserves\\nlines and parallelism, but not necessarily distance and angles. Translation,\\nscaling, similarity, reflection, rotation and shearing are all valid\\naffine transformations.\\n\\n- [BSpline Transform](https://en.wikipedia.org/wiki/B-spline):\\nThis is a deformable transformation that preserves none of the properties mentioned in the transforms describe above.\\n\\n# Getting Help\\nIf you find a bug in the elastix napari plugin, or would like support with using it, please raise an\\nissue on the [GitHub repository](https://github.com/SuperElastix/elastix_napari).\\n\\nFor question specifically about the elastix toolbox we have a [mailing list](https://groups.google.com/forum/#!forum/elastix-imageregistration).\\n\\n# Contributions\\nContributions to the elastix_napari plugin, [itkelastix](https://github.com/InsightSoftwareConsortium/ITKElastix) (the python wrapper) or [elastix](https://github.com/SuperElastix/elastix) (the C++ core) on which the plugin is build, are welcome.\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"Description\\nThis plugin makes the elastix toolbox for rigid and nonrigid registration of images available in napari.\\nelastix is open source software, based on the well-known Insight Segmentation and Registration Toolkit (ITK). The software consists of a collection of algorithms that are commonly used to solve (medical) image registration problems. The modular design of elastix allows the user to quickly configure, test, and compare different registration methods for a specific application.\\nWho is This For?\\nWith this plugin both 2D and 3D images in all file formats available in ITK can be registered.\\nThe plugin supports various transformations including rigid, affine and bspline.\\nRegistration within the plugin is done based on user defined parameters, but for novice users\\ndefaults for each transformation model are available.\\nHow to Guide\\nLoad the images you want to register into napari and select them in the fixed (or reference) and moving image dropdowns of plugin interface.\\nFor fast and easy registration only the preferred transformation (rigid, affine or bspline) has to be selected (see Transformations section for explanation).\\nFor more advanced registrations the following adjustments can be made in the plugin:\\n\\nMasks for both the fixed and the moving images can be selected to let elastix only include certain areas in the registration. These masks have to be loaded into napari and selected in the correct mask dropdown menus, which appear when the masks box is ticked.\\nPoint sets for both the fixed and the moving images can de selected to use certain points to aid registration. These point set files have to be .txt files in the following format:\\n\\nindex/point\\\\\\n  #points\\\\\\n  point1 x point1 y [point1 z]\\\\\\n  point2 x point2 y [point2 z]\\nThe first line indicates whether the points are given as “indices” (of the fixed image), or as “points” (in\\n  physical coordinates). The second line stores the number of points that will be specified. After that the\\n  point data is given. For example:\\npoint\\\\\\n  3\\\\\\n  2.32 5.34 -4.12\\\\\\n  -1.56 0.12 9.23\\\\\\n  1.00 7.34 -0.23\\n\\n\\nAn initial transform file that specifies a transform that is applied before the registration is done, can be uploaded as a .txt file. For the latest file and transform formats that are supported, see the elastix manual\\n\\n\\nFor the most common registration parameters adjustments can be made in the plugin GUI\\n\\n\\nOther, less common registration parameters can be adjusted by uploading custom transform parameter file(s). (Select 'custom' in the preset dropdown).\\n\\n\\n\\nTransformations\\nIn the plugin 3 common transformations are available as presets, other transformations can be done with the 'custom' option in the preset dropdown. The plugin then has the ability to upload custom parameter files in which other transformations can be specified.\\nThe three common transformations are:\\n\\n\\nRigid Transform:\\nAlso known as a Euclidean transformation, this transform preserves the Euclidean\\ndistance between each pair of points on the image. This includes rotation,\\ntranslation and reflection but not scaling or shearing.\\n\\n\\nAffine Transform:\\nThis transfrom preserves\\nlines and parallelism, but not necessarily distance and angles. Translation,\\nscaling, similarity, reflection, rotation and shearing are all valid\\naffine transformations.\\n\\n\\nBSpline Transform:\\nThis is a deformable transformation that preserves none of the properties mentioned in the transforms describe above.\\n\\n\\nGetting Help\\nIf you find a bug in the elastix napari plugin, or would like support with using it, please raise an\\nissue on the GitHub repository.\\nFor question specifically about the elastix toolbox we have a mailing list.\\nContributions\\nContributions to the elastix_napari plugin, itkelastix (the python wrapper) or elastix (the C++ core) on which the plugin is build, are welcome.\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"elastix-napari\",\"documentation\":\"\",\"first_released\":\"2021-03-24T08:31:46.514582Z\",\"license\":\"Apache-2.0\",\"name\":\"elastix-napari\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://elastix.lumc.nl/\",\"python_version\":\">=3.6\",\"reader_file_extensions\":[],\"release_date\":\"2021-07-05T08:34:36.293228Z\",\"report_issues\":\"\",\"requirements\":[\"itk-elastix (>=0.11.1)\",\"numpy (>=1.19.0)\",\"napari (>=0.4.6)\",\"napari-plugin-engine (>=0.1.4)\",\"magicgui (>=0.2.6)\",\"itk-napari-conversion (>=0.3.1)\",\"napari-itk-io (>=0.1.0)\"],\"summary\":\"A toolbox for rigid and nonrigid registration of images.\",\"support\":\"https://groups.google.com/g/elastix-imageregistration\",\"twitter\":\"\",\"version\":\"0.1.7\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"loic.sauteur@unibas.ch\",\"name\":\"Loïc Sauteur\"}],\"code_repository\":\"https://github.com/loicsauteur/napari-annotator\",\"conda\":[],\"description\":\"# Description\\n\\nThis light-weight plugin provides additional control over label layers.\\nIt is intended to ease your work when annotating data manually.\\n![Example screenshot](https://github.com/loicsauteur/napari-annotator/blob/main/resources/image1.png?raw=true)\\n\\nIt provides you with a widget listing all individual labels.\\nFor each label, you can:\\n- select it from the list to activate it for further drawing.\\n- toggle the visibility of individual labels\\n- locate the drawn label (i.e. move to the centroid location at the current zoom level)\\n- change the label color with a color picker\\n- erase the label (sets all the drawn pixels to the label layer background value, **not un-doable**)\\n\\n# Intended Audience & Supported Data\\n\\nEveryone that has 2D or 3D data and wants to annotate (or curate annotated data)\\nshould find a useful extension with this plugin.\\n\\nThe plugin will recognise and work only on label layers.\\n\\n**Note:**\\nThe \\\"locate center\\\" button will only work on 2D/3D label layers, i.e.: YX, ZYX, TYX, CYX.\\n\\nChannels are considered a dimension.\\n\\n# Quickstart\\n\\n1. Start napari\\n2. Open an image you want to annotate\\n   1. Best, an image with the same dimension as you labels layer should have\\n   2. e.g. ``File > Open Sample > napari > Binary Blobs (3D)``\\n3. Add (or load) a labels layer\\n4. Start the plugin ``Plugins > napari-annotator: Annotator``\\n5. Make sure the labels layer is selected\\n6. Start drawing\\n\\n#### Known limitations\\n1. Lag when drawing (see [GitHub README](https://github.com/loicsauteur/napari-annotator) for more info).\\n2. Maximum 255 labels supported (see [GitHub README](https://github.com/loicsauteur/napari-annotator) for more info).\\n\\n# Getting Help\\n\\nIf you encounter bugs, please [file an issue] along with a detailed description.\\nOr open a thread on [forum.image.sc](https://forum.image.sc) with a detailed description\\nand a [@loicsauteur](https://github.com/loicsauteur) tag.\\n\\nFor general help, reach out via the [forum.image.sc](https://forum.image.sc) with a tag [@loicsauteur](https://github.com/loicsauteur).\\n\\n# How to Cite\\n\\nNo citation needed. Honorable mention welcome.\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"Description\\nThis light-weight plugin provides additional control over label layers.\\nIt is intended to ease your work when annotating data manually.\\n\\nIt provides you with a widget listing all individual labels.\\nFor each label, you can:\\n- select it from the list to activate it for further drawing.\\n- toggle the visibility of individual labels\\n- locate the drawn label (i.e. move to the centroid location at the current zoom level)\\n- change the label color with a color picker\\n- erase the label (sets all the drawn pixels to the label layer background value, not un-doable)\\nIntended Audience & Supported Data\\nEveryone that has 2D or 3D data and wants to annotate (or curate annotated data)\\nshould find a useful extension with this plugin.\\nThe plugin will recognise and work only on label layers.\\nNote:\\nThe \\\"locate center\\\" button will only work on 2D/3D label layers, i.e.: YX, ZYX, TYX, CYX.\\nChannels are considered a dimension.\\nQuickstart\\n\\nStart napari\\nOpen an image you want to annotate\\nBest, an image with the same dimension as you labels layer should have\\ne.g. File > Open Sample > napari > Binary Blobs (3D)\\nAdd (or load) a labels layer\\nStart the plugin Plugins > napari-annotator: Annotator\\nMake sure the labels layer is selected\\nStart drawing\\n\\nKnown limitations\\n\\nLag when drawing (see GitHub README for more info).\\nMaximum 255 labels supported (see GitHub README for more info).\\n\\nGetting Help\\nIf you encounter bugs, please [file an issue] along with a detailed description.\\nOr open a thread on forum.image.sc with a detailed description\\nand a @loicsauteur tag.\\nFor general help, reach out via the forum.image.sc with a tag @loicsauteur.\\nHow to Cite\\nNo citation needed. Honorable mention welcome.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"Annotator\",\"documentation\":\"https://github.com/loicsauteur/napari-annotator#README.md\",\"first_released\":\"2022-03-07T16:39:41.535325Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-annotator\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/loicsauteur/napari-annotator\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-03-07T16:39:41.535325Z\",\"report_issues\":\"https://github.com/loicsauteur/napari-annotator/issues\",\"requirements\":[\"numpy\",\"scikit-image\"],\"summary\":\"A lightweight plugin extending label layer control\",\"support\":\"https://github.com/loicsauteur/napari-annotator/issues\",\"twitter\":\"\",\"version\":\"0.0.1\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Nicholas Sofroniew\"},{\"name\":\"Alister Burt\"},{\"name\":\"Guillaume Witz\"},{\"name\":\"Faris Abouakil\"},{\"name\":\"Talley Lambert\"}],\"code_repository\":\"https://github.com/napari/napari-animation\",\"description\":\"# napari-animation\\n\\n[![License](https://img.shields.io/pypi/l/napari-animation.svg?color=green)](https://github.com/napari/napari-animation/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-animation.svg?color=green)](https://pypi.org/project/napari-animation)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-animation.svg?color=green)](https://python.org)\\n[![tests](https://github.com/napari/napari-animation/actions/workflows/test_and_deploy.yml/badge.svg)](https://github.com/napari/napari-animation/actions)\\n[![codecov](https://codecov.io/gh/napari/napari-animation/branch/main/graph/badge.svg)](https://codecov.io/gh/napari/napari-animation)\\n\\n**napari-animation** is a plugin for making animations in [napari].\\n\\n<p align=\\\"center\\\">\\n  <img width=\\\"500\\\" src=\\\"https://user-images.githubusercontent.com/7307488/196110138-6c4663b1-67b2-4c79-97b7-57b706d1d49c.gif\\\">\\n</p>\\n\\n----------------------------------\\n\\nThis plugin is built on [naparimovie](https://github.com/guiwitz/naparimovie) from @guiwitz. naparimovie was submitted to napari in [PR#851](https://github.com/napari/napari/pull/780) before napari plugin infrastructure existed.\\n\\n----------------------------------\\n## Overview\\n\\n**napari-animation** provides a framework for the creation of animations in napari, the plugin contains:\\n- an easy to use GUI for creating animations interactively\\n- a Python package for the programmatic creation of animations\\n\\nThis plugin remains under development and contributions are very welcome, please open an issue to discuss potential improvements.\\n\\n## Installation\\n\\n### PyPI\\n`napari-animation` is available through the Python package index and can be installed using `pip`.\\n\\n```sh\\npip install napari-animation\\n```\\n\\n### Local\\nYou can clone this repository and install locally with\\n\\n    pip install -e .\\n\\n### Interactive use\\n**napari-animation** can be used interactively.\\n\\nAn animation is created by capturing [keyframes](https://en.wikipedia.org/wiki/Key_frame) containing the current viewer state.\\n\\n<p align=\\\"center\\\">\\n  <img width=\\\"600\\\" src=\\\"https://user-images.githubusercontent.com/7307488/196113682-96ce0da3-fa5c-411e-8fb1-52dc3a8f96b6.png\\\">\\n</p>\\n\\nTo activate the GUI, select **napari-animation: wizard** from the *plugins menu*\\n\\n<p align=\\\"center\\\">\\n  <img width=\\\"200\\\" src=\\\"https://user-images.githubusercontent.com/7307488/196114466-56cb5985-0d79-4cfa-96f1-38cf3ccfbc48.png\\\">\\n</p>\\n\\n### Headless\\n**napari-animation** can also be run headless, allowing for reproducible, scripted creation of animations.\\n\\n```python\\nfrom napari_animation import Animation\\n\\nanimation = Animation(viewer)\\n\\nviewer.dims.ndisplay = 3\\nviewer.camera.angles = (0.0, 0.0, 90.0)\\nanimation.capture_keyframe()\\nviewer.camera.zoom = 2.4\\nanimation.capture_keyframe()\\nviewer.camera.angles = (-7.0, 15.7, 62.4)\\nanimation.capture_keyframe(steps=60)\\nviewer.camera.angles = (2.0, -24.4, -36.7)\\nanimation.capture_keyframe(steps=60)\\nviewer.reset_view()\\nviewer.camera.angles = (0.0, 0.0, 90.0)\\nanimation.capture_keyframe()\\nanimation.animate('demo.mov', canvas_only=False)\\n```\\n\\n## Examples\\nExamples can be found in our [examples](examples) folder. Simple examples for both interactive and headless \\nuse of the plugin follow.\\n\\n## Contributing\\n\\nContributions are very welcome and a detailed contributing guide is coming soon. \\n\\nTests are run with `pytest`.\\n\\nWe use [`pre-commit`](https://pre-commit.com) to sort imports with\\n[`isort`](https://github.com/timothycrosley/isort), format code with\\n[`black`](https://github.com/psf/black), and lint with\\n[`flake8`](https://github.com/PyCQA/flake8) automatically prior to each commit.\\nTo minmize test errors when submitting pull requests, please install `pre-commit`\\nin your environment as follows:\\n\\n```sh\\npre-commit install\\n```\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-animation\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/sofroniewn/napari-animation/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-animation\\n\\n\\n\\n\\n\\nnapari-animation is a plugin for making animations in napari.\\n\\n\\n\\n\\nThis plugin is built on naparimovie from @guiwitz. naparimovie was submitted to napari in PR#851 before napari plugin infrastructure existed.\\n\\nOverview\\nnapari-animation provides a framework for the creation of animations in napari, the plugin contains:\\n- an easy to use GUI for creating animations interactively\\n- a Python package for the programmatic creation of animations\\nThis plugin remains under development and contributions are very welcome, please open an issue to discuss potential improvements.\\nInstallation\\nPyPI\\nnapari-animation is available through the Python package index and can be installed using pip.\\nsh\\npip install napari-animation\\nLocal\\nYou can clone this repository and install locally with\\npip install -e .\\n\\nInteractive use\\nnapari-animation can be used interactively.\\nAn animation is created by capturing keyframes containing the current viewer state.\\n\\n\\n\\nTo activate the GUI, select napari-animation: wizard from the plugins menu\\n\\n\\n\\nHeadless\\nnapari-animation can also be run headless, allowing for reproducible, scripted creation of animations.\\n```python\\nfrom napari_animation import Animation\\nanimation = Animation(viewer)\\nviewer.dims.ndisplay = 3\\nviewer.camera.angles = (0.0, 0.0, 90.0)\\nanimation.capture_keyframe()\\nviewer.camera.zoom = 2.4\\nanimation.capture_keyframe()\\nviewer.camera.angles = (-7.0, 15.7, 62.4)\\nanimation.capture_keyframe(steps=60)\\nviewer.camera.angles = (2.0, -24.4, -36.7)\\nanimation.capture_keyframe(steps=60)\\nviewer.reset_view()\\nviewer.camera.angles = (0.0, 0.0, 90.0)\\nanimation.capture_keyframe()\\nanimation.animate('demo.mov', canvas_only=False)\\n```\\nExamples\\nExamples can be found in our examples folder. Simple examples for both interactive and headless \\nuse of the plugin follow.\\nContributing\\nContributions are very welcome and a detailed contributing guide is coming soon. \\nTests are run with pytest.\\nWe use pre-commit to sort imports with\\nisort, format code with\\nblack, and lint with\\nflake8 automatically prior to each commit.\\nTo minmize test errors when submitting pull requests, please install pre-commit\\nin your environment as follows:\\nsh\\npre-commit install\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-animation\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 3 - Alpha\"],\"display_name\":\"napari-animation\",\"documentation\":\"\",\"first_released\":\"2021-04-23T16:11:20.051462Z\",\"license\":\"BSD 3-Clause\",\"name\":\"napari-animation\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/napari/napari-animation\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-11-12T17:21:07.189390Z\",\"report_issues\":\"\",\"requirements\":[\"imageio\",\"imageio-ffmpeg\",\"napari\",\"npe2\",\"numpy\",\"qtpy\",\"scipy\",\"tqdm (>=4.56.0)\",\"superqt\",\"pre-commit ; extra == 'dev'\",\"black ; extra == 'dev'\",\"flake8 ; extra == 'dev'\",\"check-manifest ; extra == 'dev'\",\"PyQt5 (!=5.15.0,>=5.12.3) ; extra == 'dev'\",\"pytest ; extra == 'dev'\",\"pytest ; extra == 'testing'\"],\"summary\":\"A plugin for making animations in napari\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.0.5\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Nathan Heath Patterson\",\"orcid\":\"0000-0002-0064-1583\"},{\"name\":\"Lukasz Migas\",\"orcid\":\"0000-0002-1884-6405\"}],\"code_repository\":\"https://github.com/nhpatterson/napari-imsmicrolink\",\"description\":\"# napari-imsmicrolink\\n![microlink-logo-update](https://user-images.githubusercontent.com/17855764/146078168-dd557089-ff10-46d6-b24d-268f5d21a9ee.png)\\n\\n[![License](https://img.shields.io/pypi/l/napari-imsmicrolink.svg?color=green)](https://github.com/nhpatterson/napari-imsmicrolink/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-imsmicrolink.svg?color=green)](https://pypi.org/project/napari-imsmicrolink)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-imsmicrolink.svg?color=green)](https://python.org)\\n[![tests](https://github.com/nhpatterson/napari-imsmicrolink/workflows/tests/badge.svg)](https://github.com/nhpatterson/napari-imsmicrolink/actions)\\n\\n[napari] plugin to perform MALDI IMS - microscopy registration using laser ablation marks as described in [Anal. Chem. 2018, 90, 21, 12395–12403](https://pubs.acs.org/doi/abs/10.1021/acs.analchem.8b02884). This plugin is a work-in-progress but is mostly functional.\\n\\n__N.B.__ This tool is __NOT__ a general purpose registration framework to find transforms between IMS (MALDI or otherwise)\\nand microscopy. It is built to align MALDI IMS pixels to their corresponding laser ablation marks as captured by microscopy AFTER the IMS experiment. \\nThis approach has the advantage of providing direct evidence of registration performance as IMS pixels are aligned \\nto their _explicit spatial origin_ in microscopy space, improving overall accuracy and confidence of microscopy-driven IMS \\ndata analysis.\\n\\n## Installation\\n\\nYou can install `napari-imsmicrolink` via [pip]:\\n\\n    pip install napari-imsmicrolink\\n\\n### Typical experiment workflow\\n1. Acquire pre-IMS microscopy (autofluorescence, brightfield) - _optional_\\n2. Perform normal IMS sample preparation.\\n3. Acquire post-IMS microscopy (autofluorescence, brightfield) with matrix still on sample\\nthat reveals laser ablation marks.\\n\\n4. Gather IMS data that contains XY integer coordinates for the IMS experiment\\n   (.imzML, Bruker spotlist (.txt, .csv), Bruker peaks.sqlite (_FTICR_),\\n   Bruker .tsf (TIMS qTOF only))\\n\\n5. Run `napari-imsmicrolink` with data 3 and 4\\n\\n6. Once registered, use `wsireg` to align other microscopy modalities to IMS-registered post-IMS\\nmicroscopy\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [MIT] license,\\n\\\"napari-imsmicrolink\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/nhpatterson/napari-imsmicrolink/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-imsmicrolink\\n\\n\\n\\n\\n\\nnapari plugin to perform MALDI IMS - microscopy registration using laser ablation marks as described in Anal. Chem. 2018, 90, 21, 12395–12403. This plugin is a work-in-progress but is mostly functional.\\nN.B. This tool is NOT a general purpose registration framework to find transforms between IMS (MALDI or otherwise)\\nand microscopy. It is built to align MALDI IMS pixels to their corresponding laser ablation marks as captured by microscopy AFTER the IMS experiment. \\nThis approach has the advantage of providing direct evidence of registration performance as IMS pixels are aligned \\nto their explicit spatial origin in microscopy space, improving overall accuracy and confidence of microscopy-driven IMS \\ndata analysis.\\nInstallation\\nYou can install napari-imsmicrolink via pip:\\npip install napari-imsmicrolink\\n\\nTypical experiment workflow\\n\\nAcquire pre-IMS microscopy (autofluorescence, brightfield) - optional\\nPerform normal IMS sample preparation.\\n\\nAcquire post-IMS microscopy (autofluorescence, brightfield) with matrix still on sample\\nthat reveals laser ablation marks.\\n\\n\\nGather IMS data that contains XY integer coordinates for the IMS experiment\\n   (.imzML, Bruker spotlist (.txt, .csv), Bruker peaks.sqlite (FTICR),\\n   Bruker .tsf (TIMS qTOF only))\\n\\n\\nRun napari-imsmicrolink with data 3 and 4\\n\\n\\nOnce registered, use wsireg to align other microscopy modalities to IMS-registered post-IMS\\nmicroscopy\\n\\n\\n\\nThis napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the MIT license,\\n\\\"napari-imsmicrolink\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 3 - Alpha\"],\"display_name\":\"napari-imsmicrolink\",\"documentation\":\"https://napari-imsmicrolink.readthedocs.io/en/latest/\",\"first_released\":\"2021-12-14T20:51:38.664548Z\",\"license\":\"MIT\",\"name\":\"napari-imsmicrolink\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://napari-imsmicrolink.readthedocs.io/en/latest/\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-11-18T21:33:33.140168Z\",\"report_issues\":\"https://github.com/nhpatterson/napari-imsmicrolink/issues\",\"requirements\":[\"numpy\",\"tifffile\",\"dask\",\"zarr (>=2.10.3)\",\"qtpy\",\"aicsimageio[bioformats]\",\"bioformats-jar\",\"SimpleITK\",\"pandas\",\"h5py\",\"opencv-python\",\"czifile\",\"imagecodecs\"],\"summary\":\"Plugin to perform IMS to microscopy registration using laser ablation marks.\",\"support\":\"https://github.com/nhpatterson/napari-imsmicrolink/issues\",\"twitter\":\"https://twitter.com/nheathpatterson/\",\"version\":\"0.1.8\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"matt.mccormick@kitware.com\",\"name\":\"Matt McCormick\"}],\"code_repository\":\"https://github.com/InsightSoftwareConsortium/napari-itk-io\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-itk-io\"}],\"description\":\"# napari-itk-io\\n\\n[![License](https://img.shields.io/pypi/l/napari-itk-io.svg?color=green)](https://github.com/InsightSoftwareConsortium/napari-itk-io/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-itk-io.svg?color=green)](https://pypi.org/project/napari-itk-io)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-itk-io.svg?color=green)](https://python.org)\\n[![tests](https://github.com/InsightSoftwareConsortium/napari-itk-io/workflows/tests/badge.svg)](https://github.com/InsightSoftwareConsortium/napari-itk-io/actions)\\n[![codecov](https://codecov.io/gh/InsightSoftwareConsortium/napari-itk-io/branch/master/graph/badge.svg)](https://codecov.io/gh/InsightSoftwareConsortium/napari-itk-io)\\n\\nFile IO with [itk](https://itk.org) for [napari](https://napari.org).\\n\\nImage metadata, e.g. the pixel spacing, origin, and metadata tags, are preserved and passed into napari.\\n\\nSupported image file formats:\\n\\n- [BioRad](http://www.bio-rad.com/)\\n- [BMP](https://en.wikipedia.org/wiki/BMP_file_format)\\n- [DICOM](http://dicom.nema.org/)\\n- [DICOM Series](http://dicom.nema.org/)\\n- [ITK HDF5](https://support.hdfgroup.org/HDF5/)\\n- [JPEG](https://en.wikipedia.org/wiki/JPEG_File_Interchange_Format)\\n- [GE4,GE5,GEAdw](http://www3.gehealthcare.com)\\n- [Gipl (Guys Image Processing Lab)](https://www.ncbi.nlm.nih.gov/pubmed/12956259)\\n- [LSM](http://www.openwetware.org/wiki/Dissecting_LSM_files)\\n- [MetaImage](https://itk.org/Wiki/ITK/MetaIO/Documentation)\\n- [MINC 2.0](https://en.wikibooks.org/wiki/MINC/SoftwareDevelopment/MINC2.0_File_Format_Reference)\\n- [MGH](https://surfer.nmr.mgh.harvard.edu/fswiki/FsTutorial/MghFormat)\\n- [MRC](http://www.ccpem.ac.uk/mrc_format/mrc_format.php)\\n- [NifTi](https://nifti.nimh.nih.gov/nifti-1)\\n- [NRRD](http://teem.sourceforge.net/nrrd/format.html)\\n- [Portable Network Graphics (PNG)](https://en.wikipedia.org/wiki/Portable_Network_Graphics)\\n- [Tagged Image File Format (TIFF)](https://en.wikipedia.org/wiki/TIFF)\\n- [VTK legacy file format for images](http://www.vtk.org/VTK/img/file-formats.pdf)\\n\\nFor DICOM Series, select the folder containing the series with *File -> Open\\nFolder...*. The first series will be selected and sorted spatially.\\n\\n## Installation\\n\\nYou can install `napari-itk-io` via [pip]:\\n\\n    pip install napari-itk-io\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\nFollow the [itk contributing\\nguidelines](https://github.com/InsightSoftwareConsortium/ITK/blob/master/CONTRIBUTING.md)\\nand the [itk code of\\nconduct](https://github.com/InsightSoftwareConsortium/ITK/blob/master/CODE_OF_CONDUCT.md).\\n\\n## License\\n\\nDistributed under the terms of the [Apache Software License 2.0] license,\\n\\\"napari-itk-io\\\" is free and open source software.\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/InsightSoftwareConsortium/napari-itk-io/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-itk-io\\n\\n\\n\\n\\n\\nFile IO with itk for napari.\\nImage metadata, e.g. the pixel spacing, origin, and metadata tags, are preserved and passed into napari.\\nSupported image file formats:\\n\\nBioRad\\nBMP\\nDICOM\\nDICOM Series\\nITK HDF5\\nJPEG\\nGE4,GE5,GEAdw\\nGipl (Guys Image Processing Lab)\\nLSM\\nMetaImage\\nMINC 2.0\\nMGH\\nMRC\\nNifTi\\nNRRD\\nPortable Network Graphics (PNG)\\nTagged Image File Format (TIFF)\\nVTK legacy file format for images\\n\\nFor DICOM Series, select the folder containing the series with File -> Open\\nFolder.... The first series will be selected and sorted spatially.\\nInstallation\\nYou can install napari-itk-io via pip:\\npip install napari-itk-io\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nFollow the itk contributing\\nguidelines\\nand the itk code of\\nconduct.\\nLicense\\nDistributed under the terms of the Apache Software License 2.0 license,\\n\\\"napari-itk-io\\\" is free and open source software.\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"napari-itk-io\",\"documentation\":\"\",\"first_released\":\"2021-04-28T22:49:15.780944Z\",\"license\":\"Apache-2.0\",\"name\":\"napari-itk-io\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\",\"writer\"],\"project_site\":\"https://github.com/InsightSoftwareConsortium/napari-itk-io\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[\"*\"],\"release_date\":\"2022-03-28T13:57:53.681568Z\",\"report_issues\":\"\",\"requirements\":[\"numpy\",\"napari-plugin-engine (>=0.2.0)\",\"itk-io (>=5.2.0)\",\"itk-napari-conversion\"],\"summary\":\"File IO with itk for napari\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.2.0\",\"visibility\":\"public\",\"writer_file_extensions\":[\".nii\"],\"writer_save_layers\":[\"labels\",\"image\"]}",
  "{\"authors\":[{\"name\":\"Oliver Umney\"}],\"code_repository\":\"https://github.com/oubino/napari-locpix\",\"description\":\"# napari-locpix\\n\\n[![License MIT](https://img.shields.io/pypi/l/napari-locpix.svg?color=green)](https://github.com/oubino/napari-locpix/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-locpix.svg?color=green)](https://pypi.org/project/napari-locpix)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-locpix.svg?color=green)](https://python.org)\\n[![tests](https://github.com/oubino/napari-locpix/workflows/tests/badge.svg)](https://github.com/oubino/napari-locpix/actions)\\n[![codecov](https://codecov.io/gh/oubino/napari-locpix/branch/main/graph/badge.svg)](https://codecov.io/gh/oubino/napari-locpix)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-locpix)](https://napari-hub.org/plugins/napari-locpix)\\n\\nLoad in SMLM data and annotate within napari\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/stable/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-locpix` via [pip]:\\n\\n    pip install napari-locpix\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/oubino/napari-locpix.git\\n\\n\\n## Usage\\n\\nThis plugin allows a user to \\n\\n1. Read in SMLM data\\n2. Visualise SMLM data in a histogram\\n3. Add segmentations to the data\\n4. Extract the underlying localisations from the segmentations\\n\\n## IO\\n\\nThe input data can be in the form of a .csv or .parquet.\\n\\nWe expect there to be 4 columns at least:\\n\\n* X coordinate\\n* Y coordinate\\n* Frame\\n* Channel\\n\\nIf the data has been annotated with this software we can also load this in.\\nNote however we currently only support loading in annotated data saved as a .parquet folder.\\nTherefore, we recommend always keeping a .parquet copy until loading in an annotated .csv\\nis supported.\\n\\nThe data can be outputted to a .parquet and a .csv\\n\\nThis includes the annotated data.\\n\\n## Visualisation\\n\\nUsing the render button you can render the loaded in data\\n\\n## Segmentation\\n\\nSegmentations can be added using Napari's viewer.\\n\\nSimply click the add Labels.\\n\\nNote that this software will expect the labels to be called \\\"Labels\\\"\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [MIT] license,\\n\\\"napari-locpix\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/oubino/napari-locpix/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-locpix\\n\\n\\n\\n\\n\\n\\nLoad in SMLM data and annotate within napari\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-locpix via pip:\\npip install napari-locpix\\n\\nTo install latest development version :\\npip install git+https://github.com/oubino/napari-locpix.git\\n\\nUsage\\nThis plugin allows a user to \\n\\nRead in SMLM data\\nVisualise SMLM data in a histogram\\nAdd segmentations to the data\\nExtract the underlying localisations from the segmentations\\n\\nIO\\nThe input data can be in the form of a .csv or .parquet.\\nWe expect there to be 4 columns at least:\\n\\nX coordinate\\nY coordinate\\nFrame\\nChannel\\n\\nIf the data has been annotated with this software we can also load this in.\\nNote however we currently only support loading in annotated data saved as a .parquet folder.\\nTherefore, we recommend always keeping a .parquet copy until loading in an annotated .csv\\nis supported.\\nThe data can be outputted to a .parquet and a .csv\\nThis includes the annotated data.\\nVisualisation\\nUsing the render button you can render the loaded in data\\nSegmentation\\nSegmentations can be added using Napari's viewer.\\nSimply click the add Labels.\\nNote that this software will expect the labels to be called \\\"Labels\\\"\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the MIT license,\\n\\\"napari-locpix\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari-locpix\",\"documentation\":\"https://github.com/oubino/napari-locpix#README.md\",\"first_released\":\"2023-01-16T21:22:07.765064Z\",\"license\":\"MIT\",\"name\":\"napari-locpix\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\",\"sample_data\"],\"project_site\":\"https://github.com/oubino/napari-locpix\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2023-01-23T13:14:37.166855Z\",\"report_issues\":\"https://github.com/oubino/napari-locpix/issues\",\"requirements\":[\"numpy\",\"qtpy\",\"polars\",\"pyarrow\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\"],\"summary\":\"Load in SMLM data and annotate within napari\",\"support\":\"https://github.com/oubino/napari-locpix/issues\",\"twitter\":\"\",\"version\":\"0.0.3\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"nick.schaub@nih.gov\",\"name\":\"Nick Schaub\"}],\"code_repository\":\"https://github.com/labshare/bfio\",\"conda\":[],\"description\":\"# **B**io**F**ormats **I**nput/**O**utput utility (bfio 2.3.0)\\n\\n[![Documentation Status](https://readthedocs.org/projects/bfio/badge/?version=latest)](https://bfio.readthedocs.io/en/latest/?badge=latest)\\n[![PyPI](https://img.shields.io/pypi/v/bfio)](https://pypi.org/project/filepattern/)\\n![PyPI - Downloads](https://img.shields.io/pypi/dm/bfio)\\n![Bower](https://img.shields.io/bower/l/MI)\\n\\nThis tool is a simplified but powerful interface to\\n[Bioformats](https://www.openmicroscopy.org/bio-formats/)\\nusing jpype for direct access to the library. This tool is designed with\\nscalable image analysis in mind, with a simple interface to treat any image\\nlike a memory mapped array.\\n\\nDocker containers with all necessary components are available (see\\n**Docker Containers** section).\\n\\n## Summary\\n\\n- [Installation](#installation)\\n- [Docker](#docker)\\n- [Documentation](#documentation)\\n- [Contributing](#contributing)\\n- [Versioning](#versioning)\\n- [Authors](#authors)\\n- [License](#license)\\n- [Acknowledgments](#acknowledgments)\\n\\n## Installation\\n\\n### Setting up Java\\n\\n**Note:** `bfio` can be used without Java, but only the `python` and `zarr`\\nbackends will be useable. Only files in tiled OME Tiff or OME Zarr format can be\\nread/written.\\n\\nIn order to use the `Java` backend, it is necessary to first install the JDK.\\nThe `bfio` package is generally tested with\\n[JDK 8](https://docs.oracle.com/javase/8/docs/technotes/guides/install/install_overview.html),\\nbut JDK 11 and later also appear to work.\\n\\n### Installing bfio\\n\\nThe `bfio` package and the core dependencies (numpy, tifffile, imagecodecs) can\\nbe installed using pip:\\n\\n`pip install bfio`\\n\\nAdditionally, `bfio` with other dependencies can be installed:\\n\\n1. `pip install bfio[bioformats]` - Adds support for BioFormats/Java. See [License](#license) for additional information.\\n2. `pip install bfio[zarr]` - Adds support for OME Zarr\\n3. `pip install bfio[all]` - Installs all dependencies.\\n\\n## Docker\\n\\n### labshare/polus-bfio-util:2.3.0\\n\\nUbuntu based container with bfio and all dependencies (including Java).\\n\\n### labshare/polus-bfio-util:2.3.0-imagej\\n\\nSame as above, except comes with ImageJ and PyImageJ.\\n\\n### labshare/polus-bfio-util:2.3.0-tensorflow\\n\\nTensorflow container with bfio isntalled.\\n\\n## Documentation\\n\\nDocumentation and examples are available on\\n[Read the Docs](https://bfio.readthedocs.io/en/latest/).\\n\\n## Versioning\\n\\nWe use [SemVer](http://semver.org/) for versioning. For the versions\\navailable, see the [tags on this\\nrepository](https://github.com/PurpleBooth/a-good-readme-template/tags).\\n\\n## Authors\\n\\nNick Schaub (nick.schaub@nih.gov, nick.schaub@labshare.org)\\n\\n## License\\n\\nThis project is licensed under the [MIT License](LICENSE)\\nCreative Commons License - see the [LICENSE](LICENSE) file for details.\\n\\n**NOTE**\\n\\nBioformats is licensed under GPL, and as a consequence so is the `bioformats_jar` \\npackage. These packages and libraries are installed when using the `bfio[bioformats]` option.\\n\\n## Acknowledgments\\n\\n- Parts of this code were written/modified from existing code found in\\n    `tifffile`.\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"BioFormats Input/Output utility (bfio 2.3.0)\\n\\n\\n\\n\\nThis tool is a simplified but powerful interface to\\nBioformats\\nusing jpype for direct access to the library. This tool is designed with\\nscalable image analysis in mind, with a simple interface to treat any image\\nlike a memory mapped array.\\nDocker containers with all necessary components are available (see\\nDocker Containers section).\\nSummary\\n\\nInstallation\\nDocker\\nDocumentation\\nContributing\\nVersioning\\nAuthors\\nLicense\\nAcknowledgments\\n\\nInstallation\\nSetting up Java\\nNote: bfio can be used without Java, but only the python and zarr\\nbackends will be useable. Only files in tiled OME Tiff or OME Zarr format can be\\nread/written.\\nIn order to use the Java backend, it is necessary to first install the JDK.\\nThe bfio package is generally tested with\\nJDK 8,\\nbut JDK 11 and later also appear to work.\\nInstalling bfio\\nThe bfio package and the core dependencies (numpy, tifffile, imagecodecs) can\\nbe installed using pip:\\npip install bfio\\nAdditionally, bfio with other dependencies can be installed:\\n\\npip install bfio[bioformats] - Adds support for BioFormats/Java. See License for additional information.\\npip install bfio[zarr] - Adds support for OME Zarr\\npip install bfio[all] - Installs all dependencies.\\n\\nDocker\\nlabshare/polus-bfio-util:2.3.0\\nUbuntu based container with bfio and all dependencies (including Java).\\nlabshare/polus-bfio-util:2.3.0-imagej\\nSame as above, except comes with ImageJ and PyImageJ.\\nlabshare/polus-bfio-util:2.3.0-tensorflow\\nTensorflow container with bfio isntalled.\\nDocumentation\\nDocumentation and examples are available on\\nRead the Docs.\\nVersioning\\nWe use SemVer for versioning. For the versions\\navailable, see the tags on this\\nrepository.\\nAuthors\\nNick Schaub (nick.schaub@nih.gov, nick.schaub@labshare.org)\\nLicense\\nThis project is licensed under the MIT License\\nCreative Commons License - see the LICENSE file for details.\\nNOTE\\nBioformats is licensed under GPL, and as a consequence so is the bioformats_jar \\npackage. These packages and libraries are installed when using the bfio[bioformats] option.\\nAcknowledgments\\n\\nParts of this code were written/modified from existing code found in\\n    tifffile.\\n\",\"development_status\":[\"Development Status :: 5 - Production/Stable\"],\"display_name\":\"bfio\",\"documentation\":\"https://bfio.readthedocs.io/en/latest/\",\"first_released\":\"2020-07-22T21:02:33.612752Z\",\"license\":\"MIT\",\"name\":\"bfio\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\",\"writer\"],\"project_site\":\"\",\"python_version\":\">=3.6\",\"reader_file_extensions\":[\"*\"],\"release_date\":\"2022-04-07T18:49:53.674962Z\",\"report_issues\":\"\",\"requirements\":[\"tifffile (>=2021.8.30)\",\"imagecodecs (>=2021.2.26)\",\"numpy (>=1.20.1)\",\"ome-types (>=0.2.10)\",\"lxml\",\"zarr (>=2.6.1) ; extra == 'all'\",\"bioformats-jar (==6.7.0.post2) ; extra == 'all'\",\"bioformats-jar (==6.7.0.post2) ; extra == 'bioformats'\",\"zarr (>=2.6.1) ; extra == 'dev'\",\"requests (>=2.26.0) ; extra == 'dev'\",\"bioformats-jar (==6.7.0.post2) ; extra == 'dev'\",\"zarr (>=2.6.1) ; extra == 'zarr'\"],\"summary\":\"Simple reading and writing classes for tiled tiffs using Bioformats.\",\"support\":\"\",\"twitter\":\"\",\"version\":\"2.3.0\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[\"image\"]}",
  "{\"authors\":[{\"name\":\"Adam Tyson\"}],\"category\":{\"Image modality\":[\"Multi-photon microscopy\",\"Fluorescence microscopy\"],\"Supported data\":[\"3D\"],\"Workflow step\":[\"Image feature detection\",\"Image registration\",\"Image Segmentation\",\"Image annotation\",\"Object classification\",\"Object feature extraction\"]},\"category_hierarchy\":{\"Image modality\":[[\"Multi-photon microscopy\"],[\"Fluorescence microscopy\",\"Light-sheet microscopy\"]],\"Supported data\":[[\"3D\"]],\"Workflow step\":[[\"Image feature detection\"],[\"Image registration\"],[\"Image Segmentation\"],[\"Image Segmentation\",\"Image thresholding\"],[\"Image annotation\"],[\"Object classification\"],[\"Object feature extraction\"]]},\"code_repository\":\"https://github.com/brainglobe/cellfinder-napari\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"cellfinder-napari\"}],\"description\":\"# cellfinder-napari\\n\\n[![License](https://img.shields.io/pypi/l/cellfinder-napari.svg?color=green)](https://github.com/napari/cellfinder-napari/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/cellfinder-napari.svg?color=green)](https://pypi.org/project/cellfinder-napari)\\n[![Python Version](https://img.shields.io/pypi/pyversions/cellfinder-napari.svg?color=green)](https://python.org)\\n[![tests](https://github.com/brainglobe/cellfinder-napari/workflows/tests/badge.svg)](https://github.com/brainglobe/cellfinder-napari/actions)\\n[![codecov](https://codecov.io/gh/brainglobe/cellfinder-napari/branch/main/graph/badge.svg?token=C4uzd0cm2u)](https://codecov.io/gh/brainglobe/cellfinder-napari)\\n[![Downloads](https://pepy.tech/badge/cellfinder-napari)](https://pepy.tech/project/cellfinder-napari)\\n[![Wheel](https://img.shields.io/pypi/wheel/cellfinder.svg)](https://pypi.org/project/cellfinder)\\n[![Development Status](https://img.shields.io/pypi/status/cellfinder-napari.svg)](https://github.com/brainglobe/cellfinder-napari)\\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/python/black)\\n[![Imports: isort](https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat&labelColor=ef8336)](https://pycqa.github.io/isort/)\\n[![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&logoColor=white)](https://github.com/pre-commit/pre-commit)\\n[![Contributions](https://img.shields.io/badge/Contributions-Welcome-brightgreen.svg)](https://docs.brainglobe.info/cellfinder/contributing)\\n[![Website](https://img.shields.io/website?up_message=online&url=https%3A%2F%2Fbrainglobe.info/cellfinder)](https://brainglobe.info/cellfinder)\\n[![Twitter](https://img.shields.io/twitter/follow/brain_globe?style=social)](https://twitter.com/brain_globe)\\n\\n### Efficient cell detection in large images (e.g. whole mouse brain images)\\n\\n`cellfinder-napari` is a front-end to [cellfinder-core](https://github.com/brainglobe/cellfinder-core) to allow ease of use within the [napari](https://napari.org/index.html) multidimensional image viewer. For more details on this approach, please see [Tyson, Rousseau & Niedworok et al. (2021)](https://doi.org/10.1371/journal.pcbi.1009074). This algorithm can also be used within the original\\n[cellfinder](https://github.com/brainglobe/cellfinder) software for\\nwhole-brain microscopy analysis.\\n\\n`cellfinder-napari`, `cellfinder` and `cellfinder-core` were developed by [Charly Rousseau](https://github.com/crousseau) and [Adam Tyson](https://github.com/adamltyson) in the [Margrie Lab](https://www.sainsburywellcome.org/web/groups/margrie-lab), based on previous work by [Christian Niedworok](https://github.com/cniedwor), generously supported by the [Sainsbury Wellcome Centre](https://www.sainsburywellcome.org/web/).\\n\\n----\\n![raw](https://raw.githubusercontent.com/brainglobe/cellfinder-napari/master/resources/cellfinder-napari.gif)\\n\\n**Visualising detected cells in the cellfinder napari plugin**\\n\\n----\\n## Instructions\\n\\n### Installation\\nOnce you have [installed napari](https://napari.org/index.html#installation).\\nYou can install napari either through the napari plugin installation tool, or\\ndirectly from PyPI with:\\n```bash\\npip install cellfinder-napari\\n```\\n\\n### Usage\\nFull documentation can be\\nfound [here](https://docs.brainglobe.info/cellfinder-napari).\\n\\nThis software is at a very early stage, and was written with our data in mind.\\nOver time we hope to support other data types/formats. If you have any\\nquestions or issues, please get in touch [on the forum](https://forum.image.sc/tag/brainglobe) or by\\n[raising an issue](https://github.com/brainglobe/cellfinder-napari/issues).\\n\\n\\n---\\n## Illustration\\n\\n### Introduction\\ncellfinder takes a stitched, but otherwise raw dataset with at least\\ntwo channels:\\n * Background channel (i.e. autofluorescence)\\n * Signal channel, the one with the cells to be detected:\\n\\n![raw](https://raw.githubusercontent.com/brainglobe/cellfinder/master/resources/raw.png)\\n**Raw coronal serial two-photon mouse brain image showing labelled cells**\\n\\n\\n### Cell candidate detection\\nClassical image analysis (e.g. filters, thresholding) is used to find\\ncell-like objects (with false positives):\\n\\n![raw](https://raw.githubusercontent.com/brainglobe/cellfinder/master/resources/detect.png)\\n**Candidate cells (including many artefacts)**\\n\\n\\n### Cell candidate classification\\nA deep-learning network (ResNet) is used to classify cell candidates as true\\ncells or artefacts:\\n\\n![raw](https://raw.githubusercontent.com/brainglobe/cellfinder/master/resources/classify.png)\\n**Cassified cell candidates. Yellow - cells, Blue - artefacts**\\n\\n## Contributing\\nContributions to cellfinder-napari are more than welcome. Please see the [contributing guide](https://github.com/brainglobe/.github/blob/main/CONTRIBUTING.md).\\n\\n## Citing cellfinder\\n\\nIf you find this plugin useful, and use it in your research, please cite the preprint outlining the cell detection algorithm:\\n> Tyson, A. L., Rousseau, C. V., Niedworok, C. J., Keshavarzi, S., Tsitoura, C., Cossell, L., Strom, M. and Margrie, T. W. (2021) “A deep learning algorithm for 3D cell detection in whole mouse brain image datasets’ PLOS Computational Biology, 17(5), e1009074\\n[https://doi.org/10.1371/journal.pcbi.1009074](https://doi.org/10.1371/journal.pcbi.1009074)\\n\\n\\n**If you use this, or any other tools in the brainglobe suite, please\\n [let us know](mailto:code@adamltyson.com?subject=cellfinder-napari), and\\n we'd be happy to promote your paper/talk etc.**\\n\\n---\\nThe BrainGlobe project is generously supported by the Sainsbury Wellcome Centre and the Institute of Neuroscience, Technical University of Munich, with funding from Wellcome, the Gatsby Charitable Foundation and the Munich Cluster for Systems Neurology - Synergy.\\n\\n<img src='https://brainglobe.info/images/logos_combined.png' width=\\\"550\\\">\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"cellfinder-napari\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEfficient cell detection in large images (e.g. whole mouse brain images)\\ncellfinder-napari is a front-end to cellfinder-core to allow ease of use within the napari multidimensional image viewer. For more details on this approach, please see Tyson, Rousseau & Niedworok et al. (2021). This algorithm can also be used within the original\\ncellfinder software for\\nwhole-brain microscopy analysis.\\ncellfinder-napari, cellfinder and cellfinder-core were developed by Charly Rousseau and Adam Tyson in the Margrie Lab, based on previous work by Christian Niedworok, generously supported by the Sainsbury Wellcome Centre.\\n\\n\\nVisualising detected cells in the cellfinder napari plugin\\n\\nInstructions\\nInstallation\\nOnce you have installed napari.\\nYou can install napari either through the napari plugin installation tool, or\\ndirectly from PyPI with:\\nbash\\npip install cellfinder-napari\\nUsage\\nFull documentation can be\\nfound here.\\nThis software is at a very early stage, and was written with our data in mind.\\nOver time we hope to support other data types/formats. If you have any\\nquestions or issues, please get in touch on the forum or by\\nraising an issue.\\n\\nIllustration\\nIntroduction\\ncellfinder takes a stitched, but otherwise raw dataset with at least\\ntwo channels:\\n * Background channel (i.e. autofluorescence)\\n * Signal channel, the one with the cells to be detected:\\n\\nRaw coronal serial two-photon mouse brain image showing labelled cells\\nCell candidate detection\\nClassical image analysis (e.g. filters, thresholding) is used to find\\ncell-like objects (with false positives):\\n\\nCandidate cells (including many artefacts)\\nCell candidate classification\\nA deep-learning network (ResNet) is used to classify cell candidates as true\\ncells or artefacts:\\n\\nCassified cell candidates. Yellow - cells, Blue - artefacts\\nContributing\\nContributions to cellfinder-napari are more than welcome. Please see the contributing guide.\\nCiting cellfinder\\nIf you find this plugin useful, and use it in your research, please cite the preprint outlining the cell detection algorithm:\\n\\nTyson, A. L., Rousseau, C. V., Niedworok, C. J., Keshavarzi, S., Tsitoura, C., Cossell, L., Strom, M. and Margrie, T. W. (2021) “A deep learning algorithm for 3D cell detection in whole mouse brain image datasets’ PLOS Computational Biology, 17(5), e1009074\\nhttps://doi.org/10.1371/journal.pcbi.1009074\\n\\nIf you use this, or any other tools in the brainglobe suite, please\\n let us know, and\\n we'd be happy to promote your paper/talk etc.\\n\\nThe BrainGlobe project is generously supported by the Sainsbury Wellcome Centre and the Institute of Neuroscience, Technical University of Munich, with funding from Wellcome, the Gatsby Charitable Foundation and the Munich Cluster for Systems Neurology - Synergy.\\n\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"cellfinder-napari\",\"documentation\":\"https://docs.brainglobe.info/cellfinder-napari/\",\"first_released\":\"2021-01-25T16:40:18.651958Z\",\"license\":\"BSD-3-Clause\",\"name\":\"cellfinder-napari\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\",\"sample_data\"],\"project_site\":\"https://brainglobe.info/cellfinder\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-10-21T15:12:05.089449Z\",\"report_issues\":\"https://github.com/brainglobe/cellfinder-napari/issues\",\"requirements\":[\"napari\",\"napari-plugin-engine (>=0.1.4)\",\"napari-ndtiffs\",\"brainglobe-napari-io\",\"cellfinder-core (>=0.3)\",\"pooch (>=1)\",\"black ; extra == 'dev'\",\"bump2version ; extra == 'dev'\",\"gitpython ; extra == 'dev'\",\"pre-commit ; extra == 'dev'\",\"pytest ; extra == 'dev'\",\"pytest-cov ; extra == 'dev'\",\"pytest-qt ; extra == 'dev'\"],\"summary\":\"Efficient cell detection in large images\",\"support\":\"https://forum.image.sc/tag/brainglobe\",\"twitter\":\"\",\"version\":\"0.0.20\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Aurelien Maillot\"}],\"code_repository\":\"https://github.com/AurelienMaillot/napari-bud-cell-segmenter\",\"description\":\"# napari-bud-cell-segmenter\\n\\n[![License BSD-3](https://img.shields.io/pypi/l/napari-bud-cell-segmenter.svg?color=green)](https://github.com/AurelienMaillot/napari-bud-cell-segmenter/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-bud-cell-segmenter.svg?color=green)](https://pypi.org/project/napari-bud-cell-segmenter)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-bud-cell-segmenter.svg?color=green)](https://python.org)\\n[![tests](https://github.com/AurelienMaillot/napari-bud-cell-segmenter/workflows/tests/badge.svg)](https://github.com/AurelienMaillot/napari-bud-cell-segmenter/actions)\\n[![codecov](https://codecov.io/gh/AurelienMaillot/napari-bud-cell-segmenter/branch/main/graph/badge.svg)](https://codecov.io/gh/AurelienMaillot/napari-bud-cell-segmenter)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-bud-cell-segmenter)](https://napari-hub.org/plugins/napari-bud-cell-segmenter)\\n\\nA plugin to segment embryonic mammary bud cells and detect 2 RNA probes\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/stable/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-bud-cell-segmenter` via [pip]:\\n\\n    pip install napari-bud-cell-segmenter\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/AurelienMaillot/napari-bud-cell-segmenter.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-bud-cell-segmenter\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/AurelienMaillot/napari-bud-cell-segmenter/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-bud-cell-segmenter\\n\\n\\n\\n\\n\\n\\nA plugin to segment embryonic mammary bud cells and detect 2 RNA probes\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-bud-cell-segmenter via pip:\\npip install napari-bud-cell-segmenter\\n\\nTo install latest development version :\\npip install git+https://github.com/AurelienMaillot/napari-bud-cell-segmenter.git\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-bud-cell-segmenter\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"Bud Cell Segmenter\",\"documentation\":\"https://github.com/AurelienMaillot/napari-bud-cell-segmenter#README.md\",\"first_released\":\"2022-11-14T14:07:34.318509Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-bud-cell-segmenter\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\",\"writer\",\"widget\",\"sample_data\"],\"project_site\":\"https://github.com/AurelienMaillot/napari-bud-cell-segmenter\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[\"*.npy\"],\"release_date\":\"2022-12-16T14:31:02.717645Z\",\"report_issues\":\"https://github.com/AurelienMaillot/napari-bud-cell-segmenter/issues\",\"requirements\":[\"numpy\",\"magicgui\",\"qtpy\",\"pandas\",\"scikit-image\",\"napari\",\"tifffile\",\"matplotlib\",\"scipy\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\"],\"summary\":\"A plugin to segment embryonic mammary bud cells and detect 2 RNA probes\",\"support\":\"https://github.com/AurelienMaillot/napari-bud-cell-segmenter/issues\",\"twitter\":\"\",\"version\":\"0.1.4\",\"visibility\":\"public\",\"writer_file_extensions\":[\".npy\"],\"writer_save_layers\":[\"labels\",\"image\"]}",
  "{\"authors\":[{\"name\":\"Bastian Eichenberger\"}],\"code_repository\":\"https://github.com/bbquercus/koopa\",\"description\":\"[![License MIT](https://img.shields.io/pypi/l/koopa-viz.svg?color=green)](https://github.com/bbquercus/koopa/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/koopa-viz.svg?color=green)](https://pypi.org/project/koopa-viz)\\n[![Python Version](https://img.shields.io/pypi/pyversions/koopa-viz.svg?color=green)](https://python.org)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/koopa-viz)](https://napari-hub.org/plugins/koopa-viz)\\n\\n# koopa-viz\\n\\nVizualization plugin for koopa image analysis\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\nMore information can be found on the official [GitHub repo].\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[GitHub repo]: https://github.com/bbquercus/koopa\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[file an issue]: https://github.com/bbquercus/koopa/issues\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"\\n\\n\\n\\nkoopa-viz\\nVizualization plugin for koopa image analysis\\nThis napari plugin was generated with Cookiecutter using @napari's [cookiecutter-napari-plugin] template.\\nMore information can be found on the official GitHub repo.\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"Koopa\",\"documentation\":\"https://github.com/bbquercus/koopa#README.md\",\"first_released\":\"2022-09-29T14:00:59.874419Z\",\"license\":\"MIT\",\"name\":\"koopa-viz\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/bbquercus/koopa\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2023-01-31T10:26:26.350714Z\",\"report_issues\":\"https://github.com/bbquercus/koopa/issues\",\"requirements\":null,\"summary\":\"Vizualization plugin for koopa image analysis\",\"support\":\"https://github.com/bbquercus/koopa/issues\",\"twitter\":\"\",\"version\":\"0.0.3\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Niklas Netter\"}],\"code_repository\":\"https://github.com/gatoniel/napari-merge-stardist-masks\",\"description\":\"# napari-merge-stardist-masks\\n\\n[![License BSD-3](https://img.shields.io/pypi/l/napari-merge-stardist-masks.svg?color=green)](https://github.com/gatoniel/napari-merge-stardist-masks/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-merge-stardist-masks.svg?color=green)](https://pypi.org/project/napari-merge-stardist-masks)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-merge-stardist-masks.svg?color=green)](https://python.org)\\n[![tests](https://github.com/gatoniel/napari-merge-stardist-masks/workflows/tests/badge.svg)](https://github.com/gatoniel/napari-merge-stardist-masks/actions)\\n[![codecov](https://codecov.io/gh/gatoniel/napari-merge-stardist-masks/branch/main/graph/badge.svg)](https://codecov.io/gh/gatoniel/napari-merge-stardist-masks)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-merge-stardist-masks)](https://napari-hub.org/plugins/napari-merge-stardist-masks)\\n\\nSegment non-star-convex objects with StarDist by merging masks.\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/stable/plugins/index.html\\n-->\\n\\n\\n## Run\\n\\nIn PowerShell, when you do not have sufficient GPU support, run napari without CUDA support, i.e.,:\\n```\\n$env:CUDA_VISIBLE_DEVICES=-1; napari\\n```\\n\\n\\n## Installation\\n\\nYou can install `napari-merge-stardist-masks` via [pip]:\\n\\n    pip install napari-merge-stardist-masks\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/gatoniel/napari-merge-stardist-masks.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-merge-stardist-masks\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/gatoniel/napari-merge-stardist-masks/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-merge-stardist-masks\\n\\n\\n\\n\\n\\n\\nSegment non-star-convex objects with StarDist by merging masks.\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nRun\\nIn PowerShell, when you do not have sufficient GPU support, run napari without CUDA support, i.e.,:\\n$env:CUDA_VISIBLE_DEVICES=-1; napari\\nInstallation\\nYou can install napari-merge-stardist-masks via pip:\\npip install napari-merge-stardist-masks\\n\\nTo install latest development version :\\npip install git+https://github.com/gatoniel/napari-merge-stardist-masks.git\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-merge-stardist-masks\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"Merge StarDist Masks\",\"documentation\":\"https://github.com/gatoniel/napari-merge-stardist-masks#README.md\",\"first_released\":\"2022-08-31T20:01:16.713372Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-merge-stardist-masks\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\",\"sample_data\"],\"project_site\":\"https://github.com/gatoniel/napari-merge-stardist-masks\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2023-01-16T16:23:55.839064Z\",\"report_issues\":\"https://github.com/gatoniel/napari-merge-stardist-masks/issues\",\"requirements\":[\"numpy\",\"magicgui\",\"qtpy\",\"stardist-napari (>=2022.7.5)\",\"merge-stardist-masks (>=0.1.0)\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\"],\"summary\":\"Segment non-star-convex objects with StarDist by merging masks.\",\"support\":\"https://github.com/gatoniel/napari-merge-stardist-masks/issues\",\"twitter\":\"\",\"version\":\"0.0.4\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Clement Caporal\"}],\"code_repository\":\"https://github.com/LaboratoryOpticsBiosciences/napari-brainbow-diagnose\",\"description\":\"# napari-brainbow-diagnose\\n\\n[![License BSD-3](https://img.shields.io/pypi/l/napari-brainbow-diagnose.svg?color=green)](https://github.com/LaboratoryOpticsBiosciences/napari-brainbow-diagnose/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-brainbow-diagnose.svg?color=green)](https://pypi.org/project/napari-brainbow-diagnose)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-brainbow-diagnose.svg?color=green)](https://python.org)\\n[![tests](https://github.com/LaboratoryOpticsBiosciences/napari-brainbow-diagnose/workflows/tests/badge.svg)](https://github.com/LaboratoryOpticsBiosciences/napari-brainbow-diagnose/actions)\\n[![codecov](https://codecov.io/gh/LaboratoryOpticsBiosciences/napari-brainbow-diagnose/branch/main/graph/badge.svg)](https://codecov.io/gh/LaboratoryOpticsBiosciences/napari-brainbow-diagnose)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-brainbow-diagnose)](https://napari-hub.org/plugins/napari-brainbow-diagnose)\\n\\nVisualize and Diagnose brainbow dataset in color space.\\n\\n## Pitch\\nBrainbow dataset have unique features that need to be addressed by specialized tools. This plugin aims at visualize and diagnose brainbow dataset.\\nIn particular we want to interact with the distribution of the dataset in the channel space.\\n\\n## Dev Installation\\n\\n```bash\\nconda create -n napari-brainbow-diagnose python=3.9\\nconda activate napari-brainbow-diagnose\\nconda install pip\\ngit clone git@github.com:LaboratoryOpticsBiosciences/napari-brainbow-diagnose.git\\ncd napari-brainbow-diagnose\\npip install -e .[testing]\\npre-commit install\\n```\\n\\nDo not forget to run pytest `pytest .` before you commit to check your code.\\n\\n## Installation\\n\\nYou can install `napari-brainbow-diagnose` via [pip]:\\n\\n    pip install napari-brainbow-diagnose\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/LaboratoryOpticsBiosciences/napari-brainbow-diagnose.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-brainbow-diagnose\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/LaboratoryOpticsBiosciences/napari-brainbow-diagnose/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/stable/plugins/index.html\\n-->\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-brainbow-diagnose\\n\\n\\n\\n\\n\\n\\nVisualize and Diagnose brainbow dataset in color space.\\nPitch\\nBrainbow dataset have unique features that need to be addressed by specialized tools. This plugin aims at visualize and diagnose brainbow dataset.\\nIn particular we want to interact with the distribution of the dataset in the channel space.\\nDev Installation\\nbash\\nconda create -n napari-brainbow-diagnose python=3.9\\nconda activate napari-brainbow-diagnose\\nconda install pip\\ngit clone git@github.com:LaboratoryOpticsBiosciences/napari-brainbow-diagnose.git\\ncd napari-brainbow-diagnose\\npip install -e .[testing]\\npre-commit install\\nDo not forget to run pytest pytest . before you commit to check your code.\\nInstallation\\nYou can install napari-brainbow-diagnose via pip:\\npip install napari-brainbow-diagnose\\n\\nTo install latest development version :\\npip install git+https://github.com/LaboratoryOpticsBiosciences/napari-brainbow-diagnose.git\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-brainbow-diagnose\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"Napari Brainbow Diagnose\",\"documentation\":\"https://github.com/LaboratoryOpticsBiosciences/napari-brainbow-diagnose#README.md\",\"first_released\":\"2023-01-25T01:16:18.147539Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-brainbow-diagnose\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\",\"sample_data\"],\"project_site\":\"https://github.com/LaboratoryOpticsBiosciences/napari-brainbow-diagnose\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2023-01-25T01:45:55.005197Z\",\"report_issues\":\"https://github.com/LaboratoryOpticsBiosciences/napari-brainbow-diagnose/issues\",\"requirements\":[\"numpy\",\"magicgui\",\"qtpy\",\"pooch\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"pre-commit ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\"],\"summary\":\"Visualize and Diagnose brainbow dataset in color space.\",\"support\":\"https://github.com/LaboratoryOpticsBiosciences/napari-brainbow-diagnose/issues\",\"twitter\":\"\",\"version\":\"0.0.5\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Robert Haase\"}],\"code_repository\":\"https://github.com/haesleinhuepf/napari-owncloud\",\"description\":\"# napari-owncloud\\r\\n\\r\\n[![License](https://img.shields.io/pypi/l/napari-owncloud.svg?color=green)](https://github.com/haesleinhuepf/napari-owncloud/raw/master/LICENSE)\\r\\n[![PyPI](https://img.shields.io/pypi/v/napari-owncloud.svg?color=green)](https://pypi.org/project/napari-owncloud)\\r\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-owncloud.svg?color=green)](https://python.org)\\r\\n[![tests](https://github.com/haesleinhuepf/napari-owncloud/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-owncloud/actions)\\r\\n[![codecov](https://codecov.io/gh/haesleinhuepf/napari-owncloud/branch/master/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-owncloud)\\r\\n\\r\\n## Usage\\r\\n\\r\\nBrowse folders and images in [owncloud](https://owncloud.com/) / [nextcloud](https://nextcloud.com/) servers and open them using just a double-click! \\r\\n\\r\\nLogin to an owncloud or nextcloud server by clicking the menu `Tools > Utilities > Browse owncloud / nextcloud storage`\\r\\n\\r\\n![](https://github.com/haesleinhuepf/napari-owncloud/raw/main/docs/login.png)\\r\\n\\r\\nYou can then navigate through folders by double-clicking `folder/` items in the list.\\r\\nYou can also open images by double-clicking them. Alternatively, use the arrow-up and arrow-down key to navigate the list and hit ENTER to open an image or folder.\\r\\n\\r\\n![](https://github.com/haesleinhuepf/napari-owncloud/raw/main/docs/browse.png)\\r\\n\\r\\nStore images in your cloud storage using the button `Save / upload current layer`. Note: Currently, only single selected layers can be saved.\\r\\n\\r\\n![](https://github.com/haesleinhuepf/napari-owncloud/raw/main/docs/upload.png)\\r\\n\\r\\n[Demo](https://github.com/haesleinhuepf/napari-owncloud/raw/main/docs/demo.mp4)\\r\\n\\r\\n![](https://github.com/haesleinhuepf/napari-owncloud/raw/main/docs/demo.gif)\\r\\n\\r\\n## Installation\\r\\n\\r\\nYou can install `napari-owncloud` via [pip]:\\r\\n\\r\\n    pip install napari-owncloud\\r\\n\\r\\n## Related plugins\\r\\n\\r\\nThere are other napari plugins that allow you browsing local and online image storage\\r\\n* [napari-omero](https://www.napari-hub.org/plugins/napari-omero)\\r\\n* [napari-folder-browser](https://www.napari-hub.org/plugins/napari-folder-browser)\\r\\n\\r\\n## Contributing\\r\\n\\r\\nContributions are very welcome. Tests can be run with [tox], please ensure\\r\\nthe coverage at least stays the same before you submit a pull request.\\r\\n\\r\\n## License\\r\\n\\r\\nDistributed under the terms of the [BSD-3] license,\\r\\n\\\"napari-owncloud\\\" is free and open source software\\r\\n\\r\\n## Issues\\r\\n\\r\\nIf you encounter any problems, please create a thread on [image.sc] along with a detailed description and tag [@haesleinhuepf].\\r\\n\\r\\n[napari]: https://github.com/napari/napari\\r\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\r\\n[@napari]: https://github.com/napari\\r\\n[MIT]: http://opensource.org/licenses/MIT\\r\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\r\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\r\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\r\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\r\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\r\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\r\\n\\r\\n[file an issue]: https://github.com/haesleinhuepf/napari-owncloud/issues\\r\\n\\r\\n[napari]: https://github.com/napari/napari\\r\\n[tox]: https://tox.readthedocs.io/en/latest/\\r\\n[pip]: https://pypi.org/project/pip/\\r\\n[PyPI]: https://pypi.org/\\r\\n[image.sc]: https://image.sc\\r\\n[@haesleinhuepf]: https://twitter.com/haesleinhuepf\\r\\n\\r\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-owncloud\\n\\n\\n\\n\\n\\nUsage\\nBrowse folders and images in owncloud / nextcloud servers and open them using just a double-click! \\nLogin to an owncloud or nextcloud server by clicking the menu Tools > Utilities > Browse owncloud / nextcloud storage\\n\\nYou can then navigate through folders by double-clicking folder/ items in the list.\\nYou can also open images by double-clicking them. Alternatively, use the arrow-up and arrow-down key to navigate the list and hit ENTER to open an image or folder.\\n\\nStore images in your cloud storage using the button Save / upload current layer. Note: Currently, only single selected layers can be saved.\\n\\nDemo\\n\\nInstallation\\nYou can install napari-owncloud via pip:\\npip install napari-owncloud\\n\\nRelated plugins\\nThere are other napari plugins that allow you browsing local and online image storage\\n* napari-omero\\n* napari-folder-browser\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-owncloud\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please create a thread on image.sc along with a detailed description and tag @haesleinhuepf.\",\"development_status\":[\"Development Status :: 3 - Alpha\"],\"display_name\":\"napari-owncloud\",\"documentation\":\"https://github.com/haesleinhuepf/napari-owncloud#README.md\",\"first_released\":\"2022-11-16T11:40:47.276624Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-owncloud\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/haesleinhuepf/napari-owncloud\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-11-19T10:16:27.903262Z\",\"report_issues\":\"https://github.com/haesleinhuepf/napari-owncloud/issues\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"napari-tools-menu\",\"pyocclient\"],\"summary\":\"Browse folders and images in owncloud / nextcloud servers and open them using just a double-click!\",\"support\":\"https://github.com/haesleinhuepf/napari-owncloud/issues\",\"twitter\":\"\",\"version\":\"0.1.2\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"dhole.pranjal@gmail.com\",\"name\":\"Pranjal Dhole\"}],\"code_repository\":\"https://github.com/pranjaldhole/napari-labelling-assistant\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-labelling-assistant\"}],\"description\":\"# napari-labelling-assistant\\n\\n[![License](https://img.shields.io/pypi/l/napari-labelling-assistant.svg?color=green)](https://github.com/pranjaldhole/napari-labelling-assistant/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-labelling-assistant.svg?color=green)](https://pypi.org/project/napari-labelling-assistant)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-labelling-assistant.svg?color=green)](https://python.org)\\n[![tests](https://github.com/pranjaldhole/napari-labelling-assistant/workflows/tests/badge.svg)](https://github.com/pranjaldhole/napari-labelling-assistant/actions)\\n[![codecov](https://codecov.io/gh/pranjaldhole/napari-labelling-assistant/branch/main/graph/badge.svg)](https://codecov.io/gh/pranjaldhole/napari-labelling-assistant)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-labelling-assistant)](https://napari-hub.org/plugins/napari-labelling-assistant)\\n\\nA lightweight plugin for visualizing labelling statistics.\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/plugins/stable/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-labelling-assistant` via [pip]:\\n\\n    pip install napari-labelling-assistant\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/pranjaldhole/napari-labelling-assistant.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [MIT] license,\\n\\\"napari-labelling-assistant\\\" is free and open source software.\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/pranjaldhole/napari-labelling-assistant/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-labelling-assistant\\n\\n\\n\\n\\n\\n\\nA lightweight plugin for visualizing labelling statistics.\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-labelling-assistant via pip:\\npip install napari-labelling-assistant\\n\\nTo install latest development version :\\npip install git+https://github.com/pranjaldhole/napari-labelling-assistant.git\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the MIT license,\\n\\\"napari-labelling-assistant\\\" is free and open source software.\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari-labelling-assistant\",\"documentation\":\"https://github.com/pranjaldhole/napari-labelling-assistant#README.md\",\"first_released\":\"2022-01-19T10:01:33.353562Z\",\"license\":\"MIT\",\"name\":\"napari-labelling-assistant\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/pranjaldhole/napari-labelling-assistant\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-01-24T10:04:52.881161Z\",\"report_issues\":\"https://github.com/pranjaldhole/napari-labelling-assistant/issues\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"matplotlib\"],\"summary\":\"A lightweight plugin for visualizing labelling statistics.\",\"support\":\"https://github.com/pranjaldhole/napari-labelling-assistant/issues\",\"twitter\":\"\",\"version\":\"0.0.5\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Alexander Marx\"}],\"code_repository\":\"https://github.com/marx-alex/napari-bleach-correct\",\"conda\":[],\"description\":\"## Bleach correction for napari\\n\\nThis plugin is a python implementation of three different algorithms for bleach correction and can be used \\nto correct time-lapse images that lose intensity due to photobleaching. The implementation is based on the ImageJ \\nplugin Bleach Corrector by Miura et al. All methods work with 2D and 3D time series.\\n\\nNapari Bleach correction is easy to use:\\n\\n![Demo](../data/demo.gif)\\n\\n### Ratio Method\\n\\nThis is the simplest method. Every pixel in a frame is multiplied by the ratio from the mean intensity of the \\nfirst frame to that of the *i-th* frame.\\n\\nAssumptions:\\n* the mean intensity is constant through the time-lapse\\n* the background fluorescence is the same for every pixel and frame\\n\\nParameters:\\n* Background Intensity: Must be estimated\\n\\n### Exponential Curve Fitting\\n\\nDrift estimation of fluorescence signal by fitting the mean intensity to an exponential curve.\\nThe image is corrected by the decay in the normalized exponential function.\\n\\nAssumptions:\\n* time intervals between frames are equal\\n\\nParameters:\\n* Exponential Curve: Bleaching can be modelled as a mono- or bi-exponential curve\\n\\n### Histogram Matching\\n\\nBleaching correction by matching histograms to a reference image.\\nThe correct pixel values can be calculated by the cumulative distribution function\\nof a frame and its reference frame. This method introduced by Miura et al.\\n\\nParameters:\\n* Reference Frame: Match the frame's histogram with the first our neighbor frame \\n\\nThe Histogram Matching method using the neighbor frame as reference is a good start to correct bleaching.\\nAll methods are described in detail in Miura et al.\\n\\n## References\\n\\n* Miura K. [Bleach correction ImageJ plugin for compensating the photobleaching of time-lapse sequences.](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7871415/) F1000Res. 2020 Dec 21;9:1494. doi: 10.12688/f1000research.27171.1\\n* [Documentation of the ImageJ plugin](https://wiki.cmci.info/downloads/bleach_corrector)\\n\\n<!-- This file is designed to provide you with a starting template for documenting\\nthe functionality of your plugin. Its content will be rendered on your plugin's\\nnapari hub page.\\n\\nThe sections below are given as a guide for the flow of information only, and\\nare in no way prescriptive. You should feel free to merge, remove, add and \\nrename sections at will to make this document work best for your plugin. \\n\\n## Description\\n\\nThis should be a detailed description of the context of your plugin and its \\nintended purpose.\\n\\nIf you have videos or screenshots of your plugin in action, you should include them\\nhere as well, to make them front and center for new users. \\n\\nYou should use absolute links to these assets, so that we can easily display them \\non the hub. The easiest way to include a video is to use a GIF, for example hosted\\non imgur. You can then reference this GIF as an image.\\n\\n![Example GIF hosted on Imgur](https://i.imgur.com/A5phCX4.gif)\\n\\nNote that GIFs larger than 5MB won't be rendered by GitHub - we will however,\\nrender them on the napari hub.\\n\\nThe other alternative, if you prefer to keep a video, is to use GitHub's video\\nembedding feature.\\n\\n1. Push your `DESCRIPTION.md` to GitHub on your repository (this can also be done\\nas part of a Pull Request)\\n2. Edit `.napari/DESCRIPTION.md` **on GitHub**.\\n3. Drag and drop your video into its desired location. It will be uploaded and\\nhosted on GitHub for you, but will not be placed in your repository.\\n4. We will take the resolved link to the video and render it on the hub.\\n\\nHere is an example of an mp4 video embedded this way.\\n\\nhttps://user-images.githubusercontent.com/17995243/120088305-6c093380-c132-11eb-822d-620e81eb5f0e.mp4\\n\\n## Intended Audience & Supported Data\\n\\nThis section should describe the target audience for this plugin (any knowledge,\\nskills and experience required), as well as a description of the types of data\\nsupported by this plugin.\\n\\nTry to make the data description as explicit as possible, so that users know the\\nformat your plugin expects. This applies both to reader plugins reading file formats\\nand to function/dock widget plugins accepting layers and/or layer data.\\nFor example, if you know your plugin only works with 3D integer data in \\\"tyx\\\" order,\\nmake sure to mention this.\\n\\nIf you know of researchers, groups or labs using your plugin, or if it has been cited\\nanywhere, feel free to also include this information here.\\n\\n## Quickstart\\n\\nThis section should go through step-by-step examples of how your plugin should be used.\\nWhere your plugin provides multiple dock widgets or functions, you should split these\\nout into separate subsections for easy browsing. Include screenshots and videos\\nwherever possible to elucidate your descriptions. \\n\\nIdeally, this section should start with minimal examples for those who just want a\\nquick overview of the plugin's functionality, but you should definitely link out to\\nmore complex and in-depth tutorials highlighting any intricacies of your plugin, and\\nmore detailed documentation if you have it.\\n\\n## Additional Install Steps (uncommon)\\nWe will be providing installation instructions on the hub, which will be sufficient\\nfor the majority of plugins. They will include instructions to pip install, and\\nto install via napari itself.\\n\\nMost plugins can be installed out-of-the-box by just specifying the package requirements\\nover in `setup.cfg`. However, if your plugin has any more complex dependencies, or \\nrequires any additional preparation before (or after) installation, you should add \\nthis information here.\\n\\n## Getting Help\\n\\nThis section should point users to your preferred support tools, whether this be raising\\nan issue on GitHub, asking a question on image.sc, or using some other method of contact.\\nIf you distinguish between usage support and bug/feature support, you should state that\\nhere.\\n\\n## How to Cite\\n\\nMany plugins may be used in the course of published (or publishable) research, as well as\\nduring conference talks and other public facing events. If you'd like to be cited in\\na particular format, or have a DOI you'd like used, you should provide that information here. -->\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"Bleach correction for napari\\nThis plugin is a python implementation of three different algorithms for bleach correction and can be used \\nto correct time-lapse images that lose intensity due to photobleaching. The implementation is based on the ImageJ \\nplugin Bleach Corrector by Miura et al. All methods work with 2D and 3D time series.\\nNapari Bleach correction is easy to use:\\n\\nRatio Method\\nThis is the simplest method. Every pixel in a frame is multiplied by the ratio from the mean intensity of the \\nfirst frame to that of the i-th frame.\\nAssumptions:\\n* the mean intensity is constant through the time-lapse\\n* the background fluorescence is the same for every pixel and frame\\nParameters:\\n* Background Intensity: Must be estimated\\nExponential Curve Fitting\\nDrift estimation of fluorescence signal by fitting the mean intensity to an exponential curve.\\nThe image is corrected by the decay in the normalized exponential function.\\nAssumptions:\\n* time intervals between frames are equal\\nParameters:\\n* Exponential Curve: Bleaching can be modelled as a mono- or bi-exponential curve\\nHistogram Matching\\nBleaching correction by matching histograms to a reference image.\\nThe correct pixel values can be calculated by the cumulative distribution function\\nof a frame and its reference frame. This method introduced by Miura et al.\\nParameters:\\n* Reference Frame: Match the frame's histogram with the first our neighbor frame \\nThe Histogram Matching method using the neighbor frame as reference is a good start to correct bleaching.\\nAll methods are described in detail in Miura et al.\\nReferences\\n\\nMiura K. Bleach correction ImageJ plugin for compensating the photobleaching of time-lapse sequences. F1000Res. 2020 Dec 21;9:1494. doi: 10.12688/f1000research.27171.1\\nDocumentation of the ImageJ plugin\\n\\n\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"Bleaching Correction\",\"documentation\":\"https://github.com/marx-alex/napari-bleach-correct#README.md\",\"first_released\":\"2022-09-22T19:22:06.030744Z\",\"license\":\"MIT\",\"name\":\"napari-bleach-correct\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\",\"sample_data\"],\"project_site\":\"https://github.com/marx-alex/napari-bleach-correct\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-09-22T19:22:06.030744Z\",\"report_issues\":\"https://github.com/marx-alex/napari-bleach-correct/issues\",\"requirements\":[\"numpy\",\"magicgui\",\"qtpy\",\"scikit-image\",\"scipy\",\"pyqtgraph\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\"],\"summary\":\"A napari plugin to correct time-lapse images for photobleaching.\",\"support\":\"https://github.com/marx-alex/napari-bleach-correct/issues\",\"twitter\":\"\",\"version\":\"0.0.1\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Lorenzo Gaifas\"}],\"code_repository\":\"https://github.com/brisvag/napari-gruvbox\",\"description\":\"# napari-gruvbox\\n\\n[![License GNU GPL v3.0](https://img.shields.io/pypi/l/napari-gruvbox.svg?color=green)](https://github.com/brisvag/napari-gruvbox/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-gruvbox.svg?color=green)](https://pypi.org/project/napari-gruvbox)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-gruvbox.svg?color=green)](https://python.org)\\n[![tests](https://github.com/brisvag/napari-gruvbox/workflows/tests/badge.svg)](https://github.com/brisvag/napari-gruvbox/actions)\\n[![codecov](https://codecov.io/gh/brisvag/napari-gruvbox/branch/main/graph/badge.svg)](https://codecov.io/gh/brisvag/napari-gruvbox)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-gruvbox)](https://napari-hub.org/plugins/napari-gruvbox)\\n\\nGruvbox theme for napari. Colors are taken from the palette in https://github.com/morhetz/gruvbox.\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/stable/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-gruvbox` via [pip]:\\n\\n    pip install napari-gruvbox\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/brisvag/napari-gruvbox.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [GNU GPL v3.0] license,\\n\\\"napari-gruvbox\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/brisvag/napari-gruvbox/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-gruvbox\\n\\n\\n\\n\\n\\n\\nGruvbox theme for napari. Colors are taken from the palette in https://github.com/morhetz/gruvbox.\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-gruvbox via pip:\\npip install napari-gruvbox\\n\\nTo install latest development version :\\npip install git+https://github.com/brisvag/napari-gruvbox.git\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the GNU GPL v3.0 license,\\n\\\"napari-gruvbox\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari Gruvbox\",\"documentation\":\"https://github.com/brisvag/napari-gruvbox#README.md\",\"first_released\":\"2022-12-14T13:17:54.884662Z\",\"license\":\"GPL-3.0\",\"name\":\"napari-gruvbox\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"theme\"],\"project_site\":\"https://github.com/brisvag/napari-gruvbox\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-12-14T13:17:54.884662Z\",\"report_issues\":\"https://github.com/brisvag/napari-gruvbox/issues\",\"requirements\":null,\"summary\":\"Gruvbox theme for napari.\",\"support\":\"https://github.com/brisvag/napari-gruvbox/issues\",\"twitter\":\"\",\"version\":\"0.1.0\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"stringerc@janelia.hhmi.org\",\"name\":\"Carsen Stringer\"}],\"code_repository\":\"https://github.com/Mouseland/cellpose-napari\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"cellpose-napari\"}],\"description\":\"# cellpose-napari <img src=\\\"docs/_static/favicon.ico\\\" width=\\\"50\\\" title=\\\"cellpose\\\" alt=\\\"cellpose\\\" align=\\\"right\\\" vspace = \\\"50\\\">\\n\\n[![Documentation Status](https://readthedocs.org/projects/cellpose-napari/badge/?version=latest)](https://cellpose-napari.readthedocs.io/en/latest/?badge=latest)\\n[![tests](https://github.com/mouseland/cellpose-napari/workflows/tests/badge.svg)](https://github.com/mouseland/cellpose-napari/actions)\\n[![codecov](https://codecov.io/gh/Mouseland/cellpose-napari/branch/main/graph/badge.svg)](https://codecov.io/gh/MouseLand/cellpose-napari)\\n[![PyPI version](https://badge.fury.io/py/cellpose-napari.svg)](https://badge.fury.io/py/cellpose-napari)\\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/cellpose-napari)](https://pypistats.org/packages/cellpose-napari)\\n[![Python version](https://img.shields.io/pypi/pyversions/cellpose-napari)](https://pypistats.org/packages/cellpose-napari)\\n[![License](https://img.shields.io/pypi/l/cellpose-napari.svg?color=green)](https://github.com/mouseland/cellpose-napari/raw/master/LICENSE)\\n[![Contributors](https://img.shields.io/github/contributors-anon/MouseLand/cellpose-napari)](https://github.com/MouseLand/cellpose-napari/graphs/contributors)\\n[![website](https://img.shields.io/website?url=https%3A%2F%2Fwww.cellpose.org)](https://www.cellpose.org)\\n[![GitHub stars](https://img.shields.io/github/stars/MouseLand/cellpose-napari?style=social)](https://github.com/MouseLand/cellpose-napari/)\\n[![GitHub forks](https://img.shields.io/github/forks/MouseLand/cellpose-napari?style=social)](https://github.com/MouseLand/cellpose-napari/)\\n\\na napari plugin for anatomical segmentation of general cellular images\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\\n\\nThe plugin code was written by Carsen Stringer, and the cellpose code was written by Carsen Stringer and Marius Pachitariu. To learn about Cellpose, read the [**paper**](https://t.co/kBMXmPp3Yn?amp=1) or watch this [**talk**](https://t.co/JChCsTD0SK?amp=1). \\n\\nFor support with the plugin, please open an [issue](https://github.com/MouseLand/cellpose-napari/issues). For support with cellpose, please open an [issue](https://github.com/MouseLand/cellpose/issues) on the cellpose repo. \\n\\n\\nIf you use this plugin please cite the [paper](https://www.nature.com/articles/s41592-020-01018-x):\\n::\\n    \\n      @article{stringer2021cellpose,\\n      title={Cellpose: a generalist algorithm for cellular segmentation},\\n      author={Stringer, Carsen and Wang, Tim and Michaelos, Michalis and Pachitariu, Marius},\\n      journal={Nature Methods},\\n      volume={18},\\n      number={1},\\n      pages={100--106},\\n      year={2021},\\n      publisher={Nature Publishing Group}\\n      }\\n\\n\\n![cellpose-napari_plugin](https://cellpose-napari.readthedocs.io/en/latest/_images/napari_main_demo_fast_small.gif?raw=true \\\"cellpose-napari\\\")\\n\\n## Installation\\n\\nInstall an [Anaconda](https://www.anaconda.com/download/) distribution of Python -- Choose **Python 3** and your operating system. Note you might need to use an anaconda prompt if you did not add anaconda to the path.\\n\\nInstall `napari` with pip: `pip install napari[all]`. Then install `cellpose-napari` via [pip]:\\n\\n    pip install cellpose-napari\\n    \\n Or install the plugin inside napari in the plugin window.\\n\\nIf install fails in your base environment, create a new environment:\\n1. Download the [`environment.yml`](https://github.com/MouseLand/cellpose-napari/blob/master/environment.yml?raw=true) file from the repository. You can do this by cloning the repository, or copy-pasting the text from the file into a text document on your local computer.\\n2. Open an anaconda prompt / command prompt with `conda` for **python 3** in the path\\n3. Change directories to where the `environment.yml` is and run `conda env create -f environment.yml`\\n4. To activate this new environment, run `conda activate cellpose-napari`\\n5. You should see `(cellpose-napari)` on the left side of the terminal line. \\n\\nIf you have **issues** with cellpose installation, see the [cellpose docs](https://cellpose.readthedocs.io/en/latest/installation.html) for more details, and then if the suggestions fail, open an issue.\\n\\n### Upgrading software\\n\\nYou can upgrade the plugin with\\n~~~\\npip install cellpose-napari --upgrade\\n~~~\\n\\nand you can upgrade cellpose with\\n~~~\\npip install cellpose --upgrade\\n~~~\\n\\n### GPU version (CUDA) on Windows or Linux\\n\\nIf you plan on running many images, you may want to install a GPU version of *torch* (if it isn't already installed).\\n\\nBefore installing the GPU version, remove the CPU version:\\n~~~\\npip uninstall torch\\n~~~\\n\\nFollow the instructions [here](https://pytorch.org/get-started/locally/) to determine what version to install. The Anaconda install is recommended along with CUDA version 10.2. For instance this command will install the 10.2 version on Linux and Windows (note the `torchvision` and `torchaudio` commands are removed because cellpose doesn't require them):\\n\\n~~~\\nconda install pytorch cudatoolkit=10.2 -c pytorch\\n~~~~\\n\\nWhen upgrading GPU Cellpose in the future, you will want to ignore dependencies (to ensure that the pip version of torch does not install):\\n~~~\\npip install --no-deps cellpose --upgrade\\n~~~\\n\\n### Installation of github version\\n\\nFollow steps from above to install the dependencies. In the github repository, run `pip install -e .` and the github version will be installed. If you want to go back to the pip version of cellpose-napari, then say `pip install cellpose-napari`.\\n\\n\\n## Running the software\\n\\n\\nOpen napari with the cellpose-napari dock widget open\\n```\\nnapari -w cellpose-napari\\n```\\n\\nThere is sample data in the File menu, or get started with your own images!\\n\\n### Detailed usage [documentation](https://cellpose-napari.readthedocs.io/).\\n\\n## Contributing\\n\\nContributions are very welcome. Tests are run with pytest.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"cellpose-napari\\\" is free and open source software.\\n\\n## Dependencies\\ncellpose-napari relies on the following excellent packages (which are automatically installed with conda/pip if missing):\\n- [napari](https://napari.org)\\n- [magicgui](https://napari.org/magicgui/)\\n\\ncellpose relies on the following excellent packages (which are automatically installed with conda/pip if missing):\\n- [torch](https://pytorch.org/)\\n- [numpy](http://www.numpy.org/) (>=1.16.0)\\n- [numba](http://numba.pydata.org/numba-doc/latest/user/5minguide.html)\\n- [scipy](https://www.scipy.org/)\\n- [natsort](https://natsort.readthedocs.io/en/master/)\\n- [tifffile](https://pypi.org/project/tifffile/)\\n- [opencv](https://opencv.org/)\\n\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"cellpose-napari \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\na napari plugin for anatomical segmentation of general cellular images\\n\\nThis napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.\\nThe plugin code was written by Carsen Stringer, and the cellpose code was written by Carsen Stringer and Marius Pachitariu. To learn about Cellpose, read the paper or watch this talk. \\nFor support with the plugin, please open an issue. For support with cellpose, please open an issue on the cellpose repo. \\nIf you use this plugin please cite the paper:\\n::\\n  @article{stringer2021cellpose,\\n  title={Cellpose: a generalist algorithm for cellular segmentation},\\n  author={Stringer, Carsen and Wang, Tim and Michaelos, Michalis and Pachitariu, Marius},\\n  journal={Nature Methods},\\n  volume={18},\\n  number={1},\\n  pages={100--106},\\n  year={2021},\\n  publisher={Nature Publishing Group}\\n  }\\n\\n\\nInstallation\\nInstall an Anaconda distribution of Python -- Choose Python 3 and your operating system. Note you might need to use an anaconda prompt if you did not add anaconda to the path.\\nInstall napari with pip: pip install napari[all]. Then install cellpose-napari via [pip]:\\npip install cellpose-napari\\n\\nOr install the plugin inside napari in the plugin window.\\nIf install fails in your base environment, create a new environment:\\n1. Download the environment.yml file from the repository. You can do this by cloning the repository, or copy-pasting the text from the file into a text document on your local computer.\\n2. Open an anaconda prompt / command prompt with conda for python 3 in the path\\n3. Change directories to where the environment.yml is and run conda env create -f environment.yml\\n4. To activate this new environment, run conda activate cellpose-napari\\n5. You should see (cellpose-napari) on the left side of the terminal line. \\nIf you have issues with cellpose installation, see the cellpose docs for more details, and then if the suggestions fail, open an issue.\\nUpgrading software\\nYou can upgrade the plugin with\\n~~~\\npip install cellpose-napari --upgrade\\n~~~\\nand you can upgrade cellpose with\\n~~~\\npip install cellpose --upgrade\\n~~~\\nGPU version (CUDA) on Windows or Linux\\nIf you plan on running many images, you may want to install a GPU version of torch (if it isn't already installed).\\nBefore installing the GPU version, remove the CPU version:\\n~~~\\npip uninstall torch\\n~~~\\nFollow the instructions here to determine what version to install. The Anaconda install is recommended along with CUDA version 10.2. For instance this command will install the 10.2 version on Linux and Windows (note the torchvision and torchaudio commands are removed because cellpose doesn't require them):\\n~~~\\nconda install pytorch cudatoolkit=10.2 -c pytorch\\n~~~~\\nWhen upgrading GPU Cellpose in the future, you will want to ignore dependencies (to ensure that the pip version of torch does not install):\\n~~~\\npip install --no-deps cellpose --upgrade\\n~~~\\nInstallation of github version\\nFollow steps from above to install the dependencies. In the github repository, run pip install -e . and the github version will be installed. If you want to go back to the pip version of cellpose-napari, then say pip install cellpose-napari.\\nRunning the software\\nOpen napari with the cellpose-napari dock widget open\\nnapari -w cellpose-napari\\nThere is sample data in the File menu, or get started with your own images!\\nDetailed usage documentation.\\nContributing\\nContributions are very welcome. Tests are run with pytest.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"cellpose-napari\\\" is free and open source software.\\nDependencies\\ncellpose-napari relies on the following excellent packages (which are automatically installed with conda/pip if missing):\\n- napari\\n- magicgui\\ncellpose relies on the following excellent packages (which are automatically installed with conda/pip if missing):\\n- torch\\n- numpy (>=1.16.0)\\n- numba\\n- scipy\\n- natsort\\n- tifffile\\n- opencv\",\"development_status\":[],\"display_name\":\"cellpose-napari\",\"documentation\":\"\",\"first_released\":\"2021-04-26T03:13:32.237206Z\",\"license\":\"BSD-3-Clause\",\"name\":\"cellpose-napari\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\",\"sample_data\"],\"project_site\":\"https://github.com/Mouseland/cellpose-napari\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-06-06T22:52:18.964369Z\",\"report_issues\":\"\",\"requirements\":[\"napari\",\"napari-plugin-engine (>=0.1.4)\",\"cellpose (>0.6.3)\",\"imagecodecs\",\"sphinx (>=3.0) ; extra == 'docs'\",\"sphinxcontrib-apidoc ; extra == 'docs'\",\"sphinx-rtd-theme ; extra == 'docs'\",\"sphinx-prompt ; extra == 'docs'\",\"sphinx-autodoc-typehints ; extra == 'docs'\",\"pytest ; extra == 'tests_require'\",\"pytest-qt ; extra == 'tests_require'\"],\"summary\":\"a generalist algorithm for anatomical segmentation\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.1.5\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Zach Marin\"},{\"name\":\"Talley Lambert\"}],\"code_repository\":\"https://github.com/zacsimile/napari-math\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-math\"}],\"description\":\"# napari-math\\n\\n[![License](https://img.shields.io/pypi/l/napari-math.svg?color=green)](https://github.com/zacsimile/napari-math/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-math.svg?color=green)](https://pypi.org/project/napari-math)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-math.svg?color=green)](https://python.org)\\n[![tests](https://github.com/zacsimile/napari-math/workflows/tests/badge.svg)](https://github.com/zacsimile/napari-math/actions)\\n[![codecov](https://codecov.io/gh/zacsimile/napari-math/branch/main/graph/badge.svg)](https://codecov.io/gh/zacsimile/napari-math)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-math)](https://napari-hub.org/plugins/napari-math)\\n\\nThis package provides a GUI interfrace for simple mathematical operations on image, point and surface layers.\\n\\n- addition\\n- subtraction\\n- multiplication\\n- division\\n- logical and, or, xor\\n- z-projection (mean and sum)\\n\\nOperations can be peformed on a single layer or between Image layers (functionaly pending for Surface and Point layers), \\nfor example adding one layer to another.\\n\\nWhen performing operations on two images of different sizes, the result will be the size of the smallest\\nof the two images.\\n\\n----------------------------------\\n\\n<!--\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n-->\\n\\n## Installation\\n\\nYou can install `napari-math` via [pip]:\\n\\n    pip install napari-math\\n\\n\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [MIT] license,\\n\\\"napari-math\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please file an [issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-math\\n\\n\\n\\n\\n\\n\\nThis package provides a GUI interfrace for simple mathematical operations on image, point and surface layers.\\n\\naddition\\nsubtraction\\nmultiplication\\ndivision\\nlogical and, or, xor\\nz-projection (mean and sum)\\n\\nOperations can be peformed on a single layer or between Image layers (functionaly pending for Surface and Point layers), \\nfor example adding one layer to another.\\nWhen performing operations on two images of different sizes, the result will be the size of the smallest\\nof the two images.\\n\\n\\nInstallation\\nYou can install napari-math via pip:\\npip install napari-math\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the MIT license,\\n\\\"napari-math\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an [issue] along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari math\",\"documentation\":\"https://github.com/zacsimile/napari-math#README.md\",\"first_released\":\"2022-01-18T17:06:10.593793Z\",\"license\":\"MIT\",\"name\":\"napari-math\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/zacsimile/napari-math\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-03-29T01:40:25.735077Z\",\"report_issues\":\"https://github.com/zacsimile/napari-math/issues\",\"requirements\":[\"numpy\"],\"summary\":\"Simple mathematical operations on image, point and surface layers.\",\"support\":\"https://github.com/zacsimile/napari-math/issues\",\"twitter\":\"\",\"version\":\"0.0.1b0\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Federico Gasparoli\"},{\"name\":\"Talley Lambert\"}],\"code_repository\":\"https://github.com/tlambert03/napari-micromanager\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-micromanager\"}],\"description\":\"# napari-micromanager\\n\\n[![License](https://img.shields.io/pypi/l/napari-micromanager.svg?color=green)](https://github.com/napari/napari-micromanager/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-micromanager.svg?color=green)](https://pypi.org/project/napari-micromanager)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-micromanager.svg?color=green)](https://python.org)\\n[![Tests](https://github.com/tlambert03/napari-micromanager/actions/workflows/test.yml/badge.svg)](https://github.com/tlambert03/napari-micromanager/actions/workflows/test.yml)\\n[![codecov](https://codecov.io/gh/tlambert03/napari-micromanager/branch/main/graph/badge.svg?token=tf6lYDWV1s)](https://codecov.io/gh/tlambert03/napari-micromanager)\\n\\nGUI interface between napari and micromanager powered by [pymmcore-plus](https://pymmcore-plus.readthedocs.io/).\\n\\n🚧 Experimental!  Work in progress!  Here be 🐲 🚧\\n\\n----------------------------------\\n<img width=\\\"1797\\\" alt=\\\"mm\\\" src=\\\"https://user-images.githubusercontent.com/1609449/138457506-787b7bec-7f30-4d92-b5cf-6e218c87225a.png\\\">\\n\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-micromanager` via [pip]:\\n\\n    pip install napari-micromanager\\n\\n### Getting micromanager adapters:\\n\\nThe easiest way to get the micromanager adapters is to use:\\n\\n```\\npython -m pymmcore_plus.install\\n```\\n\\nthis will install micromanager to the pymmcore_plus folder in your site-package; use this to see where:\\n\\n```\\npython -c \\\"from pymmcore_plus import find_micromanager; print(find_micromanager())\\\"\\n```\\n\\nalternatively, you can direct pymmcore_plus to your own micromanager installation with the `MICROMANAGER_PATH`\\nenvironment variable:\\n\\n```\\nexport MICROMANAGER_PATH='/path/to/Micro-Manager-...'\\n```\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n### Launching napari with plugin\\nYou can launch napari and automatically load this plugin using the `launch-dev.py` script:\\n\\n```bash\\npython launch-dev.py\\n```\\n\\nAlternatively you can run:\\n\\n```bash\\nnapari -w napari-micromanager\\n```\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-micromanager\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/tlambert03/napari-micromanager/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-micromanager\\n\\n\\n\\n\\n\\nGUI interface between napari and micromanager powered by pymmcore-plus.\\n🚧 Experimental!  Work in progress!  Here be 🐲 🚧\\n\\n\\nThis napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-micromanager via pip:\\npip install napari-micromanager\\n\\nGetting micromanager adapters:\\nThe easiest way to get the micromanager adapters is to use:\\npython -m pymmcore_plus.install\\nthis will install micromanager to the pymmcore_plus folder in your site-package; use this to see where:\\npython -c \\\"from pymmcore_plus import find_micromanager; print(find_micromanager())\\\"\\nalternatively, you can direct pymmcore_plus to your own micromanager installation with the MICROMANAGER_PATH\\nenvironment variable:\\nexport MICROMANAGER_PATH='/path/to/Micro-Manager-...'\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLaunching napari with plugin\\nYou can launch napari and automatically load this plugin using the launch-dev.py script:\\nbash\\npython launch-dev.py\\nAlternatively you can run:\\nbash\\nnapari -w napari-micromanager\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-micromanager\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 3 - Alpha\"],\"display_name\":\"napari-micromanager\",\"documentation\":\"\",\"first_released\":\"2021-08-15T00:48:42.041377Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-micromanager\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/tlambert03/napari-micromanager\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-05-19T23:49:28.692371Z\",\"report_issues\":\"\",\"requirements\":[\"napari (>=0.4.13)\",\"pymmcore-plus (>=0.4.0)\",\"useq-schema (>=0.1.0)\",\"superqt (>=0.3.1)\",\"fonticon-materialdesignicons6\",\"tifffile\",\"PyQt5 ; extra == 'pyqt5'\",\"PySide2 ; extra == 'pyside2'\",\"pytest ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\"],\"summary\":\"GUI interface between napari and micromanager\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.0.1rc7\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"jonas.windhager@uzh.ch\",\"name\":\"Jonas Windhager\"}],\"code_repository\":\"https://github.com/BodenmillerGroup/napari-czifile2\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-czifile2\"}],\"description\":\"# napari-czifile2\\n\\n<a href=\\\"https://pypi.org/project/napari-czifile2/\\\">\\n    <img src=\\\"https://img.shields.io/pypi/v/napari-czifile2\\\" alt=\\\"PyPI\\\" />\\n</a>\\n<a href=\\\"https://github.com/BodenmillerGroup/napari-czifile2/blob/main/LICENSE.md\\\">\\n    <img src=\\\"https://img.shields.io/pypi/l/napari-czifile2\\\" alt=\\\"License\\\" />\\n</a>\\n<a href=\\\"https://www.python.org/\\\">\\n    <img src=\\\"https://img.shields.io/pypi/pyversions/napari-czifile2\\\" alt=\\\"Python\\\" />\\n</a>\\n<a href=\\\"https://github.com/BodenmillerGroup/napari-czifile2/issues\\\">\\n    <img src=\\\"https://img.shields.io/github/issues/BodenmillerGroup/napari-czifile2\\\" alt=\\\"Issues\\\" />\\n</a>\\n<a href=\\\"https://github.com/BodenmillerGroup/napari-czifile2/pulls\\\">\\n    <img src=\\\"https://img.shields.io/github/issues-pr/BodenmillerGroup/napari-czifile2\\\" alt=\\\"Pull requests\\\" />\\n</a>\\n\\nCarl Zeiss Image (.czi) file type support for napari\\n\\nOpen .czi files and interactively view scenes co-registered in the machine's coordinate system using napari\\n\\n## Installation\\n\\nYou can install napari-czifile2 via [pip](https://pypi.org/project/pip/):\\n\\n    pip install napari-czifile2\\n\\nAlternatively, you can install napari-czifile2 via [conda](https://conda.io/):\\n\\n    conda install -c conda-forge napari-czifile2\\n\\n## Authors\\n\\nCreated and maintained by Jonas Windhager [jonas.windhager@uzh.ch](mailto:jonas.windhager@uzh.ch)\\n\\n## Contributing\\n\\n[Contributing](https://github.com/BodenmillerGroup/napari-czifile2/blob/main/CONTRIBUTING.md)\\n\\n## Changelog\\n\\n[Changelog](https://github.com/BodenmillerGroup/napari-czifile2/blob/main/CHANGELOG.md)\\n\\n## License\\n\\n[MIT](https://github.com/BodenmillerGroup/napari-czifile2/blob/main/LICENSE.md)\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-czifile2\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCarl Zeiss Image (.czi) file type support for napari\\nOpen .czi files and interactively view scenes co-registered in the machine's coordinate system using napari\\nInstallation\\nYou can install napari-czifile2 via pip:\\npip install napari-czifile2\\n\\nAlternatively, you can install napari-czifile2 via conda:\\nconda install -c conda-forge napari-czifile2\\n\\nAuthors\\nCreated and maintained by Jonas Windhager jonas.windhager@uzh.ch\\nContributing\\nContributing\\nChangelog\\nChangelog\\nLicense\\nMIT\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"napari-czifile2\",\"documentation\":\"https://github.com/BodenmillerGroup/napari-czifile2#README.md\",\"first_released\":\"2021-02-02T03:07:14.849183Z\",\"license\":\"MIT\",\"name\":\"napari-czifile2\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\"],\"project_site\":\"https://github.com/BodenmillerGroup/napari-czifile2\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[\"*.czi\"],\"release_date\":\"2022-06-01T12:21:38.927913Z\",\"report_issues\":\"https://github.com/BodenmillerGroup/napari-czifile2/issues\",\"requirements\":[\"czifile\",\"imagecodecs\",\"numpy\",\"tifffile\"],\"summary\":\"Carl Zeiss Image (.czi) file support for napari\",\"support\":\"https://github.com/BodenmillerGroup/napari-czifile2/issues\",\"twitter\":\"\",\"version\":\"0.2.7\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Charlotte Godard\"}],\"code_repository\":\"https://github.com/chgodard/hesperos\",\"description\":\"<div align=\\\"justify\\\">\\n    \\n# HESPEROS PLUGIN FOR NAPARI\\n\\n[![License](https://img.shields.io/pypi/l/hesperos.svg?color=green)](https://github.com/DBC/hesperos/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/hesperos.svg?color=green)](https://pypi.org/project/hesperos)\\n[![Python Version](https://img.shields.io/pypi/pyversions/hesperos.svg?color=green)](https://python.org)\\n[![tests](https://github.com/DBC/hesperos/workflows/tests/badge.svg)](https://github.com/DBC/hesperos/actions)\\n[![codecov](https://codecov.io/gh/DBC/hesperos/branch/main/graph/badge.svg)](https://codecov.io/gh/DBC/hesperos)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/hesperos)](https://napari-hub.org/plugins/hesperos)\\n\\nA Napari plugin for pre-defined manual segmentation or semi-automatic segmentation with a one-shot learning procedure. The objective was to simplify the interface as much as possible so that the user can concentrate on annotation tasks using a pen on a tablet, or a mouse on a computer. \\n    \\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n    \\n# Table of Contents\\n- [Installation and Usage](#installation-and-usage)\\n    * [Automatic installation](#automatic-installation)\\n    * [Manual installation](#manual-installation)\\n    * [Upgrade Hesperos version](#upgrade-hesperos-version)\\n- [Hesperos: *Manual Segmentation and Correction* mode](#hesperos-manual-segmentation-and-correction-mode)\\n    * [Import and adjust your image](#import-and-adjust-your-image-use-panel-1)\\n    * [Layer controls](#layer-controls)\\n    * [Annotate your image](#annotate-your-image-use-panel-2)\\n    * [Select slices of interest](#select-slices-of-interest-use-panel-3----only-displayed-for-the-shoulder-bones-category)\\n    * [Export annotations](#export-annotations-use-panel-3----or-4-if-the-shoulder-bones-category-is-selected)\\n- [Hesperos: *OneShot Segmentation* mode](#hesperos-oneshot-segmentation-mode)\\n    * [Import and adjust your image](#import-and-adjust-your-image-use-panel-1)\\n    * [Annotate your image](#annotate-your-image-use-panel-2)\\n    * [Run automatic segmentation](#run-automatic-segmentation-use-panel-3)\\n    * [Export annotations](#export-annotations-use-panel-4)\\n\\n        \\n# Installation and Usage\\nThe Hesperos plugin is designed to run on Windows (11 or less) and MacOS with Python 3.8 / 3.9 / 3.10.\\n     \\n    \\n## Automatic installation\\n1. Install [Anaconda] and unselect *Add to PATH*. Keep in mind the path where you choose to install anaconda.\\n2. Only download the *script_files* folder for [Windows](/script_files/for_Windows/) or [Macos](/script_files/for_Windows/). \\n3. Add your Anaconda path in these script files:\\n    1. <ins>For Windows</ins>: \\n    Right click on the .bat files (for [installation](/script_files/for_Windows/install_hesperos_env.bat) and [running](/script_files/for_Windows/run_hesperos.bat)) and select *Modify*. Change *PATH_TO_ADD* with your Anaconda path. Then save the changes.\\n        > for exemple:\\n        ```\\n        anaconda_dir=C:\\\\Users\\\\chgodard\\\\anaconda3\\n        ```\\n    2. <ins>For Macos</ins>:\\n        1. Right click on the .command files (for [installation](/script_files/for_Macos/install_hesperos_env.command) and [running](/script_files/for_Macos/run_hesperos.command)) and select *Open with TextEdit*. Change *PATH_TO_ADD* with your Anaconda path. Then save the changes.\\n            > for exemple:\\n            ```\\n            source ~/opt/anaconda3/etc/profile.d/conda.sh\\n            ```\\n        2. In your terminal, change the permissions to allow the following .command files to be run (change *PATH* with the path of your .command files): \\n            ``` \\n            chmod u+x PATH/install_hesperos_env.command \\n            chmod u+x PATH/run_hesperos.command \\n            ```\\n4. Double click on the **install_hesperos_env file** to create a virtual environment in Anaconda with python 3.9 and Napari 0.4.14. \\n    > /!\\\\ The Hesperos plugin is not yet compatible with Napari versions superior to 0.4.14.\\n5. Double click on the **run_hesperos file** to run Napari from your virtual environment.\\n6. In Napari: \\n    1. Go to *Plugins/Install Plugins...*\\n    2. Search for \\\"hesperos\\\" (it can take a while to load).\\n    3. Install the **hesperos** plugin.\\n    4. When the installation is done, close Napari. A restart of Napari is required to finish the plugin installation.\\n7. Double click on the **run_hesperos file** to run Napari.\\n8. In Napari, use the Hesperos plugin with *Plugins/hesperos*.\\n\\n    \\n## Manual installation\\n1. Install [Anaconda] and unselect *Add to PATH*.\\n2. Open your Anaconda prompt command.\\n3. Create a virtual environment with Python 3.8 / 3.9 / 3.10:\\n    ```\\n    conda create -n hesperos_env python=3.9\\n    ```\\n4. Install the required Python packages in your virtual environment:\\n    ```\\n    conda activate hesperos_env\\n    conda install -c conda-forge napari=0.4.14 \\n    conda install -c anaconda pyqt\\n    pip install hesperos\\n    ```\\n    > /!\\\\ Hesperos plugin is not yet compatible with napari version superior to 0.4.14.\\n5. Launch Napari:\\n    ```\\n    napari\\n    ```\\n    \\n## Upgrade Hesperos version\\n1. Double click on the **run_hesperos file** to run Napari. \\n2. In Napari: \\n    1. Go to *Plugins/Install Plugins...*\\n    2. Search for \\\"hesperos\\\" (it can take a while to load).\\n    3. Click on *Update* if a new version of Hesperos has been found. You can check the latest version of Hesperos in the [Napari Hub](https://www.napari-hub.org/plugins/hesperos).\\n    4. When the installation is done, close Napari. A restart of Napari is required to finish the plugin installation.\\n   \\n    \\n# Hesperos: *Manual Segmentation and Correction* mode\\n    \\n The ***Manual Segmentation and Correction*** mode of the Hesperos plugin is a simplified and optimized interface to do basic 2D manual segmentation of several structures in a 3D image using a mouse or a stylet with a tablet.\\n\\n    \\n <img src=\\\"https://user-images.githubusercontent.com/49953723/193262711-710673f2-5b53-4eb6-a7c7-6dada9d28d92.PNG\\\" width=\\\"1000px\\\"/>\\n    \\n## Import and adjust your image *(use Panel 1)*\\nThe Hesperos plugin can be used with Digital Imaging and COmmunications in Medicine (DICOM), Neuroimaging Informatics Technology Initiative (NIfTI) or Tagged Image File Format (TIFF) images. To improve performances, use images that are located on your own disk.\\n\\n1. To import data:\\n    - use the <img src=\\\"https://user-images.githubusercontent.com/49953723/193262334-3c28e733-36ab-4504-9a6d-acd298c15994.PNG\\\" width=\\\"100px\\\"/> button for *(.tiff, .tif, .nii or .nii.gz)* image files.\\n    - use the <img src=\\\"https://user-images.githubusercontent.com/49953723/193262624-149a4461-fbac-4498-a2b8-33bdd88e3a9f.PNG\\\" width=\\\"100px\\\"/> button for a DICOM serie. /!\\\\ Folder with multiple DICOM series is not supported.  \\n2. After the image has loaded, a slider appears that allows to zoom in/out: <img src=\\\"https://user-images.githubusercontent.com/49953723/193262738-7e6e68a9-0890-4e18-92a9-dbf2168a6bb5.PNG\\\" width=\\\"100px\\\"/>. Zooming is also possible with the <img src=\\\"https://user-images.githubusercontent.com/49953723/193262725-7d4f7b09-d119-45cf-a9d4-c42c5f848c1a.PNG\\\" width=\\\"25px\\\"/> button in the layer controls panel. \\n3. If your data is a DICOM serie, you have the possibility to directly change the contrast of the image (according to the Hounsfield Unit):\\n    - by choosing one of the two predefined contrasts: *CT bone* or *CT Soft* in <img src=\\\"https://user-images.githubusercontent.com/49953723/193262708-17e1d301-0a9a-497f-9feb-613e69893c06.PNG\\\" width=\\\"150px\\\"/>.\\n    - by creating a custom default contrast with the <img src=\\\"https://user-images.githubusercontent.com/49953723/193262707-466917b4-b885-429b-9924-6481fa6410bb.PNG\\\" width=\\\"30px\\\"/> button and selecting *Custom Contrast*. Settings can be exported as a .json file with the <img src=\\\"https://user-images.githubusercontent.com/49953723/193262709-e1ad5321-1f60-4b60-a715-7c494670e1cd.PNG\\\" width=\\\"30px\\\"/> button.\\n    - by loading a saved default contrast with the <img src=\\\"https://user-images.githubusercontent.com/49953723/193262710-c9f66354-f896-4e59-8718-70e5509875af.PNG\\\" width=\\\"30px\\\"/> button and selecting *Custom Contrast*.\\n4. In the bottom left corner of the application you also have the possibility to: \\n    - <img src=\\\"https://user-images.githubusercontent.com/49953723/193262716-d9947eb9-d87f-4251-af76-2d906cd36018.PNG\\\" width=\\\"25px\\\"/>: change the order of the visible axis (for example go to sagittal, axial or coronal planes).\\n    - <img src=\\\"https://user-images.githubusercontent.com/49953723/193262717-12afbfb1-49ae-4a77-a83e-5bc99850734a.PNG\\\" width=\\\"25px\\\"/>: transpose the 3D image on the current axis being displayed.\\n\\n\\n## Layer controls\\n\\nWhen data is loading, two layers are created: the *`image`* layer and the *`annotations`* layer. Order in the layer list correspond to the overlayed order. By clicking on these layers you will have acces to different layer controls (at the top left corner of the application). All actions can be undone/redone with the Ctrl-Z/Shift-Ctrl-Z keyboard shortcuts. You can also hide a layer by clicking on its eye icon on the layer list.\\n    \\n    \\n<ins>For the *image* layer:</ins>\\n- *`opacity`*: a slider to control the global opacity of the layer.\\n- *`contrast limits`*: a double slider to manually control the contrast of the image (same as the <img src=\\\"https://user-images.githubusercontent.com/49953723/193262708-17e1d301-0a9a-497f-9feb-613e69893c06.PNG\\\" width=\\\"150px\\\"/> option for DICOM data).\\n    \\n\\n<ins>For the *annotations* layer:</ins>\\n- <img src=\\\"https://user-images.githubusercontent.com/49953723/193262718-30882770-59eb-4d2b-9cfe-8b88537560c4.PNG\\\" width=\\\"25px\\\"/>: erase brush to erase all labels at once (if *`preserve labels`* is not selected) or only erase the selected label (if *`preserve labels`* is selected).\\n- <img src=\\\"https://user-images.githubusercontent.com/49953723/193262722-6bb6e6a4-ae7a-4ad1-b7f8-898e54ad62c3.PNG\\\" width=\\\"25px\\\"/>: paint brush with the same color than the *`label`* rectangle.\\n- <img src=\\\"https://user-images.githubusercontent.com/49953723/193262719-f816b21e-78fd-4ba7-b415-30a461cbd652.PNG\\\" width=\\\"25px\\\"/>: fill bucket with the same color than the *`label`* rectangle.\\n- <img src=\\\"https://user-images.githubusercontent.com/49953723/193262725-7d4f7b09-d119-45cf-a9d4-c42c5f848c1a.PNG\\\" width=\\\"25px\\\"/>: select to zoom in and out with the mouse wheel (same as the zoom slider at the top right corner in Panel 1).\\n- *`label`*: a colored rectangle to represent the selected label.  \\n- *`opacity`*: a slider to control the global opacity of the layer.  \\n- *`brush size limits`*: a slider to control size of the paint/erase brush.    \\n- *`preserve labels`*: if selected, all actions are applied only on the selected label (see the *`label`* rectangle); if not selected, actions are applied on all labels.\\n- *`show selected`*: if selected, only the selected label will be display on the layer; if not selected, all labels are displayed.\\n   \\n    \\n>*Remark*: a second option for filling has been added\\n>1. Drawn the egde of a closed shape with the paint brush mode.  \\n>2. Double click to activate the fill bucket.  \\n>3. Click inside the closed area to fill it.  \\n>4. Double click on the filled area to deactivate the fill bucket and reactivate the paint brush mode.\\n    \\n\\n## Annotate your image *(use Panel 2)*\\n    \\nManual annotation and correction on the segmented file is done using the layer controls of the *`annotations`* layer. Click on the layer to display them. /!\\\\ You have to choose a structure to start annotating *(see 2.)*.\\n1. To modify an existing segmentation, you can directy open the segmented file with the <img src=\\\"https://user-images.githubusercontent.com/49953723/193262702-df3b4fb8-63d0-4a1b-b1c9-8391cf8c3f22.PNG\\\" width=\\\"130px\\\"/> button. The file needs to have the same dimensions as the original image. \\n    > /!\\\\ Only .tiff, .tif, .nii and .nii.gz files are supported as segmented files.  \\n    \\n2. Choose a structure to annotate in the drop-down menu\\n    - *`Fetus`*: to annotate pregnancy image.\\n    - *`Shoulder`*: to annotate bones and muscles for shoulder surgery.\\n    - *`Shoulder Bones`*: to annotate only few bones for shoulder surgery.\\n    - *`Feta Challenge`*: to annotate fetal brain MRI with the same label than the FeTA Challenge (see ADD LIEN WEB).\\n    \\n> When selecting a structure, a new panel appears with a list of elements to annotate. Each element has its own label and color. Select one element in the list to automatically activate the paint brush mode with the corresponding color (color is updated in the *`label`* rectangle in the layer controls panel).\\n    \\n3. All actions can be undone with the <img src=\\\"https://user-images.githubusercontent.com/49953723/193265848-8c458035-609a-433e-aa82-5d9588971425.PNG\\\" width=\\\"30px\\\"/> button or Ctrl-Z.\\n    \\n4. If you need to work on a specific slice of your 3D image, but also have to explore the volume to understand some complex structures, you can use the locking option to facilitate the annotation task.\\n    - <ins>To activate the functionality</ins>: \\n        1. Go to the slice of interest.\\n        2. Click on the <img src=\\\"https://user-images.githubusercontent.com/49953723/193262706-40f3dbca-5589-406d-81e8-e150ae8bfab6.PNG\\\" width=\\\"30px\\\"/> button => will change the button to <img src=\\\"https://user-images.githubusercontent.com/49953723/193262703-2b2ea2dc-24fa-438b-a75c-3aa42b210f53.PNG\\\" width=\\\"30px\\\"/> and save the layer index.\\n        3. Scroll in the z-axis to explore the data (with the mouse wheel or the slider under the image).\\n        4. To go back to your slice of interest, click on the <img src=\\\"https://user-images.githubusercontent.com/49953723/193262703-2b2ea2dc-24fa-438b-a75c-3aa42b210f53.PNG\\\" width=\\\"30px\\\"/> button.\\n    - <ins>To deactivate the functionality</ins> (or change the locked slice index): \\n        1. Go to the locked slice.\\n        2. Click on the <img src=\\\"https://user-images.githubusercontent.com/49953723/193262703-2b2ea2dc-24fa-438b-a75c-3aa42b210f53.PNG\\\" width=\\\"30px\\\"/> button  => change the button to <img src=\\\"https://user-images.githubusercontent.com/49953723/193262706-40f3dbca-5589-406d-81e8-e150ae8bfab6.PNG\\\" width=\\\"30px\\\"/> and \\\"unlock\\\" the slice.\\n\\n\\n## Select slices of interest *(use Panel 3 -- only displayed for the Shoulder Bones category)*\\n\\nThis panel will only be displayed if the *`Shoulder Bones`* category is selected. A maxiumum of 10 slices can be selected in a 3D image and the corresponding z-indexes will be integrated in the metadata during the exportation of the segmentation file.\\n   \\n   > /!\\\\ Metadata integration is available only for exported .tiff and .tif files and with the *`Unique`* save option. \\n\\n- <img src=\\\"https://user-images.githubusercontent.com/49953723/201736039-4ed10553-4a4b-4d5e-9d61-826dc139e437.png\\\" width=\\\"25px\\\"/> : to add the currently displayed z-index in the drop-down menu.\\n- <img src=\\\"https://user-images.githubusercontent.com/49953723/201736105-a9c45264-412a-453b-8475-5a9ab856b07d.png\\\" width=\\\"25px\\\"/> : to remove the currently displayed z-index from the drop-down menu.\\n- <img src=\\\"https://user-images.githubusercontent.com/49953723/201736152-319d8559-dbfc-4e52-aeb3-e8e34445f67a.png\\\" width=\\\"25px\\\"/> : to go to the z-index selected in the drop-down menu. The icon will be checked when the currently displayed z-index matches the selected z-index in the drop-down menu.\\n- <img src=\\\"https://user-images.githubusercontent.com/49953723/201733835-7bee453a-bc07-416f-8b95-aaf803683cac.png\\\" width=\\\"100px\\\"/> : a drop-down menu containing the list of selected z-indexes. Select a z-index from the list to work with it more easily.\\n\\n\\n## Export annotations *(use Panel 3 -- or 4 if the Shoulder Bones category is selected)*\\n    \\n1. Annotations can be exported as .tif, .tiff, .nii or .nii.gz file with the <img src=\\\"https://user-images.githubusercontent.com/49953723/201735102-113f64b7-4da4-40ee-b058-9900268d270d.png\\\" width=\\\"95px\\\"/> button in one of the two following saving mode:\\n    - *`Unique`*: segmented data is exported as a unique 3D image with corresponding label ids (1-2-3-...). This file can be re-opened in the application.\\n    - *`Several`*: segmented data is exported as several binary 3D images (0 or 255), one for each label id.\\n2. <img src=\\\"https://user-images.githubusercontent.com/49953723/193262699-95758bdb-ac40-439b-8959-d924781a2368.PNG\\\" width=\\\"100px\\\"/>: delete annotation data.\\n3. *`Automatic segmentation backup`*: if selected, the segmentation data will be automatically exported as a unique 3D image when the image slice is changed.\\n    > /!\\\\ This process can slow down the display if the image is large.\\n\\n# Hesperos: *OneShot Segmentation* mode\\n    \\n The ***OneShot Segmentation*** mode of the Hesperos plugin is a 2D version of the VoxelLearning method implemented in DIVA (see [our Github](https://github.com/DecBayComp/VoxelLearning) and the latest article [Guérinot, C., Marcon, V., Godard, C., et al. (2022). New Approach to Accelerated Image Annotation by Leveraging Virtual Reality and Cloud Computing. _Frontiers in Bioinformatics_. doi:10.3389/fbinf.2021.777101](https://www.frontiersin.org/articles/10.3389/fbinf.2021.777101/full)).\\n    \\n\\nThe principle is to accelerate the segmentation without prior information. The procedure consists of:\\n1. A **rapid tagging** of few pixels in the image with two labels: one for the structure of interest (named positive tags), and one for the other structures (named negative tags).\\n2. A **training** of a simple random forest classifier with these tagged pixels and their features (mean, gaussian, ...).\\n3. An **inference** of all the pixels of the image to automatically segment the structure of interest. The output is a probability image (0-255) of belonging to a specific class.\\n4. Iterative corrections if needed.\\n    \\n<img src=\\\"https://user-images.githubusercontent.com/49953723/193262714-8699cd59-3825-4d71-b27a-bbcad1e36d55.PNG\\\" width=\\\"1000px\\\"/>\\n\\n    \\n## Import and adjust your image *(use Panel 1)*\\n    \\nSame panel as the *Manual Segmentation and Correction* mode *(see [panel 1 description](#import-and-adjust-your-image-use-panel-1))*.\\n   \\n    \\n## Annotate your image *(use Panel 2)*\\n    \\nAnnotations and corrections on the segmented file is done using the layer controls of the *`annotations`* layer. Click on the layer to display them. Only two labels are available: *`Structure of interest`* and *`Other`*. \\n\\nThe rapid manual tagging step of the one-shot learning method aims to learn and attribute different features to each label.\\n<img align=\\\"right\\\" src=\\\"https://user-images.githubusercontent.com/49953723/193262735-5dce56fb-8a2c-4aeb-9ee7-9727122d8089.PNG\\\" width=\\\"220px\\\"/> \\nTo achieve that, the user has to:\\n- with the label *`Structure of interest`*, tag few pixels of the structure of interest.\\n- with the label *`Other`*, tag the greatest diversity of uninteresting structures in the 3D image (avoid tagging too much pixels).\\n\\n> see the exemple image with *`Structure of interest`* label in red and *`Other`* label in cyan.\\n    \\n1. To modify an existing segmentation, you can directy open the segmented file with the <img src=\\\"https://user-images.githubusercontent.com/49953723/193266118-dfd241f6-8f0b-4cb9-94e7-5e74a3ce8b6e.PNG\\\" width=\\\"130px\\\"/> button. The file needs to have the same dimensions as the original image. \\n    > /!\\\\ Only .tiff, .tif, .nii and .nii.gz files are supported as segmented files. \\n2. All actions can be undone with the <img src=\\\"https://user-images.githubusercontent.com/49953723/193265848-8c458035-609a-433e-aa82-5d9588971425.PNG\\\" width=\\\"30px\\\"/> button or Ctrl-Z.\\n\\n    \\n## Run automatic segmentation *(use Panel 3)*\\n\\nFrom the previously tagged pixels, features are extracted and used to train a basic classifier : the Random Forest Classifier (RFC). When the training of the pixel classifier is done, it is applied to each pixel of the complete volume and outputs a probability to belong to the structure of interest.\\n\\nTo run training and inference, click on the <img src=\\\"https://user-images.githubusercontent.com/49953723/193262731-719c226a-f7c5-4252-b2bb-fade4ab7f5b3.PNG\\\" width=\\\"115px\\\"/> button:\\n1. You will be asked to save a .pckl file which corresponds to the model.\\n2. A new status will appears under the *Panel 4* : *`Computing...`*. You must wait for the message to change to: *`Ready`* before doing anything in the application (otherwise the application may freeze or crash).\\n3. When the processing is done, two new layers will appear:\\n    - the *`probabilities`* layer which corresponds to the direct probability (between 0 and 1) of a pixel to belong to the structure of interest. This layer is disabled by default, to enable it click on its eye icon in the layer list.\\n    - the *`segmented probabilities`* layer which corresponds to a binary image obtained from the probability image normed and thresholded according to a value manually defined with the *`Probability threshold`* slider: <img src=\\\"https://user-images.githubusercontent.com/49953723/193262730-6998c8a5-92f1-4ff1-bbf5-6972a373afd2.PNG\\\" width=\\\"80px\\\"/>.\\n\\n>Remark: If the output is not perfect, you have two possibilities to improve the result:\\n>1. Add some tags with the paint brush to take in consideration unintersting structures or add information in critical areas of your structure of interest (such as in thin sections). Then, run the training and inference process again. /!\\\\ This will overwrite all previous segmentation data.\\n>2. Export your segmentation data and re-open it with the *Manual Annotation and Correction* mode of Hesperos to manually erase or add annotations.\\n    \\n    \\n## Export annotations *(use Panel 4)*\\n    \\n1. Segmented probabilites can be exported as .tif, .tiff, .nii or .nii.gz file with the <img src=\\\"https://user-images.githubusercontent.com/49953723/193262734-57159a97-2f46-4aba-b3bf-b55a35dfacbd.PNG\\\" width=\\\"105px\\\"/> button. The image is exported as a unique 3D binary image (value 0 and 255). This file can be re-opened in the application for correction.\\n2. Probabilities can be exported as .tif, .tiff, .nii or .nii.gz file with the <img src=\\\"https://user-images.githubusercontent.com/49953723/193262733-26e37392-55b2-4c36-9287-b2f5d8d30e03.PNG\\\" width=\\\"105px\\\"/> button as a unique 3D image. The probabilities image is normed between 0 and 255.\\n3. <img src=\\\"https://user-images.githubusercontent.com/49953723/193266056-9514b648-b3e0-43f5-901a-a45fa1390f00.PNG\\\" width=\\\"100px\\\"/>: delete annotation data.\\n\\n\\n# License\\n\\nDistributed under the terms of the [BSD-3] license, **Hesperos** is a free and open source software.\\n\\n    \\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n[Anaconda]: https://www.anaconda.com/products/distribution#Downloads\\n[VoxelLearning]: https://github.com/DecBayComp/VoxelLearning\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"\\n\\n# HESPEROS PLUGIN FOR NAPARI\\n\\n[![License](https://img.shields.io/pypi/l/hesperos.svg?color=green)](https://github.com/DBC/hesperos/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/hesperos.svg?color=green)](https://pypi.org/project/hesperos)\\n[![Python Version](https://img.shields.io/pypi/pyversions/hesperos.svg?color=green)](https://python.org)\\n[![tests](https://github.com/DBC/hesperos/workflows/tests/badge.svg)](https://github.com/DBC/hesperos/actions)\\n[![codecov](https://codecov.io/gh/DBC/hesperos/branch/main/graph/badge.svg)](https://codecov.io/gh/DBC/hesperos)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/hesperos)](https://napari-hub.org/plugins/hesperos)\\n\\nA Napari plugin for pre-defined manual segmentation or semi-automatic segmentation with a one-shot learning procedure. The objective was to simplify the interface as much as possible so that the user can concentrate on annotation tasks using a pen on a tablet, or a mouse on a computer. \\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n\\n# Table of Contents\\n- [Installation and Usage](#installation-and-usage)\\n    * [Automatic installation](#automatic-installation)\\n    * [Manual installation](#manual-installation)\\n    * [Upgrade Hesperos version](#upgrade-hesperos-version)\\n- [Hesperos: *Manual Segmentation and Correction* mode](#hesperos-manual-segmentation-and-correction-mode)\\n    * [Import and adjust your image](#import-and-adjust-your-image-use-panel-1)\\n    * [Layer controls](#layer-controls)\\n    * [Annotate your image](#annotate-your-image-use-panel-2)\\n    * [Select slices of interest](#select-slices-of-interest-use-panel-3----only-displayed-for-the-shoulder-bones-category)\\n    * [Export annotations](#export-annotations-use-panel-3----or-4-if-the-shoulder-bones-category-is-selected)\\n- [Hesperos: *OneShot Segmentation* mode](#hesperos-oneshot-segmentation-mode)\\n    * [Import and adjust your image](#import-and-adjust-your-image-use-panel-1)\\n    * [Annotate your image](#annotate-your-image-use-panel-2)\\n    * [Run automatic segmentation](#run-automatic-segmentation-use-panel-3)\\n    * [Export annotations](#export-annotations-use-panel-4)\\n\\n\\n# Installation and Usage\\nThe Hesperos plugin is designed to run on Windows (11 or less) and MacOS with Python 3.8 / 3.9 / 3.10.\\n\\n\\n## Automatic installation\\n1. Install [Anaconda] and unselect *Add to PATH*. Keep in mind the path where you choose to install anaconda.\\n2. Only download the *script_files* folder for [Windows](/script_files/for_Windows/) or [Macos](/script_files/for_Windows/). \\n3. Add your Anaconda path in these script files:\\n    1. For Windows: \\n    Right click on the .bat files (for [installation](/script_files/for_Windows/install_hesperos_env.bat) and [running](/script_files/for_Windows/run_hesperos.bat)) and select *Modify*. Change *PATH_TO_ADD* with your Anaconda path. Then save the changes.\\n        > for exemple:\\n        ```\\n        anaconda_dir=C:\\\\Users\\\\chgodard\\\\anaconda3\\n        ```\\n    2. For Macos:\\n        1. Right click on the .command files (for [installation](/script_files/for_Macos/install_hesperos_env.command) and [running](/script_files/for_Macos/run_hesperos.command)) and select *Open with TextEdit*. Change *PATH_TO_ADD* with your Anaconda path. Then save the changes.\\n            > for exemple:\\n            ```\\n            source ~/opt/anaconda3/etc/profile.d/conda.sh\\n            ```\\n        2. In your terminal, change the permissions to allow the following .command files to be run (change *PATH* with the path of your .command files): \\n            ``` \\n            chmod u+x PATH/install_hesperos_env.command \\n            chmod u+x PATH/run_hesperos.command \\n            ```\\n4. Double click on the **install_hesperos_env file** to create a virtual environment in Anaconda with python 3.9 and Napari 0.4.14. \\n    > /!\\\\ The Hesperos plugin is not yet compatible with Napari versions superior to 0.4.14.\\n5. Double click on the **run_hesperos file** to run Napari from your virtual environment.\\n6. In Napari: \\n    1. Go to *Plugins/Install Plugins...*\\n    2. Search for \\\"hesperos\\\" (it can take a while to load).\\n    3. Install the **hesperos** plugin.\\n    4. When the installation is done, close Napari. A restart of Napari is required to finish the plugin installation.\\n7. Double click on the **run_hesperos file** to run Napari.\\n8. In Napari, use the Hesperos plugin with *Plugins/hesperos*.\\n\\n\\n## Manual installation\\n1. Install [Anaconda] and unselect *Add to PATH*.\\n2. Open your Anaconda prompt command.\\n3. Create a virtual environment with Python 3.8 / 3.9 / 3.10:\\n    ```\\n    conda create -n hesperos_env python=3.9\\n    ```\\n4. Install the required Python packages in your virtual environment:\\n    ```\\n    conda activate hesperos_env\\n    conda install -c conda-forge napari=0.4.14 \\n    conda install -c anaconda pyqt\\n    pip install hesperos\\n    ```\\n    > /!\\\\ Hesperos plugin is not yet compatible with napari version superior to 0.4.14.\\n5. Launch Napari:\\n    ```\\n    napari\\n    ```\\n\\n## Upgrade Hesperos version\\n1. Double click on the **run_hesperos file** to run Napari. \\n2. In Napari: \\n    1. Go to *Plugins/Install Plugins...*\\n    2. Search for \\\"hesperos\\\" (it can take a while to load).\\n    3. Click on *Update* if a new version of Hesperos has been found. You can check the latest version of Hesperos in the [Napari Hub](https://www.napari-hub.org/plugins/hesperos).\\n    4. When the installation is done, close Napari. A restart of Napari is required to finish the plugin installation.\\n\\n\\n# Hesperos: *Manual Segmentation and Correction* mode\\n\\n The ***Manual Segmentation and Correction*** mode of the Hesperos plugin is a simplified and optimized interface to do basic 2D manual segmentation of several structures in a 3D image using a mouse or a stylet with a tablet.\\n\\n\\n \\n\\n## Import and adjust your image *(use Panel 1)*\\nThe Hesperos plugin can be used with Digital Imaging and COmmunications in Medicine (DICOM), Neuroimaging Informatics Technology Initiative (NIfTI) or Tagged Image File Format (TIFF) images. To improve performances, use images that are located on your own disk.\\n\\n1. To import data:\\n    - use the  button for *(.tiff, .tif, .nii or .nii.gz)* image files.\\n    - use the  button for a DICOM serie. /!\\\\ Folder with multiple DICOM series is not supported.  \\n2. After the image has loaded, a slider appears that allows to zoom in/out: . Zooming is also possible with the  button in the layer controls panel. \\n3. If your data is a DICOM serie, you have the possibility to directly change the contrast of the image (according to the Hounsfield Unit):\\n    - by choosing one of the two predefined contrasts: *CT bone* or *CT Soft* in .\\n    - by creating a custom default contrast with the  button and selecting *Custom Contrast*. Settings can be exported as a .json file with the  button.\\n    - by loading a saved default contrast with the  button and selecting *Custom Contrast*.\\n4. In the bottom left corner of the application you also have the possibility to: \\n    - : change the order of the visible axis (for example go to sagittal, axial or coronal planes).\\n    - : transpose the 3D image on the current axis being displayed.\\n\\n\\n## Layer controls\\n\\nWhen data is loading, two layers are created: the *`image`* layer and the *`annotations`* layer. Order in the layer list correspond to the overlayed order. By clicking on these layers you will have acces to different layer controls (at the top left corner of the application). All actions can be undone/redone with the Ctrl-Z/Shift-Ctrl-Z keyboard shortcuts. You can also hide a layer by clicking on its eye icon on the layer list.\\n\\n\\nFor the *image* layer:\\n- *`opacity`*: a slider to control the global opacity of the layer.\\n- *`contrast limits`*: a double slider to manually control the contrast of the image (same as the  option for DICOM data).\\n\\n\\nFor the *annotations* layer:\\n- : erase brush to erase all labels at once (if *`preserve labels`* is not selected) or only erase the selected label (if *`preserve labels`* is selected).\\n- : paint brush with the same color than the *`label`* rectangle.\\n- : fill bucket with the same color than the *`label`* rectangle.\\n- : select to zoom in and out with the mouse wheel (same as the zoom slider at the top right corner in Panel 1).\\n- *`label`*: a colored rectangle to represent the selected label.  \\n- *`opacity`*: a slider to control the global opacity of the layer.  \\n- *`brush size limits`*: a slider to control size of the paint/erase brush.    \\n- *`preserve labels`*: if selected, all actions are applied only on the selected label (see the *`label`* rectangle); if not selected, actions are applied on all labels.\\n- *`show selected`*: if selected, only the selected label will be display on the layer; if not selected, all labels are displayed.\\n\\n\\n>*Remark*: a second option for filling has been added\\n>1. Drawn the egde of a closed shape with the paint brush mode.  \\n>2. Double click to activate the fill bucket.  \\n>3. Click inside the closed area to fill it.  \\n>4. Double click on the filled area to deactivate the fill bucket and reactivate the paint brush mode.\\n\\n\\n## Annotate your image *(use Panel 2)*\\n\\nManual annotation and correction on the segmented file is done using the layer controls of the *`annotations`* layer. Click on the layer to display them. /!\\\\ You have to choose a structure to start annotating *(see 2.)*.\\n1. To modify an existing segmentation, you can directy open the segmented file with the  button. The file needs to have the same dimensions as the original image. \\n    > /!\\\\ Only .tiff, .tif, .nii and .nii.gz files are supported as segmented files.  \\n\\n2. Choose a structure to annotate in the drop-down menu\\n    - *`Fetus`*: to annotate pregnancy image.\\n    - *`Shoulder`*: to annotate bones and muscles for shoulder surgery.\\n    - *`Shoulder Bones`*: to annotate only few bones for shoulder surgery.\\n    - *`Feta Challenge`*: to annotate fetal brain MRI with the same label than the FeTA Challenge (see ADD LIEN WEB).\\n\\n> When selecting a structure, a new panel appears with a list of elements to annotate. Each element has its own label and color. Select one element in the list to automatically activate the paint brush mode with the corresponding color (color is updated in the *`label`* rectangle in the layer controls panel).\\n\\n3. All actions can be undone with the  button or Ctrl-Z.\\n\\n4. If you need to work on a specific slice of your 3D image, but also have to explore the volume to understand some complex structures, you can use the locking option to facilitate the annotation task.\\n    - To activate the functionality: \\n        1. Go to the slice of interest.\\n        2. Click on the  button => will change the button to  and save the layer index.\\n        3. Scroll in the z-axis to explore the data (with the mouse wheel or the slider under the image).\\n        4. To go back to your slice of interest, click on the  button.\\n    - To deactivate the functionality (or change the locked slice index): \\n        1. Go to the locked slice.\\n        2. Click on the  button  => change the button to  and \\\"unlock\\\" the slice.\\n\\n\\n## Select slices of interest *(use Panel 3 -- only displayed for the Shoulder Bones category)*\\n\\nThis panel will only be displayed if the *`Shoulder Bones`* category is selected. A maxiumum of 10 slices can be selected in a 3D image and the corresponding z-indexes will be integrated in the metadata during the exportation of the segmentation file.\\n\\n   > /!\\\\ Metadata integration is available only for exported .tiff and .tif files and with the *`Unique`* save option. \\n\\n-  : to add the currently displayed z-index in the drop-down menu.\\n-  : to remove the currently displayed z-index from the drop-down menu.\\n-  : to go to the z-index selected in the drop-down menu. The icon will be checked when the currently displayed z-index matches the selected z-index in the drop-down menu.\\n-  : a drop-down menu containing the list of selected z-indexes. Select a z-index from the list to work with it more easily.\\n\\n\\n## Export annotations *(use Panel 3 -- or 4 if the Shoulder Bones category is selected)*\\n\\n1. Annotations can be exported as .tif, .tiff, .nii or .nii.gz file with the  button in one of the two following saving mode:\\n    - *`Unique`*: segmented data is exported as a unique 3D image with corresponding label ids (1-2-3-...). This file can be re-opened in the application.\\n    - *`Several`*: segmented data is exported as several binary 3D images (0 or 255), one for each label id.\\n2. : delete annotation data.\\n3. *`Automatic segmentation backup`*: if selected, the segmentation data will be automatically exported as a unique 3D image when the image slice is changed.\\n    > /!\\\\ This process can slow down the display if the image is large.\\n\\n# Hesperos: *OneShot Segmentation* mode\\n\\n The ***OneShot Segmentation*** mode of the Hesperos plugin is a 2D version of the VoxelLearning method implemented in DIVA (see [our Github](https://github.com/DecBayComp/VoxelLearning) and the latest article [Guérinot, C., Marcon, V., Godard, C., et al. (2022). New Approach to Accelerated Image Annotation by Leveraging Virtual Reality and Cloud Computing. _Frontiers in Bioinformatics_. doi:10.3389/fbinf.2021.777101](https://www.frontiersin.org/articles/10.3389/fbinf.2021.777101/full)).\\n\\n\\nThe principle is to accelerate the segmentation without prior information. The procedure consists of:\\n1. A **rapid tagging** of few pixels in the image with two labels: one for the structure of interest (named positive tags), and one for the other structures (named negative tags).\\n2. A **training** of a simple random forest classifier with these tagged pixels and their features (mean, gaussian, ...).\\n3. An **inference** of all the pixels of the image to automatically segment the structure of interest. The output is a probability image (0-255) of belonging to a specific class.\\n4. Iterative corrections if needed.\\n\\n\\n\\n\\n## Import and adjust your image *(use Panel 1)*\\n\\nSame panel as the *Manual Segmentation and Correction* mode *(see [panel 1 description](#import-and-adjust-your-image-use-panel-1))*.\\n\\n\\n## Annotate your image *(use Panel 2)*\\n\\nAnnotations and corrections on the segmented file is done using the layer controls of the *`annotations`* layer. Click on the layer to display them. Only two labels are available: *`Structure of interest`* and *`Other`*. \\n\\nThe rapid manual tagging step of the one-shot learning method aims to learn and attribute different features to each label.\\n \\nTo achieve that, the user has to:\\n- with the label *`Structure of interest`*, tag few pixels of the structure of interest.\\n- with the label *`Other`*, tag the greatest diversity of uninteresting structures in the 3D image (avoid tagging too much pixels).\\n\\n> see the exemple image with *`Structure of interest`* label in red and *`Other`* label in cyan.\\n\\n1. To modify an existing segmentation, you can directy open the segmented file with the  button. The file needs to have the same dimensions as the original image. \\n    > /!\\\\ Only .tiff, .tif, .nii and .nii.gz files are supported as segmented files. \\n2. All actions can be undone with the  button or Ctrl-Z.\\n\\n\\n## Run automatic segmentation *(use Panel 3)*\\n\\nFrom the previously tagged pixels, features are extracted and used to train a basic classifier : the Random Forest Classifier (RFC). When the training of the pixel classifier is done, it is applied to each pixel of the complete volume and outputs a probability to belong to the structure of interest.\\n\\nTo run training and inference, click on the  button:\\n1. You will be asked to save a .pckl file which corresponds to the model.\\n2. A new status will appears under the *Panel 4* : *`Computing...`*. You must wait for the message to change to: *`Ready`* before doing anything in the application (otherwise the application may freeze or crash).\\n3. When the processing is done, two new layers will appear:\\n    - the *`probabilities`* layer which corresponds to the direct probability (between 0 and 1) of a pixel to belong to the structure of interest. This layer is disabled by default, to enable it click on its eye icon in the layer list.\\n    - the *`segmented probabilities`* layer which corresponds to a binary image obtained from the probability image normed and thresholded according to a value manually defined with the *`Probability threshold`* slider: .\\n\\n>Remark: If the output is not perfect, you have two possibilities to improve the result:\\n>1. Add some tags with the paint brush to take in consideration unintersting structures or add information in critical areas of your structure of interest (such as in thin sections). Then, run the training and inference process again. /!\\\\ This will overwrite all previous segmentation data.\\n>2. Export your segmentation data and re-open it with the *Manual Annotation and Correction* mode of Hesperos to manually erase or add annotations.\\n\\n\\n## Export annotations *(use Panel 4)*\\n\\n1. Segmented probabilites can be exported as .tif, .tiff, .nii or .nii.gz file with the  button. The image is exported as a unique 3D binary image (value 0 and 255). This file can be re-opened in the application for correction.\\n2. Probabilities can be exported as .tif, .tiff, .nii or .nii.gz file with the  button as a unique 3D image. The probabilities image is normed between 0 and 255.\\n3. : delete annotation data.\\n\\n\\n# License\\n\\nDistributed under the terms of the [BSD-3] license, **Hesperos** is a free and open source software.\\n\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n[Anaconda]: https://www.anaconda.com/products/distribution#Downloads\\n[VoxelLearning]: https://github.com/DecBayComp/VoxelLearning\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"Hesperos application\",\"documentation\":\"https://github.com/chgodard/hesperos/blob/main/README.md\",\"first_released\":\"2022-05-30T20:12:28.151101Z\",\"license\":\"BSD-3-Clause\",\"name\":\"hesperos\",\"npe2\":true,\"operating_system\":[\"Operating System :: MacOS :: MacOS X\",\"Operating System :: Microsoft :: Windows\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/chgodard/hesperos\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2023-01-19T10:21:13.870150Z\",\"report_issues\":\"\",\"requirements\":[\"numpy\",\"qtpy\",\"tifffile\",\"scikit-image\",\"scikit-learn\",\"SimpleITK\",\"pandas\",\"napari (<0.4.15)\",\"napari-plugin-engine\",\"imageio-ffmpeg\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\"],\"summary\":\"A plugin to manually or semi-automatically segment medical data and correct previous segmentation data.\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.1.40\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Loic A. Royer\"}],\"code_repository\":\"https://github.com/royerloic/napari-nasa-samples\",\"conda\":[],\"description\":\"# napari-nasa-samples\\n\\n[![License Mozilla Public License 2.0](https://img.shields.io/pypi/l/napari-nasa-samples.svg?color=green)](https://github.com/royerlab/napari-nasa-samples/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-nasa-samples.svg?color=green)](https://pypi.org/project/napari-nasa-samples)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-nasa-samples.svg?color=green)](https://python.org)\\n[![tests](https://github.com/royerloic/napari-nasa-samples/workflows/tests/badge.svg)](https://github.com/royerlab/napari-nasa-samples/actions)\\n[![codecov](https://codecov.io/gh/royerloic/napari-nasa-samples/branch/main/graph/badge.svg)](https://codecov.io/gh/royerlab/napari-nasa-samples)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-nasa-samples)](https://napari-hub.org/plugins/napari-nasa-samples)\\n\\nThis napari plugin written [by Loic A. Royer](https://twitter.com/loicaroyer) provides sample datasets from NASA.\\nIn particular, you can access directly from napari the recently released images for the [James Webb Space Telescope](https://webb.nasa.gov/), as well as\\nsome of the classic and most beautiful images obtained by the venerable and still strong [Hubble Space Telescope](https://hubblesite.org/). \\nMore images will be added over time.\\n\\nThanks to (NASA)[https://www.nasa.gov/] for releasing these incredible images!\\n\\n![](https://github.com/royerloic/napari-nasa-samples/raw/main/docs/images/teaser.gif)\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-nasa-samples` via [pip]:\\n\\n    pip install napari-nasa-samples\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/royerloic/napari-nasa-samples.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [Mozilla Public License 2.0] license,\\n\\\"napari-nasa-samples\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/royerlab/napari-nasa-samples/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-nasa-samples\\n\\n\\n\\n\\n\\n\\nThis napari plugin written by Loic A. Royer provides sample datasets from NASA.\\nIn particular, you can access directly from napari the recently released images for the James Webb Space Telescope, as well as\\nsome of the classic and most beautiful images obtained by the venerable and still strong Hubble Space Telescope. \\nMore images will be added over time.\\nThanks to (NASA)[https://www.nasa.gov/] for releasing these incredible images!\\n\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-nasa-samples via pip:\\npip install napari-nasa-samples\\n\\nTo install latest development version :\\npip install git+https://github.com/royerloic/napari-nasa-samples.git\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the Mozilla Public License 2.0 license,\\n\\\"napari-nasa-samples\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"NASA sample images\",\"documentation\":\"https://github.com/royerloic/napari-nasa-samples#README.md\",\"first_released\":\"2022-07-12T22:51:43.112742Z\",\"license\":\"MPL-2.0\",\"name\":\"napari-nasa-samples\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"sample_data\"],\"project_site\":\"https://github.com/royerloic/napari-nasa-samples\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-07-16T14:41:18.493102Z\",\"report_issues\":\"https://github.com/royerloic/napari-nasa-samples/issues\",\"requirements\":[\"numpy\",\"requests\",\"pillow\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\"],\"summary\":\"This napari plugin provides sample datasets from NASA.\",\"support\":\"https://github.com/royerloic/napari-nasa-samples/issues\",\"twitter\":\"\",\"version\":\"0.0.5\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"p.schoennenbeck@fz-juelich.de\",\"name\":\"Philipp Schoennenbeck\"}],\"code_repository\":\"https://github.com/Croxa/napari-mrcfile_handler\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-mrcfile-handler\"}],\"description\":\"# napari-mrcfile_handler\\n\\n[![License](https://img.shields.io/pypi/l/napari-mrcfile_handler.svg?color=green)](https://github.com/Croxa/napari-mrcfile_handler/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-mrcfile_handler.svg?color=green)](https://pypi.org/project/napari-mrcfile_handler)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-mrcfile_handler.svg?color=green)](https://python.org)\\n[![tests](https://github.com/Croxa/napari-mrcfile_handler/workflows/tests/badge.svg)](https://github.com/Croxa/napari-mrcfile_handler/actions)\\n[![codecov](https://codecov.io/gh/Croxa/napari-mrcfile_handler/branch/master/graph/badge.svg)](https://codecov.io/gh/Croxa/napari-mrcfile_handler)\\n\\nA simple plugin to read, write and adjust mrcfiles in napari.\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-mrcfile_handler` via [pip]:\\n\\n    pip install napari-mrcfile_handler\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-mrcfile_handler\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/Croxa/napari-mrcfile_handler/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-mrcfile_handler\\n\\n\\n\\n\\n\\nA simple plugin to read, write and adjust mrcfiles in napari.\\n\\nThis napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-mrcfile_handler via pip:\\npip install napari-mrcfile_handler\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-mrcfile_handler\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari-mrcfile-handler\",\"documentation\":\"https://github.com/Croxa/napari-mrcfile_handler#README.md\",\"first_released\":\"2021-08-30T10:31:32.458679Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-mrcfile-handler\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\",\"writer\",\"widget\"],\"project_site\":\"https://github.com/Croxa/napari-mrcfile_handler\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[\"*\"],\"release_date\":\"2021-09-01T14:32:19.283708Z\",\"report_issues\":\"https://github.com/Croxa/napari-mrcfile_handler/issues\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"mrcfile\"],\"summary\":\"A simple plugin to read, write and adjust mrcfiles in napari.\",\"support\":\"https://github.com/Croxa/napari-mrcfile_handler/issues\",\"twitter\":\"\",\"version\":\"0.0.6\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[\"image\",\"labels\"]}",
  "{\"authors\":[{\"email\":\"cqzhang@g.ecc.u-tokyo.ac.jp\",\"name\":\"Chenqi Zhang\"}],\"code_repository\":\"https://github.com/zcqwh/napari-aideveloper\",\"conda\":[],\"description\":\"# napari-aideveloper\\n\\n[![License](https://img.shields.io/pypi/l/napari-aideveloper.svg?color=green)](https://github.com/zcqwh/napari-aideveloper/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-aideveloper.svg?color=green)](https://pypi.org/project/napari-aideveloper)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-aideveloper.svg?color=green)](https://python.org)\\n[![tests](https://github.com/zcqwh/napari-aideveloper/workflows/tests/badge.svg)](https://github.com/zcqwh/napari-aideveloper/actions)\\n[![codecov](https://codecov.io/gh/zcqwh/napari-aideveloper/branch/main/graph/badge.svg)](https://codecov.io/gh/zcqwh/napari-aideveloper)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-aideveloper)](https://napari-hub.org/plugins/napari-aideveloper)\\n\\n[napari_aideveloper](https://www.napari-hub.org/plugins/napari-aideveloper) is a napari-plugin derived from [AIDeveloper](https://github.com/maikherbig/AIDeveloper) that allows you to train, evaluate, and apply deep neural nets for image classification within a graphical user-interface (GUI).\\n\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-aideveloper` via [pip]:\\n\\n    pip install napari-aideveloper\\n\\n## Introduction\\n### Main functions\\n* [Build](#build)\\n* [History](#history)\\n\\n****\\n\\n### Build \\n#### 1. Load data\\nDrag and drop your data in .rtdc (HDF5) format into the file table and set the class and training/validation.\\n![alt Load_data](https://github.com/zcqwh/napari-aideveloper/blob/main/Tutorial/00_Load_data.gif?raw=true)\\n\\n#### 2. Choose Neural Networks\\n![alt Load_data](https://github.com/zcqwh/napari-aideveloper/blob/main/Tutorial/01_choose%20NN.gif?raw=true)\\n\\n#### 3. Set model storage path\\n![alt Load_data](https://github.com/zcqwh/napari-aideveloper/blob/main/Tutorial/02_save_model.gif?raw=true)\\n\\n#### 4. Start fitting\\n![alt Load_data](https://github.com/zcqwh/napari-aideveloper/blob/main/Tutorial/03_start_fitting.gif?raw=true)\\n![alt Load_data](https://github.com/zcqwh/napari-aideveloper/blob/main/Tutorial/04_fitting.gif?raw=true)\\n\\n#### Preview image\\n![alt Load_data](https://github.com/zcqwh/napari-aideveloper/blob/main/Tutorial/05_preview.gif?raw=true)\\n\\n#### Image augmentation\\n![alt Load_data](https://github.com/zcqwh/napari-aideveloper/blob/main/Tutorial/06_augmentation.gif?raw=true)\\n\\n****\\n\\n### History\\n#### 1. Load meta data\\n![alt Load_data](https://github.com/zcqwh/napari-aideveloper/blob/development/Tutorial/GIF/History/01_Load_metadata.gif?raw=true)\\n\\n#### 2. Check model details\\n![alt Load_data](https://github.com/zcqwh/napari-aideveloper/blob/development/Tutorial/GIF/History/02_model_detail.gif?raw=true)\\n\\n#### 3. Rolling median & Linear fit\\n![alt Load_data](https://github.com/zcqwh/napari-aideveloper/blob/development/Tutorial/GIF/History/03_rolling_linear.gif?raw=true)\\n\\n\\n\\n\\n\\n\\n## Contributing\\n\\nContributions are very welcome. You can submit your pull request on [GitHub](https://github.com/zcqwh/napari-aideveloper/pulls). Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-aideveloper\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue](https://github.com/zcqwh/napari-aideveloper/issues) along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-aideveloper\\n\\n\\n\\n\\n\\n\\nnapari_aideveloper is a napari-plugin derived from AIDeveloper that allows you to train, evaluate, and apply deep neural nets for image classification within a graphical user-interface (GUI).\\n\\nInstallation\\nYou can install napari-aideveloper via pip:\\npip install napari-aideveloper\\n\\nIntroduction\\nMain functions\\n\\nBuild\\nHistory\\n\\n\\nBuild\\n1. Load data\\nDrag and drop your data in .rtdc (HDF5) format into the file table and set the class and training/validation.\\n\\n2. Choose Neural Networks\\n\\n3. Set model storage path\\n\\n4. Start fitting\\n\\n\\nPreview image\\n\\nImage augmentation\\n\\n\\nHistory\\n1. Load meta data\\n\\n2. Check model details\\n\\n3. Rolling median & Linear fit\\n\\nContributing\\nContributions are very welcome. You can submit your pull request on GitHub. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-aideveloper\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari AIDeveloper\",\"documentation\":\"https://github.com/zcqwh/napari-aideveloper#README.md\",\"first_released\":\"2022-05-20T04:19:53.303321Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-aideveloper\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/zcqwh/napari-aideveloper\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-06-17T05:41:19.373520Z\",\"report_issues\":\"https://github.com/zcqwh/napari-aideveloper/issues\",\"requirements\":[\"numpy\",\"magicgui\",\"qtpy\",\"dclab (>=0.39.9)\",\"h5py (>=3.6.0)\",\"Keras-Applications (>=1.0.8)\",\"keras (>=2.8.0)\",\"keras-metrics (>=1.1.0)\",\"napari-plugin-engine (>=0.2.0)\",\"napari (>=0.4.14)\",\"onnx (>=1.11.0)\",\"opencv-contrib-python-headless (>=4.5.5.62)\",\"openpyxl (>=3.0.9)\",\"pandas (>=1.4.1)\",\"Pillow (>=9.1.1)\",\"psutil (>=5.9.0)\",\"pyqtgraph (>=0.12.3)\",\"pytest (>=7.1.2)\",\"scikit-learn (>=1.1.1)\",\"scipy (>=1.8.0)\",\"setuptools (>=58.0.4)\",\"six (>=1.16.0)\",\"tensorboard (>=2.8.0)\",\"tensorflow (>=2.8.0)\",\"tf2onnx (>=1.9.3)\",\"xlrd (>=2.0.1)\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\"],\"summary\":\"napari_aideveloper is a napari-plugin deived from AIDeveloper that allows you to train, evaluate and apply deep neural nets for image classification within a graphical user-interface (GUI).\",\"support\":\"https://github.com/zcqwh/napari-aideveloper/issues\",\"twitter\":\"\",\"version\":\"0.0.4\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"alisterburt@gmail.com\",\"name\":\"Alister Burt\"}],\"code_repository\":\"https://github.com/alisterburt/napari-mrcfile-reader\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-mrcfile-reader\"}],\"description\":\"# napari-mrcfile-reader\\n\\n[![License](https://img.shields.io/pypi/l/napari-mrcfile-reader.svg?color=green)](https://github.com/alisterburt/napari-mrcfile-reader/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-mrcfile-reader.svg?color=green)](https://pypi.org/project/napari-mrcfile-reader)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-mrcfile-reader.svg?color=green)](https://python.org)\\n[![tests](https://github.com/alisterburt/napari-mrcfile-reader/workflows/tests/badge.svg)](https://github.com/alisterburt/napari-mrcfile-reader/actions)\\n[![codecov](https://codecov.io/gh/alisterburt/napari-mrcfile-reader/branch/master/graph/badge.svg)](https://codecov.io/gh/alisterburt/napari-mrcfile-reader)\\n\\nRead MRC format image files into napari using the [mrcfile] package from [CCP-EM]\\n\\n----------------------------------\\n![example usage](example.gif)\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n\\n## Installation\\n\\nYou can install `napari-mrcfile-reader` via [pip]:\\n\\n    pip install napari-mrcfile-reader\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-mrcfile-reader\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n\\n[CCP-EM]: https://www.ccpem.ac.uk/\\n[mrcfile]: https://github.com/ccpem/mrcfile\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/alisterburt/napari-mrcfile-reader/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-mrcfile-reader\\n\\n\\n\\n\\n\\nRead MRC format image files into napari using the mrcfile package from CCP-EM\\n\\n\\nThis napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.\\nInstallation\\nYou can install napari-mrcfile-reader via pip:\\npip install napari-mrcfile-reader\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-mrcfile-reader\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"napari-mrcfile-reader\",\"documentation\":\"\",\"first_released\":\"2020-10-17T12:56:13.263981Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-mrcfile-reader\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\"],\"project_site\":\"https://github.com/alisterburt/napari-mrcfile-reader\",\"python_version\":\">=3.6\",\"reader_file_extensions\":[\"*\"],\"release_date\":\"2021-09-16T12:19:01.757212Z\",\"report_issues\":\"\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy (>=1.18)\",\"mrcfile (>=1.1)\"],\"summary\":\"read MRC format image files into napari\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.1.3\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"kevin.yamauchi@gmail.com\",\"name\":\"Kevin Yamauchi\"}],\"code_repository\":\"https://github.com/kevinyamauchi/napari-properties-viewer\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-properties-viewer\"}],\"description\":\"# napari-properties-viewer\\n\\n[![License](https://img.shields.io/pypi/l/napari-properties-viewer.svg?color=green)](https://github.com/napari/napari-properties-viewer/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-properties-viewer.svg?color=green)](https://pypi.org/project/napari-properties-viewer)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-properties-viewer.svg?color=green)](https://python.org)\\n[![tests](https://github.com/kevinyamauchi/napari-properties-viewer/workflows/tests/badge.svg)](https://github.com/kevinyamauchi/napari-properties-viewer/actions)\\n[![codecov](https://codecov.io/gh/kevinyamauchi/napari-properties-viewer/branch/master/graph/badge.svg)](https://codecov.io/gh/kevinyamauchi/napari-properties-viewer)\\n\\nA viewer for napari layer properties\\n\\n![image](resources/properties_viewer.gif)\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-properties-viewer` via [pip]:\\n\\n    pip install napari-properties-viewer\\n    \\n## Using the properties viewer table\\n\\n1. Open a a napari viewer with a layer with properties (e.g., Points)\\n2. View the properties by opening the properties viewer plugin from Plugins menu -> Add dock widget -> napari-propertiews-viewer: properties table\\n3. The layer property values are now displayed in the table widget. You can edit the values by double clicking the cell of interest and entering a new value.\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-properties-viewer\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/kevinyamauchi/napari-properties-viewer/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-properties-viewer\\n\\n\\n\\n\\n\\nA viewer for napari layer properties\\n\\nThis napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-properties-viewer via pip:\\npip install napari-properties-viewer\\n\\nUsing the properties viewer table\\n\\nOpen a a napari viewer with a layer with properties (e.g., Points)\\nView the properties by opening the properties viewer plugin from Plugins menu -> Add dock widget -> napari-propertiews-viewer: properties table\\nThe layer property values are now displayed in the table widget. You can edit the values by double clicking the cell of interest and entering a new value.\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-properties-viewer\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"napari-properties-viewer\",\"documentation\":\"https://github.com/kevinyamauchi/napari-properties-viewer\",\"first_released\":\"2021-04-28T13:47:44.032297Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-properties-viewer\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/kevinyamauchi/napari-properties-viewer\",\"python_version\":\">=3.6\",\"reader_file_extensions\":[],\"release_date\":\"2021-06-30T14:08:46.312555Z\",\"report_issues\":\"https://github.com/kevinyamauchi/napari-properties-viewer/issues\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\"],\"summary\":\"A viewer for napari layer properties\",\"support\":\"https://github.com/kevinyamauchi/napari-properties-viewer/issues\",\"twitter\":\"\",\"version\":\"0.0.2\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Robert Cudmore\"}],\"code_repository\":\"https://github.com/mapmanager/napari-layer-table\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-layer-table\"}],\"description\":\"# napari-layer-table\\n\\n[![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0)\\n[![PyPI version](https://badge.fury.io/py/napari-layer-table.svg)](https://badge.fury.io/py/napari-layer-table)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-layer-table)](https://napari-hub.org/plugins/napari-layer-table)\\n[![Python](https://img.shields.io/badge/python-3.7|3.8|3.9|3.10-blue.svg)](https://www.python.org/downloads/release/python-3100/)\\n[![OS](https://img.shields.io/badge/OS-Linux|Windows|macOS-blue.svg)]()\\n[![tests](https://github.com/mapmanager/napari-layer-table/workflows/Tests/badge.svg)](https://github.com/mapmanager/napari-layer-table/actions)\\n[![codecov](https://codecov.io/gh/mapmanager/napari-layer-table/branch/main/graph/badge.svg?token=8S8EFI8NBC)](https://codecov.io/gh/mapmanager/napari-layer-table)\\n<!-- [![PyPI](https://img.shields.io/pypi/v/napari-layer-table.svg?color=green)](https://pypi.org/project/napari-layer-table) -->\\n<!-- [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-layer-table)](https://napari-hub.org/plugins/napari-layer-table) -->\\n\\nA plugin to display a layer as a table.\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/plugins/stable/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-layer-table` via [pip]:\\n\\n    pip install napari-layer-table\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/mapmanager/napari-layer-table.git\\n\\n## Using the Plugin\\n\\nYou can use the napari-layer-table plugin to display points layer as a table.\\n\\n- Open a napari viewer with a Points layer\\n- Add the plugin to the napari viewer from Plugins menu -> Add dock widget -> napari-layer-table: Points Table\\n- The selected layer is displayed in the table.\\n- The table has columns for:\\n    - Point symbol with face color\\n    - Point coordinates (x,y,z)\\n    - If the layer has properties, they are also shown as columns\\n\\n![](plugin-2.gif)\\n\\n## Plugin Features\\n\\n- Bi-directional selection between layer and table.\\n- Bi-directional deletion between layer and table.\\n- Points added to the layer are added to the table.\\n- Points moved in the layer are updated in the table.\\n- Multiple points selected in the layer are also selected in the table\\n- Changes to face color and symbol in the layer are updated in the table.\\n- Ability to sort individual columns from low to high or high to low\\n- `Refresh` button to manually refresh the table data\\n- `btf` button to manually bring the layer whose table data is being shown to front\\n\\nRight-click for context menu to:\\n\\n- Toggle table columns on/off.\\n- Toggle shift+click to add a point to the layer (no need to switch viewer mode)\\n- Copy table to clipboard\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [GNU GPL v3.0] license,\\n\\\"napari-layer-table\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/mapmanager/napari-layer-table/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-layer-table\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nA plugin to display a layer as a table.\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-layer-table via pip:\\npip install napari-layer-table\\n\\nTo install latest development version :\\npip install git+https://github.com/mapmanager/napari-layer-table.git\\n\\nUsing the Plugin\\nYou can use the napari-layer-table plugin to display points layer as a table.\\n\\nOpen a napari viewer with a Points layer\\nAdd the plugin to the napari viewer from Plugins menu -> Add dock widget -> napari-layer-table: Points Table\\nThe selected layer is displayed in the table.\\nThe table has columns for:\\nPoint symbol with face color\\nPoint coordinates (x,y,z)\\nIf the layer has properties, they are also shown as columns\\n\\n\\n\\n\\nPlugin Features\\n\\nBi-directional selection between layer and table.\\nBi-directional deletion between layer and table.\\nPoints added to the layer are added to the table.\\nPoints moved in the layer are updated in the table.\\nMultiple points selected in the layer are also selected in the table\\nChanges to face color and symbol in the layer are updated in the table.\\nAbility to sort individual columns from low to high or high to low\\nRefresh button to manually refresh the table data\\nbtf button to manually bring the layer whose table data is being shown to front\\n\\nRight-click for context menu to:\\n\\nToggle table columns on/off.\\nToggle shift+click to add a point to the layer (no need to switch viewer mode)\\nCopy table to clipboard\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the GNU GPL v3.0 license,\\n\\\"napari-layer-table\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"Layer Table\",\"documentation\":\"https://github.com/mapmanager/napari-layer-table#README.md\",\"first_released\":\"2022-04-20T19:31:44.330744Z\",\"license\":\"GPL-3.0\",\"name\":\"napari-layer-table\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/mapmanager/napari-layer-table\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-10-23T16:38:13.083746Z\",\"report_issues\":\"https://github.com/mapmanager/napari-layer-table/issues\",\"requirements\":[\"numpy\"],\"summary\":\"A plugin to display a layer as a table.\",\"support\":\"https://github.com/mapmanager/napari-layer-table/issues\",\"twitter\":\"\",\"version\":\"0.0.10\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"William Patton\"}],\"code_repository\":\"https://github.com/pattonw/napari-affinities\",\"description\":\"# napari-affinities\\n\\n[![License](https://img.shields.io/pypi/l/napari-affinities.svg?color=green)](https://github.com/pattonw/napari-affinities/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-affinities.svg?color=green)](https://pypi.org/project/napari-affinities)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-affinities.svg?color=green)](https://python.org)\\n[![tests](https://github.com/pattonw/napari-affinities/workflows/tests/badge.svg)](https://github.com/pattonw/napari-affinities/actions)\\n[![codecov](https://codecov.io/gh/pattonw/napari-affinities/branch/main/graph/badge.svg)](https://codecov.io/gh/pattonw/napari-affinities)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-affinities)](https://napari-hub.org/plugins/napari-affinities)\\n\\nA plugin for creating, visualizing, and processing affinities\\n\\n---\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/plugins/stable/index.html\\n-->\\n\\n## Installation\\n\\nYou will need a conda environment for everything to run\\nsmoothly. Supported python versions are 3.7, 3.8, 3.9.\\n\\n### pip\\nYou can install `napari-affinities` via [pip]:\\n\\n    `pip install napari-affinities`\\n\\nTo install latest development version :\\n\\n    `pip install git+https://github.com/pattonw/napari-affinities.git`\\n\\nInstall torch according to your system [(follow the instructions here)](https://pytorch.org/get-started/locally/). For example with cuda 10.2 available, run:\\n\\n    conda install pytorch torchvision cudatoolkit=10.2 -c pytorch\\n\\nInstall conda requirements:\\n\\n    conda install -c conda-forge affogato\\n\\n### conda\\n\\nIf you install via conda, there are fewer steps since\\naffogato and pytorch will be installed for you.\\n\\nYou can install `napari-affinities` via [conda]:\\n\\n    `conda install -c conda-forge napari-affinities`\\n\\n### Download example model:\\n\\n#### 2D:\\n\\n[epithelial example model](https://oc.embl.de/index.php/s/zfWMKu7HoQnSJLs)\\nPlace the model zip file wherever you want. You can open it in the plugin with the \\\"load from file\\\" button.\\n\\n#### 3D\\n\\n[lightsheet example model](https://owncloud.gwdg.de/index.php/s/LsShICsOcilqPRs)\\nUnpack the tar file into test data (`lightsheet_nuclei_test_data` (an hdf5 file)) and model (`LightsheetNucleusSegmentation.zip` (a bioimageio model)).\\nMove the data into sample_data which will enable you to load the \\\"Lightsheet Sample\\\" data in napari.\\nPlace the model zip file anywhere you want. You can open it in the plugin with the \\\"load from file\\\" button.\\n\\n##### Workarounds to be fixed:\\n\\n1. you need to update the `rdf.yaml` in the `LightsheetNucleusSegmentation.zip` with the following:\\n   - \\\"shape\\\" for \\\"input0\\\" should be updated with a larger minimum input size and \\\"output0\\\" should be updated with a larger halo. If not fixed, there will be significant tiling artifacts.\\n   - (Optional) \\\"output0\\\" should be renamed to affinities. The plugin supports multiple outputs and relies on names for figuring out which one is which. If unrecognized names are provided we assume the outputs are ordered (affinities, fgbg, lsds) but this is less reliable than explicit names.\\n2. This model also generates foreground in the same array as affinities, i.e. a 10 channel output `(fgbg, [-1, 0, 0], [0, -1, 0], [0, 0, -1], [-2, 0, 0], ...)`. Although predictions will work, post processing such as mutex watershed will break unless you manually separate the first channel.\\n\\n## Use\\n\\nRequirements for the model:\\n\\n1. Bioimageio packaged pytorch model\\n2. Outputs with names \\\"affinities\\\", \\\"fgbg\\\"(optional) or \\\"lsds\\\"(optional)\\n   - if these names are not used, it will be assumed that the outputs are affinities, fgbg, then lsds in that order\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [MIT] license,\\n\\\"napari-affinities\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[mit]: http://opensource.org/licenses/MIT\\n[bsd-3]: http://opensource.org/licenses/BSD-3-Clause\\n[gnu gpl v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[gnu lgpl v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[apache software license 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[mozilla public license 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/pattonw/napari-affinities/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[pypi]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-affinities\\n\\n\\n\\n\\n\\n\\nA plugin for creating, visualizing, and processing affinities\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou will need a conda environment for everything to run\\nsmoothly. Supported python versions are 3.7, 3.8, 3.9.\\npip\\nYou can install napari-affinities via pip:\\n`pip install napari-affinities`\\n\\nTo install latest development version :\\n`pip install git+https://github.com/pattonw/napari-affinities.git`\\n\\nInstall torch according to your system (follow the instructions here). For example with cuda 10.2 available, run:\\nconda install pytorch torchvision cudatoolkit=10.2 -c pytorch\\n\\nInstall conda requirements:\\nconda install -c conda-forge affogato\\n\\nconda\\nIf you install via conda, there are fewer steps since\\naffogato and pytorch will be installed for you.\\nYou can install napari-affinities via [conda]:\\n`conda install -c conda-forge napari-affinities`\\n\\nDownload example model:\\n2D:\\nepithelial example model\\nPlace the model zip file wherever you want. You can open it in the plugin with the \\\"load from file\\\" button.\\n3D\\nlightsheet example model\\nUnpack the tar file into test data (lightsheet_nuclei_test_data (an hdf5 file)) and model (LightsheetNucleusSegmentation.zip (a bioimageio model)).\\nMove the data into sample_data which will enable you to load the \\\"Lightsheet Sample\\\" data in napari.\\nPlace the model zip file anywhere you want. You can open it in the plugin with the \\\"load from file\\\" button.\\nWorkarounds to be fixed:\\n\\nyou need to update the rdf.yaml in the LightsheetNucleusSegmentation.zip with the following:\\n\\\"shape\\\" for \\\"input0\\\" should be updated with a larger minimum input size and \\\"output0\\\" should be updated with a larger halo. If not fixed, there will be significant tiling artifacts.\\n(Optional) \\\"output0\\\" should be renamed to affinities. The plugin supports multiple outputs and relies on names for figuring out which one is which. If unrecognized names are provided we assume the outputs are ordered (affinities, fgbg, lsds) but this is less reliable than explicit names.\\nThis model also generates foreground in the same array as affinities, i.e. a 10 channel output (fgbg, [-1, 0, 0], [0, -1, 0], [0, 0, -1], [-2, 0, 0], ...). Although predictions will work, post processing such as mutex watershed will break unless you manually separate the first channel.\\n\\nUse\\nRequirements for the model:\\n\\nBioimageio packaged pytorch model\\nOutputs with names \\\"affinities\\\", \\\"fgbg\\\"(optional) or \\\"lsds\\\"(optional)\\nif these names are not used, it will be assumed that the outputs are affinities, fgbg, then lsds in that order\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the MIT license,\\n\\\"napari-affinities\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari Affinities\",\"documentation\":\"https://github.com/pattonw/napari-affinities#README.md\",\"first_released\":\"2022-06-12T17:19:57.548964Z\",\"license\":\"MIT\",\"name\":\"napari-affinities\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\",\"sample_data\"],\"project_site\":\"https://github.com/pattonw/napari-affinities\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-11-27T01:13:39.002044Z\",\"report_issues\":\"https://github.com/pattonw/napari-affinities/issues\",\"requirements\":[\"numpy\",\"zarr\",\"magicgui\",\"bioimageio.core\",\"gunpowder\",\"matplotlib\",\"torch\",\"lsds\"],\"summary\":\"A plugin for creating, visualizing, and processing affinities\",\"support\":\"https://github.com/pattonw/napari-affinities/issues\",\"twitter\":\"\",\"version\":\"0.1.3\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"jonas.windhager@uzh.ch\",\"name\":\"Jonas Windhager\"}],\"category\":{\"Supported data\":[\"Multi-channel\"]},\"category_hierarchy\":{\"Supported data\":[[\"Multi-channel\"]]},\"code_repository\":\"https://github.com/BodenmillerGroup/napari-imc\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-imc\"}],\"description\":\"**Supported file formats**:\\n - Fluidigm MCD\\n - Fluidigm TXT\\n\\n**Supported image types**:\\n  - Panoramas (single-channel, color)\\n  - Acquisitions (multi-channel, grayscale)\\n\\nAll images are loaded co-registered within the machine's coordinate system.\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"Supported file formats:\\n - Fluidigm MCD\\n - Fluidigm TXT\\nSupported image types:\\n  - Panoramas (single-channel, color)\\n  - Acquisitions (multi-channel, grayscale)\\nAll images are loaded co-registered within the machine's coordinate system.\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"napari-imc\",\"documentation\":\"https://github.com/BodenmillerGroup/napari-imc#README.md\",\"first_released\":\"2021-01-19T00:32:41.040698Z\",\"license\":\"MIT\",\"name\":\"napari-imc\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\",\"widget\"],\"project_site\":\"https://github.com/BodenmillerGroup/napari-imc\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[\"*.mcd\",\"*.txt\"],\"release_date\":\"2022-06-01T12:53:34.138474Z\",\"report_issues\":\"https://github.com/BodenmillerGroup/napari-imc/issues\",\"requirements\":[\"numpy\",\"qtpy\",\"readimc\",\"superqt\"],\"summary\":\"Imaging Mass Cytometry (IMC) file type support for napari\",\"support\":\"https://github.com/BodenmillerGroup/napari-imc/issues\",\"twitter\":\"\",\"version\":\"0.6.5\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Sean Martin\"}],\"code_repository\":\"https://github.com/seankmartin/napari-cookiecut\",\"conda\":[],\"description\":\"# napari-cookiecut\\n\\n[![License BSD-3](https://img.shields.io/pypi/l/napari-cookiecut.svg?color=green)](https://github.com/seankmartin/napari-cookiecut/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-cookiecut.svg?color=green)](https://pypi.org/project/napari-cookiecut)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-cookiecut.svg?color=green)](https://python.org)\\n[![tests](https://github.com/seankmartin/napari-cookiecut/workflows/tests/badge.svg)](https://github.com/seankmartin/napari-cookiecut/actions)\\n[![codecov](https://codecov.io/gh/seankmartin/napari-cookiecut/branch/main/graph/badge.svg)](https://codecov.io/gh/seankmartin/napari-cookiecut)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-cookiecut)](https://napari-hub.org/plugins/napari-cookiecut)\\n\\nA fixed version of a cookiecut napari plugin template that has been set up with all the basic functionality following the README for reference.\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/stable/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-cookiecut` via [pip]:\\n\\n    pip install napari-cookiecut\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/seankmartin/napari-cookiecut.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-cookiecut\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/seankmartin/napari-cookiecut/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-cookiecut\\n\\n\\n\\n\\n\\n\\nA fixed version of a cookiecut napari plugin template that has been set up with all the basic functionality following the README for reference.\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-cookiecut via pip:\\npip install napari-cookiecut\\n\\nTo install latest development version :\\npip install git+https://github.com/seankmartin/napari-cookiecut.git\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-cookiecut\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"Cookiecut\",\"documentation\":\"https://github.com/seankmartin/napari-cookiecut#README.md\",\"first_released\":\"2022-10-21T11:45:28.481005Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-cookiecut\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\",\"writer\",\"widget\",\"sample_data\"],\"project_site\":\"https://github.com/seankmartin/napari-cookiecut\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[\"*.npy\"],\"release_date\":\"2022-10-24T14:00:12.254912Z\",\"report_issues\":\"https://github.com/seankmartin/napari-cookiecut/issues\",\"requirements\":[\"numpy\",\"magicgui\",\"qtpy\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\"],\"summary\":\"Fixed cut\",\"support\":\"https://github.com/seankmartin/napari-cookiecut/issues\",\"twitter\":\"\",\"version\":\"0.1.1\",\"visibility\":\"public\",\"writer_file_extensions\":[\".npy\"],\"writer_save_layers\":[\"labels\",\"image\"]}",
  "{\"authors\":[{\"name\":\"Nicholas Sofroniew\"}],\"code_repository\":\"https://github.com/napari/napari-console\",\"description\":\"# napari-console (WIP, under active development)\\n\\n[![License](https://img.shields.io/pypi/l/napari-console.svg?color=green)](https://github.com/napari/napari-console/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-console.svg?color=green)](https://pypi.org/project/napari-console)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-console.svg?color=green)](https://python.org)\\n[![tests](https://github.com/sofroniewn/napari-console/workflows/tests/badge.svg)](https://github.com/sofroniewn/napari-console/actions)\\n[![codecov](https://codecov.io/gh/sofroniewn/napari-console/branch/master/graph/badge.svg)](https://codecov.io/gh/sofroniewn/napari-console)\\n\\nA plugin that adds a console to napari\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-console` via [pip]:\\n\\n    pip install napari-console\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-console\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/sofroniewn/napari-console/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-console (WIP, under active development)\\n\\n\\n\\n\\n\\nA plugin that adds a console to napari\\n\\nThis napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-console via pip:\\npip install napari-console\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-console\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"napari-console\",\"documentation\":\"\",\"first_released\":\"2021-01-21T04:42:40.342150Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-console\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[],\"project_site\":\"https://github.com/napari/napari-console\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-12-13T08:55:51.822464Z\",\"report_issues\":\"\",\"requirements\":[\"ipykernel (>=5.2.0)\",\"IPython (>=7.7.0)\",\"napari-plugin-engine (>=0.1.9)\",\"qtconsole (!=4.7.6,>=4.5.1)\",\"qtpy (>=1.7.0)\"],\"summary\":\"A plugin that adds a console to napari\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.0.7\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"cwood1967@gmail.com\",\"name\":\"Chris Wood\"}],\"code_repository\":\"https://github.com/cwood1967/napari-nikon-nd2\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-nikon-nd2\"}],\"description\":\"# napari-nikon-nd2\\n\\n[![License](https://img.shields.io/pypi/l/napari-nikon-nd2.svg?color=green)](https://github.com/cwood1967/napari-nikon-nd2/blob/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-nikon-nd2.svg?color=green)](https://pypi.org/project/napari-nikon-nd2)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-nikon-nd2.svg?color=green)](https://python.org)\\n[![tests](https://github.com/cwood1967/napari-nikon-nd2/workflows/tests/badge.svg)](https://github.com/cwood1967/napari-nikon-nd2/actions)\\n[![codecov](https://codecov.io/gh/cwood1967/napari-nikon-nd2/branch/main/graph/badge.svg)](https://codecov.io/gh/cwood1967/napari-nikon-nd2)\\n\\nOpens Nikon ND2 files into napari. This plugin uses the [nd2reader] and [pims] python packages. \\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-nikon-nd2` via [pip]:\\n\\n    pip install napari-nikon-nd2\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [Apache Software License 2.0] license,\\n\\\"napari-nikon-nd2\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n## Credits\\n\\nThis [napari] plugin was created using [Napari Delta Vision Reader] and\\nthe [Allen Institute IO] plugin as examples.\\n\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/cwood1967/napari-nikon-nd2/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n[nd2reader]: https://github.com/rbnvrw/nd2reader\\n[pims]: https://github.com/soft-matter/pims\\n[Allen Institute IO]: https://github.com/AllenCellModeling/napari-aicsimageio\\n[Napari Delta Vision Reader]: https://github.com/tlambert03/napari-dv\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-nikon-nd2\\n\\n\\n\\n\\n\\nOpens Nikon ND2 files into napari. This plugin uses the nd2reader and pims python packages. \\n\\nThis napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-nikon-nd2 via pip:\\npip install napari-nikon-nd2\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the Apache Software License 2.0 license,\\n\\\"napari-nikon-nd2\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\\nCredits\\nThis napari plugin was created using Napari Delta Vision Reader and\\nthe Allen Institute IO plugin as examples.\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"napari-nikon-nd2\",\"documentation\":\"\",\"first_released\":\"2021-02-03T21:21:34.976957Z\",\"license\":\"Apache-2.0\",\"name\":\"napari-nikon-nd2\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\"],\"project_site\":\"https://github.com/cwood1967/napari-nikon-nd2\",\"python_version\":\">=3.6\",\"reader_file_extensions\":[\"*\"],\"release_date\":\"2021-02-03T21:55:34.674256Z\",\"report_issues\":\"\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"nd2reader\"],\"summary\":\"Opens Nikon ND2 files into napari.\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.1.3\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Joel Luethi\"},{\"name\":\"Max Hess\"}],\"code_repository\":\"https://github.com/fractal-napari-plugins-collection/napari-feature-classifier\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-feature-classifier\"}],\"description\":\"# napari-feature-classifier\\n\\n[![License](https://img.shields.io/pypi/l/napari-feature-classifier.svg?color=green)](https://github.com/fractal-napari-plugins-collection/napari-feature-classifier/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-feature-classifier.svg?color=green)](https://pypi.org/project/napari-feature-classifier)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-feature-classifier.svg?color=green)](https://python.org)\\n[![tests](https://github.com/fractal-napari-plugins-collection/napari-feature-classifier/workflows/tests/badge.svg)](https://github.com/fractal-napari-plugins-collection/napari-feature-classifier/actions)\\n[![codecov](https://codecov.io/gh/fractal-napari-plugins-collection/napari-feature-classifier/branch/main/graph/badge.svg)](https://codecov.io/gh/fractal-napari-plugins-collection/napari-feature-classifier)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-feature-classifier)](https://napari-hub.org/plugins/napari-feature-classifier)\\n\\nAn interactive classifier plugin that allows the user to assign objects in a label image to multiple classes and train a classifier to learn those classes based on a feature dataframe.\\n\\n\\n## Usage\\n<p align=\\\"center\\\"><img src=\\\"https://user-images.githubusercontent.com/18033446/153727595-60380204-f299-485f-b762-d2030b75e7d3.gif\\\" /></p>\\nTo use the napari-feature-classifier, you need to have a label image and a csv file containing measurements that correspond to the object in the label image. The csv file needs to contain a column with integer values corresponding to the label values in the label image.\\nThese interactive classification workflows are well suited to visually define cell types, find mitotic cells in images, do quality control by automatically detecting missegmented cells and other tasks where a user can easily assign objects to groups.\\n\\n#### Initialize a classifier:\\n- Start the classifier in napari by going to `Plugins -> napari-feature-classifier -> Initialize a Classifier`  \\n- Provide a csv file that contains feature measurements and a column with the integer labels corresponding to the label layer you are using.\\n- Choose a name (or relative path from the current working directory) for the classifier. The classifier is initially saved in the current working directory (you can change this later on).\\n- Select the features you want to use for the classifier (you need to do the feature selection before initializing. The feature selection can't be changed after initialization anymore). Hold the command key to select multiple features.\\n<img width=\\\"1831\\\" alt=\\\"Initialize Classifier\\\" src=\\\"https://user-images.githubusercontent.com/18033446/153727784-d7b7d44b-a7b1-479f-a4af-34e0e280c8d6.png\\\">\\n\\n\\n#### Classify objects:\\n- Make sure you have the label layer selected on which you want to classify\\n- Select the current class with the radio buttons or by pressing 0, 1, 2, 3 or 4\\n- Click on label objects in the viewer to assign them to the currently selected class\\n- While you need to have the label layer active to select, sometimes you want to focus on the intensity images. You can press `v` (or manually change the opacity of the label layer) to focus on the intensity images.\\n- Once you have trained enough examples, click \\\"Run Classifier\\\" (or press `t`) to run the classifier and have it make a prediction for all objects. Aim for at least a dozen annotations per class, as the classifier divides your annotations 80/20 in training and test sets. To get good performance readouts, aim for >30 annotations per class.\\n- Once you get predictions, correct mistakes the classifier made and retrain it to improve its performance.\\n- You can save the classifier under a different name (to move it to a new folder or to have a slightly different version of the classifier - but careful, it autosaves whenever you run it). Define the new output location and then click `Save Classifier` (you need to click the Save Classifier button. Just defining the new output path does not save it yet)\\n<img width=\\\"1831\\\" alt=\\\"trainClassifier\\\" src=\\\"https://user-images.githubusercontent.com/18033446/153727960-daae2955-4368-4081-88da-1a1cdbda6e69.png\\\">\\n\\n\\n#### Apply the classifier to additional images:\\n- You can apply a classifier trained on one image to additional label images. Use `Plugins -> napari-feature-classifier -> Load Classifier`  \\n- Select the classifier (.clf file with the name you gave above) and a csv file containing the same features as the past images.\\n- Click Load Classifier, proceed as above.\\n<img width=\\\"1831\\\" alt=\\\"LoadClassifier\\\" src=\\\"https://user-images.githubusercontent.com/18033446/153728100-dd60918d-c9a4-4de8-8f0e-8fd8c6a51700.png\\\">\\n\\n\\n#### Export classifier results\\n- To export the training data and the results of the classifier, define an Export Name (full path to an output file or just a filename ending in .csv) where the results of the classifier shall be saved\\n- Click `Export Classifier Result` (Just selecting a filename is not enough, you need to click the export button)\\n- The results of the classifier are save in a csv file. The first two columns are index columns: path describes the Feature Path used (and allows you to understand which image / feature dataframe a result is from) and label is an integer of the label object within that image. The predict column contains predictions of the classifier for all objects (except those that contained NaNs in their feature data) and the train column contains the annotations you made (0 for unclassified objects, 1, 2, 3 or 4 for the classes)\\n![DataStructure](https://user-images.githubusercontent.com/18033446/153728461-d685987d-e1a9-46ff-834b-073008252ccb.png)\\n\\n\\nThere is a simple workflow for the classifier in the examples folder:\\n- Install jupyter-lab (`pip install jupyterlab`)\\n- Open the notebook in jupyter lab (Type `jupyter-lab` in the terminal when you are in the examples folder)\\n- Follow the instructions to generate an example dataframe and an example label image\\n- Use the classifier in napari with this simplified data\\n\\n## Installation\\n\\nThis plugin is written for the new napari npe2 plugin engine. Thus, it requires napari >= 0.4.13.\\nActivate your environment where you have napari installed (or install napari using `pip install \\\"napari[all]\\\"`), then install the classifier plugin:\\n\\n    pip install napari-feature-classifier\\n    \\n\\n## Contributing\\n\\nContributions are very welcome.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-feature-classifier\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n## Contributors\\n[Joel Lüthi](https://github.com/jluethi) & [Max Hess](https://github.com/MaksHess)\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-feature-classifier\\n\\n\\n\\n\\n\\n\\nAn interactive classifier plugin that allows the user to assign objects in a label image to multiple classes and train a classifier to learn those classes based on a feature dataframe.\\nUsage\\n\\nTo use the napari-feature-classifier, you need to have a label image and a csv file containing measurements that correspond to the object in the label image. The csv file needs to contain a column with integer values corresponding to the label values in the label image.\\nThese interactive classification workflows are well suited to visually define cell types, find mitotic cells in images, do quality control by automatically detecting missegmented cells and other tasks where a user can easily assign objects to groups.\\nInitialize a classifier:\\n\\nStart the classifier in napari by going to Plugins -> napari-feature-classifier -> Initialize a Classifier \\nProvide a csv file that contains feature measurements and a column with the integer labels corresponding to the label layer you are using.\\nChoose a name (or relative path from the current working directory) for the classifier. The classifier is initially saved in the current working directory (you can change this later on).\\nSelect the features you want to use for the classifier (you need to do the feature selection before initializing. The feature selection can't be changed after initialization anymore). Hold the command key to select multiple features.\\n\\n\\nClassify objects:\\n\\nMake sure you have the label layer selected on which you want to classify\\nSelect the current class with the radio buttons or by pressing 0, 1, 2, 3 or 4\\nClick on label objects in the viewer to assign them to the currently selected class\\nWhile you need to have the label layer active to select, sometimes you want to focus on the intensity images. You can press v (or manually change the opacity of the label layer) to focus on the intensity images.\\nOnce you have trained enough examples, click \\\"Run Classifier\\\" (or press t) to run the classifier and have it make a prediction for all objects. Aim for at least a dozen annotations per class, as the classifier divides your annotations 80/20 in training and test sets. To get good performance readouts, aim for >30 annotations per class.\\nOnce you get predictions, correct mistakes the classifier made and retrain it to improve its performance.\\nYou can save the classifier under a different name (to move it to a new folder or to have a slightly different version of the classifier - but careful, it autosaves whenever you run it). Define the new output location and then click Save Classifier (you need to click the Save Classifier button. Just defining the new output path does not save it yet)\\n\\n\\nApply the classifier to additional images:\\n\\nYou can apply a classifier trained on one image to additional label images. Use Plugins -> napari-feature-classifier -> Load Classifier \\nSelect the classifier (.clf file with the name you gave above) and a csv file containing the same features as the past images.\\nClick Load Classifier, proceed as above.\\n\\n\\nExport classifier results\\n\\nTo export the training data and the results of the classifier, define an Export Name (full path to an output file or just a filename ending in .csv) where the results of the classifier shall be saved\\nClick Export Classifier Result (Just selecting a filename is not enough, you need to click the export button)\\nThe results of the classifier are save in a csv file. The first two columns are index columns: path describes the Feature Path used (and allows you to understand which image / feature dataframe a result is from) and label is an integer of the label object within that image. The predict column contains predictions of the classifier for all objects (except those that contained NaNs in their feature data) and the train column contains the annotations you made (0 for unclassified objects, 1, 2, 3 or 4 for the classes)\\n\\n\\nThere is a simple workflow for the classifier in the examples folder:\\n- Install jupyter-lab (pip install jupyterlab)\\n- Open the notebook in jupyter lab (Type jupyter-lab in the terminal when you are in the examples folder)\\n- Follow the instructions to generate an example dataframe and an example label image\\n- Use the classifier in napari with this simplified data\\nInstallation\\nThis plugin is written for the new napari npe2 plugin engine. Thus, it requires napari >= 0.4.13.\\nActivate your environment where you have napari installed (or install napari using pip install \\\"napari[all]\\\"), then install the classifier plugin:\\npip install napari-feature-classifier\\n\\nContributing\\nContributions are very welcome.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-feature-classifier\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\nContributors\\nJoel Lüthi & Max Hess\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari feature classifier\",\"documentation\":\"\",\"first_released\":\"2022-02-12T22:19:29.954607Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-feature-classifier\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/fractal-napari-plugins-collection/napari-feature-classifier\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-02-12T22:19:29.954607Z\",\"report_issues\":\"\",\"requirements\":[\"numpy\",\"napari\",\"matplotlib\",\"magicgui\",\"pandas\",\"sklearn\"],\"summary\":\"An interactive classifier plugin to use with label images and feature measurements\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.0.1\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Marc Boucsein\"},{\"name\":\"Robin Koch\"}],\"code_repository\":\"https://github.com/MBPhys/Label-Creator\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"label-creator\"}],\"description\":\"# Label-Creator\\n\\n[![License](https://img.shields.io/pypi/l/napari-medical-image-formats.svg?color=green)](https://github.com/MBPhys/Label-Creator/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/Label-Creator.svg?color=green)](https://pypi.org/project/Label-Creator)\\n[![Python Version](https://img.shields.io/pypi/pyversions/Label-Creator.svg?color=green)](https://python.org)\\n\\n\\nA napari plugin for generation of Label-Layers according to selected image data shapes.\\n\\n----------------------------------\\n\\n## Installation\\n\\nYou can install `Label-Creator` via [pip]:\\n\\n    pip install Label-Creator\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"Label-Creator\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/MBPhys/Label-Creator/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"Label-Creator\\n\\n\\n\\nA napari plugin for generation of Label-Layers according to selected image data shapes.\\n\\nInstallation\\nYou can install Label-Creator via pip:\\npip install Label-Creator\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"Label-Creator\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"Label-Creator\",\"documentation\":\"\",\"first_released\":\"2022-01-12T18:26:04.391894Z\",\"license\":\"BSD-3-Clause\",\"name\":\"Label-Creator\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/MBPhys/Label-Creator\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-01-21T12:02:30.775041Z\",\"report_issues\":\"\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"dask\"],\"summary\":\"A napari plugin for generation of Label-Layers according to selected image data shapes\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.0.9\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"christinab12\"}],\"code_repository\":\"https://github.com/HelmholtzAI-Consultants-Munich/napari-organoid-counter\",\"conda\":[],\"description\":\"## Description\\n\\nA napari plugin to automatically count lung organoids from microscopy imaging data. The original implementation can be found in the [Organoid-Counting](https://github.com/HelmholtzAI-Consultants-Munich/Organoid-Counting) repository, which has been adapted here to work as a napari plugin. The CannyEdgeDetection algorithm is used for detecting the organoids and pre-processing steps ahve been added, specific to this type of data to obtain optimal resutls.\\n\\n![Alt Text](https://github.com/HelmholtzAI-Consultants-Munich/napari-organoid-counter/blob/main/readme-content/demo-plugin.gif)\\n\\n## Intended Audience & Supported Data\\n\\nThis plugin has been developed and tested with 2D CZI microscopy images of lunch organoids. The images had been previously converted from a 3D stack to 2D using an extended focus algorithm. This plugin may be used as a baseline for developers who wish to extend the plugin to work with other types of input images and/or improve the detection algorithm. \\n\\n## Dependencies\\n\\n```napari-organoid-counter``` uses the ```napari-aicsimageio```<sup>[1]</sup> plugin for reading and processing CZI images.\\n\\n[1] AICSImageIO Contributors (2021). AICSImageIO: Image Reading, Metadata Conversion, and Image Writing for Microscopy Images in Pure Python [Computer software]. GitHub. https://github.com/AllenCellModeling/aicsimageio\\n\\n## Quickstart\\n\\nThe use of the napari-organoid-counter plugin is straightforward. After loading the image or images you wish to process into the napari viewer, you must first pre-process them by clicking the _Preprocess_ button and the image layer will automatically be updated with the result. Next, you can adjust any of the parameters used in the algorithm (downsamppling, minimum organoid diamtere and sigma, i.e. kernel sixe for the Cannny Edge Detection algorithm) by using the corresponding sliders. By clicking the _Run Organoid Counter_ button the detection algorithm will run and a new shapes layer will be added to the viewer, with bounding boxes are placed around the detected organoid. You can add, edit or remove boxes using the _layer controls_ window and update the _Number of detected organoids_ displayed by clicking the _Update Number_ button. \\n\\nThe _Take Screenshot_ button has the same functionality as _File -> Save screenshot_. The _Reset Configs_ button will reset the image and all parameters to the original settings. To save your results, first select the shapes layer you wish to save form the dropdown menu. To save features of the detected organoids (diameters when approximating organoid as an ellipse and organoid area) in a csv file click _Save features_. To save the bounding boxes as a json file click _Save boxes_.\\n\\n\\n## Getting Help\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n## How to Cite\\nIf you use this plugin for your work, please cite it using the following:\\n```\\n@software{christina_bukas_2022_6457904,\\n  author       = {Christina Bukas},\\n  title        = {{HelmholtzAI-Consultants-Munich/napari-organoid- \\n                   counter: first release of napari plugin for lung\\n                   organoid counting}},\\n  month        = apr,\\n  year         = 2022,\\n  publisher    = {Zenodo},\\n  version      = {v0.1.0-beta},\\n  doi          = {10.5281/zenodo.6457904},\\n  url          = {https://doi.org/10.5281/zenodo.6457904}\\n}\\n```\\n\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"Description\\nA napari plugin to automatically count lung organoids from microscopy imaging data. The original implementation can be found in the Organoid-Counting repository, which has been adapted here to work as a napari plugin. The CannyEdgeDetection algorithm is used for detecting the organoids and pre-processing steps ahve been added, specific to this type of data to obtain optimal resutls.\\n\\nIntended Audience & Supported Data\\nThis plugin has been developed and tested with 2D CZI microscopy images of lunch organoids. The images had been previously converted from a 3D stack to 2D using an extended focus algorithm. This plugin may be used as a baseline for developers who wish to extend the plugin to work with other types of input images and/or improve the detection algorithm. \\nDependencies\\nnapari-organoid-counter uses the napari-aicsimageio[1] plugin for reading and processing CZI images.\\n[1] AICSImageIO Contributors (2021). AICSImageIO: Image Reading, Metadata Conversion, and Image Writing for Microscopy Images in Pure Python [Computer software]. GitHub. https://github.com/AllenCellModeling/aicsimageio\\nQuickstart\\nThe use of the napari-organoid-counter plugin is straightforward. After loading the image or images you wish to process into the napari viewer, you must first pre-process them by clicking the Preprocess button and the image layer will automatically be updated with the result. Next, you can adjust any of the parameters used in the algorithm (downsamppling, minimum organoid diamtere and sigma, i.e. kernel sixe for the Cannny Edge Detection algorithm) by using the corresponding sliders. By clicking the Run Organoid Counter button the detection algorithm will run and a new shapes layer will be added to the viewer, with bounding boxes are placed around the detected organoid. You can add, edit or remove boxes using the layer controls window and update the Number of detected organoids displayed by clicking the Update Number button. \\nThe Take Screenshot button has the same functionality as File -> Save screenshot. The Reset Configs button will reset the image and all parameters to the original settings. To save your results, first select the shapes layer you wish to save form the dropdown menu. To save features of the detected organoids (diameters when approximating organoid as an ellipse and organoid area) in a csv file click Save features. To save the bounding boxes as a json file click Save boxes.\\nGetting Help\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\nHow to Cite\\nIf you use this plugin for your work, please cite it using the following:\\n@software{christina_bukas_2022_6457904,\\n  author       = {Christina Bukas},\\n  title        = {{HelmholtzAI-Consultants-Munich/napari-organoid- \\n                   counter: first release of napari plugin for lung\\n                   organoid counting}},\\n  month        = apr,\\n  year         = 2022,\\n  publisher    = {Zenodo},\\n  version      = {v0.1.0-beta},\\n  doi          = {10.5281/zenodo.6457904},\\n  url          = {https://doi.org/10.5281/zenodo.6457904}\\n}\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari OrganoidCounter\",\"documentation\":\"https://github.com/HelmholtzAI-Consultants-Munich/napari-organoid-counter#README.md\",\"first_released\":\"2022-04-13T11:15:10.687948Z\",\"license\":\"MIT\",\"name\":\"napari-organoid-counter\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/HelmholtzAI-Consultants-Munich/napari-organoid-counter\",\"python_version\":\"<3.10,>=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-09-09T16:11:38.477206Z\",\"report_issues\":\"https://github.com/HelmholtzAI-Consultants-Munich/napari-organoid-counter/issues\",\"requirements\":[\"napari[all] (>=0.4.15)\",\"napari-aicsimageio (>=0.6.1)\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\"],\"summary\":\"A plugin to automatically count lung organoids\",\"support\":\"https://github.com/HelmholtzAI-Consultants-Munich/napari-organoid-counter/issues\",\"twitter\":\"\",\"version\":\"0.1.1\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"tasnadi.ervin@brc.hu\",\"name\":\"Ervin Tasnadi\"}],\"category\":{\"Image modality\":[\"Bright-field microscopy\"]},\"category_hierarchy\":{\"Image modality\":[[\"Bright-field microscopy\"]]},\"code_repository\":\"https://github.com/etasnadi/napari_nucleaizer\",\"conda\":[],\"description\":\"# napari_nucleaizer\\n\\n[![License](https://img.shields.io/pypi/l/napari-nucleaizer.svg?color=green)](https://github.com/etasnadi/napari-nucleaizer/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-nucleaizer.svg?color=green)](https://pypi.org/project/napari-nucleaizer)\\n[![Python package](https://github.com/etasnadi/napari_nucleaizer/actions/workflows/test_and_deploy.yml/badge.svg)](https://github.com/etasnadi/napari_nucleaizer/actions/workflows/test_and_deploy.yml)\\n[![codecov](https://codecov.io/gh/etasnadi/napari_nucleaizer/branch/master/graph/badge.svg?token=5XC36PA6OQ)](https://codecov.io/gh/etasnadi/napari_nucleaizer)\\n[![Documentation Status](https://readthedocs.org/projects/napari-nucleaizer-docs/badge/?version=latest)](https://napari-nucleaizer-docs.readthedocs.io/en/latest/?badge=latest)\\n\\n<!--\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-nucleaizer.svg?color=green)](https://python.org)\\n[![tests](https://github.com/etasnadi/napari_nucleaizer/workflows/tests/badge.svg)](https://github.com/etasnadi/napari-nucleaizer/actions)\\n[![codecov](https://codecov.io/gh/etasnadi/napari-nucleaizer/branch/master/graph/badge.svg)](https://codecov.io/gh/etasnadi/napari-nucleaizer)\\n-->\\n\\nGUI for the nucleaAIzer method in Napari.\\n\\n![Plugin interface in napari.](https://github.com/etasnadi/napari_nucleaizer/blob/main/napari_screenshot.png?raw=true)\\n\\n## Overview\\n\\nThis is a napari plugin to execute the nucleaizer nuclei segmentation algorithm.\\n\\n### Main functionalities\\n\\nUsing this plugin will be able to\\n\\n1. Load your image into Napar, then outline the nuclei.\\n2. Specify an image folder containing lots of images and an output folder, and automatically segment all of the images in the input folder.\\n3. If you are not satisfied with the results, you can train your own model:\\n    1. You can use our pretrained models and fine tune them on your data.\\n    2. You can skip the nucleaizer pipeline and train only on your data.\\n\\n\\n### Supported image types\\n\\nWe have several pretrained models for the following image modelities:\\n* fluorescent microscopy images\\n* IHC stained images\\n* brightfield microscopy images,\\n\\namong others. For the detailed descriptions of our models, see: https://zenodo.org/record/6800341.\\n\\n### How it works?\\n\\nFor the description of the algorithm, see our paper: \\\"Hollandi et al.: nucleAIzer: A Parameter-free Deep Learning Framework for Nucleus Segmentation Using Image Style Transfer, Cell Systems, 2020. https://doi.org/10.1016/j.cels.2020.04.003\\\"\\n\\nThe original code (https://github.com/spreka/biomagdsb) is partially transformed into a python package (nucleaizer_backend) to actually perform the operations. See the project page of the backend at: https://github.com/etasnadi/nucleaizer_backend.\\n\\nIf you wish to use the web interface, check: http://nucleaizer.org.\\n\\n![All functionalities.](https://github.com/etasnadi/napari_nucleaizer/blob/main/nucleaizer_screenshot.png?raw=true)\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Install\\n\\n1. Create an environment (recommended).\\n\\n2. Install napari: `pip install \\\"napari[pyqt5]\\\"`. Other methods: https://napari.org/tutorials/fundamentals/installation.html\\n\\n3. Install the plugin into napari:\\n\\n    * User mode from [PyPI](https://pypi.org/project/napari-nucleaizer/): start Napari (command line: `napari`) and select he **Install/Uninstall Plugins...** under the **Plugins** menu. In the popup, filter for `napari-nucleaizer`.\\n\\n    * Developer mode: clone this project and use `pythhon3 -m pip install -e <path>` to install the project locally **into the same evnrionment as napari**. It has the advantage that you will have the latest version.\\n## Run\\n\\n1. Start Napari by calling `napari` from the command line.\\n2. Then, activate the plugin in the `Plugins` menu. If you successfully installed the plugin, you have to see something like this:\\n\\n![Plugin interface in napari.](https://github.com/etasnadi/napari_nucleaizer/blob/main/napari_plugin_launch.png?raw=true)\\n\\n## Further help\\n\\nSee the [documentation](https://napari-nucleaizer-docs.readthedocs.io/en/latest/index.html) (work in progress).\\n\\n## Issues\\n\\nUse the github issue tracker if you experinece unexpected behaviour.\\n\\n## Contact\\n\\nYou can contact me in [e-mail](mailto:tasnadi.ervin@MY-INSTITUTE) where MY-INSTITUTE is `brc.hu`.\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari_nucleaizer\\n\\n\\n\\n\\n\\n\\nGUI for the nucleaAIzer method in Napari.\\n\\nOverview\\nThis is a napari plugin to execute the nucleaizer nuclei segmentation algorithm.\\nMain functionalities\\nUsing this plugin will be able to\\n\\nLoad your image into Napar, then outline the nuclei.\\nSpecify an image folder containing lots of images and an output folder, and automatically segment all of the images in the input folder.\\nIf you are not satisfied with the results, you can train your own model:\\nYou can use our pretrained models and fine tune them on your data.\\nYou can skip the nucleaizer pipeline and train only on your data.\\n\\n\\n\\nSupported image types\\nWe have several pretrained models for the following image modelities:\\n* fluorescent microscopy images\\n* IHC stained images\\n* brightfield microscopy images,\\namong others. For the detailed descriptions of our models, see: https://zenodo.org/record/6800341.\\nHow it works?\\nFor the description of the algorithm, see our paper: \\\"Hollandi et al.: nucleAIzer: A Parameter-free Deep Learning Framework for Nucleus Segmentation Using Image Style Transfer, Cell Systems, 2020. https://doi.org/10.1016/j.cels.2020.04.003\\\"\\nThe original code (https://github.com/spreka/biomagdsb) is partially transformed into a python package (nucleaizer_backend) to actually perform the operations. See the project page of the backend at: https://github.com/etasnadi/nucleaizer_backend.\\nIf you wish to use the web interface, check: http://nucleaizer.org.\\n\\n\\nInstall\\n\\n\\nCreate an environment (recommended).\\n\\n\\nInstall napari: pip install \\\"napari[pyqt5]\\\". Other methods: https://napari.org/tutorials/fundamentals/installation.html\\n\\n\\nInstall the plugin into napari:\\n\\n\\nUser mode from PyPI: start Napari (command line: napari) and select he Install/Uninstall Plugins... under the Plugins menu. In the popup, filter for napari-nucleaizer.\\n\\n\\nDeveloper mode: clone this project and use pythhon3 -m pip install -e <path> to install the project locally into the same evnrionment as napari. It has the advantage that you will have the latest version.\\n\\n\\nRun\\n\\n\\nStart Napari by calling napari from the command line.\\n\\nThen, activate the plugin in the Plugins menu. If you successfully installed the plugin, you have to see something like this:\\n\\n\\nFurther help\\nSee the documentation (work in progress).\\nIssues\\nUse the github issue tracker if you experinece unexpected behaviour.\\nContact\\nYou can contact me in e-mail where MY-INSTITUTE is brc.hu.\",\"development_status\":[\"Development Status :: 3 - Alpha\"],\"display_name\":\"Napari nucleAIzer plugin\",\"documentation\":\"https://napari-nucleaizer-docs.readthedocs.io/en/latest/index.html\",\"first_released\":\"2021-09-08T17:45:54.165468Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-nucleaizer\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/etasnadi/napari_nucleaizer\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-07-06T00:33:13.614307Z\",\"report_issues\":\"https://github.com/etasnadi/napari_nucleaizer/issues\",\"requirements\":[\"napari\",\"qtpy\",\"jsonpickle\",\"numpy\",\"scikit-image\",\"imageio\",\"nucleaizer-backend\"],\"summary\":\"A GUI interface for training and prediction using the nucleAIzer nuclei detection method.\",\"support\":\"https://github.com/etasnadi/napari_nucleaizer/issues\",\"twitter\":\"\",\"version\":\"0.2.4\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"cchiu@chanzuckerberg.com\",\"name\":\"Chi-Li Chiu\"}],\"code_repository\":\"https://github.com/chili-chiu/napari-labels-overlap\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-labels-overlap\"}],\"description\":\"# napari-labels-overlap\\n\\n[![License](https://img.shields.io/pypi/l/napari-labels-overlap.svg?color=green)](https://github.com/chili-chiu/napari-labels-overlap/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-labels-overlap.svg?color=green)](https://pypi.org/project/napari-labels-overlap)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-labels-overlap.svg?color=green)](https://python.org)\\n[![tests](https://github.com/chili-chiu/napari-labels-overlap/workflows/tests/badge.svg)](https://github.com/chili-chiu/napari-labels-overlap/actions)\\n[![codecov](https://codecov.io/gh/chili-chiu/napari-labels-overlap/branch/main/graph/badge.svg)](https://codecov.io/gh/chili-chiu/napari-labels-overlap)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-labels-overlap)](https://napari-hub.org/plugins/napari-labels-overlap)\\n\\ncreate an overlap labels layer from two labels layers\\n\\n## Description\\n\\nThis plugin takes two labels layers (layerA, layerB) as inputs, and generate the overlapped regions as a binary labels layer.\\nThree modes:<br>\\n(1) A_OR_B: new layer = layerA OR layerB (union)<br>\\n(2) A_AND_B: new layer = layerA AND layerB (intersection)<br>\\n(3) A_OUTSIDE_B: new layer = layerA OUTSIDE layerB (complement)<br>\\n\\n[comment]: <need to update the gif>\\n\\n![labels_overlap](https://user-images.githubusercontent.com/89602983/144129087-9a88d55f-f1a0-4825-bd01-770909bfc64f.gif)\\n\\n## Applicaions\\n- Object colocalization\\n- Merge separately identified objects\\n\\n## Future work\\n- Support N labels layers\\n- Basic coloc stats (% volume overlap)\\n- Output Labels with distinct IDs and links to original label IDs\\n\\n## Release log\\n- 0.0.2<br>\\n-- Run on npe2<br>\\n-- Add output types: binary/connected component<br>\\n- 0.0.1<br>\\n-- Run on npe1<br>\\n\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-labels-overlap` via [pip]:\\n\\n    pip install napari-labels-overlap\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/chili-chiu/napari-labels-overlap.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-labels-overlap\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/chili-chiu/napari-labels-overlap/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-labels-overlap\\n\\n\\n\\n\\n\\n\\ncreate an overlap labels layer from two labels layers\\nDescription\\nThis plugin takes two labels layers (layerA, layerB) as inputs, and generate the overlapped regions as a binary labels layer.\\nThree modes:\\n(1) A_OR_B: new layer = layerA OR layerB (union)\\n(2) A_AND_B: new layer = layerA AND layerB (intersection)\\n(3) A_OUTSIDE_B: new layer = layerA OUTSIDE layerB (complement)\\n[comment]: \\n\\nApplicaions\\n\\nObject colocalization\\nMerge separately identified objects\\n\\nFuture work\\n\\nSupport N labels layers\\nBasic coloc stats (% volume overlap)\\nOutput Labels with distinct IDs and links to original label IDs\\n\\nRelease log\\n\\n0.0.2\\n-- Run on npe2\\n-- Add output types: binary/connected component\\n0.0.1\\n-- Run on npe1\\n\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-labels-overlap via pip:\\npip install napari-labels-overlap\\n\\nTo install latest development version :\\npip install git+https://github.com/chili-chiu/napari-labels-overlap.git\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-labels-overlap\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari labels overlap\",\"documentation\":\"https://github.com/chili-chiu/napari-labels-overlap#README.md\",\"first_released\":\"2021-11-30T17:47:47.968332Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-labels-overlap\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/chili-chiu/napari-labels-overlap\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-06-22T22:19:03.264147Z\",\"report_issues\":\"https://github.com/chili-chiu/napari-labels-overlap/issues\",\"requirements\":[\"scikit-image\"],\"summary\":\"create an overlap labels layer from two labels layers\",\"support\":\"https://github.com/chili-chiu/napari-labels-overlap/issues\",\"twitter\":\"\",\"version\":\"0.0.3\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"talley.lambert@gmail.com\",\"name\":\"Talley Lambert\"}],\"code_repository\":\"https://github.com/tlambert03/napari-ndtiffs\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-ndtiffs\"}],\"description\":\"# napari-ndtiffs\\n\\n[![License](https://img.shields.io/pypi/l/napari-ndtiffs.svg?color=green)](https://raw.githubusercontent.com/tlambert03/napari-ndtiffs/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-ndtiffs.svg?color=green)](https://pypi.org/project/napari-ndtiffs)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-ndtiffs.svg?color=green)](https://python.org)\\n[![tests](https://github.com/tlambert03/napari-ndtiffs/workflows/tests/badge.svg)](https://github.com/tlambert03/napari-ndtiffs/actions)\\n[![codecov](https://codecov.io/gh/tlambert03/napari-ndtiffs/branch/master/graph/badge.svg)](https://codecov.io/gh/tlambert03/napari-ndtiffs)\\n\\nnapari plugin for nd tiff folders with optional OpenCl-based deskewing.\\n\\nBuilt-in support for folders of (skewed) lattice light sheet tiffs.\\n\\n![napari-ndtiffs demo](https://github.com/tlambert03/napari-ndtiffs/raw/master/demo.gif)\\n\\n----------------------------------\\n\\n*This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.*\\n\\n## Features\\n\\n- Drag and drop a folder of tiffs onto napari window to view easily \\n  - (currently designed to detect  lattice light sheet tiffs, but easily\\n    adjustable)\\n- If lattice `Settings.txt` file is found, will deskew automatically (only if\\n  necessary)\\n- Lazily loads dataset on demand.  quickly load preview your data.\\n- Handles `.zip` archives as well!  Just directly compress your tiff folder,\\n  then drop it into napari.\\n- All-openCL deskewing, works on GPU as well as CPU, falls back to scipy if\\n  pyopencl is unavailable.\\n\\nIt would not be hard to support arbitrary filenaming patterns!  If you have a\\nfolder of tiffs with a consistent naming scheme and would like to take advantage\\nof this plugin, feel free to open an issue!\\n\\n## Installation\\n\\nYou can install `napari-ndtiffs` via [pip]:\\n\\n```shell\\npip install napari-ndtiffs\\n```\\n\\nTo also install PyOpenCL (for faster deskewing):\\n\\n```shell\\npip install napari-ndtiffs[opencl]\\n```\\n\\n## Usage\\n\\nIn most cases, just drop your folder onto napari, or use `viewer.open(\\\"path\\\")`\\n\\n### Overriding parameters\\n\\nYou can control things like voxel size and deskewing angle as follows:\\n\\n```python\\nfrom napari_ndtiffs import parameter_override\\nimport napari\\n\\nviewer = napari.Viewer()\\nwith parameter_override(angle=45, name=\\\"my image\\\"):\\n    viewer.open(\\\"path/to/folder\\\", plugin=\\\"ndtiffs\\\")\\n```\\n\\nValid keys for `parameter_override` include:\\n\\n- **dx**: (`float`) the pixel size, in microns\\n- **dz**: (`float`)the z step size, in microns\\n- **deskew**: (`bool`) whether or not to deskew, (by default, will deskew if angle > 0, or if a lattice metadata file is detected that requires deskewing) \\n- **angle**: (`float`) the angle of the light sheet relative to the coverslip\\n- **padval**: (`float`) the value with which to pad the image edges when deskewing (default is 0)\\n- **contrast_limits**: (`2-tuple of int`) (min, max) contrast_limits to use when viewing the image\\n- **name**: (`str`) an optional name for the image\\n\\n### Sample data\\n\\nTry it out with test data: [download sample data](https://www.dropbox.com/s/up4ywrn2sckjunc/lls_mitosis.zip?dl=1)\\n\\nYou can unzip if you like, or just drag the zip file onto the napari window.\\n\\nOr, from command line, use:\\n\\n```bash\\nnapari path/to/lls_mitosis.zip\\n```\\n\\n## Debugging\\n\\nTo monitor file io and deskew activity, enter the following in the napari console:\\n\\n```python\\nimport logging\\nlogging.getLogger('napari_llsfolder').setLevel('DEBUG')\\n```\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-ndtiffs\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/tlambert03/napari-ndtiffs/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-ndtiffs\\n\\n\\n\\n\\n\\nnapari plugin for nd tiff folders with optional OpenCl-based deskewing.\\nBuilt-in support for folders of (skewed) lattice light sheet tiffs.\\n\\n\\nThis napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.\\nFeatures\\n\\nDrag and drop a folder of tiffs onto napari window to view easily \\n(currently designed to detect  lattice light sheet tiffs, but easily\\n    adjustable)\\nIf lattice Settings.txt file is found, will deskew automatically (only if\\n  necessary)\\nLazily loads dataset on demand.  quickly load preview your data.\\nHandles .zip archives as well!  Just directly compress your tiff folder,\\n  then drop it into napari.\\nAll-openCL deskewing, works on GPU as well as CPU, falls back to scipy if\\n  pyopencl is unavailable.\\n\\nIt would not be hard to support arbitrary filenaming patterns!  If you have a\\nfolder of tiffs with a consistent naming scheme and would like to take advantage\\nof this plugin, feel free to open an issue!\\nInstallation\\nYou can install napari-ndtiffs via pip:\\nshell\\npip install napari-ndtiffs\\nTo also install PyOpenCL (for faster deskewing):\\nshell\\npip install napari-ndtiffs[opencl]\\nUsage\\nIn most cases, just drop your folder onto napari, or use viewer.open(\\\"path\\\")\\nOverriding parameters\\nYou can control things like voxel size and deskewing angle as follows:\\n```python\\nfrom napari_ndtiffs import parameter_override\\nimport napari\\nviewer = napari.Viewer()\\nwith parameter_override(angle=45, name=\\\"my image\\\"):\\n    viewer.open(\\\"path/to/folder\\\", plugin=\\\"ndtiffs\\\")\\n```\\nValid keys for parameter_override include:\\n\\ndx: (float) the pixel size, in microns\\ndz: (float)the z step size, in microns\\ndeskew: (bool) whether or not to deskew, (by default, will deskew if angle > 0, or if a lattice metadata file is detected that requires deskewing) \\nangle: (float) the angle of the light sheet relative to the coverslip\\npadval: (float) the value with which to pad the image edges when deskewing (default is 0)\\ncontrast_limits: (2-tuple of int) (min, max) contrast_limits to use when viewing the image\\nname: (str) an optional name for the image\\n\\nSample data\\nTry it out with test data: download sample data\\nYou can unzip if you like, or just drag the zip file onto the napari window.\\nOr, from command line, use:\\nbash\\nnapari path/to/lls_mitosis.zip\\nDebugging\\nTo monitor file io and deskew activity, enter the following in the napari console:\\npython\\nimport logging\\nlogging.getLogger('napari_llsfolder').setLevel('DEBUG')\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-ndtiffs\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 3 - Alpha\"],\"display_name\":\"napari-ndtiffs\",\"documentation\":\"https://github.com/tlambert03/napari-ndtiffs#README.md\",\"first_released\":\"2020-05-06T00:05:33.846416Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-ndtiffs\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\"],\"project_site\":\"https://github.com/tlambert03/napari-ndtiffs\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[\"*\"],\"release_date\":\"2021-06-24T19:23:22.169470Z\",\"report_issues\":\"https://github.com/tlambert03/napari-ndtiffs/issues\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"dask[array]\",\"python-dateutil\",\"scipy\",\"tifffile\"],\"summary\":\"napari plugin for nd tiff folders with OpenCl deskew\",\"support\":\"https://github.com/tlambert03/napari-ndtiffs/issues\",\"twitter\":\"\",\"version\":\"0.1.2\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"david.pinto@bioch.ox.ac.uk\",\"name\":\"David Miguel Susano Pinto\"}],\"code_repository\":null,\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-microscope\"}],\"description\":\"Microscope control plugin for Napari via Python Microscope.\\n\\nCurrent development stage is whatever comes before alpha and \\\"proof of\\nconcept\\\".\\n\\nTo test\\n-------\\n\\nI haven't had access to real hardware yet, so this has all been\\ndeveloped with simulated devices.\\n\\n1. Start the device server with simulated devices.\\n\\n    a. Create a device server configuration file like so::\\n\\n        import microscope\\n        from microscope.device_server import device\\n        from microscope.simulators import (\\n            SimulatedCamera,\\n            SimulatedFilterWheel,\\n            SimulatedLightSource,\\n            SimulatedStage,\\n        )\\n\\n        DEVICES = [\\n            device(SimulatedCamera, \\\"localhost\\\", 8000,),\\n            device(SimulatedLightSource, \\\"localhost\\\", 8001),\\n            device(SimulatedFilterWheel, \\\"localhost\\\", 8002,\\n                   {\\\"positions\\\": 6}),\\n            device(SimulatedStage, \\\"localhost\\\", 8003,\\n                   {\\\"limits\\\": {\\\"X\\\": microscope.AxisLimits(0, 25000),\\n                               \\\"Y\\\": microscope.AxisLimits(0, 12000)}}),\\n        ]\\n\\n    b. Start the device server (ensure port 8000-8003 are unused)::\\n\\n        $ device-server path-to-microscope-config.py\\n\\n2. Start napari\\n\\n3. Plugins > Add Dock Widget > microscope: MicroscopeWidget\\n\\n4. Connect to the camera:\\n\\n    a. On the new widget, click on the \\\"Add device\\\" button.\\n\\n    b. Enter the camera URI `PYRO:SimulatedCamera@localhost:8000`\\n\\n5. Tick the `Enabled` box to enable the camera and then press the\\n\\\"Snap\\\" button.\\n\\n6. A random values image will appear displayed on the napari viewer.\\nKeep pressing the \\\"Snap\\\" button to get new images.  The top left\\ncorner of the image is the simulated image number.\\n\\n7. Connect to the other simulated devices.  Their URIs are:\\n\\n    a. PYRO:SimulatedLightSource@localhost:8001\\n    b. PYRO:SimulatedFilterWheel@localhost:8002\\n    c. PYRO:SimulatedStage@localhost:8003\\n\\n8. Changing the other simulated devices, doesn't really do much (but\\ndoes change state of the devices, as can be seen in the logs)\\n\\n\\nTest with stage aware camera\\n----------------------------\\n\\nThis is pretty much the same as before but one can use a large RGB\\nTIFF (histology samples are perfect) to simulate a camera that returns\\nsubsections of the image file based on the simulated stage position.\\n\\nFor quick example, try::\\n\\n    wget https://zenodo.org/record/1445489/files/B0002.tif\\n\\nAnd use the following device server configuration file::\\n\\n    from microscope.device_server import device\\n    from microscope.simulators.stage_aware_camera import simulated_setup_from_image\\n\\n    DEVICES = [\\n        device(simulated_setup_from_image, \\\"localhost\\\", 8000,\\n               conf={\\\"filepath\\\": \\\"B0002.tif\\\"}),\\n    ]\\n\\nThe URI for the devices will be::\\n\\n    PYRO:camera@localhost:8000\\n    PYRO:filterwheel@localhost:8000\\n    PYRO:stage@localhost:8000\\n\\nChanging the filterwheel changes which channel from the image is\\nreturned.  Changing the stage coordinates changes the image that is\\nreturned (but beware of the corners, pixels outside the image size are\\nnot handled yet and will give an error).\",\"description_content_type\":\"\",\"description_text\":\"Microscope control plugin for Napari via Python Microscope.\\nCurrent development stage is whatever comes before alpha and \\\"proof of\\nconcept\\\".\\nTo test\\nI haven't had access to real hardware yet, so this has all been\\ndeveloped with simulated devices.\\n\\n\\nStart the device server with simulated devices.\\na. Create a device server configuration file like so::\\nimport microscope\\nfrom microscope.device_server import device\\nfrom microscope.simulators import (\\n    SimulatedCamera,\\n    SimulatedFilterWheel,\\n    SimulatedLightSource,\\n    SimulatedStage,\\n)\\n\\nDEVICES = [\\n    device(SimulatedCamera, \\\"localhost\\\", 8000,),\\n    device(SimulatedLightSource, \\\"localhost\\\", 8001),\\n    device(SimulatedFilterWheel, \\\"localhost\\\", 8002,\\n           {\\\"positions\\\": 6}),\\n    device(SimulatedStage, \\\"localhost\\\", 8003,\\n           {\\\"limits\\\": {\\\"X\\\": microscope.AxisLimits(0, 25000),\\n                       \\\"Y\\\": microscope.AxisLimits(0, 12000)}}),\\n]\\n\\nb. Start the device server (ensure port 8000-8003 are unused)::\\n$ device-server path-to-microscope-config.py\\n\\n\\n\\nStart napari\\n\\n\\nPlugins > Add Dock Widget > microscope: MicroscopeWidget\\n\\n\\nConnect to the camera:\\na. On the new widget, click on the \\\"Add device\\\" button.\\nb. Enter the camera URI PYRO:SimulatedCamera@localhost:8000\\n\\n\\nTick the Enabled box to enable the camera and then press the\\n\\\"Snap\\\" button.\\n\\n\\nA random values image will appear displayed on the napari viewer.\\nKeep pressing the \\\"Snap\\\" button to get new images.  The top left\\ncorner of the image is the simulated image number.\\n\\n\\nConnect to the other simulated devices.  Their URIs are:\\na. PYRO:SimulatedLightSource@localhost:8001\\nb. PYRO:SimulatedFilterWheel@localhost:8002\\nc. PYRO:SimulatedStage@localhost:8003\\n\\n\\nChanging the other simulated devices, doesn't really do much (but\\ndoes change state of the devices, as can be seen in the logs)\\n\\n\\nTest with stage aware camera\\nThis is pretty much the same as before but one can use a large RGB\\nTIFF (histology samples are perfect) to simulate a camera that returns\\nsubsections of the image file based on the simulated stage position.\\nFor quick example, try::\\nwget https://zenodo.org/record/1445489/files/B0002.tif\\n\\nAnd use the following device server configuration file::\\nfrom microscope.device_server import device\\nfrom microscope.simulators.stage_aware_camera import simulated_setup_from_image\\n\\nDEVICES = [\\n    device(simulated_setup_from_image, \\\"localhost\\\", 8000,\\n           conf={\\\"filepath\\\": \\\"B0002.tif\\\"}),\\n]\\n\\nThe URI for the devices will be::\\nPYRO:camera@localhost:8000\\nPYRO:filterwheel@localhost:8000\\nPYRO:stage@localhost:8000\\n\\nChanging the filterwheel changes which channel from the image is\\nreturned.  Changing the stage coordinates changes the image that is\\nreturned (but beware of the corners, pixels outside the image size are\\nnot handled yet and will give an error).\",\"development_status\":[\"Development Status :: 3 - Alpha\"],\"display_name\":\"napari-microscope\",\"documentation\":\"\",\"first_released\":\"2021-01-22T19:50:42.567472Z\",\"license\":\"GPL-3.0-or-later\",\"name\":\"napari-microscope\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"\",\"python_version\":\">=3.6\",\"reader_file_extensions\":[],\"release_date\":\"2021-11-23T12:10:33.763843Z\",\"report_issues\":\"\",\"requirements\":null,\"summary\":\"Napari plugin for Microscope.\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.0.3\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[],\"code_repository\":\"https://github.com/AllenCellModeling/napari-aicsimageio\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-aicsimageio\"}],\"description\":\"## Features\\n\\n-   Supports reading metadata and imaging data for:\\n    -   `OME-TIFF`\\n    -   `TIFF`\\n    -   `CZI` (Zeiss)\\n    -   `LIF` (Leica)\\n    -   `ND2` (Nikon)\\n    -   `DV` (DeltaVision)\\n    -   Any formats supported by [aicsimageio](https://github.com/AllenCellModeling/aicsimageio)\\n    -   Any formats supported by [bioformats](https://github.com/tlambert03/bioformats_jar)\\n        -   `SLD` (Slidebook)\\n        -   `SVS` (Aperio)\\n        -   [Full List](https://docs.openmicroscopy.org/bio-formats/6.5.1/supported-formats.html)\\n    -   Any additional format supported by [imageio](https://github.com/imageio/imageio)\\n        -   `PNG`\\n        -   `JPG`\\n        -   `GIF`\\n        -   `AVI`\\n        -   [Full List](https://imageio.readthedocs.io/en/v2.4.1/formats.html)\\n\\n_While upstream `aicsimageio` is released under BSD-3 license, this plugin is released under GPLv3 license because it installs all format reader dependencies._\\n\\n### Reading Mode Threshold\\n\\nThis image reading plugin will load the provided image directly into memory if it meets\\nthe following two conditions:\\n\\n1. The filesize is less than 4GB.\\n2. The filesize is less than 30% of machine memory available.\\n\\nIf either of these conditions isn't met, the image is loaded in chunks only as needed.\\n\\n## Examples of Features\\n\\n#### General Image Reading\\n\\nAll image file formats supported by\\n[aicsimageio](https://github.com/AllenCellModeling/aicsimageio) will be read and all\\nraw data will be available in the napari viewer.\\n\\nIn addition, when reading an OME-TIFF, you can view all OME metadata directly in the\\nnapari viewer thanks to `ome-types`.\\n\\n![screenshot of an OME-TIFF image view, multi-channel, z-stack, with metadata viewer](https://raw.githubusercontent.com/AllenCellModeling/napari-aicsimageio/main/images/ome-tiff-with-metadata-viewer.png)\\n\\n#### Multi-Scene Selection\\n\\nWhen reading a multi-scene file, a widget will be added to the napari viewer to manage\\nscene selection (clearing the viewer each time you change scene or adding the\\nscene content to the viewer) and a list of all scenes in the file.\\n\\n![gif of drag and drop file to scene selection and management](https://raw.githubusercontent.com/AllenCellModeling/napari-aicsimageio/main/images/scene-selection.gif)\\n\\n#### Access to the AICSImage Object and Metadata\\n\\n![napari viewer with console open showing `viewer.layers[0].metadata`](https://raw.githubusercontent.com/AllenCellModeling/napari-aicsimageio/main/images/console-access.png)\\n\\nYou can access the `AICSImage` object used to load the image pixel data and\\nimage metadata using the built-in napari console:\\n\\n```python\\nimg = viewer.layers[0].metadata[\\\"aicsimage\\\"]\\nimg.dims.order  # TCZYX\\nimg.channel_names  # [\\\"Bright\\\", \\\"Struct\\\", \\\"Nuc\\\", \\\"Memb\\\"]\\nimg.get_image_dask_data(\\\"ZYX\\\")  # dask.array.Array\\n```\\n\\nThe napari layer metadata dictionary also stores a shorthand\\nfor the raw image metadata:\\n\\n```python\\nviewer.layers[0].metadata[\\\"raw_image_metadata\\\"]\\n```\\n\\nThe metadata is returned in whichever format is used by the underlying\\nfile format reader, i.e. for CZI the raw metadata is returned as\\nan `xml.etree.ElementTree.Element`, for OME-TIFF the raw metadata is returned\\nas an `OME` object from `ome-types`.\\n\\nLastly, if the underlying file format reader has an OME metadata conversion function,\\nyou may additionally see a key in the napari layer metadata dictionary\\ncalled `\\\"ome_types\\\"`. For example, because the AICSImageIO\\n`CZIReader` and `BioformatsReader` both support converting raw image metadata\\nto OME metadata, you will see an `\\\"ome_types\\\"` key that stores the metadata transformed\\ninto the OME metadata model.\\n\\n```python\\nviewer.layers[0].metadata[\\\"ome_types\\\"]  # OME object from ome-types\\n```\\n\\n#### Mosaic Reading\\n\\nWhen reading CZI or LIF images, if the image is a mosaic tiled image, `napari-aicsimageio`\\nwill return the reconstructed image:\\n\\n![screenshot of a reconstructed / restitched mosaic tile LIF](https://raw.githubusercontent.com/AllenCellModeling/napari-aicsimageio/main/images/tiled-lif.png)\\n\\n## Citation\\n\\nIf you find `aicsimageio` _(or `napari-aicsimageio`)_ useful, please cite as:\\n\\n> AICSImageIO Contributors (2021). AICSImageIO: Image Reading, Metadata Conversion, and Image Writing for Microscopy Images in Pure Python [Computer software]. GitHub. https://github.com/AllenCellModeling/aicsimageio\\n\\n_Free software: GPLv3_\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"Features\\n\\nSupports reading metadata and imaging data for:\\nOME-TIFF\\nTIFF\\nCZI (Zeiss)\\nLIF (Leica)\\nND2 (Nikon)\\nDV (DeltaVision)\\nAny formats supported by aicsimageio\\nAny formats supported by bioformats\\nSLD (Slidebook)\\nSVS (Aperio)\\nFull List\\n\\n\\nAny additional format supported by imageio\\nPNG\\nJPG\\nGIF\\nAVI\\nFull List\\n\\n\\n\\n\\n\\nWhile upstream aicsimageio is released under BSD-3 license, this plugin is released under GPLv3 license because it installs all format reader dependencies.\\nReading Mode Threshold\\nThis image reading plugin will load the provided image directly into memory if it meets\\nthe following two conditions:\\n\\nThe filesize is less than 4GB.\\nThe filesize is less than 30% of machine memory available.\\n\\nIf either of these conditions isn't met, the image is loaded in chunks only as needed.\\nExamples of Features\\nGeneral Image Reading\\nAll image file formats supported by\\naicsimageio will be read and all\\nraw data will be available in the napari viewer.\\nIn addition, when reading an OME-TIFF, you can view all OME metadata directly in the\\nnapari viewer thanks to ome-types.\\n\\nMulti-Scene Selection\\nWhen reading a multi-scene file, a widget will be added to the napari viewer to manage\\nscene selection (clearing the viewer each time you change scene or adding the\\nscene content to the viewer) and a list of all scenes in the file.\\n\\nAccess to the AICSImage Object and Metadata\\n\\nYou can access the AICSImage object used to load the image pixel data and\\nimage metadata using the built-in napari console:\\npython\\nimg = viewer.layers[0].metadata[\\\"aicsimage\\\"]\\nimg.dims.order  # TCZYX\\nimg.channel_names  # [\\\"Bright\\\", \\\"Struct\\\", \\\"Nuc\\\", \\\"Memb\\\"]\\nimg.get_image_dask_data(\\\"ZYX\\\")  # dask.array.Array\\nThe napari layer metadata dictionary also stores a shorthand\\nfor the raw image metadata:\\npython\\nviewer.layers[0].metadata[\\\"raw_image_metadata\\\"]\\nThe metadata is returned in whichever format is used by the underlying\\nfile format reader, i.e. for CZI the raw metadata is returned as\\nan xml.etree.ElementTree.Element, for OME-TIFF the raw metadata is returned\\nas an OME object from ome-types.\\nLastly, if the underlying file format reader has an OME metadata conversion function,\\nyou may additionally see a key in the napari layer metadata dictionary\\ncalled \\\"ome_types\\\". For example, because the AICSImageIO\\nCZIReader and BioformatsReader both support converting raw image metadata\\nto OME metadata, you will see an \\\"ome_types\\\" key that stores the metadata transformed\\ninto the OME metadata model.\\npython\\nviewer.layers[0].metadata[\\\"ome_types\\\"]  # OME object from ome-types\\nMosaic Reading\\nWhen reading CZI or LIF images, if the image is a mosaic tiled image, napari-aicsimageio\\nwill return the reconstructed image:\\n\\nCitation\\nIf you find aicsimageio (or napari-aicsimageio) useful, please cite as:\\n\\nAICSImageIO Contributors (2021). AICSImageIO: Image Reading, Metadata Conversion, and Image Writing for Microscopy Images in Pure Python [Computer software]. GitHub. https://github.com/AllenCellModeling/aicsimageio\\n\\nFree software: GPLv3\",\"development_status\":[\"Development Status :: 5 - Production/Stable\"],\"display_name\":\"napari-aicsimageio\",\"documentation\":\"https://github.com/AllenCellModeling/napari-aicsimageio#README.md\",\"first_released\":\"2020-03-26T20:10:54.889461Z\",\"license\":\"GPL-3.0\",\"name\":\"napari-aicsimageio\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\"],\"project_site\":\"\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[\"*.klb\",\"*.cap\",\"*.nhdr\",\"*.htm\",\"*.v\",\"*.cif\",\"*.mhd\",\"*.lms\",\"*.mos\",\"*.raf\",\"*.jpg\",\"*.dat\",\"*.dm3\",\"*.pct\",\"*.qptiff\",\"*.rgba\",\"*.sm2\",\"*.jfif\",\"*.dcx\",\"*.jif\",\"*.ffr\",\"*.arw\",\"*.oib\",\"*.mov\",\"*.ali\",\"*.psd\",\"*.fits\",\"*.cfg\",\"*.jxr\",\"*.ftc\",\"*.afi\",\"*.dds\",\"*.hdf5\",\"*.icns\",\"*.iff\",\"*.dicom\",\"*.jpk\",\"*.k25\",\"*.exp\",\"*.ras\",\"*.bin\",\"*.spe\",\"*.flc\",\"*.tnb\",\"*.mic\",\"*.obf\",\"*.tiff\",\"*.pcd\",\"*.inf\",\"*.nd2\",\"*.lfr\",\"*.mng\",\"*.avi\",\"*.g3\",\"*.nrw\",\"*.grey\",\"*.sxm\",\"*.2fl\",\"*.am\",\"*.qtk\",\"*.wap\",\"*.pef\",\"*.sr2\",\"*.bufr\",\"*.sm3\",\"*.grib\",\"*.vws\",\"*.ndpis\",\"*.sdt\",\"*.fts\",\"*.rgb\",\"*.dti\",\"*.mdb\",\"*.iiq\",\"*.lif\",\"*.cat\",\"*.drf\",\"*.i2i\",\"*.orf\",\"*.koa\",\"*.thm\",\"*.gel\",\"*.mpeg\",\"*.wmf\",\"*.fff\",\"*.jpc\",\"*.bw\",\"*.rec\",\"*.ipl\",\"*.wdp\",\"*.bmp\",\"*.xqf\",\"*.pfm\",\"*.png\",\"*.fake\",\"*.ndpi\",\"*.ch5\",\"*.tim\",\"*.pict\",\"*.oif\",\"*.aim\",\"*.iim\",\"*.zfr\",\"*.mtb\",\"*.mri\",\"*.srf\",\"*.pnl\",\"*.acff\",\"*.par\",\"*.spi\",\"*.mod\",\"*.xdce\",\"*.gif\",\"*.kc2\",\"*.cine\",\"*.arf\",\"*.scan\",\"*.labels\",\"*.dsc\",\"*.mpg\",\"*.epsi\",\"*.fit\",\"*.bay\",\"*.dm2\",\"*.ipw\",\"*.dv\",\"*.htd\",\"*.nef\",\"*.jng\",\"*.bsdf\",\"*.bmq\",\"*.oir\",\"*.j2k\",\"*.ome.tiff\",\"*.zpo\",\"*.hdp\",\"*.j2c\",\"*.ptx\",\"*.mgh\",\"*.zfp\",\"*.eps\",\"*.fz\",\"*.ano\",\"*.gipl\",\"*.jpf\",\"*.vsi\",\"*.tga\",\"*.ims\",\"*.nii.gz\",\"*.db\",\"*.pbm\",\"*.pgm\",\"*.lbm\",\"*.exr\",\"*.mha\",\"*.pxn\",\"*.sif\",\"*.fli\",\"*.rw2\",\"*.scn\",\"*.wpi\",\"*.html\",\"*.rdc\",\"*.jpx\",\"*.lim\",\"*.stk\",\"*.im3\",\"*.xml\",\"*.lsm\",\"*.ico\",\"*.kdc\",\"*.c01\",\"*.stp\",\"*.npz\",\"*.cut\",\"*.ipm\",\"*.vff\",\"*.webp\",\"*.xv\",\"*.ome\",\"*.afm\",\"*.dc2\",\"*.pxr\",\"*.fid\",\"*.mvd2\",\"*.hdr\",\"*.r3d\",\"*.frm\",\"*.lfp\",\"*.flex\",\"*.mnc\",\"*.niigz\",\"*.xpm\",\"*.csv\",\"*.svs\",\"*.ecw\",\"*.3fr\",\"*.gbr\",\"*.ppm\",\"*.swf\",\"*.tif\",\"*.xbm\",\"*.vms\",\"*.wbm\",\"*.mpo\",\"*.jpe\",\"*.msr\",\"*.hdf\",\"*.mrc\",\"*.mp4\",\"*.msp\",\"*.zvi\",\"*.xqd\",\"*.erf\",\"*.xys\",\"*.zip\",\"*.acqp\",\"*.cs1\",\"*.ics\",\"*.amiramesh\",\"*.ia\",\"*.liff\",\"*.ct.img\",\"*.h5\",\"*.ids\",\"*.pcoraw\",\"*.nii\",\"*.nd\",\"*.spc\",\"*.emf\",\"*.mrw\",\"*.lei\",\"*.nia\",\"*.hed\",\"*.czi\",\"*.top\",\"*.fpx\",\"*.st\",\"*.crw\",\"*.mef\",\"*.rcpnl\",\"*.df3\",\"*.array-like\",\"*.ftu\",\"*.rwl\",\"*.1sc\",\"*.seq\",\"*.wbmp\",\"*.ome.tif\",\"*.rwz\",\"*.nrrd\",\"*.im\",\"*.img\",\"*.wlz\",\"*.cr2\",\"*.dng\",\"*.naf\",\"*.l2d\",\"*.sld\",\"*.raw\",\"*.mdc\",\"*.vtk\",\"*.gdcm\",\"*.al3d\",\"*.jp2\",\"*.apl\",\"*.wav\",\"*.targa\",\"*.ct\",\"*.pr3\",\"*.dcr\",\"*.mkv\",\"*.tfr\",\"*.txt\",\"*.pic\",\"*.fdf\",\"*.pcx\",\"*.sti\",\"*.jpeg\",\"*.inr\",\"*.mnc2\",\"*.cxd\",\"*.dcm\",\"*.bip\",\"*.imggz\",\"*.bif\",\"*.his\",\"*.cur\",\"*.wmv\",\"*.wat\",\"*.hx\",\"*.srw\",\"*.ps\"],\"release_date\":\"2022-08-22T15:31:20.051131Z\",\"report_issues\":\"https://github.com/AllenCellModeling/napari-aicsimageio/issues\",\"requirements\":[\"aicsimageio[all] (>=4.6.3)\",\"fsspec[http] (>=2022.7.1)\",\"napari (>=0.4.11)\",\"psutil (>=5.7.0)\",\"aicspylibczi (>=3.0.5)\",\"bioformats-jar\",\"readlif (>=0.6.4)\",\"black (>=19.10b0) ; extra == 'dev'\",\"coverage (>=5.1) ; extra == 'dev'\",\"docutils (<0.16,>=0.10) ; extra == 'dev'\",\"flake8-debugger (>=3.2.1) ; extra == 'dev'\",\"flake8-pyprojecttoml ; extra == 'dev'\",\"flake8 (>=3.8.3) ; extra == 'dev'\",\"ipython (>=7.15.0) ; extra == 'dev'\",\"isort (>=5.7.0) ; extra == 'dev'\",\"mypy (>=0.800) ; extra == 'dev'\",\"pytest-runner (>=5.2) ; extra == 'dev'\",\"twine (>=3.1.1) ; extra == 'dev'\",\"wheel (>=0.34.2) ; extra == 'dev'\",\"PyQt5 ; extra == 'test'\",\"pytest (>=5.4.3) ; extra == 'test'\",\"pytest-qt (~=4.0) ; extra == 'test'\",\"pytest-cov (>=2.9.0) ; extra == 'test'\",\"pytest-raises (>=0.11) ; extra == 'test'\",\"pytest-xvfb (~=2.0) ; extra == 'test'\",\"quilt3 (~=3.4.0) ; extra == 'test'\"],\"summary\":\"AICSImageIO bindings for napari\",\"support\":\"https://github.com/AllenCellModeling/napari-aicsimageio/issues\",\"twitter\":\"\",\"version\":\"0.7.2\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Alan R. Lowe\"}],\"category\":{\"Supported data\":[\"2D\",\"3D\",\"Time series\",\"Multi-channel\"],\"Workflow step\":[\"Object tracking\"]},\"category_hierarchy\":{\"Supported data\":[[\"2D\"],[\"3D\"],[\"Time series\"],[\"Multi-channel\"]],\"Workflow step\":[[\"Object tracking\",\"Isolated object tracking\",\"Cell tracking\"],[\"Object tracking\",\"Cell lineage extraction\"],[\"Object tracking\"]]},\"code_repository\":\"https://github.com/quantumjot/BayesianTracker\",\"description\":\"[![PyPI](https://img.shields.io/pypi/v/btrack)](https://pypi.org/project/btrack)\\n[![Supported Python versions](https://img.shields.io/pypi/pyversions/btrack.svg)](https://python.org)\\n[![Downloads](https://pepy.tech/badge/btrack/month)](https://pepy.tech/project/btrack)\\n[![Black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\\n[![Tests](https://github.com/quantumjot/BayesianTracker/actions/workflows/test.yml/badge.svg)](https://github.com/quantumjot/BayesianTracker/actions/workflows/test.yml)\\n[![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&logoColor=white)](https://github.com/pre-commit/pre-commit)\\n[![Documentation](https://readthedocs.org/projects/btrack/badge/?version=latest)](https://btrack.readthedocs.io/en/latest/?badge=latest)\\n[![codecov](https://codecov.io/gh/quantumjot/BayesianTracker/branch/main/graph/badge.svg?token=QCFC9AWK0R)](https://codecov.io/gh/quantumjot/BayesianTracker)\\n[![doi:10.3389/fcomp.2021.734559](https://img.shields.io/badge/doi-10.3389%2Ffcomp.2021.734559-blue)](https://doi.org/10.3389/fcomp.2021.734559)\\n\\n![logo](https://btrack.readthedocs.io/en/latest/_images/btrack_logo.png)\\n\\n\\nBayesianTracker (`btrack`) is a multi object tracking algorithm,\\nspecifically used to reconstruct trajectories in crowded fields.  New\\nobservations are assigned to tracks by evaluating the posterior probability of\\neach potential linkage from a Bayesian belief matrix for all possible\\nlinkages.\\n\\nWe developed `btrack` for cell tracking in time-lapse microscopy data.\\n\\n![](https://raw.githubusercontent.com/lowe-lab-ucl/arboretum/master/examples/arboretum.gif)\\n\\n<!--\\n## tutorials\\n\\n* https://napari.org/tutorials/tracking/cell_tracking.html\\n-->\\n\\n\\n## associated plugins\\n\\n* [napari-arboretum](https://www.napari-hub.org/plugins/napari-arboretum) - Napari plugin to enable track graph and lineage tree visualization.\\n* [napari-btrack](https://github.com/lowe-lab-ucl/napari-btrack) - (Experimental) Napari plugin to provide a frontend GUI for `btrack`.\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBayesianTracker (btrack) is a multi object tracking algorithm,\\nspecifically used to reconstruct trajectories in crowded fields.  New\\nobservations are assigned to tracks by evaluating the posterior probability of\\neach potential linkage from a Bayesian belief matrix for all possible\\nlinkages.\\nWe developed btrack for cell tracking in time-lapse microscopy data.\\n\\n\\nassociated plugins\\n\\nnapari-arboretum - Napari plugin to enable track graph and lineage tree visualization.\\nnapari-btrack - (Experimental) Napari plugin to provide a frontend GUI for btrack.\\n\",\"development_status\":[],\"display_name\":\"btrack\",\"documentation\":\"https://btrack.readthedocs.io/en/stable/\",\"first_released\":\"2020-05-27T07:49:37.422791Z\",\"license\":\"MIT\",\"name\":\"btrack\",\"npe2\":true,\"operating_system\":[\"Operating System :: MacOS\",\"Operating System :: Microsoft :: Windows\",\"Operating System :: POSIX\",\"Operating System :: Unix\"],\"plugin_types\":[\"reader\"],\"project_site\":\"https://github.com/quantumjot/BayesianTracker\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[\"*.h5\",\"*.hdf5\",\"*.hdf\"],\"release_date\":\"2022-12-05T16:52:14.480984Z\",\"report_issues\":\"https://github.com/quantumjot/BayesianTracker/issues\",\"requirements\":[\"cvxopt (>=1.2.0)\",\"h5py (>=2.10.0)\",\"numpy (>=1.17.3)\",\"pooch (>=1.0.0)\",\"pydantic (>=1.9.0)\",\"scikit-image (>=0.16.2)\",\"scipy (>=1.3.1)\",\"numpydoc ; extra == 'docs'\",\"sphinx ; extra == 'docs'\",\"sphinx-automodapi ; extra == 'docs'\",\"sphinx-panels ; extra == 'docs'\",\"sphinx-rtd-theme ; extra == 'docs'\",\"napari (>=0.4.16) ; extra == 'napari'\"],\"summary\":\"A framework for Bayesian multi-object tracking\",\"support\":\"https://github.com/quantumjot/BayesianTracker/issues\",\"twitter\":\"\",\"version\":\"0.5.0\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[],\"code_repository\":\"https://github.com/emilmelnikov/napari-cilia-beating-frequency\",\"conda\":[],\"description\":\"# Cilia beating frequency\\n\\n> Cilia beating frequency detection.\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"Cilia beating frequency\\n\\nCilia beating frequency detection.\\n\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"Cilia beating frequency detection\",\"documentation\":\"\",\"first_released\":\"2022-07-13T08:44:09.675558Z\",\"license\":\"MIT\",\"name\":\"napari-cilia-beating-frequency\",\"npe2\":true,\"operating_system\":[],\"plugin_types\":[\"widget\"],\"project_site\":\"\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-07-13T08:44:09.675558Z\",\"report_issues\":\"\",\"requirements\":[\"magicgui\",\"napari[all]\",\"numpy (>=1.20)\",\"qtpy\",\"scipy\"],\"summary\":\"Cilia beating frequency detection\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.1\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"talley.lambert@gmail.com\",\"name\":\"Talley Lambert\"}],\"code_repository\":\"https://github.com/tlambert03/napari-bioformats\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-bioformats\"}],\"description\":\"# napari-bioformats\\n\\n[![License](https://img.shields.io/pypi/l/napari-bioformats.svg?color=green)](https://github.com/napari/napari-bioformats/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-bioformats.svg?color=green)](https://pypi.org/project/napari-bioformats)\\n[![Conda](https://img.shields.io/conda/v/conda-forge/napari-bioformats)](https://anaconda.org/conda-forge/napari-bioformats)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-bioformats.svg?color=green)](https://python.org)\\n[![tests](https://github.com/tlambert03/napari-bioformats/workflows/tests/badge.svg)](https://github.com/tlambert03/napari-bioformats/actions)\\n[![codecov](https://codecov.io/gh/tlambert03/napari-bioformats/branch/master/graph/badge.svg)](https://codecov.io/gh/tlambert03/napari-bioformats)\\n\\nBioformats plugin for napari using\\n[pims-bioformats](http://soft-matter.github.io/pims/v0.5/bioformats.html)\\n\\n----------------------------------\\n\\n## Use this plugin as a fallback!\\n\\nAnyone coming to napari from the Fiji/ImageJ world will likely be aware of the\\n_incredible_ [Bio-Formats](https://docs.openmicroscopy.org/bio-formats/6.6.1/index.html)\\nlibrary.  A heroic effort, built over years, to read\\n[more than a 100 file formats](https://docs.openmicroscopy.org/bio-formats/6.6.1/supported-formats.html).  Naturally, we want some of that goodness for `napari` ... hence this plugin.\\n\\n**However:** it's important to note that this plugin _still_\\nrequires having a java runtime engine installed.  This is easy enough to do\\n(the plugin will ask to install it for you if you're in a `conda` environment), but\\nit definitely makes for a more complicated environment setup, it's not very\\n\\\"pythonic\\\", and the performance will likely not feel as snappy as a native \\\"pure\\\"\\npython module.\\n\\nSo, before you reflexively install this plugin to fill that bio-formats\\nsized hole in your python heart, consider trying some of the other pure-python\\nplugins designed to read your format of interest:\\n\\n- **Zeiss (.czi)**: [napari-aicsimageio](https://github.com/AllenCellModeling/napari-aicsimageio), [napari-czifile2](https://github.com/BodenmillerGroup/napari-czifile2)\\n- **Nikon (.nd2)**: [napari-nikon-nd2](https://github.com/cwood1967/napari-nikon-nd2), [nd2-dask](https://github.com/DragaDoncila/nd2-dask)\\n- **Leica (.lif)**: [napari-aicsimageio](https://github.com/AllenCellModeling/napari-aicsimageio)\\n- **Olympus (.oif)**: no plugin?  (but see [oiffile](https://pypi.org/project/oiffile/) )\\n- **DeltaVision (.dv, .mrc)**: [napari-dv](https://github.com/tlambert03/napari-dv)\\n\\n> *if you have a pure-python reader for a bio-formats-supported file format that\\nyou'd like to see added to this list, please open an issue*\\n\\n## Installation\\n\\nThe easiest way to install `napari-bioformats` is via [conda], from the\\n[conda-forge] channel:\\n\\n    conda install -c conda-forge napari-bioformats\\n\\nIt is also possible to install via [pip], but you will need to have a working\\nJVM installed, and may need to set the `JAVA_HOME` environment variable\\n\\n    pip install napari-bioformats\\n\\n### First Usage\\n\\nThe first time you attempt to open a file with napari-bioformats, you will\\nlikely notice a long delay as pims downloads the `loci_tools.jar` (speed will\\ndepend on your internet connection). Subsequent files should open more quickly.\\n\\n## License\\n\\nDistributed under the terms of the [GPLv3] license,\\n\\\"napari-bioformats\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n_This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template._\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[GPLv3]: https://opensource.org/licenses/GPL-3.0\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/tlambert03/napari-bioformats/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[conda]: https://docs.conda.io/en/latest/\\n[conda-forge]: https://conda-forge.org\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-bioformats\\n\\n\\n\\n\\n\\n\\nBioformats plugin for napari using\\npims-bioformats\\n\\nUse this plugin as a fallback!\\nAnyone coming to napari from the Fiji/ImageJ world will likely be aware of the\\nincredible Bio-Formats\\nlibrary.  A heroic effort, built over years, to read\\nmore than a 100 file formats.  Naturally, we want some of that goodness for napari ... hence this plugin.\\nHowever: it's important to note that this plugin still\\nrequires having a java runtime engine installed.  This is easy enough to do\\n(the plugin will ask to install it for you if you're in a conda environment), but\\nit definitely makes for a more complicated environment setup, it's not very\\n\\\"pythonic\\\", and the performance will likely not feel as snappy as a native \\\"pure\\\"\\npython module.\\nSo, before you reflexively install this plugin to fill that bio-formats\\nsized hole in your python heart, consider trying some of the other pure-python\\nplugins designed to read your format of interest:\\n\\nZeiss (.czi): napari-aicsimageio, napari-czifile2\\nNikon (.nd2): napari-nikon-nd2, nd2-dask\\nLeica (.lif): napari-aicsimageio\\nOlympus (.oif): no plugin?  (but see oiffile )\\nDeltaVision (.dv, .mrc): napari-dv\\n\\n\\nif you have a pure-python reader for a bio-formats-supported file format that\\nyou'd like to see added to this list, please open an issue\\n\\nInstallation\\nThe easiest way to install napari-bioformats is via conda, from the\\nconda-forge channel:\\nconda install -c conda-forge napari-bioformats\\n\\nIt is also possible to install via pip, but you will need to have a working\\nJVM installed, and may need to set the JAVA_HOME environment variable\\npip install napari-bioformats\\n\\nFirst Usage\\nThe first time you attempt to open a file with napari-bioformats, you will\\nlikely notice a long delay as pims downloads the loci_tools.jar (speed will\\ndepend on your internet connection). Subsequent files should open more quickly.\\nLicense\\nDistributed under the terms of the GPLv3 license,\\n\\\"napari-bioformats\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\\nThis napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"napari-bioformats\",\"documentation\":\"https://github.com/tlambert03/napari-bioformats#README.md\",\"first_released\":\"2021-06-25T21:06:09.152167Z\",\"license\":\"GPL-3.0\",\"name\":\"napari-bioformats\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\"],\"project_site\":\"https://github.com/tlambert03/napari-bioformats\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[\"*\"],\"release_date\":\"2021-08-11T03:46:24.676720Z\",\"report_issues\":\"https://github.com/tlambert03/napari-bioformats/issues\",\"requirements\":[\"jpype1\",\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"ome-types\",\"pims\",\"requests\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\"],\"summary\":\"Bioformats for napari, using pims\",\"support\":\"https://github.com/tlambert03/napari-bioformats/issues\",\"twitter\":\"\",\"version\":\"0.2.1\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"robert.haase@tu-dresden.de\",\"name\":\"Robert Haase\"}],\"code_repository\":\"https://github.com/haesleinhuepf/napari-folder-browser\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-folder-browser\"}],\"description\":\"# napari-folder-browser\\n\\n[![License](https://img.shields.io/pypi/l/napari-folder-browser.svg?color=green)](https://github.com/haesleinhuepf/napari-folder-browser/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-folder-browser.svg?color=green)](https://pypi.org/project/napari-folder-browser)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-folder-browser.svg?color=green)](https://python.org)\\n[![tests](https://github.com/haesleinhuepf/napari-folder-browser/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-folder-browser/actions)\\n[![codecov](https://codecov.io/gh/haesleinhuepf/napari-folder-browser/branch/master/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-folder-browser)\\n\\nBrowse folders of images and open them using double-click or <ENTER>. You can also navigate through the list using arrow up/down keys.\\n\\n![](https://github.com/haesleinhuepf/napari-folder-browser/raw/main/docs/napari-folder-browser.gif)\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n## Installation\\n\\nYou can install `napari-folder-browser` from within napari by clicking menu `Plugins > Install/uninstall Plugins...` and entering here:\\n![img.png](https://github.com/haesleinhuepf/napari-folder-browser/raw/main/docs/install.png)\\n\\nYou can install `napari-folder-browser` via [pip]:\\n\\n    pip install napari-folder-browser\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-folder-browser\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please create a thread on [image.sc] along with a detailed description and tag [@haesleinhuepf].\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/haesleinhuepf/napari-folder-browser/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n[image.sc]: https://image.sc\\n[@haesleinhuepf]: https://twitter.com/haesleinhuepf\\n\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-folder-browser\\n\\n\\n\\n\\n\\nBrowse folders of images and open them using double-click or . You can also navigate through the list using arrow up/down keys.\\n\\n\\nThis napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.\\nInstallation\\nYou can install napari-folder-browser from within napari by clicking menu Plugins > Install/uninstall Plugins... and entering here:\\n\\nYou can install napari-folder-browser via pip:\\npip install napari-folder-browser\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-folder-browser\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please create a thread on image.sc along with a detailed description and tag @haesleinhuepf.\",\"development_status\":[\"Development Status :: 3 - Alpha\"],\"display_name\":\"napari-folder-browser\",\"documentation\":\"https://github.com/haesleinhuepf/napari-folder-browser#README.md\",\"first_released\":\"2021-10-03T13:57:14.143133Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-folder-browser\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/haesleinhuepf/napari-folder-browser\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-01-01T15:55:38.504053Z\",\"report_issues\":\"https://github.com/haesleinhuepf/napari-folder-browser/issues\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"napari-tools-menu\"],\"summary\":\"Browse folders of images and open them using double-click\",\"support\":\"https://github.com/haesleinhuepf/napari-folder-browser/issues\",\"twitter\":\"\",\"version\":\"0.1.3\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Blik Team\"}],\"code_repository\":\"https://github.com/gutsche-lab/blik\",\"description\":\"![logo](https://github.com/gutsche-lab/blik/raw/main/docs/images/logo.png)\\n\\n# `blik`\\n\\n![Codecov branch](https://img.shields.io/codecov/c/github/gutsche-lab/blik/main?label=codecov)\\n![PyPI](https://img.shields.io/pypi/v/blik)\\n![PyPI - Python Version](https://img.shields.io/pypi/pyversions/blik)\\n\\n*it means glance in Dutch*\\n\\n![blik showcase](https://user-images.githubusercontent.com/23482191/161224963-ad746a06-c2e5-46fe-a13b-f356bc4ad72b.png)\\n\\n**`blik`** is a tool for visualising and interacting with cryo-ET and subtomogram averaging data. It leverages the fast, multi-dimensional [napari viewer](https://napari.org) and the scientific python stack.\\n\\n**DISCLAIMER**: this package is in development phase. Expect frequent bugs and crashes. Please, report them on the issue tracker and ask if anything is unclear!\\n\\n## Installation\\n\\nYou can either install `blik` through the [napari plugin system](https://napari.org/plugins/index.html), through pip, or get both napari and blik directly with:\\n\\n```bash\\npip install \\\"blik[all]\\\"\\n```\\n\\nThe `[all]` qualifier also installs `pyqt5` as the napari GUI backend, and a few additional napari plugins that you might find useful in your workflow:\\n- [napari-properties-plotter](https://github.com/brisvag/napari-properties-plotter)\\n- [napari-properties-viewer](https://github.com/kevinyamauchi/napari-properties-viewer)\\n- [napari-label-interpolator](https://github.com/kevinyamauchi/napari-label-interpolator)\\n\\n## Basic Usage\\n\\nFrom the command line:\\n```bash\\nnapari -w blik -- /path/to.star /path/to/mrc/files/*\\n```\\n\\nThe `-w blik` is important for proper initialization of all the layers. Keep the main widget open to ensure nothing goes wrong!\\n\\n*`blik` is just `napari`*. Particles and images are exposed as simple napari layers, which can be analysed and manipulated with simple python, and most importantly other [napari plugins](https://napari-hub.org/).\\n\\n## Widget\\n\\nThe main widget has a few functions:\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"\\nblik\\n\\n\\n\\nit means glance in Dutch\\n\\nblik is a tool for visualising and interacting with cryo-ET and subtomogram averaging data. It leverages the fast, multi-dimensional napari viewer and the scientific python stack.\\nDISCLAIMER: this package is in development phase. Expect frequent bugs and crashes. Please, report them on the issue tracker and ask if anything is unclear!\\nInstallation\\nYou can either install blik through the napari plugin system, through pip, or get both napari and blik directly with:\\nbash\\npip install \\\"blik[all]\\\"\\nThe [all] qualifier also installs pyqt5 as the napari GUI backend, and a few additional napari plugins that you might find useful in your workflow:\\n- napari-properties-plotter\\n- napari-properties-viewer\\n- napari-label-interpolator\\nBasic Usage\\nFrom the command line:\\nbash\\nnapari -w blik -- /path/to.star /path/to/mrc/files/*\\nThe -w blik is important for proper initialization of all the layers. Keep the main widget open to ensure nothing goes wrong!\\nblik is just napari. Particles and images are exposed as simple napari layers, which can be analysed and manipulated with simple python, and most importantly other napari plugins.\\nWidget\\nThe main widget has a few functions:\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"\",\"documentation\":\"https://github.com/gutsche-lab/blik#README.md\",\"first_released\":\"2021-06-15T13:05:03.510893Z\",\"license\":\"GPL-3.0\",\"name\":\"blik\",\"operating_system\":[\"Operating System :: MacOS\",\"Operating System :: Microsoft :: Windows\",\"Operating System :: POSIX\",\"Operating System :: Unix\"],\"plugin_types\":[],\"project_site\":\"https://github.com/gutsche-lab/blik\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-12-07T11:29:01.544622Z\",\"report_issues\":\"https://github.com/gutsche-lab/blik/issues\",\"requirements\":[\"numpy\",\"dask\",\"pandas\",\"scipy\",\"magicgui (>=0.4.0)\",\"cryohub (>=0.3.2)\",\"cryotypes\",\"napari[all] (>=0.4.17) ; extra == 'all'\",\"napari-properties-plotter ; extra == 'all'\",\"napari-properties-viewer ; extra == 'all'\",\"napari-label-interpolator ; extra == 'all'\",\"black ; extra == 'dev'\",\"flake8 ; extra == 'dev'\",\"isort ; extra == 'dev'\",\"pre-commit ; extra == 'dev'\",\"napari[all] (>=0.4.17) ; extra == 'dev'\",\"tox ; extra == 'dev'\",\"pytest ; extra == 'dev'\",\"pytest-qt ; extra == 'dev'\",\"pyqt5 ; extra == 'dev'\",\"napari[all] (>=0.4.17) ; extra == 'testing'\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\"],\"summary\":\"Python tool for visualising and interacting with cryo-ET and subtomogram averaging data.\",\"support\":\"https://github.com/gutsche-lab/blik/issues\",\"twitter\":\"\",\"version\":\"0.3.4\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Kevin Yamauchi\"}],\"code_repository\":\"https://github.com/kevinyamauchi/morphometrics\",\"description\":\"# morphometrics\\n\\n[![License](https://img.shields.io/pypi/l/morphometrics.svg?color=green)](https://github.com/morphometrics/morphometrics/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/morphometrics.svg?color=green)](https://pypi.org/project/morphometrics)\\n[![Python Version](https://img.shields.io/pypi/pyversions/morphometrics.svg?color=green)](https://python.org)\\n[![tests](https://github.com/morphometrics/morphometrics/workflows/tests/badge.svg)](https://github.com/morphometrics/morphometrics/actions)\\n[![codecov](https://codecov.io/gh/morphometrics/morphometrics/branch/main/graph/badge.svg)](https://codecov.io/gh/morphometrics/morphometrics)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/morphometrics)](https://napari-hub.org/plugins/morphometrics)\\n\\nA plugin for quantifying shape and neighborhoods from images.\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/plugins/stable/index.html\\n-->\\n\\n## Installation\\n\\n### conda environment file\\nYou can install `morphometrics` via our conda environment file. To do so, first install anaconda or miniconda on\\nyour computer. Then, download the [`environment.yml file`](https://raw.githubusercontent.com/kevinyamauchi/morphometrics/master/environment.yml) (right click the link and \\\"Save as...\\\"). In your terminal,\\nnavigate to the directory you downloaded the `environment.yml` file to:\\n\\n```bash\\ncd <path/to/downloaded/environment.yml>\\n```\\n\\nThen create the `morphometrics` environment using\\n\\n```bash\\nconda env create -f environment.yml\\n```\\n\\nOnce the environment has been created, you can activate it and use `morphometrics` as described below.\\n\\n```bash\\nconda activate morphometrics\\n```\\n\\nIf you are on Mac OS or Linux install the following:\\n\\nMac:\\n\\n```bash\\nconda install -c conda-forge ocl_icd_wrapper_apple\\n```\\n\\nLinux:\\n\\n```bash\\nconda install -c conda-forge ocl-icd-system\\n```\\n\\n\\n### Development installation\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/kevinyamauchi/morphometrics.git\\n\\n## Example applications\\n<table border=\\\"0\\\">\\n<tr><td>\\n\\n\\n<img src=\\\"https://github.com/kevinyamauchi/morphometrics/raw/main/resources/surface_distance_measurement.gif\\\"\\nwidth=\\\"300\\\"/>\\n\\n</td><td>\\n\\n[measure the distance between surfaces](https://github.com/kevinyamauchi/morphometrics/blob/main/examples/surface_distance_measurement.ipynb)\\n\\n</td></tr><tr><td>\\n\\n<img src=\\\"https://github.com/kevinyamauchi/morphometrics/raw/main/resources/region_props_plugin.png\\\"\\nwidth=\\\"300\\\"/>\\n\\n</td><td>\\n\\n[napari plugin for measuring properties of segmented objects (regionprops)](https://github.com/kevinyamauchi/morphometrics/blob/main/examples/measure_with_widget.py)\\n\\n</td></tr><tr><td>\\n\\n<img src=\\\"https://github.com/kevinyamauchi/morphometrics/raw/main/resources/object_classification.png\\\"\\nwidth=\\\"300\\\"/>\\n\\n</td><td>\\n\\n[object classification](https://github.com/kevinyamauchi/morphometrics/blob/main/examples/object_classification.ipynb)\\n\\n</td></tr><tr><td>\\n\\n<img src=\\\"https://github.com/kevinyamauchi/morphometrics/raw/main/resources/mesh_object.png\\\"\\nwidth=\\\"300\\\"/>\\n\\n</td><td>\\n\\n[mesh binary mask](https://github.com/kevinyamauchi/morphometrics/blob/main/examples/mesh_binary_mask.ipynb)\\n\\n\\n</td></tr></table>\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"morphometrics\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/kevinyamauchi/morphometrics/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"morphometrics\\n\\n\\n\\n\\n\\n\\nA plugin for quantifying shape and neighborhoods from images.\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nconda environment file\\nYou can install morphometrics via our conda environment file. To do so, first install anaconda or miniconda on\\nyour computer. Then, download the environment.yml file (right click the link and \\\"Save as...\\\"). In your terminal,\\nnavigate to the directory you downloaded the environment.yml file to:\\nbash\\ncd <path/to/downloaded/environment.yml>\\nThen create the morphometrics environment using\\nbash\\nconda env create -f environment.yml\\nOnce the environment has been created, you can activate it and use morphometrics as described below.\\nbash\\nconda activate morphometrics\\nIf you are on Mac OS or Linux install the following:\\nMac:\\nbash\\nconda install -c conda-forge ocl_icd_wrapper_apple\\nLinux:\\nbash\\nconda install -c conda-forge ocl-icd-system\\nDevelopment installation\\nTo install latest development version :\\npip install git+https://github.com/kevinyamauchi/morphometrics.git\\n\\nExample applications\\n\\n\\n\\n\\n\\n[measure the distance between surfaces](https://github.com/kevinyamauchi/morphometrics/blob/main/examples/surface_distance_measurement.ipynb)\\n\\n\\n\\n\\n\\n[napari plugin for measuring properties of segmented objects (regionprops)](https://github.com/kevinyamauchi/morphometrics/blob/main/examples/measure_with_widget.py)\\n\\n\\n\\n\\n\\n[object classification](https://github.com/kevinyamauchi/morphometrics/blob/main/examples/object_classification.ipynb)\\n\\n\\n\\n\\n\\n[mesh binary mask](https://github.com/kevinyamauchi/morphometrics/blob/main/examples/mesh_binary_mask.ipynb)\\n\\n\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"morphometrics\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"morphometrics\",\"documentation\":\"https://github.com/kevinyamauchi/morphometrics#README.md\",\"first_released\":\"2022-03-17T16:28:24.113225Z\",\"license\":\"BSD-3-Clause\",\"name\":\"morphometrics\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\",\"sample_data\"],\"project_site\":\"https://github.com/kevinyamauchi/morphometrics\",\"python_version\":\">=3.9\",\"reader_file_extensions\":[],\"release_date\":\"2023-02-02T09:52:10.175030Z\",\"report_issues\":\"https://github.com/kevinyamauchi/morphometrics/issues\",\"requirements\":[\"glasbey\",\"imageio (!=2.11.0,!=2.22.1,>=2.5.0)\",\"leidenalg\",\"napari-skimage-regionprops\",\"napari\",\"numba\",\"numpy\",\"qtpy\",\"pandas\",\"pooch\",\"pyclesperanto-prototype (>=0.8.0)\",\"pymeshfix\",\"pyqtgraph\",\"scanpy\",\"scikit-image (>0.19.0)\",\"scikit-learn (>=0.24.2)\",\"tqdm\",\"trimesh[easy]\",\"pre-commit ; extra == 'dev'\",\"pytest ; extra == 'dev'\",\"pytest-qt ; extra == 'dev'\"],\"summary\":\"A plugin for quantifying shape and neighborhoods from images.\",\"support\":\"https://github.com/kevinyamauchi/morphometrics/issues\",\"twitter\":\"\",\"version\":\"0.0.8\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Marc Boucsein\"},{\"name\":\"Robin Koch\"}],\"code_repository\":\"https://github.com/MBPhys/napari-nd-cropper\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-nd-cropper\"}],\"description\":\"# napari-nd-cropper\\n\\n[![License](https://img.shields.io/pypi/l/napari-medical-image-formats.svg?color=green)](https://github.com/MBPhys/napari-nd-cropper/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-nd-cropper.svg?color=green)](https://pypi.org/project/napari-nd-cropper)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-nd-cropper.svg?color=green)](https://python.org)\\n\\n\\nA napari plugin in order to crop nd-images via different modes:\\n\\n- Cropping via Drag&Drop interaction box (available for napari releases > 0.4.12)\\n- Cropping of double-clicked regions based on predefined size (Integer or Tuple of integer) \\n- Cropping based on view \\n- Cropping via Sliders \\n\\n\\n----------------------------------\\n\\n## Installation\\n\\nYou can install `napari-nd-cropper` via [pip]:\\n\\n    pip install napari-nd-cropper\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-nd-cropper\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/MBPhys/napari-nd-cropper/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-nd-cropper\\n\\n\\n\\nA napari plugin in order to crop nd-images via different modes:\\n\\nCropping via Drag&Drop interaction box (available for napari releases > 0.4.12)\\nCropping of double-clicked regions based on predefined size (Integer or Tuple of integer) \\nCropping based on view \\nCropping via Sliders \\n\\n\\nInstallation\\nYou can install napari-nd-cropper via pip:\\npip install napari-nd-cropper\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-nd-cropper\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 3 - Alpha\"],\"display_name\":\"napari-nd-cropper\",\"documentation\":\"https://github.com/MBPhys/napari-nd-cropper\",\"first_released\":\"2022-01-12T11:39:06.916260Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-nd-cropper\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/MBPhys/napari-nd-cropper\",\"python_version\":\">=3.9\",\"reader_file_extensions\":[],\"release_date\":\"2022-01-12T11:39:06.916260Z\",\"report_issues\":\"https://github.com/MBPhys/napari-nd-cropper/issues\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"napari-tools-menu\",\"qtpy\",\"superqt\",\"magicgui\"],\"summary\":\"A napari plugin in order to crop nd-images via different modes\",\"support\":\"https://github.com/MBPhys/napari-nd-cropper/issues\",\"twitter\":\"\",\"version\":\"0.1.3\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Marcelo Leomil Zoccoler\"}],\"code_repository\":\"https://github.com/zoccoler/napari-metroid\",\"conda\":[],\"description\":\"# napari-metroid\\n\\n[![License](https://img.shields.io/pypi/l/napari-metroid.svg?color=green)](https://github.com/zoccoler/napari-metroid/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-metroid.svg?color=green)](https://pypi.org/project/napari-metroid)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-metroid.svg?color=green)](https://python.org)\\n[![tests](https://github.com/zoccoler/napari-metroid/workflows/tests/badge.svg)](https://github.com/zoccoler/napari-metroid/actions)\\n[![codecov](https://codecov.io/gh/zoccoler/napari-metroid/branch/main/graph/badge.svg)](https://codecov.io/gh/zoccoler/napari-metroid)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-metroid)](https://napari-hub.org/plugins/napari-metroid)\\n\\nThis napari plugin is an adaptation of [metroid](https://github.com/zoccoler/metroid). It creates several regions of interest of similar area over cells in a fluorescence video (2D+time). It then gets ROIs means over time and performs signal denoising: fixes photobleaching and separates signal from noise by means of blind source separation (with or without wavelet filtering).\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/plugins/stable/index.html\\n-->\\n\\n## A Picture (to boil down a thousand words)\\n\\nBelow is the graphical abstract of the Metroid software. This napari plugin works very similarly.\\n\\n![](https://github.com/zoccoler/metroid/blob/master/Metroid_flowchart.png)\\n\\n## Table of Contents\\n\\n- [Quick Walktrough](#quick-walkthrough)\\n- [Installation](#installation)\\n- [Usage](#usage)\\n  - [Open Sample Data](#open-sample-data)\\n  - [Open Plugin Main Interface](#open-plugin-main-interface)\\n  - [Auto-generate Cell Mask](#auto-generate-cell-mask)\\n  - [Split Mask into ROIs](#split-mask-into-rois)\\n  - [Get ROI Means over Time](#get-roi-means-over-time)\\n  - [Remove Photobleaching](#remove-photobleaching)\\n  - [Filter Signals](#filter-signals)\\n  - [Save outputs](#save-outputs)\\n- [Contributing](#contributing)\\n- [Citing napari-metroid](#citing-napari-metroid)\\n- [License](#license)\\n- [Issues](#issues)\\n\\n## Quick Walkthrough\\n\\nBelow is a full demonstration of using napari-metroid. It shows the following:\\n  * Open sample data;\\n  * Create cell mask;\\n  * Split mask into ROIs of similar area;\\n  * Get ROIs signals over time and plots two of them;\\n  * Remove photobleaching;\\n  * Remove noise:\\n    * Use ICA to decompose ROIs signals into independent components;\\n    * Plot 4 components;\\n    * Manually select the component of interest (source);\\n    * Perform inverse transformation with selected source;\\n        \\n![](https://github.com/zoccoler/napari-metroid/raw/main/figures/napari_metroid_demo.gif)\\n\\n## Installation\\n\\nDownload and install [Anaconda](https://www.anaconda.com/products/individual) or [Miniconda](https://docs.conda.io/en/latest/miniconda.html#).\\n\\nCreate a new conda environment:\\n\\n    conda create -n metroid-env python=3.8\\n\\nInstall napari, e.g. via pip:\\n\\n    pip install \\\"napari[all]\\\"\\n\\nInstall `napari-metroid` via [pip]:\\n\\n    pip install napari-metroid\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/zoccoler/napari-metroid.git\\n\\n## Usage\\n### Open Sample Data\\n\\nThis plugin comes with two sample videos:\\n- Cell1 Video Action Potential: 2D + time fluorescence video of a rat isolated cardiomyocyte labeled with a membrane potential dye upon which an external electrical field pulse is applied.\\n- Cell1 Video Electroporation: Same cell, but submitted to a strong external electrical field pulse.\\n\\nYou can open them under \\\"File -> Open Sample -> napari-metroid\\\", as shown below. Both videos are loaded from the [metroid main repository](https://github.com/zoccoler/metroid). To know more about the experimental conditions, please refer to the [original publication](https://doi.org/10.1186/s12859-020-03661-9).\\n\\n![](https://github.com/zoccoler/napari-metroid/raw/main/figures/load_sample_data.gif)\\n\\n### Open Plugin Main Interface\\n\\n![](https://github.com/zoccoler/napari-metroid/raw/main/figures/open_plugin.gif)\\n\\n### Auto-generate Cell Mask\\n\\nMetroid can generate cell binary masks automatically by cumulative sum of images until any pixel saturation happens. It then applies Otsu thresholding and removes small objects.\\n\\n![](https://github.com/zoccoler/napari-metroid/raw/main/figures/auto_create_mask.png)\\n\\n### Split Mask into ROIs\\n\\nBy default, a cell mask is split into 32 regions of interest (ROIs) in a double-layer fashion: An outer layer of ROIs and an inner layer. \\nThe method is solely based on the shape of the cell mask and the main criteria is that ROIs must have similar areas. The number of ROIs in each layer can be editted. \\n\\n![](https://github.com/zoccoler/napari-metroid/raw/main/figures/mess.png)\\n\\n### Get ROI Means over Time\\n\\nThe 'Get Signals' button serves to collect each ROI mean fluorescence over time and enable plotting. There, you can optionally provide the frame rate so that the time axis is properly displayed.\\nDouble click over a ROI to have its signal plotted. Hold the 'ALT' key to plot multiple signals together.\\n\\n![](https://github.com/zoccoler/napari-metroid/raw/main/figures/get_signals.gif)\\n\\n### Remove Photobleaching\\n\\nMetroid removes photobleaching by curve fitting over time periods that lack the cellular signal (which can be an action potential or an electroporation signal). That is why the 'Transitory' parameter is important. Action potentials are transitory signals whereas electroporation (at least for the duration of this experiment) are not, and the algorithm must be informed about that for proper trend removal.\\n\\n![](https://github.com/zoccoler/napari-metroid/raw/main/figures/remov_photob.gif)\\n\\n### Filter Signals\\n\\nCellular signals are filtered by separating signal components with either PCA or ICA (plus optional wavelet filtering). It then chooses one (or several) components and it applies the inverse transform using only the selected components. Metroid can do this component/source selection automatically based on estimations of signal power. Instead, we show below the manual selection procedure, where 4 components are plotted and the user selects one of them.\\n\\n![](https://github.com/zoccoler/napari-metroid/raw/main/figures/bssd.gif)\\n\\n### Save Outputs\\n\\nRaw, corrected and filtered signals, as well as time and components, are arranged in a table with values for each time point. The table is displayed as a widget after each Run button click. Estimated signal-to-noise (SNR) in dB for each label/ROI are also provided (in this case, each line corresponds to a ROI, not a time point).\\nThe user can save these data by clicking on the buttons \\\"Copy to clipboard\\\" or \\\"Save as csv...\\\".\\n\\n![](https://github.com/zoccoler/napari-metroid/raw/main/figures/table_widget.png)\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## Citing napari-metroid\\n\\nIf you use this plugin in your research, please be kind to cite the original paper below:\\n\\nZoccoler, M., de Oliveira, P.X. METROID: an automated method for robust quantification of subcellular fluorescence events at low SNR. BMC Bioinformatics 21, 332 (2020). https://doi.org/10.1186/s12859-020-03661-9\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-metroid\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/zoccoler/napari-metroid/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-metroid\\n\\n\\n\\n\\n\\n\\nThis napari plugin is an adaptation of metroid. It creates several regions of interest of similar area over cells in a fluorescence video (2D+time). It then gets ROIs means over time and performs signal denoising: fixes photobleaching and separates signal from noise by means of blind source separation (with or without wavelet filtering).\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nA Picture (to boil down a thousand words)\\nBelow is the graphical abstract of the Metroid software. This napari plugin works very similarly.\\n\\nTable of Contents\\n\\nQuick Walktrough\\nInstallation\\nUsage\\nOpen Sample Data\\nOpen Plugin Main Interface\\nAuto-generate Cell Mask\\nSplit Mask into ROIs\\nGet ROI Means over Time\\nRemove Photobleaching\\nFilter Signals\\nSave outputs\\nContributing\\nCiting napari-metroid\\nLicense\\nIssues\\n\\nQuick Walkthrough\\nBelow is a full demonstration of using napari-metroid. It shows the following:\\n  * Open sample data;\\n  * Create cell mask;\\n  * Split mask into ROIs of similar area;\\n  * Get ROIs signals over time and plots two of them;\\n  * Remove photobleaching;\\n  * Remove noise:\\n    * Use ICA to decompose ROIs signals into independent components;\\n    * Plot 4 components;\\n    * Manually select the component of interest (source);\\n    * Perform inverse transformation with selected source;\\n\\nInstallation\\nDownload and install Anaconda or Miniconda.\\nCreate a new conda environment:\\nconda create -n metroid-env python=3.8\\n\\nInstall napari, e.g. via pip:\\npip install \\\"napari[all]\\\"\\n\\nInstall napari-metroid via pip:\\npip install napari-metroid\\n\\nTo install latest development version :\\npip install git+https://github.com/zoccoler/napari-metroid.git\\n\\nUsage\\nOpen Sample Data\\nThis plugin comes with two sample videos:\\n- Cell1 Video Action Potential: 2D + time fluorescence video of a rat isolated cardiomyocyte labeled with a membrane potential dye upon which an external electrical field pulse is applied.\\n- Cell1 Video Electroporation: Same cell, but submitted to a strong external electrical field pulse.\\nYou can open them under \\\"File -> Open Sample -> napari-metroid\\\", as shown below. Both videos are loaded from the metroid main repository. To know more about the experimental conditions, please refer to the original publication.\\n\\nOpen Plugin Main Interface\\n\\nAuto-generate Cell Mask\\nMetroid can generate cell binary masks automatically by cumulative sum of images until any pixel saturation happens. It then applies Otsu thresholding and removes small objects.\\n\\nSplit Mask into ROIs\\nBy default, a cell mask is split into 32 regions of interest (ROIs) in a double-layer fashion: An outer layer of ROIs and an inner layer. \\nThe method is solely based on the shape of the cell mask and the main criteria is that ROIs must have similar areas. The number of ROIs in each layer can be editted. \\n\\nGet ROI Means over Time\\nThe 'Get Signals' button serves to collect each ROI mean fluorescence over time and enable plotting. There, you can optionally provide the frame rate so that the time axis is properly displayed.\\nDouble click over a ROI to have its signal plotted. Hold the 'ALT' key to plot multiple signals together.\\n\\nRemove Photobleaching\\nMetroid removes photobleaching by curve fitting over time periods that lack the cellular signal (which can be an action potential or an electroporation signal). That is why the 'Transitory' parameter is important. Action potentials are transitory signals whereas electroporation (at least for the duration of this experiment) are not, and the algorithm must be informed about that for proper trend removal.\\n\\nFilter Signals\\nCellular signals are filtered by separating signal components with either PCA or ICA (plus optional wavelet filtering). It then chooses one (or several) components and it applies the inverse transform using only the selected components. Metroid can do this component/source selection automatically based on estimations of signal power. Instead, we show below the manual selection procedure, where 4 components are plotted and the user selects one of them.\\n\\nSave Outputs\\nRaw, corrected and filtered signals, as well as time and components, are arranged in a table with values for each time point. The table is displayed as a widget after each Run button click. Estimated signal-to-noise (SNR) in dB for each label/ROI are also provided (in this case, each line corresponds to a ROI, not a time point).\\nThe user can save these data by clicking on the buttons \\\"Copy to clipboard\\\" or \\\"Save as csv...\\\".\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nCiting napari-metroid\\nIf you use this plugin in your research, please be kind to cite the original paper below:\\nZoccoler, M., de Oliveira, P.X. METROID: an automated method for robust quantification of subcellular fluorescence events at low SNR. BMC Bioinformatics 21, 332 (2020). https://doi.org/10.1186/s12859-020-03661-9\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-metroid\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari METROID\",\"documentation\":\"https://github.com/zoccoler/napari-metroid#README.md\",\"first_released\":\"2022-03-24T07:40:31.950920Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-metroid\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"writer\",\"widget\",\"sample_data\"],\"project_site\":\"https://github.com/zoccoler/napari-metroid\",\"python_version\":\"<3.9,>=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-07-20T10:56:14.990238Z\",\"report_issues\":\"https://github.com/zoccoler/napari-metroid/issues\",\"requirements\":[\"numpy\",\"scikit-learn\",\"scikit-image\",\"statsmodels\",\"scipy\",\"matplotlib\",\"napari-skimage-regionprops (>=0.3.1)\"],\"summary\":\"This napari plugin creates several regions of interest of similar area over cells in a fluorescence video (2D+time). It then gets ROIs means over time and performs signal denoising: fixes photobleaching and separates signal from noise by means of blind source separation (with or without wavelet filtering).\",\"support\":\"https://github.com/zoccoler/napari-metroid/issues\",\"twitter\":\"\",\"version\":\"0.0.5\",\"visibility\":\"public\",\"writer_file_extensions\":[\".npy\"],\"writer_save_layers\":[\"image\",\"labels\"]}",
  "{\"authors\":[{\"email\":\"robert.haase@tu-dresden.de\",\"name\":\"Robert Haase\"}],\"code_repository\":\"https://github.com/haesleinhuepf/napari-layer-details-display\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-layer-details-display\"}],\"description\":\"# napari-layer-details-display\\n\\n[![License](https://img.shields.io/pypi/l/napari-layer-details-display.svg?color=green)](https://github.com/haesleinhuepf/napari-layer-details-display/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-layer-details-display.svg?color=green)](https://pypi.org/project/napari-layer-details-display)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-layer-details-display.svg?color=green)](https://python.org)\\n[![tests](https://github.com/haesleinhuepf/napari-layer-details-display/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-layer-details-display/actions)\\n[![codecov](https://codecov.io/gh/haesleinhuepf/napari-layer-details-display/branch/main/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-layer-details-display)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-layer-details-display)](https://napari-hub.org/plugins/napari-layer-details-display)\\n\\nA display for layer information and properties\\n\\n![img.png](https://github.com/haesleinhuepf/napari-layer-details-display/raw/main/images/screenshot.png)\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n## Installation\\n\\nYou can install `napari-layer-details-display` via [pip]:\\n\\n    pip install napari-layer-details-display\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/haesleinhuepf/napari-layer-details-display.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-layer-details-display\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/haesleinhuepf/napari-layer-details-display/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-layer-details-display\\n\\n\\n\\n\\n\\n\\nA display for layer information and properties\\n\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\nInstallation\\nYou can install napari-layer-details-display via pip:\\npip install napari-layer-details-display\\n\\nTo install latest development version :\\npip install git+https://github.com/haesleinhuepf/napari-layer-details-display.git\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-layer-details-display\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari-layer-details-display\",\"documentation\":\"https://github.com/haesleinhuepf/napari-layer-details-display#README.md\",\"first_released\":\"2021-11-13T09:03:13.359953Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-layer-details-display\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/haesleinhuepf/napari-layer-details-display\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-07-10T14:03:10.876447Z\",\"report_issues\":\"https://github.com/haesleinhuepf/napari-layer-details-display/issues\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"napari-tools-menu\"],\"summary\":\"A display for layer information and properties\",\"support\":\"https://github.com/haesleinhuepf/napari-layer-details-display/issues\",\"twitter\":\"\",\"version\":\"0.1.4\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Gursharan Ahir\"},{\"name\":\"Michael Sandler\"},{\"name\":\"Ryan Thiermann\"}],\"code_repository\":\"https://github.com/ahirsharan/napari-mm3\",\"conda\":[],\"description\":\"# napari-mm3\\n\\n[![License](https://img.shields.io/pypi/l/napari-mm3.svg?color=green)](https://github.com/ahirsharan/napari-mm3/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-mm3.svg?color=green)](https://pypi.org/project/napari-mm3)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-mm3.svg?color=green)](https://python.org)\\n[![tests](https://github.com/ahirsharan/napari-mm3/workflows/tests/badge.svg)](https://github.com/ahirsharan/napari-mm3/actions)\\n[![codecov](https://codecov.io/gh/ahirsharan/napari-mm3/branch/main/graph/badge.svg)](https://codecov.io/gh/ahirsharan/napari-mm3)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-mm3)](https://napari-hub.org/plugins/napari-mm3)\\n\\nA plugin for Mother Machine Image Analysis by [Jun Lab](https://jun.ucsd.edu/).\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/plugins/stable/index.html\\n-->\\n\\n## Installation\\n\\nLoad up a new environment. We run the following command, replacing `environment-name-here` with a name of your choosing:\\n\\n`conda create -y -n environment-name-here python=3.9 napari tensorflow` \\n\\nNow, to install our code: if you would like to have the latest version, do the following.\\n\\n1. You can clone the repository with `git clone git@github.com:junlabucsd/napari-mm3.git` (SSH) or `git clone https://github.com/junlabucsd/napari-mm3.git` (https)\\n2. With your environment active, run `pip install -e .` from inside your cloned repo.\\n\\nIf you would like to have a more stable verison, simply run `pip install napari-mm3`.\\n\\nNOTE:\\nNot running the conda command and trying to install things in a different way may lead to difficult issues with PyQt5. \\nWe recommend following the above commands to simplify the situation.\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## Usage guide\\n\\n**Brief video introduction:** available [here](https://youtu.be/7MCiGTg6mq4)\\n\\n### a. Preprocessing\\n\\n* [nd2ToTIFF](https://github.com/junlabucsd/napari-mm3/blob/main/docs/nd2totiff-widget.md) -- Turn your nd2 microscopy data into TIFFs. If your data is not in the nd2 format, follow the [input image guidelines](/docs/input-images-guidelines.md). Make sure to set 'image source' in Compile to 'Other'.\\n\\n* [Compile](https://github.com/junlabucsd/napari-mm3/blob/main/docs/compile-widget.md) -- Locate traps, separate their timelapses into their own TIFFs, and return metadata.\\n\\n### b. Segmentation\\n\\n___With Otsu:___\\n\\n* [PickChannels](https://github.com/junlabucsd/napari-mm3/blob/main/docs/pickchannels-widget.md) -- User guided selection of empty and full traps.\\n\\n* [Subtract](https://github.com/junlabucsd/napari-mm3/blob/main/docs/subtract-widget.md) -- Remove (via subtraction) empty traps from the background of traps that contain cells; run this on the phase contrast channel.\\n\\n* [SegmentOtsu](https://github.com/junlabucsd/napari-mm3/blob/main/docs/segmentotsu-widget.md) -- Use Otsu segmentation to segment cells.\\n\\n___With UNet:___\\n\\n* Annotate -- annotate images for ML (U-Net or similar) training purposes; you can generate a model via TODO.\\n\\n* [SegmentUnet](https://github.com/junlabucsd/napari-mm3/blob/main/docs/segmentunet-widget.md) -- Run U-Net segmentation (you will need to supply your own model)\\n\\n### c. Tracking\\n\\n* [Track](https://github.com/junlabucsd/napari-mm3/blob/main/docs/track-widget.md) -- Acquire individual cell properties and track lineages.\\n\\n### d. Fluorescence data analysis\\n\\n* [PickChannels](https://github.com/junlabucsd/napari-mm3/blob/main/docs/pickchannels-widget.md) -- If you've already done this (e.g. for otsu segmentation), no need to do it again. User guided selection of empty and full traps. \\n\\n* [Subtract](https://github.com/junlabucsd/napari-mm3/blob/main/docs/subtract-widget.md) -- Remove (via subtraction) empty traps from the background of traps that contain cells. This time, run this on your fluorescence channels.\\n\\n* [Colors](https://github.com/junlabucsd/napari-mm3/blob/main/docs/colors-widget.md) -- Calculate fluorescence information.\\n\\n### e. (Uncommon) Foci tracking\\n\\n* [Foci](https://github.com/junlabucsd/napari-mm3/blob/main/docs/foci-widget.md) -- We use this to track `foci' (bright fluorescent spots) inside of cells.\\n\\n\\n### f. Outputs, inputs, and file structure\\nFinally, to better understand the data formats, you may wish to refer to the following documents:\\n\\n* [Input image guidelines](https://github.com/junlabucsd/napari-mm3/blob/main/docs/input-images-guidelines.md)\\n\\n* [File structure](https://github.com/junlabucsd/napari-mm3/blob/main/docs/file-structure.md)\\n\\n* [Output file structure](https://github.com/junlabucsd/napari-mm3/blob/main/docs/Cell-class-docs.md)\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-mm3\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/ahirsharan/napari-mm3/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-mm3\\n\\n\\n\\n\\n\\n\\nA plugin for Mother Machine Image Analysis by Jun Lab.\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nLoad up a new environment. We run the following command, replacing environment-name-here with a name of your choosing:\\nconda create -y -n environment-name-here python=3.9 napari tensorflow \\nNow, to install our code: if you would like to have the latest version, do the following.\\n\\nYou can clone the repository with git clone git@github.com:junlabucsd/napari-mm3.git (SSH) or git clone https://github.com/junlabucsd/napari-mm3.git (https)\\nWith your environment active, run pip install -e . from inside your cloned repo.\\n\\nIf you would like to have a more stable verison, simply run pip install napari-mm3.\\nNOTE:\\nNot running the conda command and trying to install things in a different way may lead to difficult issues with PyQt5. \\nWe recommend following the above commands to simplify the situation.\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nUsage guide\\nBrief video introduction: available here\\na. Preprocessing\\n\\n\\nnd2ToTIFF -- Turn your nd2 microscopy data into TIFFs. If your data is not in the nd2 format, follow the input image guidelines. Make sure to set 'image source' in Compile to 'Other'.\\n\\n\\nCompile -- Locate traps, separate their timelapses into their own TIFFs, and return metadata.\\n\\n\\nb. Segmentation\\nWith Otsu:\\n\\n\\nPickChannels -- User guided selection of empty and full traps.\\n\\n\\nSubtract -- Remove (via subtraction) empty traps from the background of traps that contain cells; run this on the phase contrast channel.\\n\\n\\nSegmentOtsu -- Use Otsu segmentation to segment cells.\\n\\n\\nWith UNet:\\n\\n\\nAnnotate -- annotate images for ML (U-Net or similar) training purposes; you can generate a model via TODO.\\n\\n\\nSegmentUnet -- Run U-Net segmentation (you will need to supply your own model)\\n\\n\\nc. Tracking\\n\\nTrack -- Acquire individual cell properties and track lineages.\\n\\nd. Fluorescence data analysis\\n\\n\\nPickChannels -- If you've already done this (e.g. for otsu segmentation), no need to do it again. User guided selection of empty and full traps. \\n\\n\\nSubtract -- Remove (via subtraction) empty traps from the background of traps that contain cells. This time, run this on your fluorescence channels.\\n\\n\\nColors -- Calculate fluorescence information.\\n\\n\\ne. (Uncommon) Foci tracking\\n\\nFoci -- We use this to track `foci' (bright fluorescent spots) inside of cells.\\n\\nf. Outputs, inputs, and file structure\\nFinally, to better understand the data formats, you may wish to refer to the following documents:\\n\\n\\nInput image guidelines\\n\\n\\nFile structure\\n\\n\\nOutput file structure\\n\\n\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-mm3\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari-mm3\",\"documentation\":\"https://github.com/ahirsharan/napari-mm3#README.md\",\"first_released\":\"2022-06-02T19:59:32.471036Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-mm3\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/junlabucsd/napari-mm3\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-10-06T20:52:51.778388Z\",\"report_issues\":\"https://github.com/ahirsharan/napari-mm3/issues\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"h5py\",\"tifffile (==2021.11.2)\",\"scikit-learn\",\"scikit-image\",\"tensorflow\",\"pims-nd2\",\"seaborn\"],\"summary\":\"a plugin for mother machine image analysis\",\"support\":\"https://github.com/ahirsharan/napari-mm3/issues\",\"twitter\":\"\",\"version\":\"0.0.10\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Leo Guignard\"}],\"code_repository\":\"https://github.com/GuignardLab/napari-sc3D-viewer\",\"description\":\"# napari-sc3D-viewer\\n\\n[![License](https://img.shields.io/pypi/l/napari-sc3D-viewer.svg?color=green)](https://raw.githubusercontent.com/GuignardLab/napari-sc3D-viewer/main/LICENSE)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-sc3D-viewer.svg?color=green)](https://python.org)\\n[![PyPI](https://img.shields.io/pypi/v/napari-sc3D-viewer.svg?color=green)](https://pypi.org/project/napari-sc3D-viewer)\\n[![tests](https://github.com/GuignardLab/napari-sc3D-viewer/workflows/tests/badge.svg)](https://github.com/GuignardLab/napari-sc3D-viewer/actions)\\n[![codecov](https://codecov.io/gh/GuignardLab/napari-sc3D-viewer/branch/main/graph/badge.svg)](https://codecov.io/gh/GuignardLab/napari-sc3D-viewer)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-sc3D-viewer)](https://napari-hub.org/plugins/napari-sc3D-viewer)\\n\\n\\nA plugin to visualise 3D spatial single cell omics\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n## Installation\\n\\nYou can install `napari-sc3D-viewer` via [pip]:\\n\\n    pip install .\\n(from the correct folder)\\nor\\n\\n    pip install napari-sc3D-viewer\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/GuignardLab/napari-sc3D-viewer.git\\n\\nTo install the surface computation enabled version it is necessary to use Python 3.9 (until [VTK] is ported to Python 3.10) and you can run one of the following commands:\\n\\n    pip install '.[pyvista]'\\nfrom the correct folder or\\n\\n    pip install 'napari-sc3D-viewer[pyvista]'\\n\\nto install directly from pip or\\n\\n    pip install 'napari-sc3D-viewer[pyvista] @ git+https://github.com/GuignardLab/napari-sc3D-viewer.git'\\n\\nto install the latest version\\n\\n## Usage\\n\\n`napari-sc3D-viewer` allows users to easily visualise and navigate 3D spatial single-cell transcriptomics using napari.\\n\\n### Loading and opening a dataset\\n\\n<!-- To test your the plugin you can download the following dataset composed of a id to tissue name file located [there](https://github.com/GuignardLab/sc3D/tree/main/data) and a scanpy h5ad dataset [there](https://figshare.com/s/1c29d867bc8b90d754d2). The dataset is from the following publication: [pub] -->\\n\\nThe expected dataset is a [scanpy]/[anndata] h5ad file together with an optional json file that maps cluster id numbers to actual tissue/cluster name.\\n\\nThe json file should look like that:\\n```json\\n{\\n    \\\"1\\\": \\\"Endoderm\\\",\\n    \\\"2\\\": \\\"Heart\\\",\\n    \\\"10\\\": \\\"Anterior neuroectoderm\\\"\\n}\\n```\\nIf no json file or a wrong json file is given, the original cluster id numbers are used.\\n\\nThe h5ad file should be informed in (1) and the json file in (2).\\n![loading image](images/1.loading.png)\\n\\nLet `data` be your h5ad data structure. To work properly, the viewer is expecting 4 different columns to be present in the h5ad file:\\n- the cluster id column (by default named 'predicted.id' that can be accessed as `data.obs['predicted.id']`)\\n- the 3D position column (by default named 'X_spatial_registered' that can be accessed as `data.obsm['X_spatial_registered']`)\\n- the gene names if not already in the column name (by default named 'feature_name' that can be accessed as `data.var['feature_name']`)\\n- umap coordinates (by default named 'X_umap' that can be accessed as `data.obsm['X_umap']`)\\n\\nIf the default column names are not consistent with your dataset, they can be changed in the tab `Parameters` (3) next to the tab `Loading files`\\n\\nOnce all the data paths and fields are correctly informed pressing the `Load Atlas` button (4) will load the dataset.\\n\\n### Exploring a dataset\\n\\nOnce the dataset is loaded there are few options to explore it.\\n\\nThe viewer should look like to the following:\\n![viewer](images/2.viewer.png)\\n\\nIt is divided in two main parts, the Tissue visualisation (1) part and the Metric visualisation (2) one.\\nBoth of them are themselves split in two and three tabs respectively. All these tabs allow you to visualise and explore the dataset in different fashions.\\n\\nThe Tissues tab (1.1) allows to select the tissues to display, to show the legend and to colour the cells according to their tissue types.\\n\\nThe Surfaces tab (1.2) allows to construct coarse surfaces of tissues and to display them.\\n\\nThe Single metric tab (2.1) allows to display a metric, whether it is a gene intensity or a numerical metric that is embedded in the visualised dataset. This tab also allows to threshold cells according to the viewed metric, to change the contrast and the colour map.\\n\\nThe 2 Genes (2.2) tab allows to display gene coexpression.\\n\\nThe umap tab (2.3) allows to display the umap of the selected cells and to manually select subcategories of cells to be displayed.\\n\\n![viewer](images/3.description.png)\\n\\n#### Explanatory \\\"videos\\\".\\nThe plugin is meant to be easy to use. That means that you should be able to play with it and figure things out by yourself.\\n\\nThat being said, it is not always that easy. You can find below a series of videos showing how to perform some of the main features.\\n\\n#### Loading data\\n![Loading data video](images/loading.gif)\\n\\n#### Selecting tissues\\n![Selecting tissues video](images/tissue-select.gif)\\n\\n#### Displaying one gene\\n![Displaying one gene video](images/gene1.gif)\\n\\n#### Displaying two genes co-expression\\n![Displaying genes video](images/gene2.gif)\\n\\n#### Playing with the umap\\n![Playing with the umap video](images/umap.gif)\\n\\n#### Computing and processing the surface\\n![Computing and processing the surface video](images/surfaces.gif)\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [MIT] license,\\n\\\"napari-sc3D-viewer\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/GuignardLab/napari-sc3D-viewer/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n[VTK]: https://vtk.org/\\n[scanpy]: https://scanpy.readthedocs.io/en/latest/index.html\\n[anndata]: https://anndata.readthedocs.io/en/latest/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-sc3D-viewer\\n\\n\\n\\n\\n\\n\\nA plugin to visualise 3D spatial single cell omics\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\nInstallation\\nYou can install napari-sc3D-viewer via pip:\\npip install .\\n\\n(from the correct folder)\\nor\\npip install napari-sc3D-viewer\\n\\nTo install latest development version :\\npip install git+https://github.com/GuignardLab/napari-sc3D-viewer.git\\n\\nTo install the surface computation enabled version it is necessary to use Python 3.9 (until VTK is ported to Python 3.10) and you can run one of the following commands:\\npip install '.[pyvista]'\\n\\nfrom the correct folder or\\npip install 'napari-sc3D-viewer[pyvista]'\\n\\nto install directly from pip or\\npip install 'napari-sc3D-viewer[pyvista] @ git+https://github.com/GuignardLab/napari-sc3D-viewer.git'\\n\\nto install the latest version\\nUsage\\nnapari-sc3D-viewer allows users to easily visualise and navigate 3D spatial single-cell transcriptomics using napari.\\nLoading and opening a dataset\\n\\nThe expected dataset is a scanpy/anndata h5ad file together with an optional json file that maps cluster id numbers to actual tissue/cluster name.\\nThe json file should look like that:\\njson\\n{\\n    \\\"1\\\": \\\"Endoderm\\\",\\n    \\\"2\\\": \\\"Heart\\\",\\n    \\\"10\\\": \\\"Anterior neuroectoderm\\\"\\n}\\nIf no json file or a wrong json file is given, the original cluster id numbers are used.\\nThe h5ad file should be informed in (1) and the json file in (2).\\n\\nLet data be your h5ad data structure. To work properly, the viewer is expecting 4 different columns to be present in the h5ad file:\\n- the cluster id column (by default named 'predicted.id' that can be accessed as data.obs['predicted.id'])\\n- the 3D position column (by default named 'X_spatial_registered' that can be accessed as data.obsm['X_spatial_registered'])\\n- the gene names if not already in the column name (by default named 'feature_name' that can be accessed as data.var['feature_name'])\\n- umap coordinates (by default named 'X_umap' that can be accessed as data.obsm['X_umap'])\\nIf the default column names are not consistent with your dataset, they can be changed in the tab Parameters (3) next to the tab Loading files\\nOnce all the data paths and fields are correctly informed pressing the Load Atlas button (4) will load the dataset.\\nExploring a dataset\\nOnce the dataset is loaded there are few options to explore it.\\nThe viewer should look like to the following:\\n\\nIt is divided in two main parts, the Tissue visualisation (1) part and the Metric visualisation (2) one.\\nBoth of them are themselves split in two and three tabs respectively. All these tabs allow you to visualise and explore the dataset in different fashions.\\nThe Tissues tab (1.1) allows to select the tissues to display, to show the legend and to colour the cells according to their tissue types.\\nThe Surfaces tab (1.2) allows to construct coarse surfaces of tissues and to display them.\\nThe Single metric tab (2.1) allows to display a metric, whether it is a gene intensity or a numerical metric that is embedded in the visualised dataset. This tab also allows to threshold cells according to the viewed metric, to change the contrast and the colour map.\\nThe 2 Genes (2.2) tab allows to display gene coexpression.\\nThe umap tab (2.3) allows to display the umap of the selected cells and to manually select subcategories of cells to be displayed.\\n\\nExplanatory \\\"videos\\\".\\nThe plugin is meant to be easy to use. That means that you should be able to play with it and figure things out by yourself.\\nThat being said, it is not always that easy. You can find below a series of videos showing how to perform some of the main features.\\nLoading data\\n\\nSelecting tissues\\n\\nDisplaying one gene\\n\\nDisplaying two genes co-expression\\n\\nPlaying with the umap\\n\\nComputing and processing the surface\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the MIT license,\\n\\\"napari-sc3D-viewer\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"sc3D Viewer\",\"documentation\":\"https://github.com/GuignardLab/napari-sc3D-viewer#README.md\",\"first_released\":\"2022-06-21T09:59:47.002122Z\",\"license\":\"MIT\",\"name\":\"napari-sc3D-viewer\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/GuignardLab/napari-sc3D-viewer\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-11-22T10:37:36.236467Z\",\"report_issues\":\"https://github.com/GuignardLab/napari-sc3D-viewer/issues\",\"requirements\":[\"numpy\",\"magicgui\",\"qtpy\",\"sc-3D\",\"matplotlib\",\"pyvista ; extra == 'pyvista'\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\"],\"summary\":\"A plugin to visualize 3D single cell omics\",\"support\":\"https://github.com/GuignardLab/napari-sc3D-viewer/issues\",\"twitter\":\"https://twitter.com/guignardlab\",\"version\":\"0.2.0\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"dibrov@mpi-cbg.de\",\"name\":\"Alexandr Dibrov\"}],\"code_repository\":null,\"conda\":[],\"description\":\"# napari-kics\\n\\n![napari-kics](https://github.com/mpicbg-csbd/napari-kics/raw/main/docs/banner.png?sanitize=true&raw=true)\\n\\n[![standard-readme compliant](https://img.shields.io/badge/readme%20style-standard-brightgreen.svg)](https://github.com/RichardLitt/standard-readme)\\n[![License](https://img.shields.io/pypi/l/napari-kics.svg?color=green)](./LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-kics.svg?color=green)](https://pypi.org/project/napari-kics)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-kics.svg?color=green)](https://python.org)\\n[![Python package](https://github.com/mpicbg-csbd/napari-kics/actions/workflows/python-package.yml/badge.svg)](https://github.com/mpicbg-csbd/napari-kics/actions/workflows/python-package.yml)\\n\\n\\n> A plugin to estimate chromosome sizes from karyotype images.\\n\\n<small>*This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.*</small>\\n\\n\\n## Table of Contents\\n\\n- Install\\n- Usage\\n- Example\\n- Citation\\n- Maintainer\\n- Contributing\\n- License\\n\\n\\n## Install\\n\\nYou can install `napari-kics` via [pip]:\\n\\n```sh\\npip install napari-kics\\n```\\n\\nThis will install all required dependencies as well. We recommend installing it in a virtual environment, e.g. using [conda]:\\n\\n```sh\\nconda create -n kics python\\nconda activate kics\\npip install napari-kics\\n```\\n\\nWe recommend using [mamba] as a faster alternative to conda.\\n\\n\\n## Usage\\n\\n1. Launch Napari via command line (`napari`).\\n2. Activate the plugin via menu `Plugins -> napari-kics: Karyotype Widget`.\\n3. Select file via `File -> Open File`.\\n4. Follow instructions in the panel on the right.\\n\\nYou may use the interactive analysis plots directly via command line:\\n\\n```sh\\nkaryotype-analysis-plots\\n```\\n\\n\\n## Example\\n\\n1. Launch Napari via command line (`napari`).\\n2. Activate the plugin via menu `Plugins -> napari-kics: Karyotype Widget`.\\n3. Select file via `File -> Open Sample -> napari-kics: sample`.\\n4. Follow instructions in the panel on the right.\\n\\nTry out the interactive analysis plots directly via command line:\\n\\n```sh\\nkaryotype-analysis-plots --example\\n```\\n\\n\\n## Citation\\n\\n> Arne Ludwig, Alexandr Dibrov, Gene Myers, Martin Pippel.\\n> Estimating chromosome sizes from karyotype images enables validation of\\n> *de novo* assemblies. To be published.\\n\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-kics\\\" is free and open source software\\n\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Please [file a pull request] with your\\ncontribution.\\n\\nYou can setup a local development environment for `napari-kics` via [pip]:\\n\\n```sh\\ngit clone https://github.com/mpicbg-csbd/napari-kics.git\\ncd napari-kics\\npip install -e .\\n```\\n\\n\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[@napari]: https://github.com/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[conda]: https://www.anaconda.com/products/distribution\\n[mamba]: https://github.com/mamba-org/mamba\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n[file an issue]: https://github.com/mpicbg-csbd/napari-kics/issues\\n[file a pull request]: https://github.com/mpicbg-csbd/napari-kics/pulls\\n\\n## Overview\\nhttps://user-images.githubusercontent.com/17703905/139654249-685703b5-2196-4a73-a036-d40d578ebcdf.mp4\\n\\n\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-kics\\n\\n\\n\\n\\n\\n\\n\\nA plugin to estimate chromosome sizes from karyotype images.\\n\\nThis napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.\\nTable of Contents\\n\\nInstall\\nUsage\\nExample\\nCitation\\nMaintainer\\nContributing\\nLicense\\n\\nInstall\\nYou can install napari-kics via pip:\\nsh\\npip install napari-kics\\nThis will install all required dependencies as well. We recommend installing it in a virtual environment, e.g. using conda:\\nsh\\nconda create -n kics python\\nconda activate kics\\npip install napari-kics\\nWe recommend using mamba as a faster alternative to conda.\\nUsage\\n\\nLaunch Napari via command line (napari).\\nActivate the plugin via menu Plugins -> napari-kics: Karyotype Widget.\\nSelect file via File -> Open File.\\nFollow instructions in the panel on the right.\\n\\nYou may use the interactive analysis plots directly via command line:\\nsh\\nkaryotype-analysis-plots\\nExample\\n\\nLaunch Napari via command line (napari).\\nActivate the plugin via menu Plugins -> napari-kics: Karyotype Widget.\\nSelect file via File -> Open Sample -> napari-kics: sample.\\nFollow instructions in the panel on the right.\\n\\nTry out the interactive analysis plots directly via command line:\\nsh\\nkaryotype-analysis-plots --example\\nCitation\\n\\nArne Ludwig, Alexandr Dibrov, Gene Myers, Martin Pippel.\\nEstimating chromosome sizes from karyotype images enables validation of\\nde novo assemblies. To be published.\\n\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-kics\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\\nContributing\\nContributions are very welcome. Please file a pull request with your\\ncontribution.\\nYou can setup a local development environment for napari-kics via pip:\\nsh\\ngit clone https://github.com/mpicbg-csbd/napari-kics.git\\ncd napari-kics\\npip install -e .\\nOverview\\nhttps://user-images.githubusercontent.com/17703905/139654249-685703b5-2196-4a73-a036-d40d578ebcdf.mp4\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari-kics\",\"documentation\":\"\",\"first_released\":\"2022-05-02T11:07:16.697044Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-kics\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\",\"sample_data\"],\"project_site\":\"\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-05-22T16:27:15.233710Z\",\"report_issues\":\"\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"napari[all]\",\"scikit-image\",\"pandas\",\"pulp\",\"pyqtgraph\"],\"summary\":\"A plugin to estimate chromosome sizes from karyotype images.\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.0.3rc6\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Ryan Conrad\"}],\"category\":{\"Supported data\":[\"2D\",\"3D\"],\"Workflow step\":[\"Image Segmentation\"]},\"category_hierarchy\":{\"Supported data\":[[\"2D\"],[\"3D\"]],\"Workflow step\":[[\"Image Segmentation\"]]},\"code_repository\":\"https://github.com/volume-em/empanada-napari\",\"conda\":[],\"description\":\"# empanada-napari\\n\\n**The preprint describing this work is now available [on bioRxiv](https://www.biorxiv.org/content/10.1101/2022.03.17.484806v1).**\\n\\n**Documentation for the plugin, including more detailed installation instructions, can be found [here](https://empanada.readthedocs.io/en/latest/empanada-napari.html).**\\n\\n[empanada](https://github.com/volume-em/empanada) is a tool for deep learning-based panoptic segmentation of 2D and 3D electron microscopy images of cells.\\nThis plugin allows the running of panoptic segmentation models trained in empanada within [napari](https://napari.org).\\nFor help with this plugin please open an [issue](https://github.com/volume-em/empanada-napari/issues), for issues with napari specifically\\nraise an [issue here instead](https://github.com/napari/napari/issues).\\n\\n## Implemented Models\\n\\n  - *MitoNet*: A generalist mitochondrial instance segmentation model.\\n\\n## Example Datasets\\n\\nVolume EM datasets for benchmarking mitochondrial instance segmentation are available from\\n[EMPIAR-10982](https://www.ebi.ac.uk/empiar/EMPIAR-10982/).\\n\\n## Installation\\n\\nIt's recommended to have installed napari through [conda](https://docs.conda.io/en/latest/miniconda.html).\\nThen to install this plugin:\\n\\n```shell\\npip install empanada-napari\\n```\\n\\nLaunch napari:\\n\\n```shell\\nnapari\\n```\\n\\nLook for empanada-napari under the \\\"Plugins\\\" menu.\\n\\n![empanada](images/demo.gif)\\n\\n## GPU Support\\n\\n**Note: Mac doesn't support NVIDIA GPUS. This section only applies to Windows and Linux systems.**\\n\\nAs for any deep learning models, having a GPU installed on your system will significantly\\nincrease model throughput (although we ship CPU optimized versions of all models with the plugin).\\n\\nThis plugin relies on torch for running models. If a GPU was found on your system, then you will see that the\\n\\\"Use GPU\\\" checkbox is checked by default in the \\\"2D Inference\\\" and \\\"3D Inference\\\" plugin widgets. Or if when running\\ninference you see a message that says \\\"Using CPU\\\" in the terminal that means a GPU is not being used.\\n\\nMake sure that GPU drivers are correctly installed. In terminal or command prompt:\\n\\n```shell\\nnvidia-smi\\n```\\n\\nIf this returns \\\"command not found\\\" then you need to [install the driver from NVIDIA](https://www.nvidia.com/download/index.aspx). Instead, if\\nif the driver is installed correctly, you may need to switch to the GPU enabled version of torch.\\n\\nFirst, uninstall the current version of torch:\\n\\n```shell\\npip uninstall torch\\n```\\n\\nThen [install torch >= 1.10 using conda for your system](https://pytorch.org/get-started/locally/).\\nThis command should work:\\n\\n```shell\\nconda install pytorch cudatoolkit=11.3 -c pytorch\\n```\\n\\n## Citing this work\\n\\nIf you use results generated by this plugin in a publication, please cite:\\n\\n```bibtex\\n@article {Conrad2022.03.17.484806,\\n\\tauthor = {Conrad, Ryan and Narayan, Kedar},\\n\\ttitle = {Instance segmentation of mitochondria in electron microscopy images with a generalist deep learning model},\\n\\telocation-id = {2022.03.17.484806},\\n\\tyear = {2022},\\n\\tdoi = {10.1101/2022.03.17.484806},\\n\\tpublisher = {Cold Spring Harbor Laboratory},\\n\\tURL = {https://www.biorxiv.org/content/early/2022/03/18/2022.03.17.484806},\\n\\teprint = {https://www.biorxiv.org/content/early/2022/03/18/2022.03.17.484806.full.pdf},\\n\\tjournal = {bioRxiv}\\n}\\n```\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"empanada-napari\\nThe preprint describing this work is now available on bioRxiv.\\nDocumentation for the plugin, including more detailed installation instructions, can be found here.\\nempanada is a tool for deep learning-based panoptic segmentation of 2D and 3D electron microscopy images of cells.\\nThis plugin allows the running of panoptic segmentation models trained in empanada within napari.\\nFor help with this plugin please open an issue, for issues with napari specifically\\nraise an issue here instead.\\nImplemented Models\\n\\nMitoNet: A generalist mitochondrial instance segmentation model.\\n\\nExample Datasets\\nVolume EM datasets for benchmarking mitochondrial instance segmentation are available from\\nEMPIAR-10982.\\nInstallation\\nIt's recommended to have installed napari through conda.\\nThen to install this plugin:\\nshell\\npip install empanada-napari\\nLaunch napari:\\nshell\\nnapari\\nLook for empanada-napari under the \\\"Plugins\\\" menu.\\n\\nGPU Support\\nNote: Mac doesn't support NVIDIA GPUS. This section only applies to Windows and Linux systems.\\nAs for any deep learning models, having a GPU installed on your system will significantly\\nincrease model throughput (although we ship CPU optimized versions of all models with the plugin).\\nThis plugin relies on torch for running models. If a GPU was found on your system, then you will see that the\\n\\\"Use GPU\\\" checkbox is checked by default in the \\\"2D Inference\\\" and \\\"3D Inference\\\" plugin widgets. Or if when running\\ninference you see a message that says \\\"Using CPU\\\" in the terminal that means a GPU is not being used.\\nMake sure that GPU drivers are correctly installed. In terminal or command prompt:\\nshell\\nnvidia-smi\\nIf this returns \\\"command not found\\\" then you need to install the driver from NVIDIA. Instead, if\\nif the driver is installed correctly, you may need to switch to the GPU enabled version of torch.\\nFirst, uninstall the current version of torch:\\nshell\\npip uninstall torch\\nThen install torch >= 1.10 using conda for your system.\\nThis command should work:\\nshell\\nconda install pytorch cudatoolkit=11.3 -c pytorch\\nCiting this work\\nIf you use results generated by this plugin in a publication, please cite:\\nbibtex\\n@article {Conrad2022.03.17.484806,\\n    author = {Conrad, Ryan and Narayan, Kedar},\\n    title = {Instance segmentation of mitochondria in electron microscopy images with a generalist deep learning model},\\n    elocation-id = {2022.03.17.484806},\\n    year = {2022},\\n    doi = {10.1101/2022.03.17.484806},\\n    publisher = {Cold Spring Harbor Laboratory},\\n    URL = {https://www.biorxiv.org/content/early/2022/03/18/2022.03.17.484806},\\n    eprint = {https://www.biorxiv.org/content/early/2022/03/18/2022.03.17.484806.full.pdf},\\n    journal = {bioRxiv}\\n}\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"empanada-napari\",\"documentation\":\"https://github.com/volume-em/empanada-napari#README.md\",\"first_released\":\"2022-03-04T16:37:05.562410Z\",\"license\":\"BSD-3-Clause\",\"name\":\"empanada-napari\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/volume-em/empanada-napari\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-10-04T16:16:04.635717Z\",\"report_issues\":\"https://github.com/volume-em/empanada-napari/issues\",\"requirements\":[\"napari (>=0.4.15)\",\"numpy (<1.23)\",\"napari-plugin-engine (>=0.1.4)\",\"scikit-image (>=0.19)\",\"empanada-dl (>=0.1.7)\",\"imagecodecs\"],\"summary\":\"Napari plugin of algorithms for Panoptic Segmentation of organelles in EM\",\"support\":\"https://github.com/volume-em/empanada-napari/issues\",\"twitter\":\"\",\"version\":\"0.2.3\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Riley M Shea\"}],\"code_repository\":null,\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"avidaq\"}],\"description\":\"# avidaq\\n\\n[![PyPI](https://img.shields.io/pypi/v/avidaq.svg?color=green)](https://pypi.org/project/avidaq)\\n[![Python Version](https://img.shields.io/pypi/pyversions/avidaq.svg?color=green)](https://python.org)\\n[![tests](https://github.com/optimax/avidaq/workflows/tests/badge.svg)](https://github.com/optimax/avidaq/actions)\\n[![codecov](https://codecov.io/gh/optimax/avidaq/branch/main/graph/badge.svg)](https://codecov.io/gh/optimax/avidaq)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/avidaq)](https://napari-hub.org/plugins/avidaq)\\n\\ncontrols for napari and micromanger\\n\\n---\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/plugins/index.html\\n-->\\n\\n## Installation\\n\\n### Standard installation\\n\\nYou can install `avidaq` via [pip]:\\n\\n```shell\\npip install napari[all] avidaq\\n```\\n\\n### Install from plugin menu\\n\\nAlternatively you can install `avidaq` via the [napari] plugin menu:\\n\\n## ![napari-add-plugin](napari-add-plugin.png)\\n\\n## Running\\n\\nFirst start micromanager.  Make sure the server port checkbox is activated.\\n\\nThen to start napari with the avidaq plugin active run:\\n`napari -w avidaq`\\n\\n![](screenshot.png)\\n\\n## Updating presets\\n\\nMDA presets are stored in a json file in the user's home directory.\\n\\n```shell\\n\\n`C:\\\\\\\\Users\\\\YourName\\\\.avidaq\\\\mda_presets.json`\\n```\\n\\nThis file should exist after plugin installation with some defaults. You do not need to create the file yourself.\\n\\nAdd or modify the values and reload napari to see the changes.\\n\\nAll parameter entries are optional, if not provided the default value will be used.\\n\\nThe parameter names and their descriptions can be found [here] (https://github.com/micro-manager/pycro-manager/blob/main/pycromanager/acq_util.py#L102-L115)\\n\\nThe format is as follows:\\n\\n```json\\n{\\n    \\\"gui_display_name\\\": {\\n        \\\"parameter_name\\\": value,\\n        \\\"parameter_name\\\": value,\\n        ...\\n    },\\n    \\\"gui_display_name\\\": {\\n        \\\"parameter_name\\\": value,\\n        \\\"parameter_name\\\": value,\\n        ...\\n    },\\n    ...\\n}\\n```\\n\\ndefaults:\\n\\n```json\\n{\\n  \\\"Basic\\\": {\\n    \\\"num_time_points\\\": 5,\\n    \\\"z_start\\\": 0,\\n    \\\"z_end\\\": 6,\\n    \\\"z_step\\\": 0.4\\n  },\\n  \\\"Simple\\\": {\\n    \\\"num_time_points\\\": 2,\\n    \\\"z_start\\\": 0,\\n    \\\"z_end\\\": 2,\\n    \\\"z_step\\\": 0.1\\n  },\\n  \\\"Detailed\\\": {\\n    \\\"num_time_points\\\": 10,\\n    \\\"z_start\\\": 0,\\n    \\\"z_end\\\": 12,\\n    \\\"z_step\\\": 0.2\\n  }\\n}\\n```\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n## Development\\n\\nYou should have python3.8 or higher installed.\\n\\n1. clone this repo\\n2. create a virtual environment `python -m venv .venv && source .venv/bin/activate`\\n3. run `pip install -e '.[testing,build]'`\\n4. run `pre-commit install`\\n\\n### To run unit tests\\n\\n`pytest`\\n\\n### typical workflow\\n\\n1. edit code in `/src`\\n2. run napari -w avidaq\\n3. repeat\\n\\n### Releasing to pypi\\n\\n\\nProject is automically built and deployed to pypi upon\\n\\n\\n---\\n\\n[napari]: https://github.com/napari/napari\\n[cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[mit]: http://opensource.org/licenses/MIT\\n[bsd-3]: http://opensource.org/licenses/BSD-3-Clause\\n[gnu gpl v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[gnu lgpl v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[apache software license 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[mozilla public license 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[pypi]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"avidaq\\n\\n\\n\\n\\n\\ncontrols for napari and micromanger\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nStandard installation\\nYou can install avidaq via pip:\\nshell\\npip install napari[all] avidaq\\nInstall from plugin menu\\nAlternatively you can install avidaq via the napari plugin menu:\\n\\nRunning\\nFirst start micromanager.  Make sure the server port checkbox is activated.\\nThen to start napari with the avidaq plugin active run:\\nnapari -w avidaq\\n\\nUpdating presets\\nMDA presets are stored in a json file in the user's home directory.\\n```shell\\nC:\\\\\\\\Users\\\\YourName\\\\.avidaq\\\\mda_presets.json\\n```\\nThis file should exist after plugin installation with some defaults. You do not need to create the file yourself.\\nAdd or modify the values and reload napari to see the changes.\\nAll parameter entries are optional, if not provided the default value will be used.\\nThe parameter names and their descriptions can be found [here] (https://github.com/micro-manager/pycro-manager/blob/main/pycromanager/acq_util.py#L102-L115)\\nThe format is as follows:\\njson\\n{\\n    \\\"gui_display_name\\\": {\\n        \\\"parameter_name\\\": value,\\n        \\\"parameter_name\\\": value,\\n        ...\\n    },\\n    \\\"gui_display_name\\\": {\\n        \\\"parameter_name\\\": value,\\n        \\\"parameter_name\\\": value,\\n        ...\\n    },\\n    ...\\n}\\ndefaults:\\njson\\n{\\n  \\\"Basic\\\": {\\n    \\\"num_time_points\\\": 5,\\n    \\\"z_start\\\": 0,\\n    \\\"z_end\\\": 6,\\n    \\\"z_step\\\": 0.4\\n  },\\n  \\\"Simple\\\": {\\n    \\\"num_time_points\\\": 2,\\n    \\\"z_start\\\": 0,\\n    \\\"z_end\\\": 2,\\n    \\\"z_step\\\": 0.1\\n  },\\n  \\\"Detailed\\\": {\\n    \\\"num_time_points\\\": 10,\\n    \\\"z_start\\\": 0,\\n    \\\"z_end\\\": 12,\\n    \\\"z_step\\\": 0.2\\n  }\\n}\\nIssues\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\nDevelopment\\nYou should have python3.8 or higher installed.\\n\\nclone this repo\\ncreate a virtual environment python -m venv .venv && source .venv/bin/activate\\nrun pip install -e '.[testing,build]'\\nrun pre-commit install\\n\\nTo run unit tests\\npytest\\ntypical workflow\\n\\nedit code in /src\\nrun napari -w avidaq\\nrepeat\\n\\nReleasing to pypi\\nProject is automically built and deployed to pypi upon\\n\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari avidaq\",\"documentation\":\"\",\"first_released\":\"2022-07-21T16:20:38.686709Z\",\"license\":\"BSD-3-Clause\",\"name\":\"avidaq\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\",\"writer\",\"widget\",\"sample_data\"],\"project_site\":\"\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[\"*.npy\"],\"release_date\":\"2022-08-25T15:58:05.024483Z\",\"report_issues\":\"\",\"requirements\":[\"magicgui\",\"numpy\",\"pycromanager\",\"qtpy\",\"twine ; extra == 'build'\",\"black ; extra == 'testing'\",\"ipykernel ; extra == 'testing'\",\"matplotlib ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\",\"pyright ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"tox ; extra == 'testing'\",\"yappi ; extra == 'testing'\"],\"summary\":\"controls for napari and micromanger\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.0.5\",\"writer_file_extensions\":[\".npy\"],\"writer_save_layers\":[\"image\",\"labels\"]}",
  "{\"authors\":[{\"email\":\"emmanuel.bouilhol@u-bordeaux.fr\",\"name\":\"Emmanuel Bouilhol\"}],\"code_repository\":\"https://github.com/ebouilhol/napari-DeepSpot\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-deepspot\"}],\"description\":\"# napari-DeepSpot\\n\\n[![License](https://img.shields.io/pypi/l/napari-DeepSpot.svg?color=green)](https://github.com/ebouilhol/napari-DeepSpot/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-DeepSpot.svg?color=green)](https://pypi.org/project/napari-DeepSpot)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-DeepSpot.svg?color=green)](https://python.org)\\n[![tests](https://github.com/ebouilhol/napari-DeepSpot/workflows/tests/badge.svg)](https://github.com/ebouilhol/napari-DeepSpot/actions)\\n[![codecov](https://codecov.io/gh/ebouilhol/napari-DeepSpot/branch/main/graph/badge.svg)](https://codecov.io/gh/ebouilhol/napari-DeepSpot)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-DeepSpot)](https://napari-hub.org/plugins/napari-DeepSpot)\\n\\nRNA spot enhancement for fluorescent microscopy images.\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-DeepSpot` via [pip]:\\n\\n    pip install napari-DeepSpot\\n\\n## Build from source\\n\\nThis plugin is using Tensorflow, make sure your Python environment has Tensorflow, on create a new environment using the following commands:\\n* Conda:  \\n`conda env create -f environment.yml`  \\n`conda activate deepspot-napari`\\n* Or pip:   \\n`pip install -r requirements.txt`\\n\\n## Usage\\n\\nOpen one or multiple images using Napari GUI : \\nFile > Open > Select your image\\n\\nThe images are then displayed on Napari\\n\\nLoad the Plugin:\\nPlugins > Napari-DeepSpot:Enhance Spot\\n\\n![Usage](./image/napari.png)\\n\\nClick on the right panel Button \\\"Enhance\\\"\\n\\nWait a few seconds for the magic to happen :\\n\\n![Usage](./image/napari_enhance.png)\\n\\nYou can see the original images and the enhanced version in the left panel in the layer section.\\n\\nTo save the images : File > Save all layers or File > Save selected layers.\\n\\n\\n![Usage](./image/napari_video.gif)\\n\\n\\n\\n## Citation\\nIf you use this plugin please cite the [paper](https://www.biorxiv.org/content/10.1101/2021.11.25.469984v1):\\n\\n>@article {Bouilhol2021DeepSpot,  \\n>\\t author = {Bouilhol, Emmanuel and Lefevre, Edgar and Dartigues, Benjamin and Brackin, Robyn and Savulescu, Anca Flavia and Nikolski, Macha},  \\n>\\t title = {DeepSpot: a deep neural network for RNA spot enhancement in smFISH microscopy images},  \\n>\\t elocation-id = {2021.11.25.469984},  \\n>\\t year = {2021},  \\n>\\t doi = {10.1101/2021.11.25.469984},  \\n>\\t publisher = {Cold Spring Harbor Laboratory},  \\n>\\t URL = {https://www.biorxiv.org/content/early/2021/11/25/2021.11.25.469984},  \\n>\\t eprint = {https://www.biorxiv.org/content/early/2021/11/25/2021.11.25.469984.full.pdf},  \\n>\\t journal = {bioRxiv}  \\n>}  \\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [MIT] license,\\n\\\"napari-DeepSpot\\\" is free and open source software\\n\\n## Known Issues\\n\\nIf you have troubles with the Python packages `typing extensions`, use the command :  \\n`pip install typing-extensions --upgrade`  \\n\\nWhen using \\\"Enhance\\\" on multiple images, Napari may freeze. Just wait until it comes to life again, the images will still be enhanced. This is due to Napari memory usage and will be fix one day.\\n\\n\\n## Other Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/ebouilhol/napari-DeepSpot/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-DeepSpot\\n\\n\\n\\n\\n\\n\\nRNA spot enhancement for fluorescent microscopy images.\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-DeepSpot via pip:\\npip install napari-DeepSpot\\n\\nBuild from source\\nThis plugin is using Tensorflow, make sure your Python environment has Tensorflow, on create a new environment using the following commands:\\n* Conda:\\nconda env create -f environment.yml\\nconda activate deepspot-napari\\n* Or pip: \\npip install -r requirements.txt\\nUsage\\nOpen one or multiple images using Napari GUI : \\nFile > Open > Select your image\\nThe images are then displayed on Napari\\nLoad the Plugin:\\nPlugins > Napari-DeepSpot:Enhance Spot\\n\\nClick on the right panel Button \\\"Enhance\\\"\\nWait a few seconds for the magic to happen :\\n\\nYou can see the original images and the enhanced version in the left panel in the layer section.\\nTo save the images : File > Save all layers or File > Save selected layers.\\n\\nCitation\\nIf you use this plugin please cite the paper:\\n\\n@article {Bouilhol2021DeepSpot,\\n   author = {Bouilhol, Emmanuel and Lefevre, Edgar and Dartigues, Benjamin and Brackin, Robyn and Savulescu, Anca Flavia and Nikolski, Macha},\\n   title = {DeepSpot: a deep neural network for RNA spot enhancement in smFISH microscopy images},\\n   elocation-id = {2021.11.25.469984},\\n   year = {2021},\\n   doi = {10.1101/2021.11.25.469984},\\n   publisher = {Cold Spring Harbor Laboratory},\\n   URL = {https://www.biorxiv.org/content/early/2021/11/25/2021.11.25.469984},\\n   eprint = {https://www.biorxiv.org/content/early/2021/11/25/2021.11.25.469984.full.pdf},\\n   journal = {bioRxiv}\\n}  \\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the MIT license,\\n\\\"napari-DeepSpot\\\" is free and open source software\\nKnown Issues\\nIf you have troubles with the Python packages typing extensions, use the command :\\npip install typing-extensions --upgrade \\nWhen using \\\"Enhance\\\" on multiple images, Napari may freeze. Just wait until it comes to life again, the images will still be enhanced. This is due to Napari memory usage and will be fix one day.\\nOther Issues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari-DeepSpot\",\"documentation\":\"https://github.com/ebouilhol/napari-DeepSpot#README.md\",\"first_released\":\"2021-11-29T15:55:18.777627Z\",\"license\":\"MIT\",\"name\":\"napari-DeepSpot\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/ebouilhol/napari-DeepSpot\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2021-11-30T17:41:05.396583Z\",\"report_issues\":\"https://github.com/ebouilhol/napari-DeepSpot/issues\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"pytest\",\"pytest-cov\",\"pytest-xvfb\",\"pytest-qt\",\"napari\",\"qtpy (==1.9.0)\",\"pyqt5\",\"tensorflow\",\"scikit-image\",\"opencv-python\"],\"summary\":\"RNA spot enhancement for fluorescent microscopy images\",\"support\":\"https://github.com/ebouilhol/napari-DeepSpot/issues\",\"twitter\":\"\",\"version\":\"0.0.7\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Dr. Andrew Annex\"}],\"code_repository\":\"https://github.com/AndrewAnnex/napari-rioxarray\",\"conda\":[],\"description\":\"# napari-rioxarray\\n\\n[![License BSD-3](https://img.shields.io/pypi/l/napari-rioxarray.svg?color=green)](https://github.com/AndrewAnnex/napari-rioxarray/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-rioxarray.svg?color=green)](https://pypi.org/project/napari-rioxarray)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-rioxarray.svg?color=green)](https://python.org)\\n[![tests](https://github.com/AndrewAnnex/napari-rioxarray/workflows/tests/badge.svg)](https://github.com/AndrewAnnex/napari-rioxarray/actions)\\n[![codecov](https://codecov.io/gh/AndrewAnnex/napari-rioxarray/branch/main/graph/badge.svg)](https://codecov.io/gh/AndrewAnnex/napari-rioxarray)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-rioxarray)](https://napari-hub.org/plugins/napari-rioxarray)\\n\\nA rioxarray plugin for napari supporting GDAL raster datatypes\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-rioxarray` via [pip]:\\n\\n    pip install napari-rioxarray\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/AndrewAnnex/napari-rioxarray.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-rioxarray\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/AndrewAnnex/napari-rioxarray/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-rioxarray\\n\\n\\n\\n\\n\\n\\nA rioxarray plugin for napari supporting GDAL raster datatypes\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-rioxarray via pip:\\npip install napari-rioxarray\\n\\nTo install latest development version :\\npip install git+https://github.com/AndrewAnnex/napari-rioxarray.git\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-rioxarray\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"Rioxarray Plugin\",\"documentation\":\"https://github.com/AndrewAnnex/napari-rioxarray#README.md\",\"first_released\":\"2022-09-01T02:25:14.120179Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-rioxarray\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\"],\"project_site\":\"https://github.com/AndrewAnnex/napari-rioxarray\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[\"*.IMG\",\"*.TIF\",\"*.vrt\",\"*.img\",\"*.TIFF\",\"*.LBL\",\"*.tif\",\"*.cub\",\"*.CUB\",\"*.fits\",\"*.tiff\",\"*.FITS\",\"*.lbl\"],\"release_date\":\"2022-09-01T02:25:14.120179Z\",\"report_issues\":\"https://github.com/AndrewAnnex/napari-rioxarray/issues\",\"requirements\":[\"numpy\",\"napari\",\"rioxarray\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\"],\"summary\":\"A rioxarray plugin for napari supporting GDAL raster datatypes\",\"support\":\"https://github.com/AndrewAnnex/napari-rioxarray/issues\",\"twitter\":\"\",\"version\":\"0.0.1\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Herearii Metuarea\"}],\"code_repository\":\"https://github.com/hereariim/napari-blossom\",\"description\":\"# napari-blossom\\r\\n\\r\\n[![License BSD-3](https://img.shields.io/pypi/l/napari-blossom.svg?color=green)](https://github.com/hereariim/napari-blossom/raw/main/LICENSE)\\r\\n[![PyPI](https://img.shields.io/pypi/v/napari-blossom.svg?color=green)](https://pypi.org/project/napari-blossom)\\r\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-blossom.svg?color=green)](https://python.org)\\r\\n[![tests](https://github.com/hereariim/napari-blossom/workflows/tests/badge.svg)](https://github.com/hereariim/napari-blossom/actions)\\r\\n[![codecov](https://codecov.io/gh/hereariim/napari-blossom/branch/main/graph/badge.svg)](https://codecov.io/gh/hereariim/napari-blossom)\\r\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-blossom)](https://napari-hub.org/plugins/napari-blossom)\\r\\n\\r\\nSegmentation of blossom apple tree images\\r\\n\\r\\n----------------------------------\\r\\n\\r\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\r\\n\\r\\n<!--\\r\\nDon't miss the full getting started guide to set up your new package:\\r\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\r\\n\\r\\nand review the napari docs for plugin developers:\\r\\nhttps://napari.org/plugins/index.html\\r\\n-->\\r\\n\\r\\n## Installation\\r\\n\\r\\nYou can install `napari-blossom` via [pip]:\\r\\n\\r\\n    pip install napari-blossom\\r\\n\\r\\n\\r\\n\\r\\nTo install latest development version :\\r\\n\\r\\n    pip install git+https://github.com/hereariim/napari-blossom.git\\r\\n\\r\\n## How does it works\\r\\n\\r\\nThis module offers a plugin that allows you to segment the images of the apple tree flowers. As input, you can enter a **single image** with the image selection widget. Once the image is entered in the napari window, you can segment the apple blossoms with the image segmentation widget by running the run button. The segmented image will appear in the napari window.\\r\\n\\r\\n## Contributing\\r\\n\\r\\nContributions are very welcome. Tests can be run with [tox], please ensure\\r\\nthe coverage at least stays the same before you submit a pull request.\\r\\n\\r\\n## License\\r\\n\\r\\nDistributed under the terms of the [BSD-3] license,\\r\\n\\\"napari-blossom\\\" is free and open source software\\r\\n\\r\\n## Issues\\r\\n\\r\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\r\\n\\r\\n[napari]: https://github.com/napari/napari\\r\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\r\\n[@napari]: https://github.com/napari\\r\\n[MIT]: http://opensource.org/licenses/MIT\\r\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\r\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\r\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\r\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\r\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\r\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\r\\n\\r\\n[file an issue]: https://github.com/hereariim/napari-blossom/issues\\r\\n\\r\\n[napari]: https://github.com/napari/napari\\r\\n[tox]: https://tox.readthedocs.io/en/latest/\\r\\n[pip]: https://pypi.org/project/pip/\\r\\n[PyPI]: https://pypi.org/\\r\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-blossom\\n\\n\\n\\n\\n\\n\\nSegmentation of blossom apple tree images\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-blossom via pip:\\npip install napari-blossom\\n\\nTo install latest development version :\\npip install git+https://github.com/hereariim/napari-blossom.git\\n\\nHow does it works\\nThis module offers a plugin that allows you to segment the images of the apple tree flowers. As input, you can enter a single image with the image selection widget. Once the image is entered in the napari window, you can segment the apple blossoms with the image segmentation widget by running the run button. The segmented image will appear in the napari window.\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-blossom\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"Blossom\",\"documentation\":\"https://github.com/hereariim/napari-blossom#README.md\",\"first_released\":\"2022-06-20T20:23:41.594296Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-blossom\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\",\"writer\",\"widget\",\"sample_data\"],\"project_site\":\"https://github.com/hereariim/napari-blossom\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[\"*.npy\"],\"release_date\":\"2022-12-08T12:44:23.508285Z\",\"report_issues\":\"https://github.com/hereariim/napari-blossom/issues\",\"requirements\":[\"numpy\",\"magicgui\",\"qtpy\",\"opencv-python-headless\",\"tensorflow\",\"scikit-image\",\"napari\",\"focal-loss\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\"],\"summary\":\"Segmentation of blossom apple tree images\",\"support\":\"https://github.com/hereariim/napari-blossom/issues\",\"twitter\":\"\",\"version\":\"0.0.3\",\"visibility\":\"public\",\"writer_file_extensions\":[\".npy\"],\"writer_save_layers\":[\"labels\",\"image\"]}",
  "{\"authors\":[{\"email\":\"alisterburt@gmail.com\",\"name\":\"Alister Burt\"}],\"code_repository\":\"https://github.com/alisterburt/napari-tomoslice\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-tomoslice\"}],\"description\":\"# napari-tomoslice\\n\\n[![License](https://img.shields.io/pypi/l/napari-tomoslice.svg?color=green)](https://github.com/alisterburt/napari-tomoslice/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-tomoslice.svg?color=green)](https://pypi.org/project/napari-tomoslice)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-tomoslice.svg?color=green)](https://python.org)\\n[![tests](https://github.com/alisterburt/napari-tomoslice/workflows/tests/badge.svg)](https://github.com/alisterburt/napari-tomoslice/actions)\\n[![codecov](https://codecov.io/gh/alisterburt/napari-tomoslice/branch/master/graph/badge.svg)](https://codecov.io/gh/alisterburt/napari-tomoslice)\\n\\nA napari plugin for visualising and interacting with electron cryotomograms.\\n\\n\\n## Installation\\n\\nYou can install `napari-tomoslice` via [pip]:\\n\\n    pip install napari-tomoslice\\n\\n## Usage\\n\\nThis plugin provides a user interface for opening electron cryotomograms in \\nnapari as both volumes and slices through volumes.\\n\\n![demo](https://user-images.githubusercontent.com/7307488/138575305-b05c4735-9c03-4629-bfb0-9612ea8f26fd.gif)\\n\\nThe plugin can be opened from the `plugins` menu in napari, or with \\n`napari-tomoslice` at the command line.\\n\\n![plugins-menu](https://user-images.githubusercontent.com/7307488/138575015-00ea78d9-02c1-44bc-9034-0c0a7fa8d973.png)\\n\\n```yaml\\nUsage: napari-tomoslice [TOMOGRAM_FILE]\\n\\n  An interactive tomogram slice viewer in napari.\\n\\n  Controls: \\n  x/y/z - align normal vector along x/y/z axis \\n  click and drag - shift plane along its normal vector\\n  alt-click - add point on plane (if points layer is active)\\n  o - align plane normal to view direction\\n  [] - decrease/increase plane thickness\\n\\nArguments:\\n  [TOMOGRAM_FILE]\\n\\nOptions:\\n  --help                          Show this message and exit.\\n\\n```\\n\\n## Contributing\\n\\nContributions are very welcome. \\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-tomoslice\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/alisterburt/napari-tomoslice/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-tomoslice\\n\\n\\n\\n\\n\\nA napari plugin for visualising and interacting with electron cryotomograms.\\nInstallation\\nYou can install napari-tomoslice via pip:\\npip install napari-tomoslice\\n\\nUsage\\nThis plugin provides a user interface for opening electron cryotomograms in \\nnapari as both volumes and slices through volumes.\\n\\nThe plugin can be opened from the plugins menu in napari, or with \\nnapari-tomoslice at the command line.\\n\\n```yaml\\nUsage: napari-tomoslice [TOMOGRAM_FILE]\\nAn interactive tomogram slice viewer in napari.\\nControls: \\n  x/y/z - align normal vector along x/y/z axis \\n  click and drag - shift plane along its normal vector\\n  alt-click - add point on plane (if points layer is active)\\n  o - align plane normal to view direction\\n  [] - decrease/increase plane thickness\\nArguments:\\n  [TOMOGRAM_FILE]\\nOptions:\\n  --help                          Show this message and exit.\\n```\\nContributing\\nContributions are very welcome. \\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-tomoslice\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari-tomoslice\",\"documentation\":\"https://github.com/alisterburt/napari-tomoslice#README.md\",\"first_released\":\"2021-10-24T00:48:28.316518Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-tomoslice\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/alisterburt/napari-tomoslice\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-02-01T22:21:41.010473Z\",\"report_issues\":\"https://github.com/alisterburt/napari-tomoslice/issues\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"napari (==0.4.12)\",\"mrcfile\",\"typer\"],\"summary\":\"A napari plugin for interacting with electron cryotomograms\",\"support\":\"https://github.com/alisterburt/napari-tomoslice/issues\",\"twitter\":\"\",\"version\":\"0.0.7\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Tristan Cotte\"}],\"code_repository\":\"https://github.com/tcotte/napari-calibration\",\"conda\":[],\"description\":\"# napari-calibration\\r\\n\\r\\n[![License](https://img.shields.io/pypi/l/napari-calibration.svg?color=green)](https://github.com/tcotte/napari-calibration/raw/main/LICENSE)\\r\\n[![PyPI](https://img.shields.io/pypi/v/napari-calibration.svg?color=green)](https://pypi.org/project/napari-calibration)\\r\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-calibration.svg?color=green)](https://python.org)\\r\\n[![tests](https://github.com/tcotte/napari-calibration/workflows/tests/badge.svg)](https://github.com/tcotte/napari-calibration/actions)\\r\\n[![codecov](https://codecov.io/gh/tcotte/napari-calibration/branch/main/graph/badge.svg)](https://codecov.io/gh/tcotte/napari-calibration)\\r\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-calibration)](https://napari-hub.org/plugins/napari-calibration)\\r\\n\\r\\nPlug in which enables to make camera calibration\\r\\n\\r\\n----------------------------------\\r\\n\\r\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\r\\n\\r\\n<!--\\r\\nDon't miss the full getting started guide to set up your new package:\\r\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\r\\n\\r\\nand review the napari docs for plugin developers:\\r\\nhttps://napari.org/plugins/stable/index.html\\r\\n-->\\r\\n\\r\\n## Installation\\r\\n\\r\\nYou can install `napari-calibration` via [pip]:\\r\\n\\r\\n    pip install napari-calibration\\r\\n\\r\\n\\r\\n\\r\\nTo install latest development version :\\r\\n\\r\\n    pip install git+https://github.com/tcotte/napari-calibration.git\\r\\n\\r\\n\\r\\n## Contributing\\r\\n\\r\\nContributions are very welcome. Tests can be run with [tox], please ensure\\r\\nthe coverage at least stays the same before you submit a pull request.\\r\\n\\r\\n## License\\r\\n\\r\\nDistributed under the terms of the [BSD-3] license,\\r\\n\\\"napari-calibration\\\" is free and open source software\\r\\n\\r\\n## Issues\\r\\n\\r\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\r\\n\\r\\n[napari]: https://github.com/napari/napari\\r\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\r\\n[@napari]: https://github.com/napari\\r\\n[MIT]: http://opensource.org/licenses/MIT\\r\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\r\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\r\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\r\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\r\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\r\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\r\\n\\r\\n[file an issue]: https://github.com/tcotte/napari-calibration/issues\\r\\n\\r\\n[napari]: https://github.com/napari/napari\\r\\n[tox]: https://tox.readthedocs.io/en/latest/\\r\\n[pip]: https://pypi.org/project/pip/\\r\\n[PyPI]: https://pypi.org/\\r\\n\\r\\n\\r\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-calibration\\n\\n\\n\\n\\n\\n\\nPlug in which enables to make camera calibration\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-calibration via pip:\\npip install napari-calibration\\n\\nTo install latest development version :\\npip install git+https://github.com/tcotte/napari-calibration.git\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-calibration\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari Calibration\",\"documentation\":\"https://github.com/tcotte/napari-calibration#README.md\",\"first_released\":\"2022-08-23T15:02:59.873198Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-calibration\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/tcotte/napari-calibration\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-08-29T12:09:19.883590Z\",\"report_issues\":\"https://github.com/tcotte/napari-calibration/issues\",\"requirements\":null,\"summary\":\"Plug in which enables to make camera calibration\",\"support\":\"https://github.com/tcotte/napari-calibration/issues\",\"twitter\":\"\",\"version\":\"0.0.14\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"talley.lambert@gmail.com\",\"name\":\"Talley Lambert\"}],\"code_repository\":\"https://github.com/tlambert03/napari-manual-transforms\",\"conda\":[],\"description\":\"# napari-manual-transforms\\n\\n[![License](https://img.shields.io/pypi/l/napari-manual-transforms.svg?color=green)](https://github.com/tlambert03/napari-manual-transforms/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-manual-transforms.svg?color=green)](https://pypi.org/project/napari-manual-transforms)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-manual-transforms.svg?color=green)](https://python.org)\\n[![tests](https://github.com/tlambert03/napari-manual-transforms/workflows/tests/badge.svg)](https://github.com/tlambert03/napari-manual-transforms/actions)\\n[![codecov](https://codecov.io/gh/tlambert03/napari-manual-transforms/branch/main/graph/badge.svg)](https://codecov.io/gh/tlambert03/napari-manual-transforms)\\n\\nInterface to manually edit layer affine transforms.\\n\\n- express rotations as quaternion, euler angle, or axis + angle.\\n- allows rotation around arbitrary origin\\n- currently, focusing on rigid rotations\\n- Alt-Drag to rotate a layer independently of the rest.\\n- image resampling (i.e. \\\"apply\\\" the transformation to create new dataset that can be saved)\\n\\n![Plugin Preview](/preview.jpeg)\\n\\ncaveats:\\n\\n- only works on 3D Image layers for now, open a feature request for other dims/layers.\\n- will likely result in \\\"Non-orthogonal slicing is being requested\\\" warnings in 2D view.\\n\\n## Try it out\\n\\n```python\\n\\nimport napari\\n\\nv = napari.Viewer()\\nv.dims.ndisplay = 3\\nv.open_sample('napari', 'cells3d')\\nv.window.add_plugin_dock_widget('napari-manual-transforms')\\n\\nnapari.run()\\n\\n```\\n\\n----------------------------------\\n\\n## Installation\\n\\nYou can install `napari-manual-transforms` via [pip]:\\n\\n```sh\\npip install napari-manual-transforms\\n```\\n\\nTo install latest development version :\\n\\n```sh\\npip install git+https://github.com/tlambert03/napari-manual-transforms.git\\n```\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-manual-transforms\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/tlambert03/napari-manual-transforms/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-manual-transforms\\n\\n\\n\\n\\n\\nInterface to manually edit layer affine transforms.\\n\\nexpress rotations as quaternion, euler angle, or axis + angle.\\nallows rotation around arbitrary origin\\ncurrently, focusing on rigid rotations\\nAlt-Drag to rotate a layer independently of the rest.\\nimage resampling (i.e. \\\"apply\\\" the transformation to create new dataset that can be saved)\\n\\n\\ncaveats:\\n\\nonly works on 3D Image layers for now, open a feature request for other dims/layers.\\nwill likely result in \\\"Non-orthogonal slicing is being requested\\\" warnings in 2D view.\\n\\nTry it out\\n```python\\nimport napari\\nv = napari.Viewer()\\nv.dims.ndisplay = 3\\nv.open_sample('napari', 'cells3d')\\nv.window.add_plugin_dock_widget('napari-manual-transforms')\\nnapari.run()\\n```\\n\\nInstallation\\nYou can install napari-manual-transforms via pip:\\nsh\\npip install napari-manual-transforms\\nTo install latest development version :\\nsh\\npip install git+https://github.com/tlambert03/napari-manual-transforms.git\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-manual-transforms\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"Manual Transforms\",\"documentation\":\"https://github.com/tlambert03/napari-manual-transforms#README.md\",\"first_released\":\"2022-04-28T17:16:04.435019Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-manual-transforms\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/tlambert03/napari-manual-transforms\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-04-29T20:47:53.695702Z\",\"report_issues\":\"https://github.com/tlambert03/napari-manual-transforms/issues\",\"requirements\":[\"magicgui\",\"napari\",\"numpy\",\"pytransform3d\",\"qtpy\",\"scipy\",\"vispy\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"tox ; extra == 'testing'\"],\"summary\":\"Interface to manually edit layer affine transforms\",\"support\":\"https://github.com/tlambert03/napari-manual-transforms/issues\",\"twitter\":\"\",\"version\":\"0.0.3\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Andrey Aristov\"}],\"code_repository\":\"https://github.com/aaristov/napari-segment\",\"description\":\"# napari-segment\\n\\n[![License](https://img.shields.io/pypi/l/napari-segment.svg?color=green)](https://github.com/aaristov/napari-segment/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-segment.svg?color=green)](https://pypi.org/project/napari-segment)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-segment.svg?color=green)](https://python.org)\\n[![tests](https://github.com/aaristov/napari-segment/workflows/tests/badge.svg)](https://github.com/aaristov/napari-segment/actions)\\n[![codecov](https://codecov.io/gh/aaristov/napari-segment/branch/main/graph/badge.svg)](https://codecov.io/gh/aaristov/napari-segment)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-segment)](https://napari-hub.org/plugins/napari-segment)\\n\\nInteractively segment organoids/spheroids/aggregates in brightfield/fluorescence from nd2 multipositional stack.\\n----------------------------------\\n\\n![image](https://user-images.githubusercontent.com/11408456/201948817-255717a6-5f5c-45a2-ae01-2e0cbb1e29e8.png)\\n\\n\\n## Installation\\n\\n```pip install napari-segment```\\n\\nor\\n\\nFrom napari plugin\\n\\n![image](https://user-images.githubusercontent.com/11408456/201949692-33f94eaf-ac43-44dd-8c21-e9f9a460c5b2.png)\\n\\n\\n## Usage for segmentation\\n\\n1. Drag your nd2 file into napari (otherwise try the Sample data from File / Open Sample / napari-segment)\\n2. Lauch Plugins -> napari-segment: Segment multipos\\n3. Select the brightfield channel\\n4. The data is lazily loaded from nd2 dataset and dynamically segmented in the viewer.\\n5. Binning 1-8 allows to skip small features and focus on bigger objects, also makes processing faster.\\n![image](https://user-images.githubusercontent.com/11408456/201701163-70c4af51-8a3a-42a0-adb9-32f0114eb49d.png)\\n6. Various preprocessing modes allow segmentation of different objects:\\n![image](https://user-images.githubusercontent.com/11408456/201701809-f16a23ea-d14a-4b38-8b8c-08a113416509.png)\\n\\n  - Invert: will use the dark shadow around aggregate - best for very old aggregates , out of focus (File / Open Sample / napari-segment / Old aggregate)\\n  \\n  ![image](https://user-images.githubusercontent.com/11408456/201701950-efd86fae-d85b-471c-bb44-a0e328e26adc.png)\\n\\n  - Gradient: best for very sharp edges, early aggregates, single cells (File / Open Sample / napari-segment / Early aggregate) \\n  \\n  ![image](https://user-images.githubusercontent.com/11408456/201705697-5d0d0643-44b6-4cb9-9208-4a29dd899d8c.png)\\n  \\n  \\n  - Gauss diff: Fluorescence images\\n  The result of preprocessing will be shown in the \\\"Preprocessing\\\" layer.\\n7. Smooth, Theshold and Erode parameters allow you to adjust the preliminary segmentation -> they all will appear in the \\\"Detections\\\" layer as outlines \\n\\n  ![image](https://user-images.githubusercontent.com/11408456/201703675-cff6bac1-bb2a-4d45-963f-6e6d00309c77.png)\\n\\n8. Min/max diameter and eccentricity allow you to filter out unwanted regions -> the good regions will appear in the \\\"selected labels\\\" layer as filled areas.\\n\\n![image](https://user-images.githubusercontent.com/11408456/201703754-2c83a8d6-70c2-444a-8e30-54a39c901cd0.png)\\n![image](https://user-images.githubusercontent.com/11408456/201707025-9121f0dc-3939-48f0-ae75-884891be8d66.png)\\n\\n\\n9. Once satisfied, click \\\"Save the params!\\\" - it will automatically create file.nd2.params.yml file, so you can recall how the segmentation was done. Next time you open the same dataset, the parameters will be loaded automatically from this file. \\n\\n10. Next section is for quantifying the sizes. Pixel size will be retrieved automatically from metadata. If not: update it manually and click Update plots to see the correct sizes. Click on any suspected value to see the corresponding frame and try to adjust the above parameters. \\n\\n![image](https://user-images.githubusercontent.com/11408456/201704881-b2303b9a-50c6-49c7-80ff-a6099cc2a151.png)\\n\\n11. If impossible to get good results with automatic pipeline, click Clone for manual correction: this will create an editable \\\"Manual\\\" layer which you can edin with built-in tools in napari. Click \\\"Update plots\\\" to see the updated values. \\n\\n12. \\\"Save csv!\\\" will generate a csv file with regionprops. \\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-segment\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/aaristov/napari-segment/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-segment\\n\\n\\n\\n\\n\\n\\nInteractively segment organoids/spheroids/aggregates in brightfield/fluorescence from nd2 multipositional stack.\\n\\nInstallation\\npip install napari-segment\\nor\\nFrom napari plugin\\n\\nUsage for segmentation\\n\\nDrag your nd2 file into napari (otherwise try the Sample data from File / Open Sample / napari-segment)\\nLauch Plugins -> napari-segment: Segment multipos\\nSelect the brightfield channel\\nThe data is lazily loaded from nd2 dataset and dynamically segmented in the viewer.\\nBinning 1-8 allows to skip small features and focus on bigger objects, also makes processing faster.\\n\\n\\nVarious preprocessing modes allow segmentation of different objects:\\n\\n\\n\\nInvert: will use the dark shadow around aggregate - best for very old aggregates , out of focus (File / Open Sample / napari-segment / Old aggregate)\\n\\n\\n\\n\\nGradient: best for very sharp edges, early aggregates, single cells (File / Open Sample / napari-segment / Early aggregate) \\n\\n\\n\\nGauss diff: Fluorescence images\\n  The result of preprocessing will be shown in the \\\"Preprocessing\\\" layer.\\nSmooth, Theshold and Erode parameters allow you to adjust the preliminary segmentation -> they all will appear in the \\\"Detections\\\" layer as outlines \\n\\n\\n\\nMin/max diameter and eccentricity allow you to filter out unwanted regions -> the good regions will appear in the \\\"selected labels\\\" layer as filled areas.\\n\\n\\n\\n\\n\\nOnce satisfied, click \\\"Save the params!\\\" - it will automatically create file.nd2.params.yml file, so you can recall how the segmentation was done. Next time you open the same dataset, the parameters will be loaded automatically from this file. \\n\\n\\nNext section is for quantifying the sizes. Pixel size will be retrieved automatically from metadata. If not: update it manually and click Update plots to see the correct sizes. Click on any suspected value to see the corresponding frame and try to adjust the above parameters. \\n\\n\\n\\n\\n\\nIf impossible to get good results with automatic pipeline, click Clone for manual correction: this will create an editable \\\"Manual\\\" layer which you can edin with built-in tools in napari. Click \\\"Update plots\\\" to see the updated values. \\n\\n\\n\\\"Save csv!\\\" will generate a csv file with regionprops. \\n\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-segment\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"Segment organoid\",\"documentation\":\"https://github.com/aaristov/napari-segment#README.md\",\"first_released\":\"2022-10-05T21:56:23.425221Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-segment\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\",\"writer\",\"widget\",\"sample_data\"],\"project_site\":\"https://github.com/aaristov/napari-segment\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[\"*.nd2\",\"*.npy\",\"*.tif\"],\"release_date\":\"2022-12-05T12:51:24.245652Z\",\"report_issues\":\"https://github.com/aaristov/napari-segment/issues\",\"requirements\":[\"dask\",\"imageio-ffmpeg\",\"matplotlib\",\"napari\",\"nd2\",\"numpy\",\"pytest-qt\",\"scikit-image\",\"zarr\"],\"summary\":\"Segment organoids and measure intensities\",\"support\":\"https://github.com/aaristov/napari-segment/issues\",\"twitter\":\"\",\"version\":\"0.3.7\",\"visibility\":\"public\",\"writer_file_extensions\":[\".npy\"],\"writer_save_layers\":[\"labels\",\"image\"]}",
  "{\"authors\":[],\"category\":{\"Workflow step\":[\"Image registration\"]},\"category_hierarchy\":{\"Workflow step\":[[\"Image registration\"],[\"Image registration\",\"Affine registration\"]]},\"code_repository\":\"https://github.com/brainglobe/brainreg-napari\",\"description\":\"[![Python Version](https://img.shields.io/pypi/pyversions/brainreg-napari.svg)](https://pypi.org/project/brainreg-napari)\\n[![PyPI](https://img.shields.io/pypi/v/brainreg-napari.svg)](https://pypi.org/project/brainreg-napari)\\n[![Wheel](https://img.shields.io/pypi/wheel/brainreg-napari.svg)](https://pypi.org/project/brainreg-napari)\\n[![Development Status](https://img.shields.io/pypi/status/brainreg-napari.svg)](https://github.com/brainglobe/brainreg-napari)\\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/python/black)\\n[![codecov](https://codecov.io/gh/brainglobe/brainreg-napari/branch/master/graph/badge.svg?token=HEBXJPLD2S)](https://codecov.io/gh/brainglobe/brainreg-napari)\\n\\n# brainreg-napari\\nNapari plugin to run [brainreg](https://github.com/brainglobe/brainreg),\\ndeveloped by [Stephen Lenzi](https://github.com/stephenlenzi).\\n\\n## Installation\\n```bash\\npip install brainreg-napari\\n```\\n\\n## Usage\\nDocumentation for the plugin can be found\\n[here](https://docs.brainglobe.info/brainreg-napari/introduction), and for\\nmore detail, documentation for the original\\nbrainreg can be found [here](https://docs.brainglobe.info/brainreg/introduction)\\nand a tutorial is [here](https://docs.brainglobe.info/brainreg/tutorial).\\n\\nFor segmentation of bulk structures in 3D space\\n(e.g. injection sites, Neuropixels probes), please see\\n[brainreg-segment](https://github.com/brainglobe/brainreg-segment).\\n\\nThis software is at a very early stage, and was written with our data in mind.\\nOver time we hope to support other data types/formats. If you have any issues, please get in touch [on the forum](https://forum.image.sc/tag/brainglobe) or by\\n[raising an issue](https://github.com/brainglobe/brainreg/issues).\\n\\n## Details\\nbrainreg is an update to\\n[amap](https://github.com/SainsburyWellcomeCentre/amap-python) (itself a port\\nof the [original Java software](https://www.nature.com/articles/ncomms11879))\\nto include multiple registration backends, and to support the many atlases\\nprovided by [bg-atlasapi](https://github.com/brainglobe/bg-atlasapi).\\n\\nThe aim of brainreg is to register the template brain\\n (e.g. from the [Allen Reference Atlas](https://mouse.brain-map.org/static/atlas))\\n  to the sample image. Once this is complete, any other image in the template\\n  space can be aligned with the sample (such as region annotations, for\\n  segmentation of the sample image). The template to sample transformation\\n  can also be inverted, allowing sample images to be aligned in a common\\n  coordinate space.\\n\\nTo do this, the template and sample images are filtered, and then registered in\\na three step process (reorientation, affine registration, and freeform\\nregistration.) The resulting transform from template to standard space is then\\napplied to the atlas.\\n\\nFull details of the process are in the\\n[original aMAP paper](https://www.nature.com/articles/ncomms11879).\\n![reg_process](https://user-images.githubusercontent.com/13147259/143553945-a046e918-7614-4211-814c-fc840bb0159d.png)\\n**Overview of the registration process**\\n\\n## Contributing\\nContributions to brainreg-napari are more than welcome. Please see the [contributing guide](https://github.com/brainglobe/.github/blob/main/CONTRIBUTING.md).\\n\\n### Citing brainreg\\n\\nIf you find brainreg useful, and use it in your research, please let us know and also cite the paper:\\n\\n> Tyson, A. L., V&eacute;lez-Fort, M.,  Rousseau, C. V., Cossell, L., Tsitoura, C., Lenzi, S. C., Obenhaus, H. A., Claudi, F., Branco, T.,  Margrie, T. W. (2022). Accurate determination of marker location within whole-brain microscopy images. Scientific Reports, 12, 867 [doi.org/10.1038/s41598-021-04676-9](https://doi.org/10.1038/s41598-021-04676-9)\\n\\nPlease also cite aMAP (the original pipeline from which this software is based):\\n\\n>Niedworok, C.J., Brown, A.P.Y., Jorge Cardoso, M., Osten, P., Ourselin, S., Modat, M. and Margrie, T.W., (2016). AMAP is a validated pipeline for registration and segmentation of high-resolution mouse brain data. Nature Communications. 7, 1–9. https://doi.org/10.1038/ncomms11879\\n\\nLastly, if you can, please cite the BrainGlobe Atlas API that provided the atlas:\\n\\n>Claudi, F., Petrucco, L., Tyson, A. L., Branco, T., Margrie, T. W. and Portugues, R. (2020). BrainGlobe Atlas API: a common interface for neuroanatomical atlases. Journal of Open Source Software, 5(54), 2668, https://doi.org/10.21105/joss.02668\\n\\n**Don't forget to cite the developers of the atlas that you used (e.g. the Allen Brain Atlas)!**\\n\\n\\n---\\nThe BrainGlobe project is generously supported by the Sainsbury Wellcome Centre and the Institute of Neuroscience, Technical University of Munich, with funding from Wellcome, the Gatsby Charitable Foundation and the Munich Cluster for Systems Neurology - Synergy.\\n\\n<img src='https://brainglobe.info/images/logos_combined.png' width=\\\"550\\\">\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"\\n\\n\\n\\n\\n\\nbrainreg-napari\\nNapari plugin to run brainreg,\\ndeveloped by Stephen Lenzi.\\nInstallation\\nbash\\npip install brainreg-napari\\nUsage\\nDocumentation for the plugin can be found\\nhere, and for\\nmore detail, documentation for the original\\nbrainreg can be found here\\nand a tutorial is here.\\nFor segmentation of bulk structures in 3D space\\n(e.g. injection sites, Neuropixels probes), please see\\nbrainreg-segment.\\nThis software is at a very early stage, and was written with our data in mind.\\nOver time we hope to support other data types/formats. If you have any issues, please get in touch on the forum or by\\nraising an issue.\\nDetails\\nbrainreg is an update to\\namap (itself a port\\nof the original Java software)\\nto include multiple registration backends, and to support the many atlases\\nprovided by bg-atlasapi.\\nThe aim of brainreg is to register the template brain\\n (e.g. from the Allen Reference Atlas)\\n  to the sample image. Once this is complete, any other image in the template\\n  space can be aligned with the sample (such as region annotations, for\\n  segmentation of the sample image). The template to sample transformation\\n  can also be inverted, allowing sample images to be aligned in a common\\n  coordinate space.\\nTo do this, the template and sample images are filtered, and then registered in\\na three step process (reorientation, affine registration, and freeform\\nregistration.) The resulting transform from template to standard space is then\\napplied to the atlas.\\nFull details of the process are in the\\noriginal aMAP paper.\\n\\nOverview of the registration process\\nContributing\\nContributions to brainreg-napari are more than welcome. Please see the contributing guide.\\nCiting brainreg\\nIf you find brainreg useful, and use it in your research, please let us know and also cite the paper:\\n\\nTyson, A. L., Vélez-Fort, M.,  Rousseau, C. V., Cossell, L., Tsitoura, C., Lenzi, S. C., Obenhaus, H. A., Claudi, F., Branco, T.,  Margrie, T. W. (2022). Accurate determination of marker location within whole-brain microscopy images. Scientific Reports, 12, 867 doi.org/10.1038/s41598-021-04676-9\\n\\nPlease also cite aMAP (the original pipeline from which this software is based):\\n\\nNiedworok, C.J., Brown, A.P.Y., Jorge Cardoso, M., Osten, P., Ourselin, S., Modat, M. and Margrie, T.W., (2016). AMAP is a validated pipeline for registration and segmentation of high-resolution mouse brain data. Nature Communications. 7, 1–9. https://doi.org/10.1038/ncomms11879\\n\\nLastly, if you can, please cite the BrainGlobe Atlas API that provided the atlas:\\n\\nClaudi, F., Petrucco, L., Tyson, A. L., Branco, T., Margrie, T. W. and Portugues, R. (2020). BrainGlobe Atlas API: a common interface for neuroanatomical atlases. Journal of Open Source Software, 5(54), 2668, https://doi.org/10.21105/joss.02668\\n\\nDon't forget to cite the developers of the atlas that you used (e.g. the Allen Brain Atlas)!\\n\\nThe BrainGlobe project is generously supported by the Sainsbury Wellcome Centre and the Institute of Neuroscience, Technical University of Munich, with funding from Wellcome, the Gatsby Charitable Foundation and the Munich Cluster for Systems Neurology - Synergy.\\n\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"brainreg-napari\",\"documentation\":\"\",\"first_released\":\"2021-07-13T17:35:34.578786Z\",\"license\":\"MIT\",\"name\":\"brainreg-napari\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\",\"sample_data\"],\"project_site\":\"\",\"python_version\":\">=3.8.0\",\"reader_file_extensions\":[],\"release_date\":\"2022-12-06T11:08:38.449213Z\",\"report_issues\":\"\",\"requirements\":[\"napari\",\"napari-plugin-engine (>=0.1.4)\",\"magicgui\",\"qtpy\",\"brainglobe-napari-io\",\"brainreg\",\"brainreg-segment\",\"pooch (>1)\",\"black ; extra == 'dev'\",\"pytest-cov ; extra == 'dev'\",\"pytest ; extra == 'dev'\",\"pytest-qt ; extra == 'dev'\",\"napari ; extra == 'dev'\",\"pyqt5 ; extra == 'dev'\",\"tox ; extra == 'dev'\",\"gitpython ; extra == 'dev'\",\"coverage (>=5.0.3) ; extra == 'dev'\",\"bump2version ; extra == 'dev'\",\"pre-commit ; extra == 'dev'\",\"flake8 ; extra == 'dev'\",\"check-manifest ; extra == 'dev'\",\"setuptools-scm ; extra == 'dev'\"],\"summary\":\"Multi-atlas whole-brain microscopy registration\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.1.1\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Herearii Metuarea\"}],\"code_repository\":\"https://github.com/hereariim/napari-pixel-correction\",\"conda\":[],\"description\":\"<!-- This file is a placeholder for customizing description of your plugin \\non the napari hub if you wish. The readme file will be used by default if\\nyou wish not to do any customization for the napari hub listing.\\n\\nIf you need some help writing a good description, check out our \\n[guide](https://github.com/chanzuckerberg/napari-hub/wiki/Writing-the-Perfect-Description-for-your-Plugin)\\n-->\\n\\nPlugin to correct manually pixel wrongly predicted on image by annotation\",\"description_content_type\":\"text/markdown\",\"description_text\":\"\\nPlugin to correct manually pixel wrongly predicted on image by annotation\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"Pixel correction\",\"documentation\":\"https://github.com/hereariim/napari-pixel-correction#README.md\",\"first_released\":\"2022-09-19T09:19:12.974227Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-pixel-correction\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\",\"writer\",\"widget\"],\"project_site\":\"https://github.com/hereariim/napari-pixel-correction\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[\"*.npy\"],\"release_date\":\"2022-09-21T15:24:28.107693Z\",\"report_issues\":\"https://github.com/hereariim/napari-pixel-correction/issues\",\"requirements\":[\"numpy\",\"magicgui\",\"qtpy\",\"scikit-image\",\"napari\",\"matplotlib\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\"],\"summary\":\"Plugin to correct manually pixel wrongly predicted on image by annotation\",\"support\":\"https://github.com/hereariim/napari-pixel-correction/issues\",\"twitter\":\"\",\"version\":\"0.1.4\",\"visibility\":\"public\",\"writer_file_extensions\":[\".npy\"],\"writer_save_layers\":[\"labels\",\"image\"]}",
  "{\"authors\":[{\"email\":\"tristan.cotte@sgs.com\",\"name\":\"Tristan Cotte\"}],\"code_repository\":\"https://github.com/tcotte/napari-IDS\",\"conda\":[],\"description\":\"# napari-IDS\\r\\n\\r\\n[![License](https://img.shields.io/pypi/l/napari-IDS.svg?color=green)](https://github.com/githubuser/napari-IDS/raw/main/LICENSE)\\r\\n[![PyPI](https://img.shields.io/pypi/v/napari-IDS.svg?color=green)](https://pypi.org/project/napari-IDS)\\r\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-IDS.svg?color=green)](https://python.org)\\r\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-IDS)](https://napari-hub.org/plugins/napari-IDS)\\r\\n\\r\\nA simple plugin to use with napari\\r\\n\\r\\n----------------------------------\\r\\n\\r\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\r\\n\\r\\n<!--\\r\\nDon't miss the full getting started guide to set up your new package:\\r\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\r\\n\\r\\nand review the napari docs for plugin developers:\\r\\nhttps://napari.org/plugins/stable/index.html\\r\\n-->\\r\\n\\r\\n## Installation\\r\\n\\r\\nYou can install `napari-IDS` via [pip]:\\r\\n\\r\\n    pip install napari-IDS\\r\\n\\r\\n\\r\\n\\r\\nTo install latest development version :\\r\\n\\r\\n    pip install git+https://github.com/githubuser/napari-IDS.git\\r\\n\\r\\n\\r\\n## First utilisation\\r\\n\\r\\nSuggested environment : \\r\\n- Python 3.8\\r\\n- IDS 1.2.0.5 version installed\\r\\n\\r\\nTo use this package for the first time :\\r\\n1. Install Napari `pip install \\\"napari[all]\\\"`\\r\\n2. Install napari-IDS package\\r\\n3. Install IDS Python api thanks to the command `ids_packages`\\r\\n\\r\\nIf your environment is not the suggested environment, you have to install IDS packages manually. \\r\\n\\r\\n\\r\\n## Contributing\\r\\n\\r\\nContributions are very welcome. Tests can be run with [tox], please ensure\\r\\nthe coverage at least stays the same before you submit a pull request.\\r\\n\\r\\n## License\\r\\n\\r\\nDistributed under the terms of the [BSD-3] license,\\r\\n\\\"napari-IDS\\\" is free and open source software\\r\\n\\r\\n## Issues\\r\\n\\r\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\r\\n\\r\\n[napari]: https://github.com/napari/napari\\r\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\r\\n[@napari]: https://github.com/napari\\r\\n[MIT]: http://opensource.org/licenses/MIT\\r\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\r\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\r\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\r\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\r\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\r\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\r\\n\\r\\n[file an issue]: https://github.com/githubuser/napari-IDS/issues\\r\\n\\r\\n[napari]: https://github.com/napari/napari\\r\\n[tox]: https://tox.readthedocs.io/en/latest/\\r\\n[pip]: https://pypi.org/project/pip/\\r\\n[PyPI]: https://pypi.org/\\r\\n\\r\\n\\r\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-IDS\\n\\n\\n\\n\\nA simple plugin to use with napari\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-IDS via pip:\\npip install napari-IDS\\n\\nTo install latest development version :\\npip install git+https://github.com/githubuser/napari-IDS.git\\n\\nFirst utilisation\\nSuggested environment : \\n- Python 3.8\\n- IDS 1.2.0.5 version installed\\nTo use this package for the first time :\\n1. Install Napari pip install \\\"napari[all]\\\"\\n2. Install napari-IDS package\\n3. Install IDS Python api thanks to the command ids_packages\\nIf your environment is not the suggested environment, you have to install IDS packages manually. \\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-IDS\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"napari IDS\",\"documentation\":\"https://github.com/tcotte/napari-IDS#README.md\",\"first_released\":\"2022-02-17T16:27:12.336842Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-IDS\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/tcotte/napari-IDS\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-02-23T15:00:23.108346Z\",\"report_issues\":\"https://github.com/tcotte/napari-IDS/issues\",\"requirements\":null,\"summary\":\"Plug in which enables to take photo with IDS uEye camera\",\"support\":\"https://github.com/tcotte/napari-IDS/issues\",\"twitter\":\"\",\"version\":\"0.0.8\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Johannes Elferich\"}],\"code_repository\":\"https://github.com/jojoelfe/napari-cryofibsem-monitor\",\"description\":\"# napari-cryofibsem-monitor\\n\\n[![License](https://img.shields.io/pypi/l/napari-cryofibsem-monitor.svg?color=green)](https://github.com/jojoelfe/napari-cryofibsem-monitor/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-cryofibsem-monitor.svg?color=green)](https://pypi.org/project/napari-cryofibsem-monitor)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-cryofibsem-monitor.svg?color=green)](https://python.org)\\n[![tests](https://github.com/jojoelfe/napari-cryofibsem-monitor/workflows/tests/badge.svg)](https://github.com/jojoelfe/napari-cryofibsem-monitor/actions)\\n[![codecov](https://codecov.io/gh/jojoelfe/napari-cryofibsem-monitor/branch/main/graph/badge.svg)](https://codecov.io/gh/jojoelfe/napari-cryofibsem-monitor)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-cryofibsem-monitor)](https://napari-hub.org/plugins/napari-cryofibsem-monitor)\\n\\nA plugin to monitor the creation of lamella using AutoTEM on a TFS Acquilos instrument\\n\\n\\nhttps://user-images.githubusercontent.com/6081039/201448228-fdd8b429-8ff6-4934-ad58-e80fbfcbaef0.mp4\\n\\n## Changelog\\n\\n- **v0.0.3** \\n    - Update data during milling\\n    - Align images to keep lamella in the center\\n    - Monitor thickness\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-cryofibsem-monitor` via [pip]:\\n\\n    pip install napari-cryofibsem-monitor\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/jojoelfe/napari-cryofibsem-monitor.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [MIT] license,\\n\\\"napari-cryofibsem-monitor\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/jojoelfe/napari-cryofibsem-monitor/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-cryofibsem-monitor\\n\\n\\n\\n\\n\\n\\nA plugin to monitor the creation of lamella using AutoTEM on a TFS Acquilos instrument\\nhttps://user-images.githubusercontent.com/6081039/201448228-fdd8b429-8ff6-4934-ad58-e80fbfcbaef0.mp4\\nChangelog\\n\\nv0.0.3 \\nUpdate data during milling\\nAlign images to keep lamella in the center\\nMonitor thickness\\n\\n\\n\\n\\nThis napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-cryofibsem-monitor via pip:\\npip install napari-cryofibsem-monitor\\n\\nTo install latest development version :\\npip install git+https://github.com/jojoelfe/napari-cryofibsem-monitor.git\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the MIT license,\\n\\\"napari-cryofibsem-monitor\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari-cryofibsem-monitor\",\"documentation\":\"https://github.com/jojoelfe/napari-cryofibsem-monitor#README.md\",\"first_released\":\"2021-11-05T16:10:31.864413Z\",\"license\":\"MIT\",\"name\":\"napari-cryofibsem-monitor\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/jojoelfe/napari-cryofibsem-monitor\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-11-12T01:03:51.584753Z\",\"report_issues\":\"https://github.com/jojoelfe/napari-cryofibsem-monitor/issues\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"tifffile\",\"imreg-dft\",\"matplotlib\"],\"summary\":\"A plugin to monitor the creation of lamella using AutoTEM on a TFS Acquilos instrument\",\"support\":\"https://github.com/jojoelfe/napari-cryofibsem-monitor/issues\",\"twitter\":\"\",\"version\":\"0.0.3\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Marc Boucsein\"},{\"name\":\"Robin Koch\"}],\"code_repository\":\"https://github.com/MBPhys/World2Data\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"world2data\"}],\"description\":\"# World2Data\\n\\n[![License](https://img.shields.io/pypi/l/napari-medical-image-formats.svg?color=green)](https://github.com/MBPhys/World2Data/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/World2Data.svg?color=green)](https://pypi.org/project/World2Data)\\n[![Python Version](https://img.shields.io/pypi/pyversions/World2Data.svg?color=green)](https://python.org)\\n\\n\\nA napari plugin in order to convert the world information to the data of a 2D/3D layer.\\n\\n----------------------------------\\n\\n## Installation\\n\\nYou can install `World2Data` via [pip]:\\n\\n    pip install World2Data\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"World2Data\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/MBPhys/World2Data/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"World2Data\\n\\n\\n\\nA napari plugin in order to convert the world information to the data of a 2D/3D layer.\\n\\nInstallation\\nYou can install World2Data via pip:\\npip install World2Data\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"World2Data\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"World2Data\",\"documentation\":\"\",\"first_released\":\"2022-01-14T14:59:38.742805Z\",\"license\":\"BSD-3-Clause\",\"name\":\"World2Data\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/MBPhys/World2Data\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-01-14T14:59:38.742805Z\",\"report_issues\":\"\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"gryds\",\"dask\",\"scikit-image\"],\"summary\":\"A napari plugin in order to convert the world information to the data of a 2D/3D layer\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.0.3\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"d.stansby@ucl.ac.uk\",\"name\":\"David Stansby\"}],\"code_repository\":\"https://github.com/matplotlib/napari-matplotlib\",\"conda\":[],\"description\":\"# napari-matplotlib\\n\\n[![License](https://img.shields.io/pypi/l/napari-matplotlib.svg?color=green)](https://github.com/dstansby/napari-matplotlib/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-matplotlib.svg?color=green)](https://pypi.org/project/napari-matplotlib)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-matplotlib.svg?color=green)](https://python.org)\\n[![tests](https://github.com/dstansby/napari-matplotlib/workflows/tests/badge.svg)](https://github.com/dstansby/napari-matplotlib/actions)\\n[![codecov](https://codecov.io/gh/dstansby/napari-matplotlib/branch/main/graph/badge.svg)](https://codecov.io/gh/dstansby/napari-matplotlib)\\n[![pre-commit.ci status](https://results.pre-commit.ci/badge/github/matplotlib/pytest-mpl/master.svg)](https://results.pre-commit.ci/latest/github/matplotlib/pytest-mpl/master)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-matplotlib)](https://napari-hub.org/plugins/napari-matplotlib)\\n\\nA plugin to create Matplotlib plots from napari layers\\n\\n----------------------------------\\n\\n## Introduction\\n`napari-matplotlib` is a bridge between `napari` and `matplotlib`, making it easy to create publication quality `Matplotlib` plots based on the data loaded in `napari` layers.\\n\\n## Available widgets\\n\\n### `Slice`\\nPlots 1D slices of data along a specified axis.\\n![](https://raw.githubusercontent.com/dstansby/napari-matplotlib/main/examples/slice.png)\\n\\n### `Histogram`\\nPlots histograms of individual image layers, or RGB histograms of an RGB image\\n![](https://raw.githubusercontent.com/dstansby/napari-matplotlib/main/examples/hist.png)\\n\\n### `Scatter`\\nScatters the values of two similarly sized images layers against each other.\\n![](https://raw.githubusercontent.com/dstansby/napari-matplotlib/main/examples/scatter.png)\\n\\n## Installation\\n\\nYou can install `napari-matplotlib` via [pip]:\\n\\n    pip install napari-matplotlib\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/dstansby/napari-matplotlib.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome! Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n`napari-matplotlib` is free and open source software.\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[@napari]: https://github.com/napari\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n\\n[file an issue]: https://github.com/dstansby/napari-matplotlib/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-matplotlib\\n\\n\\n\\n\\n\\n\\n\\nA plugin to create Matplotlib plots from napari layers\\n\\nIntroduction\\nnapari-matplotlib is a bridge between napari and matplotlib, making it easy to create publication quality Matplotlib plots based on the data loaded in napari layers.\\nAvailable widgets\\nSlice\\nPlots 1D slices of data along a specified axis.\\n\\nHistogram\\nPlots histograms of individual image layers, or RGB histograms of an RGB image\\n\\nScatter\\nScatters the values of two similarly sized images layers against each other.\\n\\nInstallation\\nYou can install napari-matplotlib via pip:\\npip install napari-matplotlib\\n\\nTo install latest development version :\\npip install git+https://github.com/dstansby/napari-matplotlib.git\\n\\nContributing\\nContributions are very welcome! Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\nnapari-matplotlib is free and open source software.\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 3 - Alpha\"],\"display_name\":\"napari Matplotlib\",\"documentation\":\"https://github.com/matplotlib/napari-matplotlib#README.md\",\"first_released\":\"2022-05-02T09:05:57.008000Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-matplotlib\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/matplotlib/napari-matplotlib\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-06-06T10:10:57.406724Z\",\"report_issues\":\"https://github.com/matplotlib/napari-matplotlib/issues\",\"requirements\":[\"matplotlib\",\"napari\",\"numpy\",\"numpydoc ; extra == 'docs'\",\"pydata-sphinx-theme ; extra == 'docs'\",\"qtgallery ; extra == 'docs'\",\"sphinx ; extra == 'docs'\",\"sphinx-automodapi ; extra == 'docs'\",\"sphinx-gallery ; extra == 'docs'\",\"napari[pyqt5] ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"tox ; extra == 'testing'\",\"pytest-xvfb ; (sys_platform == \\\"linux\\\") and extra == 'testing'\"],\"summary\":\"A plugin to use Matplotlib with napari\",\"support\":\"https://github.com/matplotlib/napari-matplotlib/issues\",\"twitter\":\"\",\"version\":\"0.2.1\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Robert Haase\"},{\"name\":\"Marcelo Leomil Zoccoler\"}],\"code_repository\":\"https://github.com/haesleinhuepf/napari-plot-profile\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-plot-profile\"}],\"description\":\"# napari-plot-profile (npp)\\n\\n[![License](https://img.shields.io/pypi/l/napari-plot-profile.svg?color=green)](https://github.com/haesleinhuepf/napari-plot-profile/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-plot-profile.svg?color=green)](https://pypi.org/project/napari-plot-profile)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-plot-profile.svg?color=green)](https://python.org)\\n[![tests](https://github.com/haesleinhuepf/napari-plot-profile/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-plot-profile/actions)\\n[![codecov](https://codecov.io/gh/haesleinhuepf/napari-plot-profile/branch/master/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-plot-profile)\\n[![Development Status](https://img.shields.io/pypi/status/napari-plot-profile.svg)](https://en.wikipedia.org/wiki/Software_release_life_cycle#Alpha)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-plot-profile)](https://napari-hub.org/plugins/napari-plot-profile)\\n\\n## Plot a Line Profile\\n\\nPlot intensities along a line in [napari].\\n\\n![img.png](https://github.com/haesleinhuepf/napari-plot-profile/raw/main/docs/napari-plot-profile-screencast.gif)\\n\\n* Open some images in [napari].\\n  \\n* Add a shapes layer.\\n\\n![img.png](https://github.com/haesleinhuepf/napari-plot-profile/raw/main/docs/add_shapes_layer_screenshot.png)\\n  \\n* Activate the line drawing tool or the path tool and draw a line.\\n\\n![img.png](https://github.com/haesleinhuepf/napari-plot-profile/raw/main/docs/draw_line_tool_screenshot.png)\\n  \\n* After drawing a line, click on the menu Plugins > Measurements (Plot Profile)\\n* If you modify the line, you may want to click the \\\"Refresh\\\" button to redraw the profile.\\n\\n![img.png](https://github.com/haesleinhuepf/napari-plot-profile/raw/main/docs/redraw_screenshot.png)\\n\\nTo see how these steps can be done programmatically from python, check out the [demo notebook](https://github.com/haesleinhuepf/napari-plot-profile/blob/main/docs/demo.ipynb)\\n\\n## Create a Topographical View\\n\\nCreate a 3D view of a 2D image by warping pixel intensities to heights. It can be displayed as a 3D image layer, a points cloud layer or a surface layer.\\n\\n![](https://github.com/haesleinhuepf/napari-plot-profile/raw/main/docs/topographical_view_screencast.gif)\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n----------------------------------\\n\\n## Installation\\n\\nYou can install `napari-plot-profile` via [pip]:\\n\\n    pip install napari-plot-profile\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-plot-profile\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please create a thread on [image.sc] along with a detailed description and tag [@haesleinhuepf].\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/haesleinhuepf/napari-plot-profile/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n[image.sc]: https://image.sc\\n[@haesleinhuepf]: https://twitter.com/haesleinhuepf\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-plot-profile (npp)\\n\\n\\n\\n\\n\\n\\n\\nPlot a Line Profile\\nPlot intensities along a line in napari.\\n\\n\\n\\nOpen some images in napari.\\n\\n\\nAdd a shapes layer.\\n\\n\\n\\n\\nActivate the line drawing tool or the path tool and draw a line.\\n\\n\\n\\nAfter drawing a line, click on the menu Plugins > Measurements (Plot Profile)\\nIf you modify the line, you may want to click the \\\"Refresh\\\" button to redraw the profile.\\n\\n\\nTo see how these steps can be done programmatically from python, check out the demo notebook\\nCreate a Topographical View\\nCreate a 3D view of a 2D image by warping pixel intensities to heights. It can be displayed as a 3D image layer, a points cloud layer or a surface layer.\\n\\nThis napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-plot-profile via pip:\\npip install napari-plot-profile\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-plot-profile\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please create a thread on image.sc along with a detailed description and tag @haesleinhuepf.\",\"development_status\":[\"Development Status :: 3 - Alpha\"],\"display_name\":\"napari-plot-profile\",\"documentation\":\"https://github.com/haesleinhuepf/napari-plot-profile#README.md\",\"first_released\":\"2021-08-20T20:36:22.546745Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-plot-profile\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/haesleinhuepf/napari-plot-profile\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-10-05T15:15:04.990462Z\",\"report_issues\":\"https://github.com/haesleinhuepf/napari-plot-profile/issues\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"pyqtgraph\",\"napari\",\"napari-tools-menu\",\"napari-skimage-regionprops (>=0.2.4)\",\"imageio (!=2.22.1)\"],\"summary\":\"Plot intensity along a line and create topographical views in napari\",\"support\":\"https://github.com/haesleinhuepf/napari-plot-profile/issues\",\"twitter\":\"\",\"version\":\"0.2.2\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"dev@biberger.xyz\",\"name\":\"Simon Biberger\"}],\"code_repository\":\"https://github.com/biberger/napari-ccp4map\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-ccp4map\"}],\"description\":\"# napari-ccp4map\\n\\n[![License](https://img.shields.io/pypi/l/napari-ccp4map.svg?color=green)](https://github.com/biberger/napari-ccp4map/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-ccp4map.svg?color=green)](https://pypi.org/project/napari-ccp4map)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-ccp4map.svg?color=green)](https://python.org)\\n[![tests](https://github.com/biberger/napari-ccp4map/workflows/tests/badge.svg)](https://github.com/biberger/napari-ccp4map/actions)\\n[![codecov](https://codecov.io/gh/biberger/napari-ccp4map/branch/master/graph/badge.svg)](https://codecov.io/gh/biberger/napari-ccp4map)\\n\\nEnables napari to read .map files in the ccp4 format.\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-ccp4map` via [pip]:\\n\\n    pip install napari-ccp4map\\n\\n## Usage\\nIf the plugin was installed correctly, it will pop up in a napari window under Plugins->Install/Uninstall Plugins.\\nYou can either drag&drop filed into the window to read them, or search for a folder/file using Ctrl+O.\\n\\n## How it works\\nThis plugin simply reads a file and allows [gemmi](https://github.com/project-gemmi/gemmi) to interact with it. Then, numpy turns the file into an array.\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-ccp4map\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/biberger/napari-ccp4map/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-ccp4map\\n\\n\\n\\n\\n\\nEnables napari to read .map files in the ccp4 format.\\n\\nThis napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-ccp4map via pip:\\npip install napari-ccp4map\\n\\nUsage\\nIf the plugin was installed correctly, it will pop up in a napari window under Plugins->Install/Uninstall Plugins.\\nYou can either drag&drop filed into the window to read them, or search for a folder/file using Ctrl+O.\\nHow it works\\nThis plugin simply reads a file and allows gemmi to interact with it. Then, numpy turns the file into an array.\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-ccp4map\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari-ccp4map\",\"documentation\":\"https://github.com/biberger/napari-ccp4map#README.md\",\"first_released\":\"2021-10-04T17:00:03.473074Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-ccp4map\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\"],\"project_site\":\"https://github.com/biberger/napari-ccp4map\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[\"*\"],\"release_date\":\"2021-10-04T17:49:51.073658Z\",\"report_issues\":\"https://github.com/biberger/napari-ccp4map/issues\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"gemmi\"],\"summary\":\"Enables napari to read .map files in the ccp4 format. Drag&Drop or press Ctrl+O to read files.\",\"support\":\"https://github.com/biberger/napari-ccp4map/issues\",\"twitter\":\"\",\"version\":\"1.0\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"brisvag@gmail.com\",\"name\":\"Lorenzo Gaifas\"}],\"code_repository\":\"https://github.com/brisvag/napari-molecule-reader\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-molecule-reader\"}],\"description\":\"# napari-molecule-reader\\n\\n[![License](https://img.shields.io/pypi/l/napari-molecule-reader.svg?color=green)](https://github.com/brisvag/napari-molecule-reader/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-molecule-reader.svg?color=green)](https://pypi.org/project/napari-molecule-reader)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-molecule-reader.svg?color=green)](https://python.org)\\n[![tests](https://github.com/brisvag/napari-molecule-reader/workflows/tests/badge.svg)](https://github.com/brisvag/napari-molecule-reader/actions)\\n[![codecov](https://codecov.io/gh/brisvag/napari-molecule-reader/branch/main/graph/badge.svg)](https://codecov.io/gh/brisvag/napari-molecule-reader)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-molecule-reader)](https://napari-hub.org/plugins/napari-molecule-reader)\\n\\nA napari plugin that read molecular structure files. It reads PDB and MMCIF files using [`atomium`](https://github.com/samirelanduk/atomium), expanding molecular assemblies to a full visualization. Data is loaded into napari as `Points` for ball representation and `Vectors` for stick representation. If multiple models or assemblies are detected, they will be loaded as separate objects.\\n\\nhttps://user-images.githubusercontent.com/23482191/150109390-bd7fb3b4-79b4-43da-aafc-20921714df25.mp4\\n\\nTODO list:\\n- [] handle alternate locations (i.e: different conformations in the same pdb model)\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/plugins/stable/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-molecule-reader` via [pip]:\\n\\n    pip install napari-molecule-reader\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/brisvag/napari-molecule-reader.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-molecule-reader\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/brisvag/napari-molecule-reader/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-molecule-reader\\n\\n\\n\\n\\n\\n\\nA napari plugin that read molecular structure files. It reads PDB and MMCIF files using atomium, expanding molecular assemblies to a full visualization. Data is loaded into napari as Points for ball representation and Vectors for stick representation. If multiple models or assemblies are detected, they will be loaded as separate objects.\\nhttps://user-images.githubusercontent.com/23482191/150109390-bd7fb3b4-79b4-43da-aafc-20921714df25.mp4\\nTODO list:\\n- [] handle alternate locations (i.e: different conformations in the same pdb model)\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-molecule-reader via pip:\\npip install napari-molecule-reader\\n\\nTo install latest development version :\\npip install git+https://github.com/brisvag/napari-molecule-reader.git\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-molecule-reader\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari molecule reader\",\"documentation\":\"https://github.com/brisvag/napari-molecule-reader#README.md\",\"first_released\":\"2021-12-21T11:39:40.715897Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-molecule-reader\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\"],\"project_site\":\"https://github.com/brisvag/napari-molecule-reader\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[\"*.pdb\",\"*.cif\"],\"release_date\":\"2022-01-31T17:59:24.494345Z\",\"report_issues\":\"https://github.com/brisvag/napari-molecule-reader/issues\",\"requirements\":[\"numpy\",\"pandas\",\"scipy\",\"atomium\"],\"summary\":\"A napari plugin that read molecular structure files.\",\"support\":\"https://github.com/brisvag/napari-molecule-reader/issues\",\"twitter\":\"\",\"version\":\"0.1.2\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"code@arlowe.co.uk\",\"name\":\"Alan R. Lowe\"}],\"category\":{\"Supported data\":[\"Time series\"],\"Workflow step\":[\"Object tracking\"]},\"category_hierarchy\":{\"Supported data\":[[\"Time series\"]],\"Workflow step\":[[\"Object tracking\"]]},\"code_repository\":\"https://github.com/quantumjot/napari-btrack-reader\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-btrack-reader\"}],\"description\":\"# napari-btrack-reader\\n\\n[![License](https://img.shields.io/pypi/l/napari-btrack-reader.svg?color=green)](https://github.com/napari/napari-btrack-reader/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-btrack-reader.svg?color=green)](https://pypi.org/project/napari-btrack-reader)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-btrack-reader.svg?color=green)](https://python.org)\\n[![tests](https://github.com/quantumjot/napari-btrack-reader/workflows/tests/badge.svg)](https://github.com/quantumjot/napari-btrack-reader/actions)\\n[![codecov](https://codecov.io/gh/quantumjot/napari-btrack-reader/branch/master/graph/badge.svg)](https://codecov.io/gh/quantumjot/napari-btrack-reader)\\n\\nA plugin to load btrack files\\n\\n----------------------------------\\n\\nThis plugin reads tracking data generated by BayesianTracker (btrack).\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-btrack-reader` via [pip]:\\n\\n    pip install napari-btrack-reader\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-btrack-reader\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/quantumjot/napari-btrack-reader/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-btrack-reader\\n\\n\\n\\n\\n\\nA plugin to load btrack files\\n\\nThis plugin reads tracking data generated by BayesianTracker (btrack).\\nThis napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-btrack-reader via pip:\\npip install napari-btrack-reader\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-btrack-reader\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"napari-btrack-reader\",\"documentation\":\"\",\"first_released\":\"2020-11-05T10:23:48.064223Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-btrack-reader\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\"],\"project_site\":\"https://github.com/quantumjot/napari-btrack-reader\",\"python_version\":\">=3.6\",\"reader_file_extensions\":[\"*\"],\"release_date\":\"2020-11-05T10:23:48.064223Z\",\"report_issues\":\"\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"btrack (>=0.4.0)\"],\"summary\":\"A plugin to load btrack files\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.1.0\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Sebastian Rhode\"}],\"code_repository\":\"https://github.com/sebi06/napari-czann-segment\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-czann-segment\"}],\"description\":\"# napari-czann-segment\\n\\n[![License](https://img.shields.io/pypi/l/napari-czann-segment.svg?color=green)](https://github.com/sebi06/napari-czann-segment/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-czann-segment.svg?color=green)](https://pypi.org/project/napari-czann-segment)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-czann-segment.svg?color=green)](https://python.org)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-czann-segment)](https://napari-hub.org/plugins/napari-czann-segment)\\n\\nSemantic Segmentation of multi-dimensional images using Deep Learning ONNX models packaged as *.czann files.\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n![Train on APEER and use model in Napari](https://github.com/sebi06/napari-czann-segment/raw/main/readme_images/Train_APEER_run_Napari_CZANN_no_highlights_small.gif)\\n\\n## Installation\\n\\nBefore installing, please setup a conda environment. If you have never worked with conda environments, go through [this tutorial](https://biapol.github.io/blog/johannes_mueller/anaconda_getting_started/) first.\\n\\nYou can then install `napari-czann-segment` via [pip]:\\n\\n    pip install napari-czann-segment\\n\\n## What does the plugin do\\n\\nThe plugin allows you to:\\n\\n- Use a *.czann file containing the Deep Neural Network (ONNX) for semantic segmentation and metadata\\n- Segmentation will be applied per 2D plane for all dimensions\\n- Processing larger multi-dimensional images it uses the [cztile] package to chunk the individual 2d arrays using a specific overlap.\\n- multi-dimensional images will be processed plane-by-plane\\n\\n## What does the plugin NOT do\\n\\n**Before one can actually use a model it needs to be trained, which is NOT done by this plugin**.\\n\\nTherer two main ways hwo such a model can be created:\\n\\n- Train the segmentation model fully automated on [APEER] and download the *.czann file\\n- Train your model in a Jupyter notebook etc. and package it using the [czmodel] python package as an *.czann\\n\\n## Using this plugin\\n\\n### Sample Data\\n\\nA test image and a *.czann model file can be downloaded [here](https://github.com/sebi06/napari-czann-segment/tree/main/src/napari_czann_segment/_data).\\n\\n- `PGC_20X.ome.tiff` --> use `PGC_20X_nucleus_detector.czann` to segment\\n\\nIn order to use this plugin the user has to do the following things:\\n\\n- Open the image using \\\"File - Open Files(s)\\\" (requires [napari-aicsimageio] plugin).\\n- Click **napari-czann-segment: Segment with CZANN model** in the \\\"Plugins\\\" menu.\\n- **Select a *.czann file** to use the model for segmentation.\\n- metadata of the model will be shown (see example below)\\n\\n| Parameter    | Value                                        | Explanation                                             |\\n| :----------- | :------------------------------------------- | ------------------------------------------------------- |\\n| model_type   | ModelType.SINGLE_CLASS_SEMANTIC_SEGMENTATION | see: [czmodel] for details                              |\\n| input_shape  | [1024, 1024, 1]                              | tile dimensions of model input                          |\\n| output_shape | [1024, 1024, 3]                              | tile dimensions of model output                         |\\n| model_id     | ba32bc6d-6bc9-4774-8b47-20646c7cb838         | unique GUID for that model                              |\\n| min_overlap  | [128, 128]                                   | tile overlap used during training (for this model)      |\\n| classes      | ['background', 'grains', 'inclusions']       | availbale classes                                       |\\n| model_name   | APEER-trained model                          | name of the model                                       |\\n\\n![Napari - Image loaded and czann selected](https://github.com/sebi06/napari-czann-segment/raw/main/readme_images/napari_czann1.png)\\n\\n- Adjust the **minimum overlap** for the tiling (optional, see [cztile] for details).\\n- Select the **layer** to be segmented.\\n- Press **Segment Selected Image Layer** to run the segmentation.\\n\\n![Napari - Image successfully segmented](https://github.com/sebi06/napari-czann-segment/raw/main/readme_images/napari_czann2.png)\\n\\nA successful is obviously only the starting point for further image analysis steps to extract the desired numbers from the segmented image. Another example is shown below demonstrating a simple \\\"Grain Size Analysis\\\" using a deep-learning model trained on [APEER] used in [napari]\\n\\n![Napari - Simple Grain Size Analysis](https://github.com/sebi06/napari-czann-segment/raw/main/readme_images/grainsize_czann_napari.png)\\n\\n### Remarks\\n\\n> **IMPORTANT**: Currently the plugin only supports using models trained on a **single channel** image. Therefore make sure that during the training on [APEER] or somewhere else the correct inputs images are used.\\n> It is quite simple to train an single RGB image, which actually has three channels, load this image in [napari] and notice only then that the model will not work, because the image will 3 channels inside [napari].\\n\\n- Only the CPU will be used for the inference using the ONNX runtime for the [ONNX-CPU] runtime\\n- GPUs are not supported yet and will require [ONNX-GPU] runtime\\n\\n## For developers\\n\\n- **Please clone this repository first using your favorite tool.**\\n\\n- **Ideally one creates a new [conda] environment or use an existing environment that already contains [Napari].**\\n\\nFeel free to create a new environment using the [YAML](env_napari_czann_segment.yml) file at your own risk:\\n\\n    cd the-github-repo-with-YAML-file\\n    conda env create --file conda_env_napari_czann_segment.yml\\n    conda activate napari_czmodel\\n\\n- **Install the plugin locally**\\n\\nPlease run the the following command:\\n\\n    pip install -e .\\n\\nTo install latest development version:\\n\\n    pip install git+https://github.com/sebi06/napari_czann_segment.git\\n\\n## Contributing\\n\\nContributions and Feedback are very welcome.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-czann-segment\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/sebi06/napari-czann-segment/issues\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n[czmodel]: https://pypi.org/project/czmodel/\\n[cztile]: https://pypi.org/project/cztile/\\n[APEER]: https://www.apeer.com\\n[napari-aicsimageio]: https://github.com/AllenCellModeling/napari-aicsimageio\\n[ONNX-GPU]: https://pypi.org/project/onnxruntime-gpu/\\n[ONNX-CPU]: https://pypi.org/project/onnxruntime/\\n[conda]: https://docs.conda.io/projects/conda/en/latest/user-guide/getting-started.html\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-czann-segment\\n\\n\\n\\n\\nSemantic Segmentation of multi-dimensional images using Deep Learning ONNX models packaged as *.czann files.\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nBefore installing, please setup a conda environment. If you have never worked with conda environments, go through this tutorial first.\\nYou can then install napari-czann-segment via pip:\\npip install napari-czann-segment\\n\\nWhat does the plugin do\\nThe plugin allows you to:\\n\\nUse a *.czann file containing the Deep Neural Network (ONNX) for semantic segmentation and metadata\\nSegmentation will be applied per 2D plane for all dimensions\\nProcessing larger multi-dimensional images it uses the cztile package to chunk the individual 2d arrays using a specific overlap.\\nmulti-dimensional images will be processed plane-by-plane\\n\\nWhat does the plugin NOT do\\nBefore one can actually use a model it needs to be trained, which is NOT done by this plugin.\\nTherer two main ways hwo such a model can be created:\\n\\nTrain the segmentation model fully automated on APEER and download the *.czann file\\nTrain your model in a Jupyter notebook etc. and package it using the czmodel python package as an *.czann\\n\\nUsing this plugin\\nSample Data\\nA test image and a *.czann model file can be downloaded here.\\n\\nPGC_20X.ome.tiff --> use PGC_20X_nucleus_detector.czann to segment\\n\\nIn order to use this plugin the user has to do the following things:\\n\\nOpen the image using \\\"File - Open Files(s)\\\" (requires napari-aicsimageio plugin).\\nClick napari-czann-segment: Segment with CZANN model in the \\\"Plugins\\\" menu.\\nSelect a *.czann file to use the model for segmentation.\\nmetadata of the model will be shown (see example below)\\n\\n| Parameter    | Value                                        | Explanation                                             |\\n| :----------- | :------------------------------------------- | ------------------------------------------------------- |\\n| model_type   | ModelType.SINGLE_CLASS_SEMANTIC_SEGMENTATION | see: czmodel for details                              |\\n| input_shape  | [1024, 1024, 1]                              | tile dimensions of model input                          |\\n| output_shape | [1024, 1024, 3]                              | tile dimensions of model output                         |\\n| model_id     | ba32bc6d-6bc9-4774-8b47-20646c7cb838         | unique GUID for that model                              |\\n| min_overlap  | [128, 128]                                   | tile overlap used during training (for this model)      |\\n| classes      | ['background', 'grains', 'inclusions']       | availbale classes                                       |\\n| model_name   | APEER-trained model                          | name of the model                                       |\\n\\n\\nAdjust the minimum overlap for the tiling (optional, see cztile for details).\\nSelect the layer to be segmented.\\nPress Segment Selected Image Layer to run the segmentation.\\n\\n\\nA successful is obviously only the starting point for further image analysis steps to extract the desired numbers from the segmented image. Another example is shown below demonstrating a simple \\\"Grain Size Analysis\\\" using a deep-learning model trained on APEER used in napari\\n\\nRemarks\\n\\nIMPORTANT: Currently the plugin only supports using models trained on a single channel image. Therefore make sure that during the training on APEER or somewhere else the correct inputs images are used.\\nIt is quite simple to train an single RGB image, which actually has three channels, load this image in napari and notice only then that the model will not work, because the image will 3 channels inside napari.\\n\\n\\nOnly the CPU will be used for the inference using the ONNX runtime for the ONNX-CPU runtime\\nGPUs are not supported yet and will require ONNX-GPU runtime\\n\\nFor developers\\n\\n\\nPlease clone this repository first using your favorite tool.\\n\\n\\nIdeally one creates a new conda environment or use an existing environment that already contains Napari.\\n\\n\\nFeel free to create a new environment using the YAML file at your own risk:\\ncd the-github-repo-with-YAML-file\\nconda env create --file conda_env_napari_czann_segment.yml\\nconda activate napari_czmodel\\n\\n\\nInstall the plugin locally\\n\\nPlease run the the following command:\\npip install -e .\\n\\nTo install latest development version:\\npip install git+https://github.com/sebi06/napari_czann_segment.git\\n\\nContributing\\nContributions and Feedback are very welcome.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-czann-segment\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 3 - Alpha\"],\"display_name\":\"CZANN Segmentation\",\"documentation\":\"https://github.com/sebi06/napari-czann-segment#README.md\",\"first_released\":\"2022-07-11T12:10:58.248435Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-czann-segment\",\"npe2\":true,\"operating_system\":[\"Operating System :: Microsoft :: Windows\",\"Operating System :: Unix\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/sebi06/napari-czann-segment\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-10-04T13:43:07.505874Z\",\"report_issues\":\"https://github.com/sebi06/napari-czann-segment/issues\",\"requirements\":[\"numpy\",\"magicgui\",\"qtpy\",\"napari\",\"cztile\",\"czmodel[pytorch]\",\"onnxruntime\",\"aicsimageio\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\"],\"summary\":\"Semantic Segmentation using Deep Learning ONNX models packaged as *.czann files\",\"support\":\"https://github.com/sebi06/napari-czann-segment/issues\",\"twitter\":\"\",\"version\":\"0.0.16\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"robert.haase@tu-dresden.de\",\"name\":\"Robert Haase\"}],\"code_repository\":\"https://github.com/haesleinhuepf/napari-mouse-controls\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-mouse-controls\"}],\"description\":\"# napari-mouse-controls\\n\\n[![License](https://img.shields.io/pypi/l/napari-mouse-controls.svg?color=green)](https://github.com/haesleinhuepf/napari-mouse-controls/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-mouse-controls.svg?color=green)](https://pypi.org/project/napari-mouse-controls)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-mouse-controls.svg?color=green)](https://python.org)\\n[![tests](https://github.com/haesleinhuepf/napari-mouse-controls/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-mouse-controls/actions)\\n[![codecov](https://codecov.io/gh/haesleinhuepf/napari-mouse-controls/branch/main/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-mouse-controls)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-mouse-controls)](https://napari-hub.org/plugins/napari-mouse-controls)\\n\\nControl zoom, slicing and contrast windowing with mouse and touch screen\\n\\n----------------------------------\\n\\n## Usage\\n\\nYou find the mouse control panel in the menu `Tools > Utilities > Mouse controls`\\n\\n### Zoom\\n\\nAfter clicking the Zoom button ![](https://github.com/haesleinhuepf/napari-mouse-controls/raw/main/src/napari_mouse_controls/icons/Zoom.png), you can click in the napari canvas and move the mouse up and down to zoom in and out.\\n\\n![](https://github.com/haesleinhuepf/napari-mouse-controls/raw/main/docs/zoom.gif)\\n\\n### Slicing\\n\\nAfter clicking the Slicing button ![](https://github.com/haesleinhuepf/napari-mouse-controls/raw/main/src/napari_mouse_controls/icons/Slicing.png), you can control the currently displayed slice by moving the mouse.\\nBy moving the mouse up and down, you control the currently selected Z-plane.\\nBy moving the mouse left and right, you control the currently seleted time point.\\n\\n![](https://github.com/haesleinhuepf/napari-mouse-controls/raw/main/docs/slicing.gif)\\n\\n### Windowing\\n\\nAfter clicking the Windowing button ![](https://github.com/haesleinhuepf/napari-mouse-controls/raw/main/src/napari_mouse_controls/icons/Windowing.png), you can modify the brightness and contrast by moving the mouse. \\nBy moving the mouse up and down, you control window width of the range of displayed grey values (max - min).\\nBy moving the mouse left and right, you control the center of the grey value window. \\n\\n![](https://github.com/haesleinhuepf/napari-mouse-controls/raw/main/docs/windowing.gif)\\n\\n### Normal / default mode\\n\\nClick the Default button ![](https://github.com/haesleinhuepf/napari-mouse-controls/raw/main/src/napari_mouse_controls/icons/Default.png)\\nto return to napari's normal mode.\\n\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n\\n## Installation\\n\\nYou can install `napari-mouse-controls` via [pip]:\\n\\n    pip install napari-mouse-controls\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-mouse-controls\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please create a thread on [image.sc] along with a detailed description and tag [@haesleinhuepf].\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/haesleinhuepf/napari-mouse-controls/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n[image.sc]: https://image.sc\\n\\n[@haesleinhuepf]: https://twitter.com/haesleinhuepf\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-mouse-controls\\n\\n\\n\\n\\n\\n\\nControl zoom, slicing and contrast windowing with mouse and touch screen\\n\\nUsage\\nYou find the mouse control panel in the menu Tools > Utilities > Mouse controls\\nZoom\\nAfter clicking the Zoom button , you can click in the napari canvas and move the mouse up and down to zoom in and out.\\n\\nSlicing\\nAfter clicking the Slicing button , you can control the currently displayed slice by moving the mouse.\\nBy moving the mouse up and down, you control the currently selected Z-plane.\\nBy moving the mouse left and right, you control the currently seleted time point.\\n\\nWindowing\\nAfter clicking the Windowing button , you can modify the brightness and contrast by moving the mouse. \\nBy moving the mouse up and down, you control window width of the range of displayed grey values (max - min).\\nBy moving the mouse left and right, you control the center of the grey value window. \\n\\nNormal / default mode\\nClick the Default button \\nto return to napari's normal mode.\\nThis napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.\\nInstallation\\nYou can install napari-mouse-controls via pip:\\npip install napari-mouse-controls\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-mouse-controls\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please create a thread on image.sc along with a detailed description and tag @haesleinhuepf.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari-mouse-controls\",\"documentation\":\"https://github.com/haesleinhuepf/napari-mouse-controls#README.md\",\"first_released\":\"2021-10-30T19:11:36.638852Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-mouse-controls\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/haesleinhuepf/napari-mouse-controls\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-01-01T16:28:08.331706Z\",\"report_issues\":\"https://github.com/haesleinhuepf/napari-mouse-control/issues\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"napari\",\"napari-tools-menu\"],\"summary\":\"Control napari using a touch screen\",\"support\":\"https://github.com/haesleinhuepf/napari-mouse-controls/issues\",\"twitter\":\"\",\"version\":\"0.1.3\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Reka Hollandi\",\"orcid\":\"0000-0002-4052-6846\"}],\"category\":{\"Image modality\":[\"Fluorescence microscopy\",\"Confocal microscopy\",\"Electron microscopy\",\"Medical imaging\"],\"Supported data\":[\"2D\",\"Multi-channel\"],\"Workflow step\":[\"Image annotation\",\"Image Segmentation\",\"Object classification\"]},\"category_hierarchy\":{\"Image modality\":[[\"Fluorescence microscopy\"],[\"Confocal microscopy\"],[\"Electron microscopy\"],[\"Medical imaging\"]],\"Supported data\":[[\"2D\"],[\"Multi-channel\"]],\"Workflow step\":[[\"Image annotation\"],[\"Image Segmentation\"],[\"Image Segmentation\",\"Cell segmentation\"],[\"Image Segmentation\",\"Model-based segmentation\"],[\"Object classification\"]]},\"code_repository\":\"https://github.com/spreka/napari-annotatorj\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-annotatorj\"}],\"description\":\"\\n\\n<!-- This file is designed to provide you with a starting template for documenting\\nthe functionality of your plugin. Its content will be rendered on your plugin's\\nnapari hub page.\\n\\nThe sections below are given as a guide for the flow of information only, and\\nare in no way prescriptive. You should feel free to merge, remove, add and \\nrename sections at will to make this document work best for your plugin. \\n\\n# Description\\n\\nThis should be a detailed description of the context of your plugin and its \\nintended purpose.\\n\\nIf you have videos or screenshots of your plugin in action, you should include them\\nhere as well, to make them front and center for new users. \\n\\nYou should use absolute links to these assets, so that we can easily display them \\non the hub. The easiest way to include a video is to use a GIF, for example hosted\\non imgur. You can then reference this GIF as an image.\\n\\n![Example GIF hosted on Imgur](https://i.imgur.com/A5phCX4.gif)\\n\\nNote that GIFs larger than 5MB won't be rendered by GitHub - we will however,\\nrender them on the napari hub.\\n\\nThe other alternative, if you prefer to keep a video, is to use GitHub's video\\nembedding feature.\\n\\n1. Push your `DESCRIPTION.md` to GitHub on your repository (this can also be done\\nas part of a Pull Request)\\n2. Edit `.napari/DESCRIPTION.md` **on GitHub**.\\n3. Drag and drop your video into its desired location. It will be uploaded and\\nhosted on GitHub for you, but will not be placed in your repository.\\n4. We will take the resolved link to the video and render it on the hub.\\n\\nHere is an example of an mp4 video embedded this way.\\n\\nhttps://user-images.githubusercontent.com/17995243/120088305-6c093380-c132-11eb-822d-620e81eb5f0e.mp4\\n\\n# Intended Audience & Supported Data\\n\\nThis section should describe the target audience for this plugin (any knowledge,\\nskills and experience required), as well as a description of the types of data\\nsupported by this plugin.\\n\\nTry to make the data description as explicit as possible, so that users know the\\nformat your plugin expects. This applies both to reader plugins reading file formats\\nand to function/dock widget plugins accepting layers and/or layer data.\\nFor example, if you know your plugin only works with 3D integer data in \\\"tyx\\\" order,\\nmake sure to mention this.\\n\\nIf you know of researchers, groups or labs using your plugin, or if it has been cited\\nanywhere, feel free to also include this information here.\\n\\n# Quickstart\\n\\nThis section should go through step-by-step examples of how your plugin should be used.\\nWhere your plugin provides multiple dock widgets or functions, you should split these\\nout into separate subsections for easy browsing. Include screenshots and videos\\nwherever possible to elucidate your descriptions. \\n\\nIdeally, this section should start with minimal examples for those who just want a\\nquick overview of the plugin's functionality, but you should definitely link out to\\nmore complex and in-depth tutorials highlighting any intricacies of your plugin, and\\nmore detailed documentation if you have it.\\n\\n# Additional Install Steps (uncommon)\\nWe will be providing installation instructions on the hub, which will be sufficient\\nfor the majority of plugins. They will include instructions to pip install, and\\nto install via napari itself.\\n\\nMost plugins can be installed out-of-the-box by just specifying the package requirements\\nover in `setup.cfg`. However, if your plugin has any more complex dependencies, or \\nrequires any additional preparation before (or after) installation, you should add \\nthis information here.\\n\\n# Getting Help\\n\\nThis section should point users to your preferred support tools, whether this be raising\\nan issue on GitHub, asking a question on image.sc, or using some other method of contact.\\nIf you distinguish between usage support and bug/feature support, you should state that\\nhere.\\n\\n# How to Cite\\n\\nMany plugins may be used in the course of published (or publishable) research, as well as\\nduring conference talks and other public facing events. If you'd like to be cited in\\na particular format, or have a DOI you'd like used, you should provide that information here. -->\\n\\n# Description\\nThis plugin allows easy object annotation on 2D images. Annotation is made quick, easy and fun, just start drawing! See a [quick start](#quick-start) guide below.\\n\\n![image](https://drive.google.com/uc?export=view&id=1fVfvanffTdrXvLE0m1Yo6FV5TAjh6sb2)\\n\\nIt is the napari version of the ImageJ plugin [AnnotatorJ](https://github.com/spreka/annotatorj).\\n\\n## What kind of data it works on\\n**2D images**. That's the only requirement. Whether you have microscopy images of cells or tissue, natural photos of cats and dogs, vehichle dash-cam footage, industrial pipeline monitoring etc., just open the image and you can start annotating.\\n\\nAnnotations are save to ImageJ-compatible roi.zip files. [Export](#export) is possible to several file formats depending on the intended application.\\n\\n## Intended users\\n**Anyone**. No experience in computer science or underlying technology is needed; if you know how to use MS Paint, you are ready to start annotating. Biologists, programmers, even children can use it. See [quick start](#quick-start) guide or [demos](#demo). If you experience any issues or have questions, feel free to open an [issue](https://github.com/spreka/napari-annotatorj/issues) on GitHub.\\n\\n## Main features\\n\\nWhy choose napari-annotatorj?\\n- Assisted annotation is possible with automatic deep learning-based [contour suggestion](#contour-assist-mode),\\n- freehand contour drawing in [instance annotation](#instance-annotation),\\n- shape [editing](#edit-mode) via painting labels,\\n- [class annotation](#class-mode),\\n- [export](#export) to formats directly suitable for deep CNN training\\n- import of previous annotations as [overlay](#overlay); e.g. when comparing annotations or curating\\n- and more.\\n\\nSee [demos](#demo).\\n\\n## Quick start\\nDemo data is available in the GitHub repository's [demo](https://github.com/spreka/napari-annotatorj/tree/main/demo) folder.\\n\\nnapari-annotatorj has several convenient functions to speed up the annotation process, make it easier and more fun. These *modes* can be activated by their corresponding checkboxes on the left side of the main AnnotatorJ widget.\\n\\n- [Contour assist mode](#contour-assist-mode)\\n- [Edit mode](#edit-mode)\\n- [Class mode](#class-mode)\\n- [Overlay](#overlay)\\n\\nFreehand drawing is enabled in the plugin. The \\\"Add polygon\\\" tool is selected by default upon startup. To draw a freehand object (shape) simply hold the mouse and drag it around the object. The contour is visualized when the mouse button is released.\\n\\nSee the [guide](#how-to-annotate) below or a [demo](#demo-scripts) script.\\n\\n## Instance annotation\\nAllows freehand drawing of object contours (shapes) with the mouse as in ImageJ.\\n\\nShape contour points are tracked automatically when the left mouse button is held and dragged to draw a shape. The shape is closed when the mouse button is released, automatically, and added to the default shapes layer (named \\\"ROI\\\"). In direct selection mode (from the layer controls panel), you can see the saved contour points. The slower you drag the mouse, the more contour points saved, i.e. the more refined your contour will be.\\n\\nClick to watch demo video below.\\n\\n[![instance-annot-demo](https://drive.google.com/uc?export=view&id=1sBg19d_hqGH-UI8irkrwame7ZjrldwHr)](https://drive.google.com/uc?export=view&id=1wELreE9MdCZq4Kf4oCWdxIw4e5o05XzK \\\"Click to watch instance annotation demo\\\")\\n\\n## How to annotate\\n\\n1. Open --> opens an image\\n2. (Optionally) \\n\\t- ... --> Select annotation type --> Ok --> a default tool is selected from the toolbar that fits the selected annotation type\\n\\t- The default annotation type is instance\\n\\t- Selected annotation type is saved to a config file\\n3. Start annotating objects\\n\\t- [instance](#instance-annotation): draw contours around objects\\n\\t- [semantic](#semantic-annotation): paint the objects' area\\n\\t- [bounding box](#bounding-box-annotation): draw rectangles around the objects\\n4. Save --> Select class --> saves the annotation to a file in a sub-folder of the original image folder with the name of the selected class\\n\\n5. (Optionally)\\n\\t- Load --> continue a previous annotation\\n\\t- Overlay --> display a different annotation as overlay (semi-transparent) on the currently opened image\\n\\t- Colours --> select annotation and overlay colours\\n\\t- ... (coming soon) --> set options for semantic segmentation and *Contour assist* mode\\n\\t- checkboxes --> Various options\\n\\t\\t- (default) Add automatically --> adds the most recent annotation to the ROI list automatically when releasing the left mouse button\\n\\t\\t- Smooth (coming soon) --> smooths the contour (in instance annotation type only)\\n\\t\\t- Show contours --> displays all the contours in the ROI list\\n\\t\\t- Contours assist --> suggests a contour in the region of an initial, lazily drawn contour using the deep learning method U-Net\\n\\t\\t- Show overlay --> displays the overlayed annotation if loaded with the Overlay button\\n\\t\\t- Edit mode --> edits a selected, already saved contour in the ROI list by clicking on it on the image\\n\\t\\t- Class mode --> assigns the selected class to the selected contour in the ROI list by clicking on it on the image and displays its contour in the class's colour (can be set in the Class window); clicking on the object a second time unclassifies it\\n\\t- [^] --> quick export in 16-bit multi-labelled .tiff format; if classified, also exports by classes\\n\\n\\n## Semantic annotation\\nAllows painting with the brush tool (labels).\\n\\nUseful for semantic (e.g. scene) annotation. Currently saves all labels to binary mask only (foreground-background).\\n\\n## Bounding box annotation\\nAllows drawing bounding boxes (shapes, rectangles) around objects with the mouse.\\n\\nUseful for object detection annotation.\\n\\n\\n## Demo\\n## Instance annotation mode\\nSee [above](#instance-annotation).\\n\\n## Contour assist mode\\nAssisted annotation via a pre-trained deep learning model's suggested contour.\\n\\n1. initialize a contour with mouse drag around an object\\n2. the suggested contour is displayed automatically\\n3. modify the contour:\\n    - edit with mouse drag or \\n    - erase holding \\\"Alt\\\" or\\n\\t- invert with pressing \\\"u\\\"\\n4. finalize it\\n    - accept with pressing \\\"q\\\" or\\n    - reject with pressing \\\"Ctrl\\\" + \\\"Del\\\"\\n\\n- if the suggested contour is a merge of multiple objects, you can erase the dividing line around the object you wish to keep, and keep erasing (or splitting with the eraser) until the object you wish to keep is the largest, then press \\\"q\\\" to accept it\\n- this mode requires a Keras model to be present in the [model folder](#configure-model-folder)\\n\\nClick to watch demo video below\\n\\n[![contour-assist-demo](https://drive.google.com/uc?export=view&id=1Mw2fCPdm5WHBVRgNnp8fGNmqxI84F_9I)](https://drive.google.com/uc?export=view&id=1VTd6RScjNfAwi3vMk-bU87U4ucPmOO_M \\\"Click to watch contour assist demo\\\")\\n\\n\\n## Edit mode\\nAllows to modify created objects with a brush tool.\\n\\n1. select an object (shape) to modify by clicking on it\\n2. an editing layer (labels layer) is created for painting automatically\\n3. modify the contour:\\n    - edit with mouse drag or \\n    - erase holding \\\"Alt\\\"\\n4. finalize it\\n    - accept with pressing \\\"q\\\" or\\n    - delete with pressing \\\"Ctrl\\\" + \\\"Del\\\" or\\n    - revert changes with pressing \\\"Esc\\\" (to the state before editing)\\n\\n- if the edited contour is a merge of multiple objects, you can erase the dividing line around the object you wish to keep, and keep erasing (or splitting with the eraser) until the object you wish to keep is the largest, then press \\\"q\\\" to accept it\\n\\nClick to watch demo video below\\n\\n[![edit-mode-demo](https://drive.google.com/uc?export=view&id=1M-XdEWPXMsIOtO0ncyUtvGACS0SRX-3K)](https://drive.google.com/uc?export=view&id=10MQm53hblLKQlfBNrfUsi1vxvIdTbzCZ \\\"Click to watch edit mode demo\\\")\\n\\n\\n## Class mode\\nAllows to assign class labels to objects by clicking on shapes.\\n\\n1. select a class from the class list to assign\\n2. click on an object (shape) to assign the selected class label to it\\n3. the contour colour of the clicked object will be updated to the selected class colour, plus the class label is updated in the text properties of object (turn on \\\"display text\\\" on the layer control panel to see the text properties as `objectID:(classLabel)` e.g. 1:(0) for the first object)\\n\\n- optionally, you can set a default class for all currently unlabelled objects on the ROI (shapes) layer by selecting a class from the drop-down menu on the right to the text label \\\"Default class\\\"\\n- class colours can be changed with the drop-down menu right to the class list; upon selection, all objects whose class label is the currently selected class will have their contour colour updated to the selected colour\\n- clicking on an object that has already been assigned a class label will unclassify it: assign the label *0* to it\\n\\nClick to watch demo video below\\n\\n[![class-mode-demo](https://drive.google.com/uc?export=view&id=1EV1cn_mySO11S_ZDFv6Dl1laAk30jGJk)](https://drive.google.com/uc?export=view&id=1uOmznUvfHEFvviWTtOnUHty8rkKyWR7Q \\\"Click to watch class mode demo\\\")\\n\\n\\n## Export\\nSee also: [Quick export](#quick-export)\\n\\nThe exporter plugin AnnotatorJExport can be invoked from the Plugins menu under the plugin name `napari-annotatorj`. It is used for batch export of annotations to various formats directly suitable to train different types of deep learning models. See a [demonstrative figure](https://raw.githubusercontent.com/spreka/annotatorj/master/demos/annotation_and_export_types.png) in the [AnnotatorJ repository](https://github.com/spreka/annotatorj) and further description in its [README](https://github.com/spreka/annotatorj#export) or [documentation](https://github.com/spreka/annotatorj/blob/master/AnnotatorJ_documentation.pdf).\\n\\n1. browse original image folder with either the\\n    - \\\"Browse original...\\\" button or\\n    - text input field next to it\\n2. browse annotation folder with either the\\n    - \\\"Browse annot...\\\" button or\\n    - text input field next to it\\n3. select the export options you wish to export the annotations to (see tooltips on hover for help)\\n    - at least one export option must be selected to start export\\n    - (optional) right click on the checkbox \\\"Coordinates\\\" to switch between the default COCO format and YOLO format; see [explanation](#coordinate-formats)\\n4. click on \\\"Export masks\\\" to start the export\\n    - this will open a progress bar in the napari window and close it upon finish\\n\\nThe folder structure required by the exporter is as follows:\\n\\n```\\nimage_folder\\n\\t|--- image1.png\\n\\t|--- another_image.png\\n\\t|--- something.png\\n\\t|--- ...\\n\\nannotation_folder\\n\\t|--- image1_ROIs.zip\\n\\t|--- another_image_ROIs.zip\\n\\t|--- something_ROIs.zip\\n\\t|--- ...\\n```\\n\\nMultiple export options can be selected at once, any selected will create a subfolder in the folder where the annotations are saved.\\n\\nClick to watch demo video below\\n\\n[![exporter-demo](https://drive.google.com/uc?export=view&id=1QoaJrI9pKziUzYwiZNdWlfRD7PcvJB9U)](https://drive.google.com/uc?export=view&id=1uJz-x_ypEOjc7SYPUTqrEt0ieyNLFy6u \\\"Click to watch exporter demo\\\")\\n\\n\\n## Quick export\\nClick on the \\\"[^]\\\" button to quickly save annotations and export to mask image. It saves the current annotations (shapes) to an ImageJ-compatible roi.zip file and a generated a 16-bit multi-labelled mask image to the subfolder \\\"masks\\\" under the current original image's folder.\\n\\n\\n## Coordinate formats\\nIn the AnnotatorJExport plugin 2 coordinates formats can be selecting by right clicking on the Coordinates checkbox: COCO or YOLO. The default is COCO.\\n\\n*COCO format*:\\n- `[x, y, width, height]` based on the top-left corner of the bounding box around the object\\n- coordinates are not normalized\\n- annotations are saved with header to \\n    - .csv file\\n    - tab delimeted\\n\\n*YOLO format*:\\n- `[class, x, y, width, height]` based on the center point of the bounding box around the object\\n- coordinates are normalized to the image size as floating point values between 0 and 1\\n- annotations are saved with header to\\n    - .txt file\\n    - whitespace delimeted\\n    - class is saved as the 1st column\\n\\n\\n## Overlay\\nA separate annotation file can be loaded as overlay for convenience, e.g. to compare annotations.\\n\\n1. load another annotation file with the \\\"Overlay\\\" button\\n\\n- (optional) switch its visibility with the \\\"Show overlay\\\" checkbox\\n- (optional) change the contour colour of the overlay shapes with the [\\\"Colours\\\" button](#change-colours)\\n\\n\\n## Change colours\\nClicking on the \\\"Colours\\\" button opens the Colours widget where you can set the annotation and overlay colours.\\n\\n1. select a colour from the drop-down list either next to the text label \\\"overlay\\\" or \\\"annotation\\\"\\n2. click the \\\"Ok\\\" button to apply changes\\n\\n- contour colour of shapes on the annotation shapes layer (named \\\"ROI\\\") that already have a class label assigned to them will **not** be updated to the new annotation colour, only those not having a class label (the class label can be displayed with the \\\"display text\\\" checkbox on the layer controls panel as `objectID:(classLabel)` e.g. 1:(0) for the first object)\\n- contour colour of shapes on the overlay shapes layer (named \\\"overlay\\\") will all have the overlay colour set, regardless of any existing class information saved to the annotation file loaded as overlay\\n\\n\\n# For coding users\\n## Demo scripts\\nRun a demo of napari-annotatorj with sample data: a small 3-channel RGB image as original image and an ImageJ roi.zip file as annotations loaded.\\n\\n```shell\\n    # from the napari-annotatorj folder\\n\\tpython src/napari_annotatorj/load_imagej_roi.py\\n```\\nAlternatively, you can startup the napari-annotatorj plugin by running\\n\\n```shell\\n    # from the napari-annotatorj folder\\n\\tpython src/napari_annotatorj/startup_annotatorj.py\\n```\\n\\n## Configure model folder\\nThe Contour assist mode imports a pre-trained Keras model from a folder named *models* under exactly the path *napari_annotatorj*. This is automatically created on the first startup in your user folder:\\n- `C:\\\\Users\\\\Username\\\\.napari_annotatorj` on Windows\\n- `\\\\home\\\\username\\\\.napari_annotatorj` on Linux\\n\\nA pre-trained model for nucleus segmentation is automatically downloaded from the GitHub repository of the [ImageJ version of AnnotatorJ](https://github.com/spreka/annotatorj/releases/tag/v0.0.2-model). The model will be saved to `[your user folder]\\\\.napari_annotatorj\\\\models\\\\model_real.h5`. This location is printed to the console (command prompt or powershell on Windows, terminal on Linux).\\n\\n(deprecated) When bulding from source the model folder is located at *path\\\\to\\\\napari-annotatorj\\\\src\\\\napari_annotatorj\\\\models* whereas installing from pypi it is located at *path\\\\to\\\\virtualenv\\\\Lib\\\\site-packages\\\\napari_annotatorj\\\\models*.\\n\\nThe model must be in either of these file formats:\\n- config .json file + weights file: *model_real.json* and *model_real_weights.h5*\\n- combined weights file: *model_real.hdf5*\\n\\nYou can also train a new model on your own data in e.g. Python and save it with this code block:\\n\\n```python\\n\\t# save model as json\\n\\tmodel_json=model.to_json()\\n\\twith open(‘model_real.json’, ‘w’) as f:\\n\\t\\tf.write(model_json)\\n\\t\\n\\t# save weights too\\n\\tmodel.save_weights(‘model_real_weights.h5’)\\n\\n```\\nThis configuration will change in the next release to allow model browse and custom model name in an options widget.\\n\\n## Setting device for deep learning model prediction\\nThe [Contour assist](#contour-assist-mode) mode uses a pre-trained U-Net model for suggesting contours based on a lazily initialized contour drawn by the user. The default configuration loads and runs the model on the CPU so all users can run it. It is possible to switch to GPU if you have:\\n- a CUDA-capable GPU in your computer\\n- nVidia's CUDA toolkit + cuDNN installed\\n\\nSee installation guide on [nVidia's website](https://developer.nvidia.com/cuda-downloads) according to your system.\\n\\nTo switch to GPU utilization, edit [_dock_widget.py](https://github.com/spreka/napari-annotatorj/blob/main/src/napari_annotatorj/_dock_widget.py#L112) and set to the device you would like to use. Valid values are `'cpu','0','1','2',...`. The default value is `cpu`. The default GPU device is `0` if your system has any CUDA-capable GPU. If the device you set cannot be found or utilized by the code, it will fall back to `cpu`. An informative message is printed to the console upon plugin startup.\\n\\n## License\\nDistributed under the terms of the [BSD-3](https://opensource.org/licenses/BSD-3-Clause) license,\\n\\\"napari-annotatorj\\\" is free and open source software.\\n\\n## Getting help\\nIf you experience any issues or have questions, feel free to open an [issue](https://github.com/spreka/napari-annotatorj/issues) on GitHub.\\n\\n## How to cite\\nRéka Hollandi, Ákos Diósdi, Gábor Hollandi, Nikita Moshkov, Péter Horváth (2020): “AnnotatorJ: an ImageJ plugin to ease hand-annotation of cellular compartments”, Molecular Biology of the Cell, Vol. 31, No. 20, 2179-2186, https://doi.org/10.1091/mbc.E20-02-0156\",\"description_content_type\":\"text/markdown\",\"description_text\":\"\\nDescription\\nThis plugin allows easy object annotation on 2D images. Annotation is made quick, easy and fun, just start drawing! See a quick start guide below.\\n\\nIt is the napari version of the ImageJ plugin AnnotatorJ.\\nWhat kind of data it works on\\n2D images. That's the only requirement. Whether you have microscopy images of cells or tissue, natural photos of cats and dogs, vehichle dash-cam footage, industrial pipeline monitoring etc., just open the image and you can start annotating.\\nAnnotations are save to ImageJ-compatible roi.zip files. Export is possible to several file formats depending on the intended application.\\nIntended users\\nAnyone. No experience in computer science or underlying technology is needed; if you know how to use MS Paint, you are ready to start annotating. Biologists, programmers, even children can use it. See quick start guide or demos. If you experience any issues or have questions, feel free to open an issue on GitHub.\\nMain features\\nWhy choose napari-annotatorj?\\n- Assisted annotation is possible with automatic deep learning-based contour suggestion,\\n- freehand contour drawing in instance annotation,\\n- shape editing via painting labels,\\n- class annotation,\\n- export to formats directly suitable for deep CNN training\\n- import of previous annotations as overlay; e.g. when comparing annotations or curating\\n- and more.\\nSee demos.\\nQuick start\\nDemo data is available in the GitHub repository's demo folder.\\nnapari-annotatorj has several convenient functions to speed up the annotation process, make it easier and more fun. These modes can be activated by their corresponding checkboxes on the left side of the main AnnotatorJ widget.\\n\\nContour assist mode\\nEdit mode\\nClass mode\\nOverlay\\n\\nFreehand drawing is enabled in the plugin. The \\\"Add polygon\\\" tool is selected by default upon startup. To draw a freehand object (shape) simply hold the mouse and drag it around the object. The contour is visualized when the mouse button is released.\\nSee the guide below or a demo script.\\nInstance annotation\\nAllows freehand drawing of object contours (shapes) with the mouse as in ImageJ.\\nShape contour points are tracked automatically when the left mouse button is held and dragged to draw a shape. The shape is closed when the mouse button is released, automatically, and added to the default shapes layer (named \\\"ROI\\\"). In direct selection mode (from the layer controls panel), you can see the saved contour points. The slower you drag the mouse, the more contour points saved, i.e. the more refined your contour will be.\\nClick to watch demo video below.\\n\\nHow to annotate\\n\\nOpen --> opens an image\\n(Optionally) \\n... --> Select annotation type --> Ok --> a default tool is selected from the toolbar that fits the selected annotation type\\nThe default annotation type is instance\\nSelected annotation type is saved to a config file\\n\\n\\nStart annotating objects\\ninstance: draw contours around objects\\nsemantic: paint the objects' area\\nbounding box: draw rectangles around the objects\\n\\n\\n\\nSave --> Select class --> saves the annotation to a file in a sub-folder of the original image folder with the name of the selected class\\n\\n\\n(Optionally)\\n\\nLoad --> continue a previous annotation\\nOverlay --> display a different annotation as overlay (semi-transparent) on the currently opened image\\nColours --> select annotation and overlay colours\\n... (coming soon) --> set options for semantic segmentation and Contour assist mode\\ncheckboxes --> Various options\\n(default) Add automatically --> adds the most recent annotation to the ROI list automatically when releasing the left mouse button\\nSmooth (coming soon) --> smooths the contour (in instance annotation type only)\\nShow contours --> displays all the contours in the ROI list\\nContours assist --> suggests a contour in the region of an initial, lazily drawn contour using the deep learning method U-Net\\nShow overlay --> displays the overlayed annotation if loaded with the Overlay button\\nEdit mode --> edits a selected, already saved contour in the ROI list by clicking on it on the image\\nClass mode --> assigns the selected class to the selected contour in the ROI list by clicking on it on the image and displays its contour in the class's colour (can be set in the Class window); clicking on the object a second time unclassifies it\\n\\n\\n[^] --> quick export in 16-bit multi-labelled .tiff format; if classified, also exports by classes\\n\\n\\n\\nSemantic annotation\\nAllows painting with the brush tool (labels).\\nUseful for semantic (e.g. scene) annotation. Currently saves all labels to binary mask only (foreground-background).\\nBounding box annotation\\nAllows drawing bounding boxes (shapes, rectangles) around objects with the mouse.\\nUseful for object detection annotation.\\nDemo\\nInstance annotation mode\\nSee above.\\nContour assist mode\\nAssisted annotation via a pre-trained deep learning model's suggested contour.\\n\\ninitialize a contour with mouse drag around an object\\nthe suggested contour is displayed automatically\\nmodify the contour:\\nedit with mouse drag or \\nerase holding \\\"Alt\\\" or\\ninvert with pressing \\\"u\\\"\\n\\n\\n\\nfinalize it\\n\\naccept with pressing \\\"q\\\" or\\nreject with pressing \\\"Ctrl\\\" + \\\"Del\\\"\\n\\n\\n\\nif the suggested contour is a merge of multiple objects, you can erase the dividing line around the object you wish to keep, and keep erasing (or splitting with the eraser) until the object you wish to keep is the largest, then press \\\"q\\\" to accept it\\n\\nthis mode requires a Keras model to be present in the model folder\\n\\nClick to watch demo video below\\n\\nEdit mode\\nAllows to modify created objects with a brush tool.\\n\\nselect an object (shape) to modify by clicking on it\\nan editing layer (labels layer) is created for painting automatically\\nmodify the contour:\\nedit with mouse drag or \\nerase holding \\\"Alt\\\"\\n\\n\\n\\nfinalize it\\n\\naccept with pressing \\\"q\\\" or\\ndelete with pressing \\\"Ctrl\\\" + \\\"Del\\\" or\\nrevert changes with pressing \\\"Esc\\\" (to the state before editing)\\n\\n\\n\\nif the edited contour is a merge of multiple objects, you can erase the dividing line around the object you wish to keep, and keep erasing (or splitting with the eraser) until the object you wish to keep is the largest, then press \\\"q\\\" to accept it\\n\\n\\nClick to watch demo video below\\n\\nClass mode\\nAllows to assign class labels to objects by clicking on shapes.\\n\\nselect a class from the class list to assign\\nclick on an object (shape) to assign the selected class label to it\\n\\nthe contour colour of the clicked object will be updated to the selected class colour, plus the class label is updated in the text properties of object (turn on \\\"display text\\\" on the layer control panel to see the text properties as objectID:(classLabel) e.g. 1:(0) for the first object)\\n\\n\\noptionally, you can set a default class for all currently unlabelled objects on the ROI (shapes) layer by selecting a class from the drop-down menu on the right to the text label \\\"Default class\\\"\\n\\nclass colours can be changed with the drop-down menu right to the class list; upon selection, all objects whose class label is the currently selected class will have their contour colour updated to the selected colour\\nclicking on an object that has already been assigned a class label will unclassify it: assign the label 0 to it\\n\\nClick to watch demo video below\\n\\nExport\\nSee also: Quick export\\nThe exporter plugin AnnotatorJExport can be invoked from the Plugins menu under the plugin name napari-annotatorj. It is used for batch export of annotations to various formats directly suitable to train different types of deep learning models. See a demonstrative figure in the AnnotatorJ repository and further description in its README or documentation.\\n\\nbrowse original image folder with either the\\n\\\"Browse original...\\\" button or\\ntext input field next to it\\n\\n\\nbrowse annotation folder with either the\\n\\\"Browse annot...\\\" button or\\ntext input field next to it\\n\\n\\nselect the export options you wish to export the annotations to (see tooltips on hover for help)\\nat least one export option must be selected to start export\\n(optional) right click on the checkbox \\\"Coordinates\\\" to switch between the default COCO format and YOLO format; see explanation\\n\\n\\nclick on \\\"Export masks\\\" to start the export\\nthis will open a progress bar in the napari window and close it upon finish\\n\\n\\n\\nThe folder structure required by the exporter is as follows:\\n```\\nimage_folder\\n    |--- image1.png\\n    |--- another_image.png\\n    |--- something.png\\n    |--- ...\\nannotation_folder\\n    |--- image1_ROIs.zip\\n    |--- another_image_ROIs.zip\\n    |--- something_ROIs.zip\\n    |--- ...\\n```\\nMultiple export options can be selected at once, any selected will create a subfolder in the folder where the annotations are saved.\\nClick to watch demo video below\\n\\nQuick export\\nClick on the \\\"[^]\\\" button to quickly save annotations and export to mask image. It saves the current annotations (shapes) to an ImageJ-compatible roi.zip file and a generated a 16-bit multi-labelled mask image to the subfolder \\\"masks\\\" under the current original image's folder.\\nCoordinate formats\\nIn the AnnotatorJExport plugin 2 coordinates formats can be selecting by right clicking on the Coordinates checkbox: COCO or YOLO. The default is COCO.\\nCOCO format:\\n- [x, y, width, height] based on the top-left corner of the bounding box around the object\\n- coordinates are not normalized\\n- annotations are saved with header to \\n    - .csv file\\n    - tab delimeted\\nYOLO format:\\n- [class, x, y, width, height] based on the center point of the bounding box around the object\\n- coordinates are normalized to the image size as floating point values between 0 and 1\\n- annotations are saved with header to\\n    - .txt file\\n    - whitespace delimeted\\n    - class is saved as the 1st column\\nOverlay\\nA separate annotation file can be loaded as overlay for convenience, e.g. to compare annotations.\\n\\n\\nload another annotation file with the \\\"Overlay\\\" button\\n\\n\\n(optional) switch its visibility with the \\\"Show overlay\\\" checkbox\\n\\n(optional) change the contour colour of the overlay shapes with the \\\"Colours\\\" button\\n\\nChange colours\\nClicking on the \\\"Colours\\\" button opens the Colours widget where you can set the annotation and overlay colours.\\n\\nselect a colour from the drop-down list either next to the text label \\\"overlay\\\" or \\\"annotation\\\"\\n\\nclick the \\\"Ok\\\" button to apply changes\\n\\n\\ncontour colour of shapes on the annotation shapes layer (named \\\"ROI\\\") that already have a class label assigned to them will not be updated to the new annotation colour, only those not having a class label (the class label can be displayed with the \\\"display text\\\" checkbox on the layer controls panel as objectID:(classLabel) e.g. 1:(0) for the first object)\\n\\ncontour colour of shapes on the overlay shapes layer (named \\\"overlay\\\") will all have the overlay colour set, regardless of any existing class information saved to the annotation file loaded as overlay\\n\\nFor coding users\\nDemo scripts\\nRun a demo of napari-annotatorj with sample data: a small 3-channel RGB image as original image and an ImageJ roi.zip file as annotations loaded.\\nshell\\n    # from the napari-annotatorj folder\\n    python src/napari_annotatorj/load_imagej_roi.py\\nAlternatively, you can startup the napari-annotatorj plugin by running\\nshell\\n    # from the napari-annotatorj folder\\n    python src/napari_annotatorj/startup_annotatorj.py\\nConfigure model folder\\nThe Contour assist mode imports a pre-trained Keras model from a folder named models under exactly the path napari_annotatorj. This is automatically created on the first startup in your user folder:\\n- C:\\\\Users\\\\Username\\\\.napari_annotatorj on Windows\\n- \\\\home\\\\username\\\\.napari_annotatorj on Linux\\nA pre-trained model for nucleus segmentation is automatically downloaded from the GitHub repository of the ImageJ version of AnnotatorJ. The model will be saved to [your user folder]\\\\.napari_annotatorj\\\\models\\\\model_real.h5. This location is printed to the console (command prompt or powershell on Windows, terminal on Linux).\\n(deprecated) When bulding from source the model folder is located at path\\\\to\\\\napari-annotatorj\\\\src\\\\napari_annotatorj\\\\models whereas installing from pypi it is located at path\\\\to\\\\virtualenv\\\\Lib\\\\site-packages\\\\napari_annotatorj\\\\models.\\nThe model must be in either of these file formats:\\n- config .json file + weights file: model_real.json and model_real_weights.h5\\n- combined weights file: model_real.hdf5\\nYou can also train a new model on your own data in e.g. Python and save it with this code block:\\n```python\\n    # save model as json\\n    model_json=model.to_json()\\n    with open(‘model_real.json’, ‘w’) as f:\\n        f.write(model_json)\\n# save weights too\\nmodel.save_weights(‘model_real_weights.h5’)\\n\\n```\\nThis configuration will change in the next release to allow model browse and custom model name in an options widget.\\nSetting device for deep learning model prediction\\nThe Contour assist mode uses a pre-trained U-Net model for suggesting contours based on a lazily initialized contour drawn by the user. The default configuration loads and runs the model on the CPU so all users can run it. It is possible to switch to GPU if you have:\\n- a CUDA-capable GPU in your computer\\n- nVidia's CUDA toolkit + cuDNN installed\\nSee installation guide on nVidia's website according to your system.\\nTo switch to GPU utilization, edit _dock_widget.py and set to the device you would like to use. Valid values are 'cpu','0','1','2',.... The default value is cpu. The default GPU device is 0 if your system has any CUDA-capable GPU. If the device you set cannot be found or utilized by the code, it will fall back to cpu. An informative message is printed to the console upon plugin startup.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-annotatorj\\\" is free and open source software.\\nGetting help\\nIf you experience any issues or have questions, feel free to open an issue on GitHub.\\nHow to cite\\nRéka Hollandi, Ákos Diósdi, Gábor Hollandi, Nikita Moshkov, Péter Horváth (2020): “AnnotatorJ: an ImageJ plugin to ease hand-annotation of cellular compartments”, Molecular Biology of the Cell, Vol. 31, No. 20, 2179-2186, https://doi.org/10.1091/mbc.E20-02-0156\",\"development_status\":[\"Development Status :: 3 - Alpha\"],\"display_name\":\"napari-annotatorj\",\"documentation\":\"https://github.com/spreka/napari-annotatorj#README.md\",\"first_released\":\"2022-05-26T12:19:13.088484Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-annotatorj\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\",\"writer\",\"widget\"],\"project_site\":\"https://github.com/spreka/napari-annotatorj\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[\"<EDIT_ME>\"],\"release_date\":\"2022-07-21T13:13:37.100077Z\",\"report_issues\":\"https://github.com/spreka/napari-annotatorj/issues\",\"requirements\":[\"napari\",\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"roifile\",\"scikit-image\",\"opencv-python (>=4.5.5)\",\"keras\",\"tensorflow (>=2.5.0)\",\"tifffile\",\"imagecodecs\"],\"summary\":\"The napari adaptation of the ImageJ/Fiji plugin AnnotatorJ for easy and fun image annotation.\",\"support\":\"https://github.com/spreka/napari-annotatorj/issues\",\"twitter\":\"\",\"version\":\"0.0.7\",\"visibility\":\"public\",\"writer_file_extensions\":[\".tiff\"],\"writer_save_layers\":[\"labels\"]}",
  "{\"authors\":[{\"email\":\"susmi06@yahoo.com\",\"name\":\"SUSMITA SAHA\"}],\"code_repository\":null,\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-mri\"}],\"description\":\"# napari-mri\\n\\n[![License](https://img.shields.io/pypi/l/napari-mri.svg?color=green)](https://github.com/sahas111/napari-mri/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-mri.svg?color=green)](https://pypi.org/project/napari-mri)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-mri.svg?color=green)](https://python.org)\\n[![tests](https://github.com/sahas111/napari-mri/workflows/tests/badge.svg)](https://github.com/sahas111/napari-mri/actions)\\n[![codecov](https://codecov.io/gh/sahas111/napari-mri/branch/master/graph/badge.svg)](https://codecov.io/gh/sahas111/napari-mri)\\n\\nA simple plugin to use with napari for 3D-viewing of Magnetic Resonance Imaging file formats\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-mri` via [pip]:\\n\\n    pip install napari-mri\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-mri\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/sahas111/napari-mri/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-mri\\n\\n\\n\\n\\n\\nA simple plugin to use with napari for 3D-viewing of Magnetic Resonance Imaging file formats\\n\\nThis napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-mri via pip:\\npip install napari-mri\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-mri\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"\",\"documentation\":\"\",\"first_released\":\"2021-03-21T06:12:30.232144Z\",\"license\":\"\",\"name\":\"napari-mri\",\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[],\"project_site\":\"\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2021-03-21T06:12:30.232144Z\",\"report_issues\":\"\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"nibabel\",\"numpy\",\"pytest-runner (>=5.2) ; extra == 'setup'\",\"setuptools-scm ; extra == 'setup'\",\"codecov (>=2.1.4) ; extra == 'test'\",\"flake8 (>=3.8.3) ; extra == 'test'\",\"flake8-debugger (>=3.2.1) ; extra == 'test'\",\"pytest (>=5.4.3) ; extra == 'test'\",\"pytest-cov (>=2.9.0) ; extra == 'test'\",\"pytest-raises (>=0.11) ; extra == 'test'\"],\"summary\":\"A simple plugin to use with napari for 3D-viewing of                  Magnetic Resonance Imaging file formats\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.1.0\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"robert.haase@tu-dresden.de\",\"name\":\"Robert Haase\"}],\"code_repository\":\"https://github.com/haesleinhuepf/napari-animated-gif-io\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-animated-gif-io\"}],\"description\":\"# napari-animated-gif-io\\n\\n[![License](https://img.shields.io/pypi/l/napari-animated-gif-io.svg?color=green)](https://github.com/haesleinhuepf/napari-animated-gif-io/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-animated-gif-io.svg?color=green)](https://pypi.org/project/napari-animated-gif-io)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-animated-gif-io.svg?color=green)](https://python.org)\\n[![tests](https://github.com/haesleinhuepf/napari-animated-gif-io/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-animated-gif-io/actions)\\n[![codecov](https://codecov.io/gh/haesleinhuepf/napari-animated-gif-io/branch/main/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-animated-gif-io)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-animated-gif-io)](https://napari-hub.org/plugins/napari-animated-gif-io)\\n\\nOpen and save 3D image stacks as animated gifs\\n\\nYou find the menus for opening and saving animated gifs in the `Tools > File Import/Export` menu:\\n\\n![img.png](https://github.com/haesleinhuepf/napari-animated-gif-io/raw/main/docs/screenshot.png)\\n\\nFurthermore, if in 3D view, you can save the current view with a little tilt animation as animated gif.\\nUnder the hood this uses the [microfilm](https://github.com/guiwitz/microfilm) library.\\n\\n![img.png](https://github.com/haesleinhuepf/napari-animated-gif-io/raw/main/docs/video.gif)\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n## Installation\\n\\nYou can install `napari-animated-gif-io` via [pip]:\\n\\n    pip install napari-animated-gif-io\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/haesleinhuepf/napari-animated-gif-io.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-animated-gif-io\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/haesleinhuepf/napari-animated-gif-io/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-animated-gif-io\\n\\n\\n\\n\\n\\n\\nOpen and save 3D image stacks as animated gifs\\nYou find the menus for opening and saving animated gifs in the Tools > File Import/Export menu:\\n\\nFurthermore, if in 3D view, you can save the current view with a little tilt animation as animated gif.\\nUnder the hood this uses the microfilm library.\\n\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\nInstallation\\nYou can install napari-animated-gif-io via pip:\\npip install napari-animated-gif-io\\n\\nTo install latest development version :\\npip install git+https://github.com/haesleinhuepf/napari-animated-gif-io.git\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-animated-gif-io\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari-animated-gif-io\",\"documentation\":\"https://github.com/haesleinhuepf/napari-animated-gif-io#README.md\",\"first_released\":\"2021-12-03T21:52:38.121004Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-animated-gif-io\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"writer\",\"widget\"],\"project_site\":\"https://github.com/haesleinhuepf/napari-animated-gif-io\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-04-15T12:26:28.329312Z\",\"report_issues\":\"https://github.com/haesleinhuepf/napari-animated-gif-io/issues\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"imageio\",\"napari-tools-menu\",\"napari\",\"microfilm\"],\"summary\":\"Save 3D image stacks as animated gifs\",\"support\":\"https://github.com/haesleinhuepf/napari-animated-gif-io/issues\",\"twitter\":\"\",\"version\":\"0.1.2\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[\"image\"]}",
  "{\"authors\":[{\"email\":\"ome-team@openmicroscopy.org\",\"name\":\"OME Team\"}],\"citations\":{\"APA\":\"Moore J., Moore W., Besson S., Doncila Pop D. napari-ome-zarr DOI: 10.5281/zenodo.5620851 URL: https://github.com/ome/napari-ome-zarr\\n\",\"BibTex\":\"@misc{YourReferenceHere,\\nauthor = {Moore, Josh and Moore, Will and Besson, Sébastien and Doncila Pop, Draga},\\ndoi = {10.5281/zenodo.5620851},\\ntitle = {napari-ome-zarr},\\nurl = {https://github.com/ome/napari-ome-zarr}\\n}\\n\",\"RIS\":\"TY  - GEN\\nAU  - Moore, Josh\\nAU  - Moore, Will\\nAU  - Besson, Sébastien\\nAU  - Doncila Pop, Draga\\nDO  - 10.5281/zenodo.5620851\\nTI  - napari-ome-zarr\\nUR  - https://github.com/ome/napari-ome-zarr\\nER\\n\",\"citation\":\"# YAML 1.2\\n# Metadata for citation of this software according to the CFF format (https://citation-file-format.github.io/)\\ncff-version: 1.2.0\\nmessage: If you use this software, please cite it using these metadata.\\ntitle: napari-ome-zarr\\ndoi: 10.5281/zenodo.5620851\\nauthors:\\n- given-names: Josh\\n  family-names: Moore\\n  affiliation: '@openmicroscopy'\\n- given-names: Will\\n  family-names: Moore\\n  affiliation: '@openmicroscopy'\\n- given-names: Sébastien\\n  family-names: Besson\\n  affiliation: University of Dundee\\n- given-names: Draga\\n  family-names: Doncila Pop\\nrepository-code: https://github.com/ome/napari-ome-zarr\\nreferences:\\n  - type: article\\n    title: \\\"OME-NGFF: a next-generation file format for expanding bioimaging data-access strategies \\\"\\n    journal: Nature Methods\\n    volume: 18\\n    number: 12\\n    pages: 1496–1498\\n    doi: 10.1038/s41592-021-01326-w\\n    year: 2021\\n    date-published: 2021-11-29\\n    authors:\\n      - family-names: Moore\\n        given-names: Josh\\n        orcid: \\\"https://orcid.org/0000-0003-4028-811X\\\"\\n      - family-names: Allan\\n        given-names: Chris\\n      - family-names: Besson\\n        given-names: Sébastien\\n        orcid: \\\"https://orcid.org/0000-0001-8783-1429\\\"\\n      - family-names: Burel\\n        given-names: Jean-Marie\\n      - family-names: Diel\\n        given-names: Erin\\n        orcid: \\\"https://orcid.org/0000-0003-2526-3512\\\"\\n      - family-names: Gault\\n        given-names: David\\n      - family-names: Kozlowski\\n        given-names: Kevin\\n      - family-names: Lindner\\n        given-names: Dominik\\n      - family-names: Linkert\\n        given-names: Melissa\\n      - family-names: Manz\\n        given-names: Trevor\\n        orcid: \\\"https://orcid.org/0000-0001-7694-5164\\\"\\n      - family-names: Moore\\n        given-names: Will\\n        orcid: \\\"https://orcid.org/0000-0002-7264-8338\\\"\\n      - family-names: Pape\\n        given-names: Constantin\\n        orcid: \\\"https://orcid.org/0000-0001-6562-7187\\\"\\n      - family-names: Tischer\\n        given-names: Christian\\n      - family-names: Swedlow\\n        given-names: Jason\\n        orcid: \\\"https://orcid.org/0000-0002-2198-1958\\\"\\n\"},\"code_repository\":\"https://github.com/ome/napari-ome-zarr\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-ome-zarr\"}],\"description\":\"# Description\\n\\nThis plugin provides a reader for zarr backed OME-NGFF images in napari. The reader\\nwill inspect the `.zattrs` metadata provided and pass any relevant metadata, including channel, scale and colormap metadata.\\n\\n![Opening an ome-zarr image in napari](https://i.imgur.com/tf9IqRA.gif)\\n\\nThe example above uses the image at https://idr.openmicroscopy.org/webclient/?show=image-6001240\\n\\n# Supported Data\\n\\nThis plugin is designed to allow bioimaging researchers and analysts to explore their\\nmulti-resolution images stored in Zarr filesets (according to the [OME zarr spec](https://ngff.openmicroscopy.org/latest/))\\nwithout needing an intricate understanding of zarr, or the spec itself.\\n\\nThis plugin supports reading all images recognised as ome-zarr, namely, containing\\nwell-formed `.zattrs` and `.zgroup` files, as well as the appropriate directory\\nhierarchy as described in the [spec](https://ngff.openmicroscopy.org/latest/).\\nThe image metadata from OMERO will be used to set channel names, colormaps and rendering settings in napari.\\n\\n# Quickstart\\n\\nYou can open local or remote images using `napari` at the terminal and the path to your file:\\n\\n```\\n$ napari 'https://uk1s3.embassy.ebi.ac.uk/idr/zarr/v0.1/6001240.zarr/'\\n\\n# also works with local files\\n$ napari 6001240.zarr\\n```\\n\\nOR in python:\\n\\n```python\\nimport napari\\n\\nviewer = napari.Viewer()\\nviewer.open('https://uk1s3.embassy.ebi.ac.uk/idr/zarr/v0.1/6001240.zarr/')\\nnapari.run()\\n```\\nIf a single zarray is passed to the plugin, it will be opened without the use of\\nthe metadata:\\n\\n```\\n$ napari '/tmp/6001240.zarr/0'\\n```\\n\\nIf an image group contains labels, they will also be opened, and added as a\\nseparate layer in napari.\\n\\nWhen the labels group metadata additionally contains `\\\"rgba\\\"` and `\\\"properties\\\"` keys,\\nthe labels will be given appropriate colors and the properties will be displayed\\nin the status bar.\\n\\nWorking with ome-zarr images can be more convenient using the command-line interface\\nand utility functions of our associated library `ome-zarr`. For more information\\nplease see the [package documentation](https://pypi.org/project/ome-zarr/) for `ome-zarr`.\\n\\n# Getting Help\\n\\nIf you discover a bug with the plugin, or would like to request a new feature, please\\nraise an issue on our repository at https://github.com/ome/napari-ome-zarr.\\n\\nIf you would like assistance with using the plugin, or converting images to\\nome-zarr format, please reach out on [image.sc](https://forum.image.sc/).\\n\\n# How to Cite OME-NGFF:\\n\\n[Next-generation file format (NGFF) specifications for storing bioimaging data in the cloud](https://ngff.openmicroscopy.org/0.1/). J. Moore, et al. Editors. Open Microscopy Environment Consortium, 20 November 2020. This edition of the specification is https://ngff.openmicroscopy.org/0.1/. The latest edition is available at https://ngff.openmicroscopy.org/latest/. ([doi:10.5281/zenodo.4282107](https://doi.org/10.5281/zenodo.4282107))\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"Description\\nThis plugin provides a reader for zarr backed OME-NGFF images in napari. The reader\\nwill inspect the .zattrs metadata provided and pass any relevant metadata, including channel, scale and colormap metadata.\\n\\nThe example above uses the image at https://idr.openmicroscopy.org/webclient/?show=image-6001240\\nSupported Data\\nThis plugin is designed to allow bioimaging researchers and analysts to explore their\\nmulti-resolution images stored in Zarr filesets (according to the OME zarr spec)\\nwithout needing an intricate understanding of zarr, or the spec itself.\\nThis plugin supports reading all images recognised as ome-zarr, namely, containing\\nwell-formed .zattrs and .zgroup files, as well as the appropriate directory\\nhierarchy as described in the spec.\\nThe image metadata from OMERO will be used to set channel names, colormaps and rendering settings in napari.\\nQuickstart\\nYou can open local or remote images using napari at the terminal and the path to your file:\\n```\\n$ napari 'https://uk1s3.embassy.ebi.ac.uk/idr/zarr/v0.1/6001240.zarr/'\\nalso works with local files\\n$ napari 6001240.zarr\\n```\\nOR in python:\\n```python\\nimport napari\\nviewer = napari.Viewer()\\nviewer.open('https://uk1s3.embassy.ebi.ac.uk/idr/zarr/v0.1/6001240.zarr/')\\nnapari.run()\\n```\\nIf a single zarray is passed to the plugin, it will be opened without the use of\\nthe metadata:\\n$ napari '/tmp/6001240.zarr/0'\\nIf an image group contains labels, they will also be opened, and added as a\\nseparate layer in napari.\\nWhen the labels group metadata additionally contains \\\"rgba\\\" and \\\"properties\\\" keys,\\nthe labels will be given appropriate colors and the properties will be displayed\\nin the status bar.\\nWorking with ome-zarr images can be more convenient using the command-line interface\\nand utility functions of our associated library ome-zarr. For more information\\nplease see the package documentation for ome-zarr.\\nGetting Help\\nIf you discover a bug with the plugin, or would like to request a new feature, please\\nraise an issue on our repository at https://github.com/ome/napari-ome-zarr.\\nIf you would like assistance with using the plugin, or converting images to\\nome-zarr format, please reach out on image.sc.\\nHow to Cite OME-NGFF:\\nNext-generation file format (NGFF) specifications for storing bioimaging data in the cloud. J. Moore, et al. Editors. Open Microscopy Environment Consortium, 20 November 2020. This edition of the specification is https://ngff.openmicroscopy.org/0.1/. The latest edition is available at https://ngff.openmicroscopy.org/latest/. (doi:10.5281/zenodo.4282107)\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"napari-ome-zarr\",\"documentation\":\"https://github.com/ome/napari-ome-zarr#README.md\",\"first_released\":\"2021-06-14T08:43:26.418782Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-ome-zarr\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\"],\"project_site\":\"https://github.com/ome/napari-ome-zarr\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[\"*.zarr\",\"*.zarr*\"],\"release_date\":\"2022-07-06T14:07:17.676453Z\",\"report_issues\":\"https://github.com/ome/napari-ome-zarr/issues\",\"requirements\":[\"ome-zarr (>=0.3.0)\",\"numpy\",\"vispy\",\"napari (>=0.4.13)\"],\"summary\":\"A reader for zarr backed OME-NGFF images.\",\"support\":\"https://github.com/ome/napari-ome-zarr/issues\",\"twitter\":\"https://twitter.com/openmicroscopy\",\"version\":\"0.5.2\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Lorenzo Gaifas\"}],\"code_repository\":\"https://github.com/brisvag/napari-help\",\"conda\":[],\"description\":\"# napari-help\\n\\n[![License GNU GPL v3.0](https://img.shields.io/pypi/l/napari-help.svg?color=green)](https://github.com/brisvag/napari-help/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-help.svg?color=green)](https://pypi.org/project/napari-help)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-help.svg?color=green)](https://python.org)\\n[![tests](https://github.com/brisvag/napari-help/workflows/tests/badge.svg)](https://github.com/brisvag/napari-help/actions)\\n[![codecov](https://codecov.io/gh/brisvag/napari-help/branch/main/graph/badge.svg)](https://codecov.io/gh/brisvag/napari-help)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-help)](https://napari-hub.org/plugins/napari-help)\\n\\nHelpful tooltips for napari.\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/stable/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-help` via [pip]:\\n\\n    pip install napari-help\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/brisvag/napari-help.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [GNU GPL v3.0] license,\\n\\\"napari-help\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/brisvag/napari-help/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-help\\n\\n\\n\\n\\n\\n\\nHelpful tooltips for napari.\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-help via pip:\\npip install napari-help\\n\\nTo install latest development version :\\npip install git+https://github.com/brisvag/napari-help.git\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the GNU GPL v3.0 license,\\n\\\"napari-help\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari Help\",\"documentation\":\"https://github.com/brisvag/napari-help#README.md\",\"first_released\":\"2022-07-28T20:10:18.515120Z\",\"license\":\"GPL-3.0\",\"name\":\"napari-help\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/brisvag/napari-help\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-07-28T20:10:18.515120Z\",\"report_issues\":\"https://github.com/brisvag/napari-help/issues\",\"requirements\":[\"napari\",\"qtpy\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"tox ; extra == 'testing'\"],\"summary\":\"Helpful tooltips for napari.\",\"support\":\"https://github.com/brisvag/napari-help/issues\",\"twitter\":\"\",\"version\":\"0.1.0\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Kyle Harrington\"}],\"code_repository\":\"https://github.com/kephale/napari-stable-diffusion\",\"conda\":[],\"description\":\"# napari-stable-diffusion\\n\\n[![License BSD-3](https://img.shields.io/pypi/l/napari-stable-diffusion.svg?color=green)](https://github.com/kephale/napari-stable-diffusion/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-stable-diffusion.svg?color=green)](https://pypi.org/project/napari-stable-diffusion)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-stable-diffusion.svg?color=green)](https://python.org)\\n[![tests](https://github.com/kephale/napari-stable-diffusion/workflows/tests/badge.svg)](https://github.com/kephale/napari-stable-diffusion/actions)\\n[![codecov](https://codecov.io/gh/kephale/napari-stable-diffusion/branch/main/graph/badge.svg)](https://codecov.io/gh/kephale/napari-stable-diffusion)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-stable-diffusion)](https://napari-hub.org/plugins/napari-stable-diffusion)\\n\\nA demo of stable diffusion in napari.\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n![demo image of napari-stable-diffusion of the prompt \\\"a unicorn and a dinosaur eating cookies and drinking tea\\\"](./napari_stable_diffusion_demo.png)\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/stable/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-stable-diffusion` via [pip]:\\n\\n    pip install napari-stable-diffusion\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/kephale/napari-stable-diffusion.git\\n\\nYou will also need to sign up with HuggingFace and [generate an access\\ntoken](https://huggingface.co/docs/hub/security-tokens) to get access to the\\nStable Diffusion model we use.\\n\\nWhen you have generated your access token you can either permanently\\nset the `HF_TOKEN_SD` environment variable in your `.bashrc` or whichever file\\nyour OS uses, or you can include it on the command line\\n\\n```\\nHF_TOKEN_SD=\\\"hf_aaaAaaaasdadsadsaoaoaoasoidijo\\\" napari\\n```\\n\\nFor more information on the Stable Diffusion model itself, please see https://huggingface.co/CompVis/stable-diffusion-v1-4.\\n\\n### Apple M1 specific instructions\\n\\nTo utilize the M1 GPU, the nightly version of PyTorch needs to be\\ninstalled. Consider using `conda` or `mamba` like this:\\n\\n```\\nmamba create -c pytorch-nightly -n napari-stable-diffusion python=3.9 pip pyqt pytorch torchvision\\npip install git+https://github.com/kephale/napari-stable-diffusion.git\\n```\\n\\n## Next steps\\n\\n- Image 2 Image support\\n- Inpainting support\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-stable-diffusion\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/kephale/napari-stable-diffusion/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-stable-diffusion\\n\\n\\n\\n\\n\\n\\nA demo of stable diffusion in napari.\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\n\\nInstallation\\nYou can install napari-stable-diffusion via pip:\\npip install napari-stable-diffusion\\n\\nTo install latest development version :\\npip install git+https://github.com/kephale/napari-stable-diffusion.git\\n\\nYou will also need to sign up with HuggingFace and generate an access\\ntoken to get access to the\\nStable Diffusion model we use.\\nWhen you have generated your access token you can either permanently\\nset the HF_TOKEN_SD environment variable in your .bashrc or whichever file\\nyour OS uses, or you can include it on the command line\\nHF_TOKEN_SD=\\\"hf_aaaAaaaasdadsadsaoaoaoasoidijo\\\" napari\\nFor more information on the Stable Diffusion model itself, please see https://huggingface.co/CompVis/stable-diffusion-v1-4.\\nApple M1 specific instructions\\nTo utilize the M1 GPU, the nightly version of PyTorch needs to be\\ninstalled. Consider using conda or mamba like this:\\nmamba create -c pytorch-nightly -n napari-stable-diffusion python=3.9 pip pyqt pytorch torchvision\\npip install git+https://github.com/kephale/napari-stable-diffusion.git\\nNext steps\\n\\nImage 2 Image support\\nInpainting support\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-stable-diffusion\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"Stable Diffusion\",\"documentation\":\"https://github.com/kephale/napari-stable-diffusion#README.md\",\"first_released\":\"2022-10-27T21:23:10.927779Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-stable-diffusion\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/kephale/napari-stable-diffusion\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-10-27T21:23:10.927779Z\",\"report_issues\":\"https://github.com/kephale/napari-stable-diffusion/issues\",\"requirements\":[\"napari\",\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"magicgui\",\"qtpy\",\"diffusers\",\"transformers\",\"torch\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\"],\"summary\":\"A demo of stable diffusion in napari\",\"support\":\"https://github.com/kephale/napari-stable-diffusion/issues\",\"twitter\":\"\",\"version\":\"0.0.1\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Lorenzo Gaifas\"}],\"code_repository\":\"https://github.com/brisvag/napari-label-interpolator\",\"conda\":[],\"description\":\"# napari-label-interpolator\\n\\n[![License GNU GPL v3.0](https://img.shields.io/pypi/l/napari-label-interpolator.svg?color=green)](https://github.com/brisvag/napari-label-interpolator/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-label-interpolator.svg?color=green)](https://pypi.org/project/napari-label-interpolator)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-label-interpolator.svg?color=green)](https://python.org)\\n[![tests](https://github.com/brisvag/napari-label-interpolator/workflows/tests/badge.svg)](https://github.com/brisvag/napari-label-interpolator/actions)\\n[![codecov](https://codecov.io/gh/brisvag/napari-label-interpolator/branch/main/graph/badge.svg)](https://codecov.io/gh/brisvag/napari-label-interpolator)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-label-interpolator)](https://napari-hub.org/plugins/napari-label-interpolator)\\n\\nA napari plugin to interpolate any number of (n-1)d-labels across a single dimension.\\n\\nTo use, simply label a few slices along the desired dimension, then use the widget to interpolate along the desired axis.\\n\\n![https://user-images.githubusercontent.com/23482191/189153632-40ef38b7-be89-40b3-b583-b17f3241c67b.png]()\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-label-interpolator` via [pip]:\\n\\n    pip install napari-label-interpolator\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/brisvag/napari-label-interpolator.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [GNU GPL v3.0] license,\\n\\\"napari-label-interpolator\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/brisvag/napari-label-interpolator/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-label-interpolator\\n\\n\\n\\n\\n\\n\\nA napari plugin to interpolate any number of (n-1)d-labels across a single dimension.\\nTo use, simply label a few slices along the desired dimension, then use the widget to interpolate along the desired axis.\\n\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-label-interpolator via pip:\\npip install napari-label-interpolator\\n\\nTo install latest development version :\\npip install git+https://github.com/brisvag/napari-label-interpolator.git\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the GNU GPL v3.0 license,\\n\\\"napari-label-interpolator\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari label interpolator\",\"documentation\":\"https://github.com/brisvag/napari-label-interpolator#README.md\",\"first_released\":\"2022-09-08T15:27:52.824433Z\",\"license\":\"GPL-3.0\",\"name\":\"napari-label-interpolator\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/brisvag/napari-label-interpolator\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-09-08T15:27:52.824433Z\",\"report_issues\":\"https://github.com/brisvag/napari-label-interpolator/issues\",\"requirements\":[\"magicgui\",\"edt\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\"],\"summary\":\"A napari plugin to interpolate any number of (n-1)d-labels across a single dimension.\",\"support\":\"https://github.com/brisvag/napari-label-interpolator/issues\",\"twitter\":\"\",\"version\":\"0.1.0\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Thorsten Beier\"}],\"code_repository\":\"https://github.com/uhlmanngroup/napari-splineit\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-splineit\"}],\"description\":\"# napari-splineit\\n\\n[![License](https://img.shields.io/pypi/l/napari-splineit.svg?color=green)](https://github.com/uhlmanngroup/napari-splineit/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-splineit.svg?color=green)](https://pypi.org/project/napari-splineit)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-splineit.svg?color=green)](https://python.org)\\n[![tests](https://github.com/uhlmanngroup/napari-splineit/workflows/tests/badge.svg)](https://github.com/uhlmanngroup/napari-splineit/actions)\\n[![codecov](https://codecov.io/gh/uhlmanngroup/napari-splineit/branch/main/graph/badge.svg)](https://codecov.io/gh/uhlmanngroup/napari-splineit)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-splineit)](https://napari-hub.org/plugins/napari-splineit)\\n\\nA napari plugin for the interactive manipulation of spline-interpolation based geometrical models\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/plugins/stable/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-splineit` via [pip]:\\n\\n    pip install napari-splineit\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/uhlmanngroup/napari-splineit.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-splineit\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/uhlmanngroup/napari-splineit/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-splineit\\n\\n\\n\\n\\n\\n\\nA napari plugin for the interactive manipulation of spline-interpolation based geometrical models\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-splineit via pip:\\npip install napari-splineit\\n\\nTo install latest development version :\\npip install git+https://github.com/uhlmanngroup/napari-splineit.git\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-splineit\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"Napari SplineIt2\",\"documentation\":\"https://github.com/uhlmanngroup/napari-splineit#README.md\",\"first_released\":\"2022-07-05T10:07:35.650619Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-splineit\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\",\"writer\",\"widget\",\"sample_data\"],\"project_site\":\"https://github.com/uhlmanngroup/napari-splineit\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[\"*.splineit\"],\"release_date\":\"2022-10-24T13:39:06.290642Z\",\"report_issues\":\"https://github.com/uhlmanngroup/napari-splineit/issues\",\"requirements\":[\"numpy\",\"qtpy\",\"scikit-image\",\"scipy\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"tox ; extra == 'testing'\"],\"summary\":\"A napari plugin for spline manipulation\",\"support\":\"https://github.com/uhlmanngroup/napari-splineit/issues\",\"twitter\":\"\",\"version\":\"0.3.0\",\"visibility\":\"public\",\"writer_file_extensions\":[\".splineit\"],\"writer_save_layers\":[\"shapes\"]}",
  "{\"authors\":[{\"name\":\"Geneva Schlafly\"},{\"name\":\"Amitabh Verma\"},{\"name\":\"Rudolf Oldenbourg\"}],\"code_repository\":\"https://github.com/PolarizedLightFieldMicroscopy/napari-LF\",\"conda\":[],\"description\":\"\\n\\n<!-- This file is designed to provide you with a starting template for documenting\\nthe functionality of your plugin. Its content will be rendered on your plugin's\\nnapari hub page.\\n\\nThe sections below are given as a guide for the flow of information only, and\\nare in no way prescriptive. You should feel free to merge, remove, add and \\nrename sections at will to make this document work best for your plugin. \\n\\n# Description\\n\\nThis should be a detailed description of the context of your plugin and its \\nintended purpose.\\n\\nIf you have videos or screenshots of your plugin in action, you should include them\\nhere as well, to make them front and center for new users. \\n\\nYou should use absolute links to these assets, so that we can easily display them \\non the hub. The easiest way to include a video is to use a GIF, for example hosted\\non imgur. You can then reference this GIF as an image.\\n\\n![Example GIF hosted on Imgur](https://i.imgur.com/A5phCX4.gif)\\n\\nNote that GIFs larger than 5MB won't be rendered by GitHub - we will however,\\nrender them on the napari hub.\\n\\nThe other alternative, if you prefer to keep a video, is to use GitHub's video\\nembedding feature.\\n\\n1. Push your `DESCRIPTION.md` to GitHub on your repository (this can also be done\\nas part of a Pull Request)\\n2. Edit `.napari/DESCRIPTION.md` **on GitHub**.\\n3. Drag and drop your video into its desired location. It will be uploaded and\\nhosted on GitHub for you, but will not be placed in your repository.\\n4. We will take the resolved link to the video and render it on the hub.\\n\\nHere is an example of an mp4 video embedded this way.\\n\\nhttps://user-images.githubusercontent.com/17995243/120088305-6c093380-c132-11eb-822d-620e81eb5f0e.mp4\\n\\n# Intended Audience & Supported Data\\n\\nThis section should describe the target audience for this plugin (any knowledge,\\nskills and experience required), as well as a description of the types of data\\nsupported by this plugin.\\n\\nTry to make the data description as explicit as possible, so that users know the\\nformat your plugin expects. This applies both to reader plugins reading file formats\\nand to function/dock widget plugins accepting layers and/or layer data.\\nFor example, if you know your plugin only works with 3D integer data in \\\"tyx\\\" order,\\nmake sure to mention this.\\n\\nIf you know of researchers, groups or labs using your plugin, or if it has been cited\\nanywhere, feel free to also include this information here.\\n\\n# Quickstart\\n\\nThis section should go through step-by-step examples of how your plugin should be used.\\nWhere your plugin provides multiple dock widgets or functions, you should split these\\nout into separate subsections for easy browsing. Include screenshots and videos\\nwherever possible to elucidate your descriptions. \\n\\nIdeally, this section should start with minimal examples for those who just want a\\nquick overview of the plugin's functionality, but you should definitely link out to\\nmore complex and in-depth tutorials highlighting any intricacies of your plugin, and\\nmore detailed documentation if you have it.\\n\\n# Additional Install Steps (uncommon)\\nWe will be providing installation instructions on the hub, which will be sufficient\\nfor the majority of plugins. They will include instructions to pip install, and\\nto install via napari itself.\\n\\nMost plugins can be installed out-of-the-box by just specifying the package requirements\\nover in `setup.cfg`. However, if your plugin has any more complex dependencies, or \\nrequires any additional preparation before (or after) installation, you should add \\nthis information here.\\n\\n# Getting Help\\n\\nThis section should point users to your preferred support tools, whether this be raising\\nan issue on GitHub, asking a question on image.sc, or using some other method of contact.\\nIf you distinguish between usage support and bug/feature support, you should state that\\nhere.\\n\\n# How to Cite\\n\\nMany plugins may be used in the course of published (or publishable) research, as well as\\nduring conference talks and other public facing events. If you'd like to be cited in\\na particular format, or have a DOI you'd like used, you should provide that information here. -->\\n\\n\\nDeconvolves a 4D light field image into a full 3D focal stack reconstruction\\n\\nhttps://user-images.githubusercontent.com/23206511/180571940-9500dd19-119b-4d0d-8b33-5ab1705e9b6f.mov\\n\\nnapari-LF provides three basic processes to Calibrate, Rectify, and Deconvolve light field images:\\n\\nThe **Calibrate** process generates a calibration file that represents the optical setup that was used to record the light field images. The same calibration file can be used to rectify and deconvolve all light field images that were recorded with the same optical setup, usually the same microscope and light field camera. The Calibrate process requires as input the radiometry frame, dark frame, optical parameters, and volume parameters to generate the calibration file, which is subsequently used to rectify and deconvolve related light field images. The calibration file includes a point spread function (PSF) derived from the optical and volume parameters and is stored in HDF5 file format.\\n\\nThe **Rectify** process uses the calibration file for an affine transformation to scale and rotate experimental light field images that were recorded with a light field camera whose microlens array was (slightly) rotated with respect to the pixel array of the area detector and whose pixel pitch is not commensurate with the microlens pitch. After rectification, the rectified light field has the same integer number of pixels behind each microlens. When the Deconvolve process is called for an experimental light field image, rectifying the light field image is automatically applied before the iterative deconvolution does begin. However, the rectified light field image is not saved and is not available for viewing. Therefore, by pushing the Rectify button in the middle of the napari-LF widget, only the rectification step is invoked and the rectified light field image is saved to the project directory.\\n\\nThe **Deconvolve** process uses the PSF and a wave optics model to iteratively deconvolve a light field image into a stack of optical sections.\\n\\nThe **Parameter** panels, located in the lower half of the napari-LF widget, allows the user to specify settings for the reconstruction process. Once the appropriate parameters are selected, the Calibrate button followed by the Deconvolve button can be pushed to complete the reconstruction.\\n\\n## Quickstart\\n1. Install the napari-LF plugin into your napari environment, as described below under **Installation**.\\n1. From the napari Plugins menu, select the napari-LF plugin to install its widget into the napari viewer\\n1. Near the top of the widget, select your project folder containing the following images: light field, radiometry, and dark frame.\\n1. Write the name of the metadata file you want for recording your reconstruction settings, e.g. metadata.txt. This file will be updated each time a calibration process is started.\\n1. Calibration\\n    - In the parameters panel, navigate to **Calibrate, Required** (top tab **Calibrate**, bottom tab **Required**), which is the default selection.\\n    - Select **radiometry** and **dark frame** images from pull down menus.\\n    - Write the name of the **calibration file** you would like to produce, e.g. calibration.lfc.\\n    - Enter the appropriate **optical parameters** according to your microscope and sample material.\\n    - Enter the **volume parameters** you would like for your 3D reconstuction.\\n    - Push the `Calibrate` button.\\n1. Deconvolution\\n    - In the parameters panel, navigate to **Deconvolve, Required**.\\n    - Select **light field** image and **calibration file** from pull down menus.\\n    - Write the name of the **output image stack** you would like to produce, e.g. output_stack.tif.\\n    - Push the `Deconvolve` button.\\n3D focal stack reconstruction will display in the napari viewer and be saved in your original project folder.\\n\\n## Getting Help\\nFor details about each parameter, hover over each parameter textbox to read the tooltip description.\\nFor additional information about the reconstruction process, see our [User Guide](https://github.com/PolarizedLightFieldMicroscopy/napari-LF/blob/description/docs/napari-LF_UserGuide_5July2022.docx) along with our general documentation on [GitHub](https://github.com/PolarizedLightFieldMicroscopy/napari-LF).\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-LF\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[file an issue]: https://github.com/PolarizedLightFieldMicroscopy/napari-LF/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"\\nDeconvolves a 4D light field image into a full 3D focal stack reconstruction\\nhttps://user-images.githubusercontent.com/23206511/180571940-9500dd19-119b-4d0d-8b33-5ab1705e9b6f.mov\\nnapari-LF provides three basic processes to Calibrate, Rectify, and Deconvolve light field images:\\nThe Calibrate process generates a calibration file that represents the optical setup that was used to record the light field images. The same calibration file can be used to rectify and deconvolve all light field images that were recorded with the same optical setup, usually the same microscope and light field camera. The Calibrate process requires as input the radiometry frame, dark frame, optical parameters, and volume parameters to generate the calibration file, which is subsequently used to rectify and deconvolve related light field images. The calibration file includes a point spread function (PSF) derived from the optical and volume parameters and is stored in HDF5 file format.\\nThe Rectify process uses the calibration file for an affine transformation to scale and rotate experimental light field images that were recorded with a light field camera whose microlens array was (slightly) rotated with respect to the pixel array of the area detector and whose pixel pitch is not commensurate with the microlens pitch. After rectification, the rectified light field has the same integer number of pixels behind each microlens. When the Deconvolve process is called for an experimental light field image, rectifying the light field image is automatically applied before the iterative deconvolution does begin. However, the rectified light field image is not saved and is not available for viewing. Therefore, by pushing the Rectify button in the middle of the napari-LF widget, only the rectification step is invoked and the rectified light field image is saved to the project directory.\\nThe Deconvolve process uses the PSF and a wave optics model to iteratively deconvolve a light field image into a stack of optical sections.\\nThe Parameter panels, located in the lower half of the napari-LF widget, allows the user to specify settings for the reconstruction process. Once the appropriate parameters are selected, the Calibrate button followed by the Deconvolve button can be pushed to complete the reconstruction.\\nQuickstart\\n\\nInstall the napari-LF plugin into your napari environment, as described below under Installation.\\nFrom the napari Plugins menu, select the napari-LF plugin to install its widget into the napari viewer\\nNear the top of the widget, select your project folder containing the following images: light field, radiometry, and dark frame.\\nWrite the name of the metadata file you want for recording your reconstruction settings, e.g. metadata.txt. This file will be updated each time a calibration process is started.\\nCalibration\\nIn the parameters panel, navigate to Calibrate, Required (top tab Calibrate, bottom tab Required), which is the default selection.\\nSelect radiometry and dark frame images from pull down menus.\\nWrite the name of the calibration file you would like to produce, e.g. calibration.lfc.\\nEnter the appropriate optical parameters according to your microscope and sample material.\\nEnter the volume parameters you would like for your 3D reconstuction.\\nPush the Calibrate button.\\n\\n\\nDeconvolution\\nIn the parameters panel, navigate to Deconvolve, Required.\\nSelect light field image and calibration file from pull down menus.\\nWrite the name of the output image stack you would like to produce, e.g. output_stack.tif.\\nPush the Deconvolve button.\\n3D focal stack reconstruction will display in the napari viewer and be saved in your original project folder.\\n\\n\\n\\nGetting Help\\nFor details about each parameter, hover over each parameter textbox to read the tooltip description.\\nFor additional information about the reconstruction process, see our User Guide along with our general documentation on GitHub.\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-LF\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari LF\",\"documentation\":\"https://github.com/PolarizedLightFieldMicroscopy/napari-LF#README.md\",\"first_released\":\"2022-07-21T17:24:54.916244Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-LF\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\",\"writer\",\"widget\",\"sample_data\"],\"project_site\":\"https://github.com/PolarizedLightFieldMicroscopy/napari-LF\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[\"*.npy\"],\"release_date\":\"2022-07-28T20:10:48.107469Z\",\"report_issues\":\"https://github.com/PolarizedLightFieldMicroscopy/napari-LF/issues\",\"requirements\":[\"numpy\",\"h5py\",\"pyopencl\",\"napari\",\"opencv-contrib-python\"],\"summary\":\"Light field imaging plugin for napari\",\"support\":\"https://github.com/PolarizedLightFieldMicroscopy/napari-LF/issues\",\"twitter\":\"\",\"version\":\"0.0.4\",\"visibility\":\"public\",\"writer_file_extensions\":[\".npy\"],\"writer_save_layers\":[\"image\",\"labels\"]}",
  "{\"authors\":[{\"name\":\"Marc Boucsein\"},{\"name\":\"Marc Buckmakowski\"}],\"code_repository\":\"https://github.com/MBPhys/napari-medical-image-formats\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-medical-image-formats\"}],\"description\":\"# napari-medical-image-formats\\n\\n[![License](https://img.shields.io/pypi/l/napari-medical-image-formats.svg?color=green)](https://github.com/MBPhys/napari-medical-image-formats/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-medical-image-formats.svg?color=green)](https://pypi.org/project/napari-medical-image-formats)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-medical-image-formats.svg?color=green)](https://python.org)\\n\\n\\nA Plugin in order to read and write medical image formats such as DICOM, DICOM Series and NIfTI. The meta information is supported by the package napari-itk-io. \\n\\n----------------------------------\\n\\n## Installation\\n\\nYou can install `napari-medical-image-formats` via [pip]:\\n\\n    pip install napari-medical-image-formats\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-medical-image-formats\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/MBPhys/napari-medical-image-formats/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-medical-image-formats\\n\\n\\n\\nA Plugin in order to read and write medical image formats such as DICOM, DICOM Series and NIfTI. The meta information is supported by the package napari-itk-io. \\n\\nInstallation\\nYou can install napari-medical-image-formats via pip:\\npip install napari-medical-image-formats\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-medical-image-formats\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"napari-medical-image-formats\",\"documentation\":\"\",\"first_released\":\"2021-04-24T14:22:51.473417Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-medical-image-formats\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\",\"writer\"],\"project_site\":\"https://github.com/MBPhys/napari-medical-image-formats\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[\"*\"],\"release_date\":\"2022-01-11T09:47:49.304049Z\",\"report_issues\":\"\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"pydicom\",\"SimpleITK\",\"itk\",\"itk-napari-conversion\"],\"summary\":\"A Plugin in order to read medical image formats such as DICOM and NIfTI\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.3.8\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[\"labels\",\"image\"]}",
  "{\"authors\":[{\"name\":\"Kyle Harrington\"}],\"code_repository\":\"https://github.com/kephale/napari-tyssue\",\"description\":\"# napari-tyssue\\n\\n[![License BSD-3](https://img.shields.io/pypi/l/napari-tyssue.svg?color=green)](https://github.com/kephale/napari-tyssue/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-tyssue.svg?color=green)](https://pypi.org/project/napari-tyssue)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-tyssue.svg?color=green)](https://python.org)\\n[![tests](https://github.com/kephale/napari-tyssue/workflows/tests/badge.svg)](https://github.com/kephale/napari-tyssue/actions)\\n[![codecov](https://codecov.io/gh/kephale/napari-tyssue/branch/main/graph/badge.svg)](https://codecov.io/gh/kephale/napari-tyssue)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-tyssue)](https://napari-hub.org/plugins/napari-tyssue)\\n\\nA napari plugin for use with the tyssue library\\n\\n![napari-tyssue demo of apoptosis model](https://github.com/kephale/napari-tyssue/raw/main/assets/napari_tyssue_apoptosis.gif)\\n\\n\\nExample video of apoptosis demo simulation created based on the\\napoptosis demo from\\n[tyssue-demo](https://github.com/DamCB/tyssue-demo).\\n\\n![napari-tyssue demo of invagination model](https://github.com/kephale/napari-tyssue/raw/main/assets/napari_tyssue_invagination_3x.gif)\\n\\n\\nExample video of apoptosis demo simulation created based on work under\\nrevision by Suzanne group at U Toulouse entitled\\n\\\"Epithelio-mesenchymal transition generates an apico-basal driving\\nforce required for tissue remodeling\\\" [available here](https://github.com/DamCB/invagination).\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/stable/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou are better off using conda. You will need pytables, and ideally CGAL.\\n\\nYou can install `napari-tyssue` via [pip]:\\n\\n    pip install napari-tyssue\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/kephale/napari-tyssue.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-tyssue\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/kephale/napari-tyssue/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-tyssue\\n\\n\\n\\n\\n\\n\\nA napari plugin for use with the tyssue library\\n\\nExample video of apoptosis demo simulation created based on the\\napoptosis demo from\\ntyssue-demo.\\n\\nExample video of apoptosis demo simulation created based on work under\\nrevision by Suzanne group at U Toulouse entitled\\n\\\"Epithelio-mesenchymal transition generates an apico-basal driving\\nforce required for tissue remodeling\\\" available here.\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou are better off using conda. You will need pytables, and ideally CGAL.\\nYou can install napari-tyssue via pip:\\npip install napari-tyssue\\n\\nTo install latest development version :\\npip install git+https://github.com/kephale/napari-tyssue.git\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-tyssue\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari tyssue\",\"documentation\":\"https://github.com/kephale/napari-tyssue#README.md\",\"first_released\":\"2022-10-20T13:58:45.132746Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-tyssue\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/kephale/napari-tyssue\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-11-10T13:41:10.697955Z\",\"report_issues\":\"https://github.com/kephale/napari-tyssue/issues\",\"requirements\":[\"numpy\",\"magicgui\",\"qtpy\",\"tyssue\",\"quantities\",\"pooch\",\"tables\",\"imageio-ffmpeg\",\"invagination (==0.0.2)\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\"],\"summary\":\"A napari plugin for use with the tyssue library\",\"support\":\"https://github.com/kephale/napari-tyssue/issues\",\"twitter\":\"\",\"version\":\"0.1.1\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"robert.haase@tu-dresden.de\",\"name\":\"Robert Haase\"}],\"code_repository\":\"https://github.com/haesleinhuepf/napari-workflow-inspector\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-workflow-inspector\"}],\"description\":\"# napari-workflow-inspector\\n\\n[![License](https://img.shields.io/pypi/l/napari-workflow-inspector.svg?color=green)](https://github.com/haesleinhuepf/napari-workflow-inspector/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-workflow-inspector.svg?color=green)](https://pypi.org/project/napari-workflow-inspector)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-workflow-inspector.svg?color=green)](https://python.org)\\n[![tests](https://github.com/haesleinhuepf/napari-workflow-inspector/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-workflow-inspector/actions)\\n[![codecov](https://codecov.io/gh/haesleinhuepf/napari-workflow-inspector/branch/main/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-workflow-inspector)\\n[![Development Status](https://img.shields.io/pypi/status/napari-workflow-inspector.svg)](https://en.wikipedia.org/wiki/Software_release_life_cycle#Alpha)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-workflow-inspector)](https://napari-hub.org/plugins/napari-workflow-inspector)\\n\\nInspect relationships between image processing operations in active workflows in napari. Open the inspector by clicking the menu `Tools > Visualization > Workflow Inspector`.\\n\\n![img_1.png](https://github.com/haesleinhuepf/napari-workflow-inspector/raw/main/docs/screenshot_graph.png)\\n\\nAlso install the [napari-script-editor](https://www.napari-hub.org/plugins/napari-script-editor) \\nto generate code from active workflows.\\n\\n![img_2.png](https://github.com/haesleinhuepf/napari-workflow-inspector/raw/main/docs/screenshot_script_editor.png)\\n\\nFor recording workflows, all napari image processing plugins that use the `@time_slicer` interface are supported. See\\n[napari-time-slicer](https://www.napari-hub.org/plugins/napari-time-slicer) for a list. More to come, stay tuned.\\n\\n## Installation\\n\\nYou can install `napari-workflow-inspector` via [pip]:\\n\\n```\\npip install napari-workflow-inspector\\n```\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-workflow-inspector\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/haesleinhuepf/napari-workflow-inspector/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-workflow-inspector\\n\\n\\n\\n\\n\\n\\n\\nInspect relationships between image processing operations in active workflows in napari. Open the inspector by clicking the menu Tools > Visualization > Workflow Inspector.\\n\\nAlso install the napari-script-editor \\nto generate code from active workflows.\\n\\nFor recording workflows, all napari image processing plugins that use the @time_slicer interface are supported. See\\nnapari-time-slicer for a list. More to come, stay tuned.\\nInstallation\\nYou can install napari-workflow-inspector via pip:\\npip install napari-workflow-inspector\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-workflow-inspector\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 3 - Alpha\"],\"display_name\":\"napari-workflow-inspector\",\"documentation\":\"https://github.com/haesleinhuepf/napari-workflow-inspector#README.md\",\"first_released\":\"2021-12-04T14:21:01.302048Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-workflow-inspector\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/haesleinhuepf/napari-workflow-inspector\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-05-15T12:00:02.019533Z\",\"report_issues\":\"https://github.com/haesleinhuepf/napari-workflow-inspector/issues\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"napari-tools-menu\",\"networkx\",\"matplotlib\",\"napari-workflows\"],\"summary\":\"Inspect relationships between image processing operations in active workflows in napari\",\"support\":\"https://github.com/haesleinhuepf/napari-workflow-inspector/issues\",\"twitter\":\"\",\"version\":\"0.2.2\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"\",\"name\":\"\"}],\"code_repository\":null,\"conda\":[],\"description\":\"# napari-bioimageio\\n\\nnapari plugin for managing AI models in the [BioImage Model Zoo](https://bioimage.io).\\n\\n> **WARNING**: This is an alpha release. The API may change in future versions, and please feel free to create issues to report bugs or provide feedbacks.\\n\\n![](assets/screenshot-model-manager-1.png)\\n\\n## Installation\\n\\n```\\npip install napari-bioimageio\\n```\\n\\n(If you don't have napari installed, run `pip install napari[pyqt5]`)\\n\\n## Usage\\n\\nThis library is meant for helping developers to ease the handling of models in napari.\\n\\nWe provide a set of API functions for managing and selecting models.\\n### `show_model_manager()`\\nShow the model manager with a model list pulled from the BioImage Model Zoo, the user can explore all the available models, download or remove models.\\n\\n### `show_model_selector(filter=None)`\\nDisplay a dialog for selecting models from the BioImage Model Zoo, the user can either select an existing model or download from the BioImage Model Zoo.\\n\\nThe selecte model information (a dictionary) will be returned if the user selected a model, otherwise it returns `None`.\\n\\nOnce the user selected the model, you can access the name, and also the file path to the model resource description file (via the `rdf_source` key). With the `bioimageio.core` library (installed via `pip install bioimageio.core` or `conda install -c conda-forge bioimageio.core`), you can run inference directly, the following examples shows how to implement it:\\n\\n```python\\n# Popup a model selection dialog for choosing the model\\nmodel_info = show_model_selector(filter=nuclear_segmentation_model_filter)\\n\\nif model_info:\\n  self.nucseg_model_source = model_info[\\\"rdf_source\\\"]\\n  # Load model \\n  model_description = bioimageio.core.load_resource_description(model_info[\\\"rdf_source\\\"])\\n  input_image = imageio.imread(\\\"./my-image.tif\\\")\\n\\n  with bioimageio.core.create_prediction_pipeline(\\n      bioimageio_model=model_description\\n  ) as pipeline:\\n    output_image = bioimageio.core.prediction.predict_with_padding(\\n        pipeline, input_image, padding=padding\\n    )\\n```\\nNote: To run the models, you need to setup the conda environment properly according to the [installation guide of bioimageio.core](https://github.com/bioimage-io/core-bioimage-io-python#installation).\\n\\nFor more examples, see [this example notebook](https://github.com/bioimage-io/core-bioimage-io-python/blob/main/example/bioimageio-core-usage.ipynb) for `bioimageio.core`.\\n\\nYou can also access the weight files directly by searching the model folder (e.g. extract the model folder path via `os.path.dirname(model_description[\\\"rdf_source\\\"])`), this will be useful if you prefer to use your own model inference logic.\\n### `show_model_uploader()`\\nDisplay a dialog to instruct the user to upload a model package to the BioImage Model Zoo.\\nCurrently, it only shows a message, in the future, we will try to support direct uploading with user's credentials obtained from Zenodo (a public data repository used by the BioImage Model Zoo to store models).\\n\\nTo create a BioImageIO-compatible model package, you can use the `build_model` function as demonstrated in [this notebook]((https://github.com/bioimage-io/core-bioimage-io-python/blob/main/example/bioimageio-core-usage.ipynb)).\\n\\n## Development\\n\\n- Install and set up development environment.\\n\\n  ```sh\\n  pip install -r requirements_dev.txt\\n  ```\\n\\n  This will install all requirements.\\nIt will also install this package in development mode, so that code changes are applied immediately without reinstall necessary.\\n\\n- Here's a list of development tools we use.\\n  - [black](https://pypi.org/project/black/)\\n  - [flake8](https://pypi.org/project/flake8/)\\n  - [mypy](https://pypi.org/project/mypy/)\\n  - [pydocstyle](https://pypi.org/project/pydocstyle/)\\n  - [pylint](https://pypi.org/project/pylint/)\\n  - [pytest](https://pypi.org/project/pytest/)\\n  - [tox](https://pypi.org/project/tox/)\\n- It's recommended to use the corresponding code formatter and linters also in your code editor to get instant feedback. A popular editor that can do this is [`vscode`](https://code.visualstudio.com/).\\n- Run all tests, check formatting and linting.\\n\\n  ```sh\\n  tox\\n  ```\\n\\n- Run a single tox environment.\\n\\n  ```sh\\n  tox -e lint\\n  ```\\n\\n- Reinstall all tox environments.\\n\\n  ```sh\\n  tox -r\\n  ```\\n\\n- Run pytest and all tests.\\n\\n  ```sh\\n  pytest\\n  ```\\n\\n- Run pytest and calculate coverage for the package.\\n\\n  ```sh\\n  pytest --cov-report term-missing --cov=napari-bioimageio\\n  ```\\n\\n- Continuous integration is by default supported via [GitHub actions](https://help.github.com/en/actions). GitHub actions is free for public repositories and comes with 2000 free Ubuntu build minutes per month for private repositories.\\n\",\"description_content_type\":\"text/markdown; charset=UTF-8; variant=GFM\",\"description_text\":\"napari-bioimageio\\nnapari plugin for managing AI models in the BioImage Model Zoo.\\n\\nWARNING: This is an alpha release. The API may change in future versions, and please feel free to create issues to report bugs or provide feedbacks.\\n\\n\\nInstallation\\npip install napari-bioimageio\\n(If you don't have napari installed, run pip install napari[pyqt5])\\nUsage\\nThis library is meant for helping developers to ease the handling of models in napari.\\nWe provide a set of API functions for managing and selecting models.\\nshow_model_manager()\\nShow the model manager with a model list pulled from the BioImage Model Zoo, the user can explore all the available models, download or remove models.\\nshow_model_selector(filter=None)\\nDisplay a dialog for selecting models from the BioImage Model Zoo, the user can either select an existing model or download from the BioImage Model Zoo.\\nThe selecte model information (a dictionary) will be returned if the user selected a model, otherwise it returns None.\\nOnce the user selected the model, you can access the name, and also the file path to the model resource description file (via the rdf_source key). With the bioimageio.core library (installed via pip install bioimageio.core or conda install -c conda-forge bioimageio.core), you can run inference directly, the following examples shows how to implement it:\\n```python\\nPopup a model selection dialog for choosing the model\\nmodel_info = show_model_selector(filter=nuclear_segmentation_model_filter)\\nif model_info:\\n  self.nucseg_model_source = model_info[\\\"rdf_source\\\"]\\n  # Load model \\n  model_description = bioimageio.core.load_resource_description(model_info[\\\"rdf_source\\\"])\\n  input_image = imageio.imread(\\\"./my-image.tif\\\")\\nwith bioimageio.core.create_prediction_pipeline(\\n      bioimageio_model=model_description\\n  ) as pipeline:\\n    output_image = bioimageio.core.prediction.predict_with_padding(\\n        pipeline, input_image, padding=padding\\n    )\\n```\\nNote: To run the models, you need to setup the conda environment properly according to the installation guide of bioimageio.core.\\nFor more examples, see this example notebook for bioimageio.core.\\nYou can also access the weight files directly by searching the model folder (e.g. extract the model folder path via os.path.dirname(model_description[\\\"rdf_source\\\"])), this will be useful if you prefer to use your own model inference logic.\\nshow_model_uploader()\\nDisplay a dialog to instruct the user to upload a model package to the BioImage Model Zoo.\\nCurrently, it only shows a message, in the future, we will try to support direct uploading with user's credentials obtained from Zenodo (a public data repository used by the BioImage Model Zoo to store models).\\nTo create a BioImageIO-compatible model package, you can use the build_model function as demonstrated in this notebook.\\nDevelopment\\n\\nInstall and set up development environment.\\n\\nsh\\n  pip install -r requirements_dev.txt\\nThis will install all requirements.\\nIt will also install this package in development mode, so that code changes are applied immediately without reinstall necessary.\\n\\nHere's a list of development tools we use.\\nblack\\nflake8\\nmypy\\npydocstyle\\npylint\\npytest\\ntox\\nIt's recommended to use the corresponding code formatter and linters also in your code editor to get instant feedback. A popular editor that can do this is vscode.\\nRun all tests, check formatting and linting.\\n\\nsh\\n  tox\\n\\nRun a single tox environment.\\n\\nsh\\n  tox -e lint\\n\\nReinstall all tox environments.\\n\\nsh\\n  tox -r\\n\\nRun pytest and all tests.\\n\\nsh\\n  pytest\\n\\nRun pytest and calculate coverage for the package.\\n\\nsh\\n  pytest --cov-report term-missing --cov=napari-bioimageio\\n\\nContinuous integration is by default supported via GitHub actions. GitHub actions is free for public repositories and comes with 2000 free Ubuntu build minutes per month for private repositories.\\n\",\"development_status\":[],\"display_name\":\"BioImage.IO Model Manager\",\"documentation\":\"\",\"first_released\":\"2022-07-05T18:33:59.492886Z\",\"license\":\"\",\"name\":\"napari-bioimageio\",\"npe2\":true,\"operating_system\":[],\"plugin_types\":[\"widget\"],\"project_site\":\"\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-07-07T06:49:26.410343Z\",\"report_issues\":\"\",\"requirements\":[\"napari\",\"bioimageio.core (>=0.5.1)\",\"PyYAML (>=6.0)\"],\"summary\":\"\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.1.3\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Christopher Nauroth-Kress\"}],\"code_repository\":\"https://github.com/ch-n/napari-time_series_plotter\",\"description\":\"# napari-time_series_plotter\\n\\n[![License](https://img.shields.io/pypi/l/napari-time_series_plotter.svg?color=green)](https://github.com/ch-n/napari-time_series_plotter/raw/main/LICENSE)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-time_series_plotter.svg?color=green)](https://python.org)\\n[![PyPI](https://img.shields.io/pypi/v/napari-time_series_plotter.svg?color=green)](https://pypi.org/project/napari-time_series_plotter)\\n[![Anaconda-Server Badge](https://anaconda.org/conda-forge/napari-time-series-plotter/badges/version.svg)](https://anaconda.org/conda-forge/napari-time-series-plotter)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-time-series-plotter)](https://napari-hub.org/plugins/napari-time-series-plotter)\\n[![tests](https://github.com/ch-n/napari-time_series_plotter/workflows/tests/badge.svg)](https://github.com/ch-n/napari-time_series_plotter/actions)\\n[![codecov](https://codecov.io/gh/ch-n/napari-time_series_plotter/branch/main/graph/badge.svg)](https://codecov.io/gh/ch-n/napari-time_series_plotter)\\n\\n\\n## Description\\nNapari-time_series_plotter (TSP) is a plugin for the `napari` ndimensional image viewer. \\n\\nTSP adds live plotting of time-resolved images to napari. With the TSPExplorer widget, you can select and visualize pixel/voxel or ROI mean values from one or multiple image layers as intensity-over-time line plots. The first image dimension is handled as time. TSP supports 3D to nD images (3D: t+2D, nD: t+nD).\\n\\nThe TSPExplorer offers three different plotting modes: Voxel, Shapes, Points\\n--> Voxel mode offers live plotting while moving the cursor over an image layer\\n--> Shapes mode offers shape-based ROI plotting the ROI combination method can be one of [Mean, Median, STD, Sum, Min, Max]; multiple ROIs can be plotted simultaneously\\n--> Points mode offers simultaneous, point-based plotting of multiple voxels\\n\\nYou can modify and save the plots through the canvas toolbar.\\nPlotting powered by `napari-matplotlib`.\\n\\n----------------------------------\\n\\n## Installation\\nYou can either install the latest version via pip or conda.\\n\\n**pip:**\\n\\n    pip install napari-time-series-plotter\\n\\nor download the packaged `tar.gz` file from the release assets and install it with \\n    \\n    pip install /path/to/file.tar.gz\\n\\n**conda:**\\n\\n    conda install -c conda-forge napari-time-series-plotter\\n\\n\\nAlternatively, you can install the plugin directly in the `napari` viewer plugin manager, the napari hub, or the release assets.\\n\\n<br>\\n\\nTo install the latest development version install directly from the relevant GitHub branch.\\n\\n## Usage\\n<p align=\\\"center\\\">\\n  <img src=\\\"https://github.com/ch-n/napari-time_series_plotter/raw/main/napari-time-series-plotter_demo.gif\\\" alt=\\\"Demo gif\\\" />\\n</p>\\n    \\n- Select the TSPExplorer widget in the `Plugins` tab of the napari viewer\\n- Use the LayerSelector to choose the image layers you want to source for plotting\\n- Select the plotting mode via the options tab (Voxel mode is the default)\\n\\nVoxel mode:\\n- Move the mouse over the image while holding \\\"Shift\\\"\\n- The plotter will display the hovered voxel intensity over time for all selected layers\\n\\nShapes mode:\\n- Add one or more shapes to the ROI selection layer\\n- Position it as you need\\n- The plotter will display the combined intensity of the ROI over time for all selected layers\\n    - The shapes are 2D only; 3D ROIs are not supported\\n    - All shapes are on the currently displayed slice\\n    - The ROI combination mode can be selected in the options tab, default: mean\\n\\nPoints mode:\\n- Add one or more points to the Point selection layer\\n- The plotter will display a time series plot for each point on all selected layers\\n    - The points can be on different slices (3D and 4D support only) or images (grid mode)\\n    - Adding or moving points will regenerate the plots\\n\\n- Set custom title or axe labels in the options tab\\n- Switch between autoscaling and manually defined max and min values of the axes in the options tab\\n- Switch to label truncation in the options tab if your layer names are too long for the figure legend (set max length manually)\\n- Set a scaling factor for the X-axis in the options tab\\n\\n## ToDo (help welcome)\\n- [ ] Add Sphinx documentation\\n\\n## Version 0.1.0 Milestones\\n- [X] Update to napari-plugin-engine2 [#5](https://github.com/ch-n/napari-time_series_plotter/issues/5)\\n- [X] Update widget GUI [#6](https://github.com/ch-n/napari-time_series_plotter/issues/6)\\n- [ ] Add widget to save pixel/voxel time series to file [#7](https://github.com/ch-n/napari-time_series_plotter/issues/7)\\n- [X] Add ROI and multi-voxel plotting [#14](https://github.com/ch-n/napari-time_series_plotter/issues/14)\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-time_series_plotter\\\" is free and open-source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n--------------\\n\\n## References\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\nImages used in the demo gif were taken from [The Cancer Imaging Archive] <br>\\n\\n    DOI: https://doi.org/10.7937/K9/TCIA.2015.VOSN3HN1\\n    Images: 1.3.6.1.4.1.9328.50.16.281868838636204210586871132130856898223\\n            1.3.6.1.4.1.9328.50.16.254461916058189583774506642993503110733\\n\\n[The Cancer Imaging Archive]: https://www.cancerimagingarchive.net/\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/ch-n/napari-time_series_plotter/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-time_series_plotter\\n\\n\\n\\n\\n\\n\\n\\nDescription\\nNapari-time_series_plotter (TSP) is a plugin for the napari ndimensional image viewer. \\nTSP adds live plotting of time-resolved images to napari. With the TSPExplorer widget, you can select and visualize pixel/voxel or ROI mean values from one or multiple image layers as intensity-over-time line plots. The first image dimension is handled as time. TSP supports 3D to nD images (3D: t+2D, nD: t+nD).\\nThe TSPExplorer offers three different plotting modes: Voxel, Shapes, Points\\n--> Voxel mode offers live plotting while moving the cursor over an image layer\\n--> Shapes mode offers shape-based ROI plotting the ROI combination method can be one of [Mean, Median, STD, Sum, Min, Max]; multiple ROIs can be plotted simultaneously\\n--> Points mode offers simultaneous, point-based plotting of multiple voxels\\nYou can modify and save the plots through the canvas toolbar.\\nPlotting powered by napari-matplotlib.\\n\\nInstallation\\nYou can either install the latest version via pip or conda.\\npip:\\npip install napari-time-series-plotter\\n\\nor download the packaged tar.gz file from the release assets and install it with \\npip install /path/to/file.tar.gz\\n\\nconda:\\nconda install -c conda-forge napari-time-series-plotter\\n\\nAlternatively, you can install the plugin directly in the napari viewer plugin manager, the napari hub, or the release assets.\\n\\nTo install the latest development version install directly from the relevant GitHub branch.\\nUsage\\n\\n\\n\\n\\nSelect the TSPExplorer widget in the Plugins tab of the napari viewer\\nUse the LayerSelector to choose the image layers you want to source for plotting\\nSelect the plotting mode via the options tab (Voxel mode is the default)\\n\\nVoxel mode:\\n- Move the mouse over the image while holding \\\"Shift\\\"\\n- The plotter will display the hovered voxel intensity over time for all selected layers\\nShapes mode:\\n- Add one or more shapes to the ROI selection layer\\n- Position it as you need\\n- The plotter will display the combined intensity of the ROI over time for all selected layers\\n    - The shapes are 2D only; 3D ROIs are not supported\\n    - All shapes are on the currently displayed slice\\n    - The ROI combination mode can be selected in the options tab, default: mean\\nPoints mode:\\n- Add one or more points to the Point selection layer\\n- The plotter will display a time series plot for each point on all selected layers\\n    - The points can be on different slices (3D and 4D support only) or images (grid mode)\\n    - Adding or moving points will regenerate the plots\\n\\nSet custom title or axe labels in the options tab\\nSwitch between autoscaling and manually defined max and min values of the axes in the options tab\\nSwitch to label truncation in the options tab if your layer names are too long for the figure legend (set max length manually)\\nSet a scaling factor for the X-axis in the options tab\\n\\nToDo (help welcome)\\n\\n[ ] Add Sphinx documentation\\n\\nVersion 0.1.0 Milestones\\n\\n[X] Update to napari-plugin-engine2 #5\\n[X] Update widget GUI #6\\n[ ] Add widget to save pixel/voxel time series to file #7\\n[X] Add ROI and multi-voxel plotting #14\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-time_series_plotter\\\" is free and open-source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\\n\\nReferences\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\nImages used in the demo gif were taken from The Cancer Imaging Archive \\nDOI: https://doi.org/10.7937/K9/TCIA.2015.VOSN3HN1\\nImages: 1.3.6.1.4.1.9328.50.16.281868838636204210586871132130856898223\\n        1.3.6.1.4.1.9328.50.16.254461916058189583774506642993503110733\\n\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari-time-series-plotter\",\"documentation\":\"https://github.com/ch-n/napari-time_series_plotter#README.md\",\"first_released\":\"2021-12-01T11:59:13.160554Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-time-series-plotter\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/ch-n/napari-time_series_plotter\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-11-16T11:56:49.975469Z\",\"report_issues\":\"https://github.com/ch-n/napari-time_series_plotter/issues\",\"requirements\":[\"napari-plugin-engine (>=0.2.0)\",\"napari-matplotlib\",\"numpy\",\"qtpy\",\"napari ; extra == 'test'\",\"pytest ; extra == 'test'\",\"pytest-qt ; extra == 'test'\",\"pytest-cov ; extra == 'test'\"],\"summary\":\"A Plugin for napari to visualize pixel values over the first dimension (time -> t+3D, t+2D) as graphs.\",\"support\":\"https://github.com/ch-n/napari-time_series_plotter/issues\",\"twitter\":\"\",\"version\":\"0.0.5\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"juan.nunez-iglesias@monash.edu\",\"name\":\"Juan Nunez-Iglesias\"}],\"category\":{\"Supported data\":[\"2D\"],\"Workflow step\":[\"Image registration\"]},\"category_hierarchy\":{\"Supported data\":[[\"2D\"]],\"Workflow step\":[[\"Image registration\"],[\"Image registration\",\"Affine registration\"],[\"Image registration\",\"Affine registration\",\"Rigid registration\"]]},\"code_repository\":\"https://github.com/jni/affinder\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"affinder\"}],\"description\":\"# Description\\n\\nThis GUI plugin allows you to quickly find the affine matrix mapping\\none image to another using manual correspondence points annotation.\\n\\nMore simply, this plugin allows you to select corresponding points\\non an image, and a second image you wish to transform. It computes \\nthe requisite transformation matrix using Affine Transform, Euclidean Transform, \\nor Similarity Transform, and performs this transformation on the\\nmoving image, aligning it to the reference image.\\n\\nhttps://user-images.githubusercontent.com/17995243/120086403-f1d0b300-c121-11eb-8000-a44a2ac54339.mp4\\n\\n\\n# Who is This For?\\n\\nThis is a simple plugin which can be used on any 2D images, provided\\nthey can be loaded as layers into napari. The images need not be the same\\nfile format and this plugin also works with labels layers.\\n\\nNo prior understanding of the transformation methods is required, as\\nthey perform in the background based on the reference points selected.\\n\\n# How to Guide\\n\\nYou will need a combination of two or more 2D image and/or labels layers \\nloaded into napari. Once you have installed affinder, you can find it in\\nthe dock widgets menu.\\n\\n![Affinder widget in the Plugins->Add Dock Widget menu](https://i.imgur.com/w7MCXQy.png)\\n\\nThe first two dropdown boxes will be populated with the layers currently\\nloaded into napari. Select a layer to use as reference, and another to\\ntransform.\\n\\n![Dropdowns allow you to select the reference and moving layers](https://i.imgur.com/Tdbm1sX.png)\\n\\nNext, you can select the transformation model to use (affine is selected by default\\nand is the least rigid transformation of those available). See [below](#transformation-models) for a\\ndescription of the different models.\\n\\nFinally, you can optionally select a path to a text file for saving out the\\nresulting transformation matrix.\\n\\nWhen you click Start, affinder will add two points layers to napari. \\nThe plugin will also bring your reference image in focus, and its associated points\\nlayer. You can then start adding reference points by clicking on your image.\\n\\n![Adding reference points to layer](https://i.imgur.com/WPzNtyy.png)\\n\\nOnce three points are added, affinder will switch focus to the moving image,\\nand you should then proceed to select the corresponding three points.\\n\\n![Adding corresponding points to newly focused layer](https://i.imgur.com/JVZCvmp.png)\\n\\naffinder will immediately transform the moving image to align the points you've\\nselected when you add your third corresponding point to your moving image.\\n\\n![The moving image is transformed once three points are added](https://i.imgur.com/NTne9fj.png)\\n\\nFrom there, you can continue iteratively adding points until you \\nare happy with the alignment. Affinder will switch focus between\\nreference and moving image with each point.\\n\\nClick Finish to exit affinder.\\n\\n## Transformation Models\\n\\nThere are three transformation models available for use with affinder.\\nThey are listed here in order of increasing rigidity in the types of\\ntransforms they will allow. The eponymous Affine Transform is the \\nleast rigid and is the default choice.\\n\\n- [**Affine Transform**](https://en.wikipedia.org/wiki/Affine_transformation): \\nthe least rigid transformation, it preserves\\nlines and parallelism, but not necessarily distance and angles. Translation,\\nscaling, similarity, reflection, rotation and shearing are all valid\\naffine transformations.\\n\\n- [**Similarity Transform**](https://en.wikipedia.org/wiki/Similarity_(geometry)): \\nthis is a \\\"shape preserving\\\" transformation, producing objects which are \\ngeometrically similar. Translation, rotation, reflection and uniform scaling are \\nvalid similarity transforms. Shearing is not.\\n\\n- [**Euclidean Transform**](https://en.wikipedia.org/wiki/Rigid_transformation):\\nAlso known as a rigid transformation, this transform preserves the Euclidean\\ndistance between each pair of points on the image. This includes rotation,\\ntranslation and reflection but not scaling or shearing.\\n\\n# Getting Help\\n\\nIf you find a bug with affinder, or would like support with using it, please raise an\\nissue on the [GitHub repository](https://github.com/jni/affinder).\\n\\n# How to Cite\\n\\nMany plugins may be used in the course of published (or publishable) research, as well as\\nduring conference talks and other public facing events. If you'd like to be cited in\\na particular format, or have a DOI you'd like used, you should provide that information here.\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"Description\\nThis GUI plugin allows you to quickly find the affine matrix mapping\\none image to another using manual correspondence points annotation.\\nMore simply, this plugin allows you to select corresponding points\\non an image, and a second image you wish to transform. It computes \\nthe requisite transformation matrix using Affine Transform, Euclidean Transform, \\nor Similarity Transform, and performs this transformation on the\\nmoving image, aligning it to the reference image.\\nhttps://user-images.githubusercontent.com/17995243/120086403-f1d0b300-c121-11eb-8000-a44a2ac54339.mp4\\nWho is This For?\\nThis is a simple plugin which can be used on any 2D images, provided\\nthey can be loaded as layers into napari. The images need not be the same\\nfile format and this plugin also works with labels layers.\\nNo prior understanding of the transformation methods is required, as\\nthey perform in the background based on the reference points selected.\\nHow to Guide\\nYou will need a combination of two or more 2D image and/or labels layers \\nloaded into napari. Once you have installed affinder, you can find it in\\nthe dock widgets menu.\\n\\nThe first two dropdown boxes will be populated with the layers currently\\nloaded into napari. Select a layer to use as reference, and another to\\ntransform.\\n\\nNext, you can select the transformation model to use (affine is selected by default\\nand is the least rigid transformation of those available). See below for a\\ndescription of the different models.\\nFinally, you can optionally select a path to a text file for saving out the\\nresulting transformation matrix.\\nWhen you click Start, affinder will add two points layers to napari. \\nThe plugin will also bring your reference image in focus, and its associated points\\nlayer. You can then start adding reference points by clicking on your image.\\n\\nOnce three points are added, affinder will switch focus to the moving image,\\nand you should then proceed to select the corresponding three points.\\n\\naffinder will immediately transform the moving image to align the points you've\\nselected when you add your third corresponding point to your moving image.\\n\\nFrom there, you can continue iteratively adding points until you \\nare happy with the alignment. Affinder will switch focus between\\nreference and moving image with each point.\\nClick Finish to exit affinder.\\nTransformation Models\\nThere are three transformation models available for use with affinder.\\nThey are listed here in order of increasing rigidity in the types of\\ntransforms they will allow. The eponymous Affine Transform is the \\nleast rigid and is the default choice.\\n\\n\\nAffine Transform: \\nthe least rigid transformation, it preserves\\nlines and parallelism, but not necessarily distance and angles. Translation,\\nscaling, similarity, reflection, rotation and shearing are all valid\\naffine transformations.\\n\\n\\nSimilarity Transform: \\nthis is a \\\"shape preserving\\\" transformation, producing objects which are \\ngeometrically similar. Translation, rotation, reflection and uniform scaling are \\nvalid similarity transforms. Shearing is not.\\n\\n\\nEuclidean Transform:\\nAlso known as a rigid transformation, this transform preserves the Euclidean\\ndistance between each pair of points on the image. This includes rotation,\\ntranslation and reflection but not scaling or shearing.\\n\\n\\nGetting Help\\nIf you find a bug with affinder, or would like support with using it, please raise an\\nissue on the GitHub repository.\\nHow to Cite\\nMany plugins may be used in the course of published (or publishable) research, as well as\\nduring conference talks and other public facing events. If you'd like to be cited in\\na particular format, or have a DOI you'd like used, you should provide that information here.\",\"development_status\":[\"Development Status :: 3 - Alpha\"],\"display_name\":\"affinder\",\"documentation\":\"\",\"first_released\":\"2021-02-04T10:12:07.298699Z\",\"license\":\"BSD-3-Clause\",\"name\":\"affinder\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/jni/affinder\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-04-08T01:03:20.850508Z\",\"report_issues\":\"\",\"requirements\":[\"napari (>=0.4.12)\",\"npe2 (>=0.1.2)\",\"numpy\",\"scikit-image\",\"magicgui (>=0.3.7)\",\"toolz\",\"furo ; extra == 'docs'\",\"myst-parser ; extra == 'docs'\",\"coverage ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"scikit-image[data] ; extra == 'testing'\",\"napari[pyqt5] ; extra == 'testing'\"],\"summary\":\"Quickly find the affine matrix mapping one image to another using manual correspondence points annotation\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.2.3\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Ruben Lopez\"}],\"code_repository\":\"https://github.com/rjlopez2/napari-sif-reader\",\"description\":\"# napari-sif-reader\\n\\n[![License BSD-3](https://img.shields.io/pypi/l/napari-sif-reader.svg?color=green)](https://github.com/rjlopez2/napari-sif-reader/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-sif-reader.svg?color=green)](https://pypi.org/project/napari-sif-reader)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-sif-reader.svg?color=green)](https://python.org)\\n[![tests](https://github.com/rjlopez2/napari-sif-reader/workflows/tests/badge.svg)](https://github.com/rjlopez2/napari-sif-reader/actions)\\n[![codecov](https://codecov.io/gh/rjlopez2/napari-sif-reader/branch/main/graph/badge.svg)](https://codecov.io/gh/rjlopez2/napari-sif-reader)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-sif-reader)](https://napari-hub.org/plugins/napari-sif-reader)\\n\\nThis is a simple wraper to read .sif format files from Andor Technology.\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/stable/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-sif-reader` via [pip]:\\n\\n    pip install napari-sif-reader\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/rjlopez2/napari-sif-reader.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-sif-reader\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/rjlopez2/napari-sif-reader/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-sif-reader\\n\\n\\n\\n\\n\\n\\nThis is a simple wraper to read .sif format files from Andor Technology.\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-sif-reader via pip:\\npip install napari-sif-reader\\n\\nTo install latest development version :\\npip install git+https://github.com/rjlopez2/napari-sif-reader.git\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-sif-reader\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari sif file reader\",\"documentation\":\"https://github.com/rjlopez2/napari-sif-reader#README.md\",\"first_released\":\"2022-11-03T08:11:24.248334Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-sif-reader\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\",\"sample_data\"],\"project_site\":\"https://github.com/rjlopez2/napari-sif-reader\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[\"*.sif\"],\"release_date\":\"2022-11-03T08:11:24.248334Z\",\"report_issues\":\"https://github.com/rjlopez2/napari-sif-reader/issues\",\"requirements\":[\"numpy\",\"sif-parser\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\"],\"summary\":\"This is a simple wraper to read .sif format files from Andor Technology.\",\"support\":\"https://github.com/rjlopez2/napari-sif-reader/issues\",\"twitter\":\"\",\"version\":\"0.0.1\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Andrea Bassi\"},{\"name\":\"Mark Neil\"}],\"category\":{\"Image modality\":[\"Fluorescence microscopy\"],\"Supported data\":[\"3D\"],\"Workflow step\":[\"Image reconstruction\"]},\"category_hierarchy\":{\"Image modality\":[[\"Fluorescence microscopy\",\"Super-resolution microscopy\"]],\"Supported data\":[[\"3D\"]],\"Workflow step\":[[\"Image reconstruction\",\"Structured illumination reconstruction\"]]},\"code_repository\":\"https://github.com/andreabassi78/napari-sim-processor\",\"description\":\"# napari-sim-processor\\n\\n[![License](https://img.shields.io/pypi/l/napari-sim-processor.svg?color=green)](https://github.com/andreabassi78/napari-sim-processor/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-sim-processor.svg?color=green)](https://pypi.org/project/napari-sim-processor)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-sim-processor.svg?color=green)](https://python.org)\\n[![tests](https://github.com/andreabassi78/napari-sim-processor/workflows/tests/badge.svg)](https://github.com/andreabassi78/napari-sim-processor/actions)\\n[![codecov](https://codecov.io/gh/andreabassi78/napari-sim-processor/branch/main/graph/badge.svg)](https://codecov.io/gh/andreabassi78/napari-sim-processor)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-sim-processor)](https://napari-hub.org/plugins/napari-sim-processor)\\n\\nA Napari plugin for the reconstruction of Structured Illumination Microscopy (SIM) with GPU acceleration (pytorch/cupy if installed).\\nCurrently supports:    \\n   - conventional SIM data with a generic number of angles and phases (typically, 3 angles and 3 phases are used for resolution improvement in 2D, but any combination can be processed by the widget)\\n   - hexagonal SIM data with 7 phases.\\n\\nThe SIM processing widget accepts image stacks organized in 5D (`angle`,`phase`,`z`,`y`,`x`).\\n\\nThe reshape widget can be used to easily reshape the data if they are not organized as 5D (angle,phase,z,y,x).\\nCurrently only square images are supported (`x`=`y`)\\n\\nFor 3D stacks (raw images) with multiple z-frames, a batch reconstruction method is available, as described here:\\n\\thttps://doi.org/10.1098/rsta.2020.0162\\n        \\nSupport for 3D SIM with enhanced resolution in all directions is not yet available.\\nMulticolor reconstruction is not yet available.  \\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-sim-processor` via [pip]:\\n\\n    pip install napari-sim-processor\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/andreabassi78/napari-sim-processor.git\\n\\n\\n## Usage\\n\\n1) Open napari. \\n\\n2) Launch the reshape and sim-processor widgets.\\n\\n3) Open your raw image stack (using the napari built-in or your own file openers).\\n\\n![raw](https://github.com/andreabassi78/napari-sim-processor/raw/main/images/Picture1.png)\\n\\n4) If your image is ordered as a 5D stack (angle, phase, z-frame, y, x) go to point 6. \\n\\n5) In the reshape widget, select the actual number of acquired angles, phases, and frames (red arrow) and press `Reshape Stack`.\\n Note that the label axis of the viewer will be updated (green arrow).\\n\\n![raw](https://github.com/andreabassi78/napari-sim-processor/raw/main/images/Picture1b.png)\\n\\n6) In the sim-reconstruction widget press the Select image layer button. Note that the number of phases and angles will be updated (blue arrow). \\n\\n7) Choose the correct parameters of the SIM acquisition (`NA`, `pixelsize`, `M`, etc.) and processing parameters (`alpha`, `beta`, w, `eta`, `group`):\\n   - `w`: parameter of the Weiner filter.\\n   - `eta`: constant used for calibration. It should be slightly smaller than the carrier frequency (in pupil radius units).\\n   - `group`: for stacks with multiple z-frames, it is the number of frames that are used together for the calibration process.\\n\\t\\nFor details on the other parameters see https://doi.org/10.1098/rsta.2020.0162.\\n\\n8) Calibrate the SIM processor, pressing the `Calibrate` button. This will find the carrier frequencies (red circles if the `Show Carrier` checkbox is selected), the modulation amplitude and the phase, using cross correlation analysis.\\n\\n9) Click on the checkboxes to show the power spectrum of the raw image (`Show power spectrum`) or the cross-correlation (`Show Xcorr`), to see if the found carrier frequency is correct.\\n\\n![raw](https://github.com/andreabassi78/napari-sim-processor/raw/main/images/Picture2b.png)\\n**Napari viewer showing the power spectrum of the raw stack. The pupil circle is in blue. A circle corresponding to `eta` is shown in green.**\\n\\n![raw](https://github.com/andreabassi78/napari-sim-processor/raw/main/images/Picture2.png)\\n**Napari viewer showing the cross-correlation of the raw stack. The red circles indicate the found carrier frequencies**\\n\\n10) Run the reconstruction of a single plane (`SIM reconstruction`) or of a stack (`Stack reconstruction`). After execution, a new image_layer will be added to the napari viewer. Click on the `Batch reconstruction` checkbox in order to process an entire stack in one shot. Click on the pytorch checkbox for gpu acceleration.\\n\\n![raw](https://github.com/andreabassi78/napari-sim-processor/raw/main/images/Picture3b.png)\\n**Napari viewer with widgets showing a pseudo-widefield reconstruction**\\n\\n![raw](https://github.com/andreabassi78/napari-sim-processor/raw/main/images/Picture3.png)\\n**Napari viewer with widgets showing a SIM reconstruction**\\n\\n## GPU processing\\n\\nThe underlying processing classes will use numpy (and FFTW if available) for \\nits calculations. For GPU accelerated processing you need to have either the \\nPyTorch (tested with torch v1.11.0+cu113) or the CuPy (tested with cupy-cuda113 \\nv10.4.0) package installed.  Make sure to match the package cuda version to the CUDA library \\ninstalled on your system otherwise PyTorch will default to CPU and CuPy will not work at all.  \\n\\nBoth packages give significant speedup on even relatively modest CUDA GPUs compared \\nto Numpy, and PyTorch running on the CPU only can show improvements relative to numpy \\nand FFTW. Selection of which processing package to use is via a ComboBox in the \\nnapari_sim_processor widget.  Only available packages are shown. \\n\\nOther than requiring a CUDA GPU it is advisable to have significant GPU memory \\navailable, particularly when processing large datasets.  Batch processing is the \\nmost memory hungry of the methods, but can process 280x512x512 datasets on a 4GB GPU.\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-sim-processor\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/andreabassi78/napari-sim-processor/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-sim-processor\\n\\n\\n\\n\\n\\n\\nA Napari plugin for the reconstruction of Structured Illumination Microscopy (SIM) with GPU acceleration (pytorch/cupy if installed).\\nCurrently supports:  \\n   - conventional SIM data with a generic number of angles and phases (typically, 3 angles and 3 phases are used for resolution improvement in 2D, but any combination can be processed by the widget)\\n   - hexagonal SIM data with 7 phases.\\nThe SIM processing widget accepts image stacks organized in 5D (angle,phase,z,y,x).\\nThe reshape widget can be used to easily reshape the data if they are not organized as 5D (angle,phase,z,y,x).\\nCurrently only square images are supported (x=y)\\nFor 3D stacks (raw images) with multiple z-frames, a batch reconstruction method is available, as described here:\\n    https://doi.org/10.1098/rsta.2020.0162\\nSupport for 3D SIM with enhanced resolution in all directions is not yet available.\\nMulticolor reconstruction is not yet available.  \\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-sim-processor via pip:\\npip install napari-sim-processor\\n\\nTo install latest development version :\\npip install git+https://github.com/andreabassi78/napari-sim-processor.git\\n\\nUsage\\n1) Open napari. \\n2) Launch the reshape and sim-processor widgets.\\n3) Open your raw image stack (using the napari built-in or your own file openers).\\n\\n4) If your image is ordered as a 5D stack (angle, phase, z-frame, y, x) go to point 6. \\n5) In the reshape widget, select the actual number of acquired angles, phases, and frames (red arrow) and press Reshape Stack.\\n Note that the label axis of the viewer will be updated (green arrow).\\n\\n6) In the sim-reconstruction widget press the Select image layer button. Note that the number of phases and angles will be updated (blue arrow). \\n7) Choose the correct parameters of the SIM acquisition (NA, pixelsize, M, etc.) and processing parameters (alpha, beta, w, eta, group):\\n   - w: parameter of the Weiner filter.\\n   - eta: constant used for calibration. It should be slightly smaller than the carrier frequency (in pupil radius units).\\n   - group: for stacks with multiple z-frames, it is the number of frames that are used together for the calibration process.\\nFor details on the other parameters see https://doi.org/10.1098/rsta.2020.0162.\\n8) Calibrate the SIM processor, pressing the Calibrate button. This will find the carrier frequencies (red circles if the Show Carrier checkbox is selected), the modulation amplitude and the phase, using cross correlation analysis.\\n9) Click on the checkboxes to show the power spectrum of the raw image (Show power spectrum) or the cross-correlation (Show Xcorr), to see if the found carrier frequency is correct.\\n\\nNapari viewer showing the power spectrum of the raw stack. The pupil circle is in blue. A circle corresponding to eta is shown in green.\\n\\nNapari viewer showing the cross-correlation of the raw stack. The red circles indicate the found carrier frequencies\\n10) Run the reconstruction of a single plane (SIM reconstruction) or of a stack (Stack reconstruction). After execution, a new image_layer will be added to the napari viewer. Click on the Batch reconstruction checkbox in order to process an entire stack in one shot. Click on the pytorch checkbox for gpu acceleration.\\n\\nNapari viewer with widgets showing a pseudo-widefield reconstruction\\n\\nNapari viewer with widgets showing a SIM reconstruction\\nGPU processing\\nThe underlying processing classes will use numpy (and FFTW if available) for \\nits calculations. For GPU accelerated processing you need to have either the \\nPyTorch (tested with torch v1.11.0+cu113) or the CuPy (tested with cupy-cuda113 \\nv10.4.0) package installed.  Make sure to match the package cuda version to the CUDA library \\ninstalled on your system otherwise PyTorch will default to CPU and CuPy will not work at all.  \\nBoth packages give significant speedup on even relatively modest CUDA GPUs compared \\nto Numpy, and PyTorch running on the CPU only can show improvements relative to numpy \\nand FFTW. Selection of which processing package to use is via a ComboBox in the \\nnapari_sim_processor widget.  Only available packages are shown. \\nOther than requiring a CUDA GPU it is advisable to have significant GPU memory \\navailable, particularly when processing large datasets.  Batch processing is the \\nmost memory hungry of the methods, but can process 280x512x512 datasets on a 4GB GPU.\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-sim-processor\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari SIM processor\",\"documentation\":\"https://github.com/andreabassi78/napari-sim-processor#README.md\",\"first_released\":\"2022-05-04T16:42:50.109160Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-sim-processor\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/andreabassi78/napari-sim-processor\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2023-02-01T13:15:54.048059Z\",\"report_issues\":\"https://github.com/andreabassi78/napari-sim-processor/issues\",\"requirements\":[\"numpy\",\"magicgui\",\"qtpy\",\"matplotlib\",\"superqt (>=0.3.2)\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\",\"matplotlib ; extra == 'testing'\"],\"summary\":\"A plugin to process Structured Illumination Microscopy data with gpu acceleration\",\"support\":\"https://github.com/andreabassi78/napari-sim-processor/issues\",\"twitter\":\"\",\"version\":\"0.0.10\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"talley.lambert@gmail.com\",\"name\":\"Talley Lambert\"}],\"code_repository\":\"https://github.com/tlambert03/napari-error-reporter\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-error-reporter\"}],\"description\":\"# 🐛 napari-error-reporter\\n\\n[![License](https://img.shields.io/pypi/l/napari-error-reporter.svg?color=green)](https://github.com/tlambert03/napari-error-reporter/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-error-reporter.svg?color=green)](https://pypi.org/project/napari-error-reporter)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-error-reporter.svg?color=green)](https://python.org)\\n[![CI](https://github.com/tlambert03/napari-error-reporter/actions/workflows/ci.yml/badge.svg)](https://github.com/tlambert03/napari-error-reporter/actions/workflows/ci.yml)\\n[![codecov](https://codecov.io/gh/tlambert03/napari-error-reporter/branch/main/graph/badge.svg)](https://codecov.io/gh/tlambert03/napari-error-reporter)\\n\\nWant to help out napari?  Install this plugin!\\n\\nThis plugin will automatically send error reports to napari (via\\n[sentry.io](https://sentry.io)) whenever an exception occurs while you are using\\nnapari.\\n\\nThe first time you run napari after installing this plugin an opt-in\\nnotification will appear (Be sure to click \\\"yes\\\", otherwise no reports will be\\ncollected or sent).  You may opt back out at any time in napari's help menu.\\n\\nEvery effort is made to strip these reports of personally identifiable\\ninformation.  Here is an example exception event:\\n\\n<details>\\n\\n<summary>Example bug report</summary>\\n\\n```python\\n{\\n    'breadcrumbs': {\\n        'values': [\\n            {\\n                'category': 'subprocess',\\n                'data': {},\\n                'message': 'sw_vers -productVersion',\\n                'timestamp': '2022-02-02T01:30:00.216738Z',\\n                'type': 'subprocess'\\n            }\\n        ]\\n    },\\n    'contexts': {\\n        'runtime': {\\n            'build': '3.9.9 | packaged by conda-forge | (main, Dec 20 2021, 02:41:37) \\\\n[Clang 11.1.0 ]',\\n            'name': 'CPython',\\n            'version': '3.9.9'\\n        }\\n    },\\n    'environment': 'macOS-10.15.7-x86_64-i386-64bit',\\n    'event_id': '02dd8ddd3a4b4743af3d7d7a09949df4',\\n    'exception': {\\n        'values': [\\n            {\\n                'mechanism': None,\\n                'module': None,\\n                'stacktrace': {\\n                    'frames': [\\n                        {\\n                            'context_line': '                x = 1 / 0',\\n                            'filename': 'napari_error_reporter/_util.py',\\n                            'function': 'get_sample_event',\\n                            'in_app': True,\\n                            'lineno': 130,\\n                            'module': 'napari_error_reporter._util',\\n                            'post_context': [\\n                                '            except Exception:',\\n                                '                with sentry_sdk.push_scope() as scope:',\\n                                '                    for k, v in _get_tags().items():',\\n                                '                        scope.set_tag(k, v)',\\n                                '                    del v, k, scope'\\n                            ],\\n                            'pre_context': [\\n                                \\\"            # remove locals that wouldn't really be there\\\",\\n                                '            del settings, _trans, kwargs, client, EVENT',\\n                                '            try:',\\n                                '                some_variable = 1',\\n                                '                another_variable = \\\"my_string\\\"'\\n                            ]\\n                        }\\n                    ]\\n                },\\n                'type': 'ZeroDivisionError',\\n                'value': 'division by zero'\\n            }\\n        ]\\n    },\\n    'extra': {'sys.argv': ['napari']},\\n    'level': 'error',\\n    'modules': {\\n        'aicsimageio': '4.5.2',\\n        'aicspylibczi': '3.0.4',\\n        'aiohttp': '3.8.1',\\n        'aiosignal': '1.2.0',\\n        'alabaster': '0.7.12',\\n        'anyio': '3.5.0',\\n        'appdirs': '1.4.4',\\n        'appnope': '0.1.2',\\n        'argon2-cffi': '21.3.0',\\n        'argon2-cffi-bindings': '21.2.0',\\n        'arrow': '1.2.1',\\n        'asciitree': '0.3.3',\\n        'asttokens': '2.0.5',\\n        'async-timeout': '4.0.2',\\n        'atomium': '1.0.11',\\n        'attrs': '21.4.0',\\n        'autopep8': '1.6.0',\\n        'babel': '2.9.1',\\n        'backcall': '0.2.0',\\n        'bcrypt': '3.2.0',\\n        'beautifulsoup4': '4.10.0',\\n        'binaryornot': '0.4.4',\\n        'black': '20.8b1',\\n        'bleach': '4.1.0',\\n        'bracex': '2.2.1',\\n        'build': '0.7.0',\\n        'cachey': '0.2.1',\\n        'cellpose': '0.6.5',\\n        'certifi': '2021.10.8',\\n        'cffi': '1.15.0',\\n        'cfgv': '3.3.1',\\n        'chardet': '4.0.0',\\n        'charset-normalizer': '2.0.10',\\n        'check-manifest': '0.47',\\n        'click': '7.1.2',\\n        'click-option-group': '0.5.3',\\n        'cloudpickle': '2.0.0',\\n        'colorama': '0.4.4',\\n        'commonmark': '0.9.1',\\n        'cookiecutter': '1.7.3',\\n        'coverage': '6.2',\\n        'cryptography': '36.0.1',\\n        'cycler': '0.11.0',\\n        'dask': '2022.1.0',\\n        'debugpy': '1.5.1',\\n        'decorator': '5.1.1',\\n        'defusedxml': '0.7.1',\\n        'distlib': '0.3.4',\\n        'dnspython': '2.2.0',\\n        'docstring-parser': '0.13',\\n        'docutils': '0.16',\\n        'elementpath': '2.4.0',\\n        'email-validator': '1.1.3',\\n        'entrypoints': '0.3',\\n        'executing': '0.8.2',\\n        'fancycompleter': '0.9.1',\\n        'fasteners': '0.17.2',\\n        'fastremap': '1.12.2',\\n        'filelock': '3.4.2',\\n        'flake8': '3.8.4',\\n        'fonttools': '4.28.5',\\n        'freetype-py': '2.2.0',\\n        'frozenlist': '1.3.0',\\n        'fsspec': '2022.1.0',\\n        'furo': '2022.1.2',\\n        'gitdb': '4.0.9',\\n        'gitpython': '3.1.26',\\n        'greenlet': '1.1.2',\\n        'heapdict': '1.0.1',\\n        'hsluv': '5.0.2',\\n        'hypothesis': '6.35.1',\\n        'identify': '2.4.4',\\n        'idna': '3.3',\\n        'imagecodecs': '2021.11.20',\\n        'imageio': '2.10.5',\\n        'imageio-ffmpeg': '0.4.5',\\n        'imagesize': '1.3.0',\\n        'importlib-metadata': '4.10.1',\\n        'iniconfig': '1.1.1',\\n        'install': '1.3.5',\\n        'intervaltree': '3.1.0',\\n        'ipykernel': '6.7.0',\\n        'ipython': '8.0.0',\\n        'ipython-genutils': '0.2.0',\\n        'ipywidgets': '7.6.5',\\n        'jedi': '0.18.1',\\n        'jinja2': '3.0.3',\\n        'jinja2-time': '0.2.0',\\n        'jsonschema': '3.2.0',\\n        'jupyter': '1.0.0',\\n        'jupyter-book': '0.12.1',\\n        'jupyter-cache': '0.4.3',\\n        'jupyter-client': '7.1.1',\\n        'jupyter-console': '6.4.0',\\n        'jupyter-core': '4.9.1',\\n        'jupyter-server': '1.13.3',\\n        'jupyter-server-mathjax': '0.2.3',\\n        'jupyter-sphinx': '0.3.2',\\n        'jupyterlab-pygments': '0.1.2',\\n        'jupyterlab-widgets': '1.0.2',\\n        'jupytext': '1.11.5',\\n        'kiwisolver': '1.3.2',\\n        'latexcodec': '2.0.1',\\n        'linkify-it-py': '1.0.3',\\n        'llvmlite': '0.38.0',\\n        'locket': '0.2.1',\\n        'loguru': '0.5.3',\\n        'lxml': '4.7.1',\\n        'magicgui': '0.3.5.dev18+g78d1687',\\n        'markdown-it-py': '1.1.0',\\n        'markupsafe': '2.0.1',\\n        'matplotlib': '3.5.1',\\n        'matplotlib-inline': '0.1.3',\\n        'mccabe': '0.6.1',\\n        'mdit-py-plugins': '0.2.8',\\n        'meshzoo': '0.9.2',\\n        'mistune': '0.8.4',\\n        'mrc': '0.2.0',\\n        'msgpack': '1.0.3',\\n        'multidict': '5.2.0',\\n        'mypy': '0.931',\\n        'mypy-extensions': '0.4.3',\\n        'myst-nb': '0.13.1',\\n        'myst-parser': '0.15.2',\\n        'napari': '0.4.14rc1.dev4+gcdf58d44b',\\n        'napari-aicsimageio': '0.4.1',\\n        'napari-console': '0.0.4',\\n        'napari-dv': '0.2.7.dev0+g54e1691.d20220128',\\n        'napari-error-reporter': '0.1.dev1+g1b388f2.d20220201',\\n        'napari-hello': '0.0.1',\\n        'napari-math': '0.0.1a0',\\n        'napari-micromanager': '0.0.1rc6.dev14+g5149788.d20220128',\\n        'napari-molecule-reader': '0.1.2.dev1+gc2ec2de',\\n        'napari-plugin-engine': '0.2.0',\\n        'napari-pyclesperanto-assistant': '0.12.0',\\n        'napari-skimage-regionprops': '0.2.9',\\n        'napari-svg': '0.1.6',\\n        'napari-time-slicer': '0.4.2',\\n        'napari-workflows': '0.1.2',\\n        'natsort': '8.0.2',\\n        'nbclient': '0.5.10',\\n        'nbconvert': '6.4.0',\\n        'nbdime': '3.1.1',\\n        'nbformat': '5.1.3',\\n        'nd2': '0.1.4',\\n        'nest-asyncio': '1.5.4',\\n        'networkx': '2.6.3',\\n        'nodeenv': '1.6.0',\\n        'notebook': '6.4.7',\\n        'npe2': '0.1.1',\\n        'numba': '0.55.0',\\n        'numcodecs': '0.9.1',\\n        'numpy': '1.20.3',\\n        'numpydoc': '1.1.0',\\n        'ome-types': '0.2.10',\\n        'opencv-python-headless': '4.5.5.62',\\n        'packaging': '21.3',\\n        'pandas': '1.3.5',\\n        'pandocfilters': '1.5.0',\\n        'paramiko': '2.9.2',\\n        'parso': '0.8.3',\\n        'partd': '1.2.0',\\n        'pathspec': '0.9.0',\\n        'pdbpp': '0.10.3',\\n        'peewee': '3.14.8',\\n        'pep517': '0.12.0',\\n        'pexpect': '4.8.0',\\n        'pickleshare': '0.7.5',\\n        'pillow': '8.4.0',\\n        'pint': '0.18',\\n        'pip': '21.3.1',\\n        'platformdirs': '2.4.1',\\n        'pluggy': '1.0.0',\\n        'pooch': '1.5.2',\\n        'poyo': '0.5.0',\\n        'pre-commit': '2.16.0',\\n        'prometheus-client': '0.12.0',\\n        'prompt-toolkit': '3.0.24',\\n        'psutil': '5.9.0',\\n        'psygnal': '0.2.0',\\n        'ptyprocess': '0.7.0',\\n        'pure-eval': '0.2.1',\\n        'py': '1.11.0',\\n        'pybtex': '0.24.0',\\n        'pybtex-docutils': '1.0.1',\\n        'pyclesperanto-prototype': '0.12.0',\\n        'pycodestyle': '2.8.0',\\n        'pycparser': '2.21',\\n        'pydantic': '1.9.0',\\n        'pydata-sphinx-theme': '0.7.2',\\n        'pyflakes': '2.2.0',\\n        'pygments': '2.11.2',\\n        'pymmcore': '10.1.1.70.5',\\n        'pymmcore-plus': '0.1.8',\\n        'pynacl': '1.5.0',\\n        'pyopencl': '2021.2.13',\\n        'pyopengl': '3.1.5',\\n        'pyparsing': '3.0.6',\\n        'pyperclip': '1.8.2',\\n        'pyrepl': '0.9.0',\\n        'pyro5': '5.13.1',\\n        'pyrsistent': '0.18.1',\\n        'pyside2': '5.15.2.1',\\n        'pytest': '6.2.5',\\n        'pytest-cookies': '0.6.1',\\n        'pytest-cov': '3.0.0',\\n        'pytest-faulthandler': '2.0.1',\\n        'pytest-order': '1.0.1',\\n        'pytest-qt': '4.0.2',\\n        'python-dateutil': '2.8.2',\\n        'python-dotenv': '0.19.2',\\n        'python-slugify': '5.0.2',\\n        'pytomlpp': '1.0.10',\\n        'pytools': '2021.2.9',\\n        'pytz': '2021.3',\\n        'pywavelets': '1.2.0',\\n        'pyyaml': '6.0',\\n        'pyzmq': '22.3.0',\\n        'qtconsole': '5.2.2',\\n        'qtpy': '2.0.0',\\n        'regex': '2021.11.10',\\n        'requests': '2.27.1',\\n        'rich': '11.0.0',\\n        'rmsd': '1.4',\\n        'ruamel.yaml': '0.17.20',\\n        'ruamel.yaml.clib': '0.2.6',\\n        'scikit-image': '0.19.1',\\n        'scipy': '1.7.3',\\n        'semgrep': '0.78.0',\\n        'send2trash': '1.8.0',\\n        'sentry-sdk': '1.5.4',\\n        'serpent': '1.40',\\n        'setuptools': '60.5.0',\\n        'shiboken2': '5.15.2.1',\\n        'six': '1.16.0',\\n        'smmap': '5.0.0',\\n        'sniffio': '1.2.0',\\n        'snowballstemmer': '2.2.0',\\n        'sortedcontainers': '2.4.0',\\n        'soupsieve': '2.3.1',\\n        'sourcery-cli': '0.10.0',\\n        'sphinx': '4.4.0',\\n        'sphinx-autodoc-typehints': '1.12.0',\\n        'sphinx-book-theme': '0.1.10',\\n        'sphinx-comments': '0.0.3',\\n        'sphinx-copybutton': '0.4.0',\\n        'sphinx-external-toc': '0.2.3',\\n        'sphinx-jupyterbook-latex': '0.4.6',\\n        'sphinx-multitoc-numbering': '0.1.3',\\n        'sphinx-panels': '0.6.0',\\n        'sphinx-tabs': '3.2.0',\\n        'sphinx-thebe': '0.0.10',\\n        'sphinx-togglebutton': '0.2.3',\\n        'sphinxcontrib-applehelp': '1.0.2',\\n        'sphinxcontrib-bibtex': '2.2.1',\\n        'sphinxcontrib-devhelp': '1.0.2',\\n        'sphinxcontrib-htmlhelp': '2.0.0',\\n        'sphinxcontrib-jsmath': '1.0.1',\\n        'sphinxcontrib-qthelp': '1.0.3',\\n        'sphinxcontrib-serializinghtml': '1.1.5',\\n        'sqlalchemy': '1.4.29',\\n        'stack-data': '0.1.4',\\n        'superqt': '0.2.5.post2.dev7+ga49bcd7',\\n        'tensorstore': '0.1.16',\\n        'terminado': '0.12.1',\\n        'testpath': '0.5.0',\\n        'text-unidecode': '1.3',\\n        'tifffile': '2021.11.2',\\n        'toml': '0.10.2',\\n        'tomli': '2.0.0',\\n        'toolz': '0.11.2',\\n        'torch': '1.10.1',\\n        'tornado': '6.1',\\n        'tox': '3.24.5',\\n        'tox-conda': '0.9.1',\\n        'tqdm': '4.62.3',\\n        'traitlets': '5.1.1',\\n        'transforms3d': '0.3.1',\\n        'transitions': '0.8.10',\\n        'typed-ast': '1.5.1',\\n        'typer': '0.4.0',\\n        'typing-extensions': '4.0.1',\\n        'uc-micro-py': '1.0.1',\\n        'urllib3': '1.26.8',\\n        'useq-schema': '0.1.1.dev13+g01d1b46.d20220120',\\n        'valerius': '0.2.0',\\n        'virtualenv': '20.13.0',\\n        'vispy': '0.9.4',\\n        'watchdog': '2.1.6',\\n        'wcmatch': '8.3',\\n        'wcwidth': '0.2.5',\\n        'webencodings': '0.5.1',\\n        'websocket-client': '1.2.3',\\n        'wheel': '0.37.1',\\n        'widgetsnbextension': '3.5.2',\\n        'wmctrl': '0.4',\\n        'wrapt': '1.13.3',\\n        'wurlitzer': '3.0.2',\\n        'xarray': '0.20.2',\\n        'xmlschema': '1.9.2',\\n        'yarl': '1.7.2',\\n        'zarr': '2.10.3',\\n        'zipp': '3.7.0'\\n    },\\n    'platform': 'python',\\n    'release': '0.4.14rc1.dev4+gcdf58d44b',\\n    'sdk': {\\n        'integrations': [\\n            'aiohttp',\\n            'argv',\\n            'atexit',\\n            'dedupe',\\n            'excepthook',\\n            'logging',\\n            'modules',\\n            'sqlalchemy',\\n            'stdlib',\\n            'threading',\\n            'tornado'\\n        ],\\n        'name': 'sentry.python',\\n        'packages': [{'name': 'pypi:sentry-sdk', 'version': '1.5.4'}],\\n        'version': '1.5.4'\\n    },\\n    'server_name': '',\\n    'tags': {\\n        'platform.name': 'MacOS 10.15.7',\\n        'platform.system': 'Darwin',\\n        'qtpy.API_NAME': 'PySide2',\\n        'qtpy.QT_VERSION': '5.15.2'\\n    },\\n    'timestamp': '2022-02-02T01:30:00.229122Z'\\n}\\n```\\n\\n</details>\\n\\n> ***NOTE**: in the opt-in dialog, there is a checkbox labeled \\\"include local variables\\\",\\nchecking this will include the value of variables in the local scope when an exception\\noccurs.  While these can be very useful when interpreting a bug report, they may\\noccasionally include local file path strings.  If that concerns you, please leave this\\nbox unchecked*\\n\\n## Install\\n\\nThis plugin requires napari version 0.4.15 or greater, or the `main` branch with PR\\n[napari/napari#4055](https://github.com/napari/napari/pull/4055).\\n\\nInstall via pip with:\\n\\n```sh\\npip install napari-error-reporter\\n```\\n\\nor in the built-in plugin installer (a restart will be required):\\n\\n<img width=\\\"503\\\" alt=\\\"Untitled\\\" src=\\\"https://user-images.githubusercontent.com/1609449/153915128-09a5e3d7-8561-4c17-b543-5ea172e3e860.png\\\">\\n\\n\\nThank you!!\\n\\n## Privacy FAQ\\n\\nEven with the multiple layers of opt-ins, and the attempts to wipe all personal info\\nprior to sending reports, we understand that privacy is always a concern.\\n\\n### Do you collect personal info?\\n\\nWe make every attempt to collect ***no*** personally identifiable information.  No\\nname, location, IP address, etc...  We do collect your\\n([`uuid.getnode()`](https://docs.python.org/3.10/library/uuid.html#uuid.getnode)) to\\nbe able to track bug resolution over time. As mentioned above, allowing local\\nvariables to be collected may occasionally include a file path in the log.\\nIf that concerns you, please leave that unchecked.\\n\\n### Is this shipped with napari?\\n\\n`napari-error-reporter` is **not** bundled with napari or listed as a napari dependency.\\nIn order for reports to be sent, you must first install this plugin yourself, and then\\nopt in on the next launch.  If you uninstall the plugin, no more reports can be sent.\\n\\n### Who can access these reports?\\n\\nOnly the following napari core developers have access to these reports.\\nIf [this](https://raw.githubusercontent.com/tlambert03/napari-error-reporter/main/ADMINS)\\nlist changes in the future, you will be asked to opt-in again in napari:\\n\\n- Juan Nunez-Iglesias ([@jni](https://github.com/jni))\\n- Talley Lambert ([@tlambert03](https://github.com/tlambert03))\\n\\n*This plugin is **not** associated with the Chan Zuckerberg Initiative*.\\n\\n### How will these reports be used?\\n\\nCommonly occuring errors will be will be manually purged of file paths and\\nlocal variables and posted to https://github.com/napari/napari/issues\\n\\n### How long is data retained\\n\\nSentry retains event data for 90 days by default.  For complete details,\\nsee Sentry's page on [Security & Compliance](https://sentry.io/security/)\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"🐛 napari-error-reporter\\n\\n\\n\\n\\n\\nWant to help out napari?  Install this plugin!\\nThis plugin will automatically send error reports to napari (via\\nsentry.io) whenever an exception occurs while you are using\\nnapari.\\nThe first time you run napari after installing this plugin an opt-in\\nnotification will appear (Be sure to click \\\"yes\\\", otherwise no reports will be\\ncollected or sent).  You may opt back out at any time in napari's help menu.\\nEvery effort is made to strip these reports of personally identifiable\\ninformation.  Here is an example exception event:\\n\\nExample bug report\\n\\n```python\\n{\\n    'breadcrumbs': {\\n        'values': [\\n            {\\n                'category': 'subprocess',\\n                'data': {},\\n                'message': 'sw_vers -productVersion',\\n                'timestamp': '2022-02-02T01:30:00.216738Z',\\n                'type': 'subprocess'\\n            }\\n        ]\\n    },\\n    'contexts': {\\n        'runtime': {\\n            'build': '3.9.9 | packaged by conda-forge | (main, Dec 20 2021, 02:41:37) \\\\n[Clang 11.1.0 ]',\\n            'name': 'CPython',\\n            'version': '3.9.9'\\n        }\\n    },\\n    'environment': 'macOS-10.15.7-x86_64-i386-64bit',\\n    'event_id': '02dd8ddd3a4b4743af3d7d7a09949df4',\\n    'exception': {\\n        'values': [\\n            {\\n                'mechanism': None,\\n                'module': None,\\n                'stacktrace': {\\n                    'frames': [\\n                        {\\n                            'context_line': '                x = 1 / 0',\\n                            'filename': 'napari_error_reporter/_util.py',\\n                            'function': 'get_sample_event',\\n                            'in_app': True,\\n                            'lineno': 130,\\n                            'module': 'napari_error_reporter._util',\\n                            'post_context': [\\n                                '            except Exception:',\\n                                '                with sentry_sdk.push_scope() as scope:',\\n                                '                    for k, v in _get_tags().items():',\\n                                '                        scope.set_tag(k, v)',\\n                                '                    del v, k, scope'\\n                            ],\\n                            'pre_context': [\\n                                \\\"            # remove locals that wouldn't really be there\\\",\\n                                '            del settings, _trans, kwargs, client, EVENT',\\n                                '            try:',\\n                                '                some_variable = 1',\\n                                '                another_variable = \\\"my_string\\\"'\\n                            ]\\n                        }\\n                    ]\\n                },\\n                'type': 'ZeroDivisionError',\\n                'value': 'division by zero'\\n            }\\n        ]\\n    },\\n    'extra': {'sys.argv': ['napari']},\\n    'level': 'error',\\n    'modules': {\\n        'aicsimageio': '4.5.2',\\n        'aicspylibczi': '3.0.4',\\n        'aiohttp': '3.8.1',\\n        'aiosignal': '1.2.0',\\n        'alabaster': '0.7.12',\\n        'anyio': '3.5.0',\\n        'appdirs': '1.4.4',\\n        'appnope': '0.1.2',\\n        'argon2-cffi': '21.3.0',\\n        'argon2-cffi-bindings': '21.2.0',\\n        'arrow': '1.2.1',\\n        'asciitree': '0.3.3',\\n        'asttokens': '2.0.5',\\n        'async-timeout': '4.0.2',\\n        'atomium': '1.0.11',\\n        'attrs': '21.4.0',\\n        'autopep8': '1.6.0',\\n        'babel': '2.9.1',\\n        'backcall': '0.2.0',\\n        'bcrypt': '3.2.0',\\n        'beautifulsoup4': '4.10.0',\\n        'binaryornot': '0.4.4',\\n        'black': '20.8b1',\\n        'bleach': '4.1.0',\\n        'bracex': '2.2.1',\\n        'build': '0.7.0',\\n        'cachey': '0.2.1',\\n        'cellpose': '0.6.5',\\n        'certifi': '2021.10.8',\\n        'cffi': '1.15.0',\\n        'cfgv': '3.3.1',\\n        'chardet': '4.0.0',\\n        'charset-normalizer': '2.0.10',\\n        'check-manifest': '0.47',\\n        'click': '7.1.2',\\n        'click-option-group': '0.5.3',\\n        'cloudpickle': '2.0.0',\\n        'colorama': '0.4.4',\\n        'commonmark': '0.9.1',\\n        'cookiecutter': '1.7.3',\\n        'coverage': '6.2',\\n        'cryptography': '36.0.1',\\n        'cycler': '0.11.0',\\n        'dask': '2022.1.0',\\n        'debugpy': '1.5.1',\\n        'decorator': '5.1.1',\\n        'defusedxml': '0.7.1',\\n        'distlib': '0.3.4',\\n        'dnspython': '2.2.0',\\n        'docstring-parser': '0.13',\\n        'docutils': '0.16',\\n        'elementpath': '2.4.0',\\n        'email-validator': '1.1.3',\\n        'entrypoints': '0.3',\\n        'executing': '0.8.2',\\n        'fancycompleter': '0.9.1',\\n        'fasteners': '0.17.2',\\n        'fastremap': '1.12.2',\\n        'filelock': '3.4.2',\\n        'flake8': '3.8.4',\\n        'fonttools': '4.28.5',\\n        'freetype-py': '2.2.0',\\n        'frozenlist': '1.3.0',\\n        'fsspec': '2022.1.0',\\n        'furo': '2022.1.2',\\n        'gitdb': '4.0.9',\\n        'gitpython': '3.1.26',\\n        'greenlet': '1.1.2',\\n        'heapdict': '1.0.1',\\n        'hsluv': '5.0.2',\\n        'hypothesis': '6.35.1',\\n        'identify': '2.4.4',\\n        'idna': '3.3',\\n        'imagecodecs': '2021.11.20',\\n        'imageio': '2.10.5',\\n        'imageio-ffmpeg': '0.4.5',\\n        'imagesize': '1.3.0',\\n        'importlib-metadata': '4.10.1',\\n        'iniconfig': '1.1.1',\\n        'install': '1.3.5',\\n        'intervaltree': '3.1.0',\\n        'ipykernel': '6.7.0',\\n        'ipython': '8.0.0',\\n        'ipython-genutils': '0.2.0',\\n        'ipywidgets': '7.6.5',\\n        'jedi': '0.18.1',\\n        'jinja2': '3.0.3',\\n        'jinja2-time': '0.2.0',\\n        'jsonschema': '3.2.0',\\n        'jupyter': '1.0.0',\\n        'jupyter-book': '0.12.1',\\n        'jupyter-cache': '0.4.3',\\n        'jupyter-client': '7.1.1',\\n        'jupyter-console': '6.4.0',\\n        'jupyter-core': '4.9.1',\\n        'jupyter-server': '1.13.3',\\n        'jupyter-server-mathjax': '0.2.3',\\n        'jupyter-sphinx': '0.3.2',\\n        'jupyterlab-pygments': '0.1.2',\\n        'jupyterlab-widgets': '1.0.2',\\n        'jupytext': '1.11.5',\\n        'kiwisolver': '1.3.2',\\n        'latexcodec': '2.0.1',\\n        'linkify-it-py': '1.0.3',\\n        'llvmlite': '0.38.0',\\n        'locket': '0.2.1',\\n        'loguru': '0.5.3',\\n        'lxml': '4.7.1',\\n        'magicgui': '0.3.5.dev18+g78d1687',\\n        'markdown-it-py': '1.1.0',\\n        'markupsafe': '2.0.1',\\n        'matplotlib': '3.5.1',\\n        'matplotlib-inline': '0.1.3',\\n        'mccabe': '0.6.1',\\n        'mdit-py-plugins': '0.2.8',\\n        'meshzoo': '0.9.2',\\n        'mistune': '0.8.4',\\n        'mrc': '0.2.0',\\n        'msgpack': '1.0.3',\\n        'multidict': '5.2.0',\\n        'mypy': '0.931',\\n        'mypy-extensions': '0.4.3',\\n        'myst-nb': '0.13.1',\\n        'myst-parser': '0.15.2',\\n        'napari': '0.4.14rc1.dev4+gcdf58d44b',\\n        'napari-aicsimageio': '0.4.1',\\n        'napari-console': '0.0.4',\\n        'napari-dv': '0.2.7.dev0+g54e1691.d20220128',\\n        'napari-error-reporter': '0.1.dev1+g1b388f2.d20220201',\\n        'napari-hello': '0.0.1',\\n        'napari-math': '0.0.1a0',\\n        'napari-micromanager': '0.0.1rc6.dev14+g5149788.d20220128',\\n        'napari-molecule-reader': '0.1.2.dev1+gc2ec2de',\\n        'napari-plugin-engine': '0.2.0',\\n        'napari-pyclesperanto-assistant': '0.12.0',\\n        'napari-skimage-regionprops': '0.2.9',\\n        'napari-svg': '0.1.6',\\n        'napari-time-slicer': '0.4.2',\\n        'napari-workflows': '0.1.2',\\n        'natsort': '8.0.2',\\n        'nbclient': '0.5.10',\\n        'nbconvert': '6.4.0',\\n        'nbdime': '3.1.1',\\n        'nbformat': '5.1.3',\\n        'nd2': '0.1.4',\\n        'nest-asyncio': '1.5.4',\\n        'networkx': '2.6.3',\\n        'nodeenv': '1.6.0',\\n        'notebook': '6.4.7',\\n        'npe2': '0.1.1',\\n        'numba': '0.55.0',\\n        'numcodecs': '0.9.1',\\n        'numpy': '1.20.3',\\n        'numpydoc': '1.1.0',\\n        'ome-types': '0.2.10',\\n        'opencv-python-headless': '4.5.5.62',\\n        'packaging': '21.3',\\n        'pandas': '1.3.5',\\n        'pandocfilters': '1.5.0',\\n        'paramiko': '2.9.2',\\n        'parso': '0.8.3',\\n        'partd': '1.2.0',\\n        'pathspec': '0.9.0',\\n        'pdbpp': '0.10.3',\\n        'peewee': '3.14.8',\\n        'pep517': '0.12.0',\\n        'pexpect': '4.8.0',\\n        'pickleshare': '0.7.5',\\n        'pillow': '8.4.0',\\n        'pint': '0.18',\\n        'pip': '21.3.1',\\n        'platformdirs': '2.4.1',\\n        'pluggy': '1.0.0',\\n        'pooch': '1.5.2',\\n        'poyo': '0.5.0',\\n        'pre-commit': '2.16.0',\\n        'prometheus-client': '0.12.0',\\n        'prompt-toolkit': '3.0.24',\\n        'psutil': '5.9.0',\\n        'psygnal': '0.2.0',\\n        'ptyprocess': '0.7.0',\\n        'pure-eval': '0.2.1',\\n        'py': '1.11.0',\\n        'pybtex': '0.24.0',\\n        'pybtex-docutils': '1.0.1',\\n        'pyclesperanto-prototype': '0.12.0',\\n        'pycodestyle': '2.8.0',\\n        'pycparser': '2.21',\\n        'pydantic': '1.9.0',\\n        'pydata-sphinx-theme': '0.7.2',\\n        'pyflakes': '2.2.0',\\n        'pygments': '2.11.2',\\n        'pymmcore': '10.1.1.70.5',\\n        'pymmcore-plus': '0.1.8',\\n        'pynacl': '1.5.0',\\n        'pyopencl': '2021.2.13',\\n        'pyopengl': '3.1.5',\\n        'pyparsing': '3.0.6',\\n        'pyperclip': '1.8.2',\\n        'pyrepl': '0.9.0',\\n        'pyro5': '5.13.1',\\n        'pyrsistent': '0.18.1',\\n        'pyside2': '5.15.2.1',\\n        'pytest': '6.2.5',\\n        'pytest-cookies': '0.6.1',\\n        'pytest-cov': '3.0.0',\\n        'pytest-faulthandler': '2.0.1',\\n        'pytest-order': '1.0.1',\\n        'pytest-qt': '4.0.2',\\n        'python-dateutil': '2.8.2',\\n        'python-dotenv': '0.19.2',\\n        'python-slugify': '5.0.2',\\n        'pytomlpp': '1.0.10',\\n        'pytools': '2021.2.9',\\n        'pytz': '2021.3',\\n        'pywavelets': '1.2.0',\\n        'pyyaml': '6.0',\\n        'pyzmq': '22.3.0',\\n        'qtconsole': '5.2.2',\\n        'qtpy': '2.0.0',\\n        'regex': '2021.11.10',\\n        'requests': '2.27.1',\\n        'rich': '11.0.0',\\n        'rmsd': '1.4',\\n        'ruamel.yaml': '0.17.20',\\n        'ruamel.yaml.clib': '0.2.6',\\n        'scikit-image': '0.19.1',\\n        'scipy': '1.7.3',\\n        'semgrep': '0.78.0',\\n        'send2trash': '1.8.0',\\n        'sentry-sdk': '1.5.4',\\n        'serpent': '1.40',\\n        'setuptools': '60.5.0',\\n        'shiboken2': '5.15.2.1',\\n        'six': '1.16.0',\\n        'smmap': '5.0.0',\\n        'sniffio': '1.2.0',\\n        'snowballstemmer': '2.2.0',\\n        'sortedcontainers': '2.4.0',\\n        'soupsieve': '2.3.1',\\n        'sourcery-cli': '0.10.0',\\n        'sphinx': '4.4.0',\\n        'sphinx-autodoc-typehints': '1.12.0',\\n        'sphinx-book-theme': '0.1.10',\\n        'sphinx-comments': '0.0.3',\\n        'sphinx-copybutton': '0.4.0',\\n        'sphinx-external-toc': '0.2.3',\\n        'sphinx-jupyterbook-latex': '0.4.6',\\n        'sphinx-multitoc-numbering': '0.1.3',\\n        'sphinx-panels': '0.6.0',\\n        'sphinx-tabs': '3.2.0',\\n        'sphinx-thebe': '0.0.10',\\n        'sphinx-togglebutton': '0.2.3',\\n        'sphinxcontrib-applehelp': '1.0.2',\\n        'sphinxcontrib-bibtex': '2.2.1',\\n        'sphinxcontrib-devhelp': '1.0.2',\\n        'sphinxcontrib-htmlhelp': '2.0.0',\\n        'sphinxcontrib-jsmath': '1.0.1',\\n        'sphinxcontrib-qthelp': '1.0.3',\\n        'sphinxcontrib-serializinghtml': '1.1.5',\\n        'sqlalchemy': '1.4.29',\\n        'stack-data': '0.1.4',\\n        'superqt': '0.2.5.post2.dev7+ga49bcd7',\\n        'tensorstore': '0.1.16',\\n        'terminado': '0.12.1',\\n        'testpath': '0.5.0',\\n        'text-unidecode': '1.3',\\n        'tifffile': '2021.11.2',\\n        'toml': '0.10.2',\\n        'tomli': '2.0.0',\\n        'toolz': '0.11.2',\\n        'torch': '1.10.1',\\n        'tornado': '6.1',\\n        'tox': '3.24.5',\\n        'tox-conda': '0.9.1',\\n        'tqdm': '4.62.3',\\n        'traitlets': '5.1.1',\\n        'transforms3d': '0.3.1',\\n        'transitions': '0.8.10',\\n        'typed-ast': '1.5.1',\\n        'typer': '0.4.0',\\n        'typing-extensions': '4.0.1',\\n        'uc-micro-py': '1.0.1',\\n        'urllib3': '1.26.8',\\n        'useq-schema': '0.1.1.dev13+g01d1b46.d20220120',\\n        'valerius': '0.2.0',\\n        'virtualenv': '20.13.0',\\n        'vispy': '0.9.4',\\n        'watchdog': '2.1.6',\\n        'wcmatch': '8.3',\\n        'wcwidth': '0.2.5',\\n        'webencodings': '0.5.1',\\n        'websocket-client': '1.2.3',\\n        'wheel': '0.37.1',\\n        'widgetsnbextension': '3.5.2',\\n        'wmctrl': '0.4',\\n        'wrapt': '1.13.3',\\n        'wurlitzer': '3.0.2',\\n        'xarray': '0.20.2',\\n        'xmlschema': '1.9.2',\\n        'yarl': '1.7.2',\\n        'zarr': '2.10.3',\\n        'zipp': '3.7.0'\\n    },\\n    'platform': 'python',\\n    'release': '0.4.14rc1.dev4+gcdf58d44b',\\n    'sdk': {\\n        'integrations': [\\n            'aiohttp',\\n            'argv',\\n            'atexit',\\n            'dedupe',\\n            'excepthook',\\n            'logging',\\n            'modules',\\n            'sqlalchemy',\\n            'stdlib',\\n            'threading',\\n            'tornado'\\n        ],\\n        'name': 'sentry.python',\\n        'packages': [{'name': 'pypi:sentry-sdk', 'version': '1.5.4'}],\\n        'version': '1.5.4'\\n    },\\n    'server_name': '',\\n    'tags': {\\n        'platform.name': 'MacOS 10.15.7',\\n        'platform.system': 'Darwin',\\n        'qtpy.API_NAME': 'PySide2',\\n        'qtpy.QT_VERSION': '5.15.2'\\n    },\\n    'timestamp': '2022-02-02T01:30:00.229122Z'\\n}\\n```\\n\\n\\n\\nNOTE: in the opt-in dialog, there is a checkbox labeled \\\"include local variables\\\",\\nchecking this will include the value of variables in the local scope when an exception\\noccurs.  While these can be very useful when interpreting a bug report, they may\\noccasionally include local file path strings.  If that concerns you, please leave this\\nbox unchecked\\n\\nInstall\\nThis plugin requires napari version 0.4.15 or greater, or the main branch with PR\\nnapari/napari#4055.\\nInstall via pip with:\\nsh\\npip install napari-error-reporter\\nor in the built-in plugin installer (a restart will be required):\\n\\nThank you!!\\nPrivacy FAQ\\nEven with the multiple layers of opt-ins, and the attempts to wipe all personal info\\nprior to sending reports, we understand that privacy is always a concern.\\nDo you collect personal info?\\nWe make every attempt to collect no personally identifiable information.  No\\nname, location, IP address, etc...  We do collect your\\n(uuid.getnode()) to\\nbe able to track bug resolution over time. As mentioned above, allowing local\\nvariables to be collected may occasionally include a file path in the log.\\nIf that concerns you, please leave that unchecked.\\nIs this shipped with napari?\\nnapari-error-reporter is not bundled with napari or listed as a napari dependency.\\nIn order for reports to be sent, you must first install this plugin yourself, and then\\nopt in on the next launch.  If you uninstall the plugin, no more reports can be sent.\\nWho can access these reports?\\nOnly the following napari core developers have access to these reports.\\nIf this\\nlist changes in the future, you will be asked to opt-in again in napari:\\n\\nJuan Nunez-Iglesias (@jni)\\nTalley Lambert (@tlambert03)\\n\\nThis plugin is not associated with the Chan Zuckerberg Initiative.\\nHow will these reports be used?\\nCommonly occuring errors will be will be manually purged of file paths and\\nlocal variables and posted to https://github.com/napari/napari/issues\\nHow long is data retained\\nSentry retains event data for 90 days by default.  For complete details,\\nsee Sentry's page on Security & Compliance\",\"development_status\":[\"Development Status :: 3 - Alpha\"],\"display_name\":\"Napari Error Reporter\",\"documentation\":\"\",\"first_released\":\"2022-02-13T12:21:26.571245Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-error-reporter\",\"npe2\":true,\"operating_system\":[],\"plugin_types\":[],\"project_site\":\"https://github.com/tlambert03/napari-error-reporter\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-06-21T16:36:23.036467Z\",\"report_issues\":\"\",\"requirements\":[\"appdirs\",\"qtpy\",\"sentry-sdk\",\"black ; extra == 'dev'\",\"flake8 ; extra == 'dev'\",\"flake8-docstrings ; extra == 'dev'\",\"ipython ; extra == 'dev'\",\"isort ; extra == 'dev'\",\"jedi (<0.18.0) ; extra == 'dev'\",\"mypy ; extra == 'dev'\",\"pre-commit ; extra == 'dev'\",\"pydocstyle ; extra == 'dev'\",\"pytest ; extra == 'dev'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"tox ; extra == 'testing'\",\"tox-conda ; extra == 'testing'\"],\"summary\":\"Opt-in automated bug/error reporting for napari\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.3.1\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Pranjal Dhole\"},{\"name\":\"Duway Nicolas Lesmes Leon\"}],\"code_repository\":\"https://github.com/yapic/napari-hdf5-labels-io\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-hdf5-labels-io\"}],\"description\":\"# Description\\n\\nThis IO plugin lets you to store your progress in a single file (.h5 extension). It stores not only the layer's data but also its metadata, meaning that in some way, this IO can be seen as a project file generator.\\n\\nThe current supported layer types are: images, labels and dots.\\n\\nThis plugin was developed to create a connection between napari (for labeling) and YAPiC (a deep learning segmentation tool).\\n\\n# Who is this for?\\n\\nThis plugin is meant to be used for any napari user wanting to store their progress in a single file and for those which use napari as a labeling tool.\\n\\nIt supports any data dimensionality and it was designed to improve the memory efficiency when storing label layers.\\n\\nAdditionally, YAPiC supports the files generated by this IO to perform image segmentation.\\n\\n# Quick start\\n\\n## Saving .h5 files\\n\\nWith `napari-hdf5-labels-io installed`, use napari as alway. once you are done, click in File, click in Save Selected Layer(s)... (Ctrl+S) or Save All Layers... (Ctrl+Shift+S) and write the output file name as `filename.h5`. Including the \\\".h5\\\" extension at the end of the name will automatically activate the plugin.\\n\\n## Opening .h5 files\\n\\nTo open a .h5 file written with this plugin, you can open this file as any other (either by the Open File option in the File menu or dragging it to the main window).\",\"description_content_type\":\"text/markdown\",\"description_text\":\"Description\\nThis IO plugin lets you to store your progress in a single file (.h5 extension). It stores not only the layer's data but also its metadata, meaning that in some way, this IO can be seen as a project file generator.\\nThe current supported layer types are: images, labels and dots.\\nThis plugin was developed to create a connection between napari (for labeling) and YAPiC (a deep learning segmentation tool).\\nWho is this for?\\nThis plugin is meant to be used for any napari user wanting to store their progress in a single file and for those which use napari as a labeling tool.\\nIt supports any data dimensionality and it was designed to improve the memory efficiency when storing label layers.\\nAdditionally, YAPiC supports the files generated by this IO to perform image segmentation.\\nQuick start\\nSaving .h5 files\\nWith napari-hdf5-labels-io installed, use napari as alway. once you are done, click in File, click in Save Selected Layer(s)... (Ctrl+S) or Save All Layers... (Ctrl+Shift+S) and write the output file name as filename.h5. Including the \\\".h5\\\" extension at the end of the name will automatically activate the plugin.\\nOpening .h5 files\\nTo open a .h5 file written with this plugin, you can open this file as any other (either by the Open File option in the File menu or dragging it to the main window).\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"napari-hdf5-labels-io\",\"documentation\":\"https://yapic.github.io/napari-hdf5-labels-io/\",\"first_released\":\"2021-03-04T09:44:01.947204Z\",\"license\":\"GPL-3.0\",\"name\":\"napari-hdf5-labels-io\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\",\"writer\"],\"project_site\":\"https://yapic.github.io/napari-hdf5-labels-io/\",\"python_version\":\"<3.9\",\"reader_file_extensions\":[\"*\"],\"release_date\":\"2021-11-30T06:00:41.776331Z\",\"report_issues\":\"https://github.com/yapic/napari-hdf5-labels-io/issues\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"typing\",\"numpy\",\"sparse\",\"h5py (==2.10.0)\",\"zarr\"],\"summary\":\"Napari plugin to store set of layers in a .h5 file. Label layer are stored in a sparse representation.\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.3.dev16\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[\"labels\",\"points\",\"image\"]}",
  "{\"authors\":[{\"name\":\"Johannes Müller\"}],\"code_repository\":\"https://github.com/jo-mueller/napari-stl-exporter.git\",\"description\":\"# napari-stl-exporter\\n\\n[![License](https://img.shields.io/pypi/l/napari-stl-exporter.svg?color=green)](https://github.com/jo-mueller/napari-stl-exporter/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-stl-exporter.svg?color=green)](https://pypi.org/project/napari-stl-exporter)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-stl-exporter.svg?color=green)](https://python.org)\\n[![tests](https://github.com/jo-mueller/napari-stl-exporter/workflows/tests/badge.svg)](https://github.com/jo-mueller/napari-stl-exporter/actions)\\n[![codecov](https://codecov.io/gh/jo-mueller/napari-stl-exporter/branch/main/graph/badge.svg?token=9zctLzazD9)](https://codecov.io/gh/jo-mueller/napari-stl-exporter)\\n\\nThis plugin allows to import and export surface data in Napari to common file formats. The generated file formats can be read by other common applications, and - in particular - allow *3D-printing*.\\n\\n![](https://github.com/jo-mueller/napari-stl-exporter/raw/main/doc/model_and_printed_model.png)\\n\\n\\n### Supported file formats:\\nCurrently supported file formats for export include and rely on the [vedo io API](https://vedo.embl.es/autodocs/content/vedo/io.html#vedo.io).\\n* *.stl*: [Standard triangle language](https://en.wikipedia.org/wiki/STL_%28file_format%29)\\n* *.ply*: [Polygon file format](https://en.wikipedia.org/wiki/PLY_(file_format))\\n* *.obj*: [Wavefront object](https://en.wikipedia.org/wiki/Wavefront_.obj_file)\\n\\n### Supported Napari layers:\\n\\nCurrently supported Napari layer types are:\\n* [Surface layers](https://napari.org/howtos/layers/surface.html)\\n* [Label layers](https://napari.org/howtos/layers/labels.html): The label data is converted to surface data under the hood using the [marching cubes algorithm](https://scikit-image.org/docs/dev/api/skimage.measure.html#skimage.measure.marching_cubes) implemented in [scikit-image](https://scikit-image.org/) and is then exported using [Vedo](https://vedo.embl.es/). Warning: This can be slow for large image data!\\n\\n### Import/export\\n\\n**Interactively:** To export the data, simply save the selected layer with `File > Save Selected Layer(s)` and specify the file ending to be `some_file.[file_ending]`, for supported file types, see above. Similarly, supported file types can be imported into Napari with `File > `\\n\\n**From code**: A [Napari Label layer](https://napari.org/api/napari.layers.Labels.html) can be added to the viewer as described in the [napari reference](https://napari.org/gallery/add_labels.html?highlight=add_labels) with this code snippet:\\n\\n```python\\nimport napari\\nimport numpy as np\\n\\n# Load and binarize image\\nlabel_image = np.zeros((100, 100, 100), dtype=int)\\nlabel_image[25:75, 25:75, 25:75] = 1\\n\\n# Add data to viewer\\nviewer = napari.Viewer()\\nlabel_layer = viewer.add_labels(data, name='3D object')\\n\\n# save the layer as 3D printable file to disc\\nnapari.save_layers(r'/some/path/test.stl', [label_layer])\\n```\\n\\n### Sample data\\nYou can create sample label/surface data for export using the built-in functions as shown here:\\n\\n![](https://github.com/jo-mueller/napari-stl-exporter/raw/main/doc/1_sample_data.png)\\n\\n...or from code with\\n\\n```Python\\nimport napari_stl_exporter\\n\\npyramid = napari_stl_exporter.make_pyramid_surface()\\n\\n```\\n\\n### 3D-printing\\nTo actually send your object to a 3D-printer, it has to be further converted to the *.gcode* format with a Slicer program. The latter convert the 3D object to machine-relevant parameters (printing detail, motor trajectories, etc). Popular slicers are:\\n\\n* [Slic3r](https://slic3r.org/): Documentation [here](https://manual.slic3r.org/intro/overview)\\n* [Prusa Slicer](https://www.prusa3d.com/prusaslicer/): Tutorial [here](https://help.prusa3d.com/en/article/first-print-with-prusaslicer_1753)\\n\\n*Note*: You can also upload the STL file to [github.com](https://github.com) and interact with it in the browser:\\n\\n![](https://github.com/jo-mueller/napari-stl-exporter/raw/main/doc/pyramid_browser_screenshot.png)\\n\\n#### Digital elevation models\\n\\nDIgital elevation models (DEMs) can be printed with the napari-stl-exporter following these steps:\\n\\n1. Go to the [open topography repository](https://portal.opentopography.org/raster?opentopoID=OTSDEM.032021.4326.2) and select a region of your choice, then download it as a GeoTiff file (`.tif`, intensity encodes elevation)\\n2. Open the downloaded tif image use the image conversion plugin (´Plugins > napari-stl-exporter > 2D image to surface´) to convert the downloaded image to a surface. CHeck the `solidify` option to make it readily 3D-printable.\\n\\n![](https://github.com/jo-mueller/napari-stl-exporter/raw/main/doc/landscape_to_surface.png)\\n\\n3. Export the created surface layer as `.stl` or `.ply` file. Open it in your Slicer of choice (you may have to scale it according to the size limitations of your printer) and off you go!\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-stl-exporter` via [pip]:\\n\\n    pip install napari-stl-exporter\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-stl-exporter\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue](https://github.com/jo-mueller/napari-stl-exporter/issues) along with a detailed description or post to image.sc and tag ```El_Pollo_Diablo```\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-stl-exporter\\n\\n\\n\\n\\n\\nThis plugin allows to import and export surface data in Napari to common file formats. The generated file formats can be read by other common applications, and - in particular - allow 3D-printing.\\n\\nSupported file formats:\\nCurrently supported file formats for export include and rely on the vedo io API.\\n* .stl: Standard triangle language\\n* .ply: Polygon file format\\n* .obj: Wavefront object\\nSupported Napari layers:\\nCurrently supported Napari layer types are:\\n* Surface layers\\n* Label layers: The label data is converted to surface data under the hood using the marching cubes algorithm implemented in scikit-image and is then exported using Vedo. Warning: This can be slow for large image data!\\nImport/export\\nInteractively: To export the data, simply save the selected layer with File > Save Selected Layer(s) and specify the file ending to be some_file.[file_ending], for supported file types, see above. Similarly, supported file types can be imported into Napari with File >\\nFrom code: A Napari Label layer can be added to the viewer as described in the napari reference with this code snippet:\\n```python\\nimport napari\\nimport numpy as np\\nLoad and binarize image\\nlabel_image = np.zeros((100, 100, 100), dtype=int)\\nlabel_image[25:75, 25:75, 25:75] = 1\\nAdd data to viewer\\nviewer = napari.Viewer()\\nlabel_layer = viewer.add_labels(data, name='3D object')\\nsave the layer as 3D printable file to disc\\nnapari.save_layers(r'/some/path/test.stl', [label_layer])\\n```\\nSample data\\nYou can create sample label/surface data for export using the built-in functions as shown here:\\n\\n...or from code with\\n```Python\\nimport napari_stl_exporter\\npyramid = napari_stl_exporter.make_pyramid_surface()\\n```\\n3D-printing\\nTo actually send your object to a 3D-printer, it has to be further converted to the .gcode format with a Slicer program. The latter convert the 3D object to machine-relevant parameters (printing detail, motor trajectories, etc). Popular slicers are:\\n\\nSlic3r: Documentation here\\nPrusa Slicer: Tutorial here\\n\\nNote: You can also upload the STL file to github.com and interact with it in the browser:\\n\\nDigital elevation models\\nDIgital elevation models (DEMs) can be printed with the napari-stl-exporter following these steps:\\n\\nGo to the open topography repository and select a region of your choice, then download it as a GeoTiff file (.tif, intensity encodes elevation)\\nOpen the downloaded tif image use the image conversion plugin (´Plugins > napari-stl-exporter > 2D image to surface´) to convert the downloaded image to a surface. CHeck the solidify option to make it readily 3D-printable.\\n\\n\\n\\nExport the created surface layer as .stl or .ply file. Open it in your Slicer of choice (you may have to scale it according to the size limitations of your printer) and off you go!\\n\\n\\nThis napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-stl-exporter via pip:\\npip install napari-stl-exporter\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-stl-exporter\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description or post to image.sc and tag El_Pollo_Diablo\",\"development_status\":[\"Development Status :: 3 - Alpha\"],\"display_name\":\"napari-stl-exporter\",\"documentation\":\"https://pypi.org/project/napari-stl-exporter/\",\"first_released\":\"2021-10-06T12:49:22.533995Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-stl-exporter\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\",\"writer\",\"widget\",\"sample_data\"],\"project_site\":\"https://github.com/jo-mueller/napari-stl-exporter.git\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[\"*.stl\",\"*.ply\",\"*.obj\"],\"release_date\":\"2023-01-27T15:14:39.039369Z\",\"report_issues\":\"\",\"requirements\":[\"napari\",\"scikit-image\",\"vedo\",\"npe2\",\"numpy (<1.24.0)\"],\"summary\":\"Exports label images to 3D-printable stl files.\",\"support\":\"https://github.com/jo-mueller/napari-stl-exporter/issues\",\"twitter\":\"https://twitter.com/jm_mightypirate\",\"version\":\"0.1.4\",\"visibility\":\"public\",\"writer_file_extensions\":[\".obj\",\".ply\",\".stl\"],\"writer_save_layers\":[\"surface\",\"labels\"]}",
  "{\"authors\":[{\"email\":\"sebastian.gonzalez@embl.de\",\"name\":\"Sebastian Gonzalez-Tirado\"}],\"code_repository\":\"https://github.com/sebgoti/napari-spacetx-explorer\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-spacetx-explorer\"}],\"description\":\"# napari-spacetx-explorer\\n\\n[![License](https://img.shields.io/pypi/l/napari-spacetx-explorer.svg?color=green)](https://github.com/sebgoti/napari-spacetx-explorer/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-spacetx-explorer.svg?color=green)](https://pypi.org/project/napari-spacetx-explorer)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-spacetx-explorer.svg?color=green)](https://python.org)\\n[![tests](https://github.com/sebgoti/napari-spacetx-explorer/workflows/tests/badge.svg)](https://github.com/sebgoti/napari-spacetx-explorer/actions)\\n[![codecov](https://codecov.io/gh/sebgoti/napari-spacetx-explorer/branch/master/graph/badge.svg)](https://codecov.io/gh/sebgoti/napari-spacetx-explorer)\\n\\nA napari plugin for interactive visualization of decoded spots from spatial transcriptomic data stored as CSV\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\\n\\nThe plugin code was written by Sebastian Gonzalez-Tirado.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n## Reader hookspec\\n\\n`napari-spacetx-explorer` allows the user to open and visualize CSV files that\\nhave point-data stored in a given format. The main target is for users who\\nwant to analyze decoded spot maps from spatial omics experiments but it can\\nused as well for any other type of coordinate data where each point has assigned\\na label (e. g. a gene) as a string and the x and y-coordinates of the point's center.\\nThe header for these data must be 'target', 'xc', and 'yc', respectively.\\n\\n![img.png](https://github.com/sebgoti/napari-spacetx-explorer/raw/main/docs/Read_Hookspec.png)\\n\\n## Selecting genes\\n\\nAfter loading the gene/target maps it is possible to select specific groups for better visualization.\\nThis creates a new \\\"Points\\\" layer in napari with the selected groups displayed in different colors.\\n\\n![img.png](https://github.com/sebgoti/napari-spacetx-explorer/raw/main/docs/_function_hookspec.png)\\n\\n## Loading data in OME.ZARR format\\n\\nThe plugin napari-ome-zarr can be used to display whole-tissue images in addition to the spot maps produced with the \\n`napari-spacetx-explorer` plugin.\\n\\n![img.png](https://github.com/sebgoti/napari-spacetx-explorer/raw/main/docs/_ome_zarr_napari_spacetx_explorer.png)\\n\\n## Installation\\n\\nThe easiest installation is via the \\\"Install/Uninstall Plugins...\\\" under the Plugins menu in napari.  \\nAnother way is through [pip] \\n\\n    pip install napari-spacetx-explorer\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-spacetx-explorer\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems or would like some support, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/sebgoti/napari-spacetx-explorer/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-spacetx-explorer\\n\\n\\n\\n\\n\\nA napari plugin for interactive visualization of decoded spots from spatial transcriptomic data stored as CSV\\n\\nThis napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.\\nThe plugin code was written by Sebastian Gonzalez-Tirado.\\n\\nReader hookspec\\nnapari-spacetx-explorer allows the user to open and visualize CSV files that\\nhave point-data stored in a given format. The main target is for users who\\nwant to analyze decoded spot maps from spatial omics experiments but it can\\nused as well for any other type of coordinate data where each point has assigned\\na label (e. g. a gene) as a string and the x and y-coordinates of the point's center.\\nThe header for these data must be 'target', 'xc', and 'yc', respectively.\\n\\nSelecting genes\\nAfter loading the gene/target maps it is possible to select specific groups for better visualization.\\nThis creates a new \\\"Points\\\" layer in napari with the selected groups displayed in different colors.\\n\\nLoading data in OME.ZARR format\\nThe plugin napari-ome-zarr can be used to display whole-tissue images in addition to the spot maps produced with the \\nnapari-spacetx-explorer plugin.\\n\\nInstallation\\nThe easiest installation is via the \\\"Install/Uninstall Plugins...\\\" under the Plugins menu in napari.\\nAnother way is through pip \\npip install napari-spacetx-explorer\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-spacetx-explorer\\\" is free and open source software\\nIssues\\nIf you encounter any problems or would like some support, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 3 - Alpha\"],\"display_name\":\"napari-spacetx-explorer\",\"documentation\":\"https://github.com/sebgoti/napari-spacetx-explorer#README.md\",\"first_released\":\"2021-09-28T13:19:17.894005Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-spacetx-explorer\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\",\"widget\"],\"project_site\":\"https://github.com/sebgoti/napari-spacetx-explorer\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[\"*\"],\"release_date\":\"2021-12-10T10:54:48.965377Z\",\"report_issues\":\"https://github.com/sebgoti/napari-spacetx-explorer/issues\",\"requirements\":[\"napari\",\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"pandas\"],\"summary\":\"visualizer for spatial omic data\",\"support\":\"https://github.com/sebgoti/napari-spacetx-explorer/issues\",\"twitter\":\"\",\"version\":\"0.1.8\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"sylvain.prigent@inria.fr\",\"name\":\"Sylvain Prigent\"}],\"category\":{\"Supported data\":[\"2D\",\"3D\",\"Time series\"],\"Workflow step\":[\"Object tracking\"]},\"category_hierarchy\":{\"Supported data\":[[\"2D\"],[\"3D\"],[\"Time series\"]],\"Workflow step\":[[\"Object tracking\",\"Isolated object tracking\",\"Particle tracking\"]]},\"code_repository\":\"https://github.com/sylvainprigent/napari-stracking\",\"conda\":[],\"description\":\"# Description\\n\\nThe STracking suite provides a set of plugins for particles tracking in 2D+t and 3D+t images. \\n\\n<video src=\\\"https://raw.githubusercontent.com/sylvainprigent/napari-stracking/main/docs/images/intro.mp4\\\" controls=\\\"controls\\\" style=\\\"max-width: 730px;\\\">\\n</video>\\n   \\nA full documentation is available [here](<https://sylvainprigent.github.io/napari-stracking/quickstart.html>)   \\n    \\n## Installation\\n\\nYou can install `napari-stracking` via [pip]:\\n\\n    pip install napari-stracking\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-tracks-reader\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/sylvainprigent/napari-stracking/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"Description\\nThe STracking suite provides a set of plugins for particles tracking in 2D+t and 3D+t images. \\n\\n\\nA full documentation is available here \\nInstallation\\nYou can install napari-stracking via pip:\\npip install napari-stracking\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-tracks-reader\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari-stracking\",\"documentation\":\"https://sylvainprigent.github.io/napari-stracking/guide.html\",\"first_released\":\"2021-08-11T12:26:25.759055Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-stracking\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://sylvainprigent.github.io/napari-stracking/\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-04-29T08:24:06.683008Z\",\"report_issues\":\"https://github.com/sylvainprigent/napari-stracking/issues\",\"requirements\":[\"napari\",\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"stracking (>=0.1.9)\"],\"summary\":\"Linking and tracks analysis\",\"support\":\"https://github.com/sylvainprigent/napari-stracking/issues\",\"twitter\":\"https://twitter.com/SylvainMPrigent\",\"version\":\"0.1.9\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"David Bauer\"}],\"code_repository\":\"https://github.com/bauerdavid/napari-input-visualizer\",\"conda\":[],\"description\":\"# napari-input-visualizer\\n\\n[![License BSD-3](https://img.shields.io/pypi/l/napari-input-visualizer.svg?color=green)](https://github.com/bauerdavid/napari-input-visualizer/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-input-visualizer.svg?color=green)](https://pypi.org/project/napari-input-visualizer)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-input-visualizer.svg?color=green)](https://python.org)\\n[![tests](https://github.com/bauerdavid/napari-input-visualizer/workflows/tests/badge.svg)](https://github.com/bauerdavid/napari-input-visualizer/actions)\\n[![codecov](https://codecov.io/gh/bauerdavid/napari-input-visualizer/branch/main/graph/badge.svg)](https://codecov.io/gh/bauerdavid/napari-input-visualizer)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-input-visualizer)](https://napari-hub.org/plugins/napari-input-visualizer)\\n\\nVisualize keyboard and mouse button presses\\n\\nA simple tool to visualize input events like keyboard presses or mouse clicking and scrolling. Use it to create tutorial videos or to demonstrate a bug you encountered!\\n\\n## Demo:\\n\\n\\nhttps://user-images.githubusercontent.com/36735863/194586424-1e6288d3-2c2f-412c-a1cb-91d139f787bd.mp4\\n\\n\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/stable/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-input-visualizer` via [pip]:\\n\\n    pip install napari-input-visualizer\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/bauerdavid/napari-input-visualizer.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-input-visualizer\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/bauerdavid/napari-input-visualizer/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-input-visualizer\\n\\n\\n\\n\\n\\n\\nVisualize keyboard and mouse button presses\\nA simple tool to visualize input events like keyboard presses or mouse clicking and scrolling. Use it to create tutorial videos or to demonstrate a bug you encountered!\\nDemo:\\nhttps://user-images.githubusercontent.com/36735863/194586424-1e6288d3-2c2f-412c-a1cb-91d139f787bd.mp4\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-input-visualizer via pip:\\npip install napari-input-visualizer\\n\\nTo install latest development version :\\npip install git+https://github.com/bauerdavid/napari-input-visualizer.git\\n\\nContributing\\nContributions are very welcome.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-input-visualizer\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"Input Visualizer\",\"documentation\":\"https://github.com/bauerdavid/napari-input-visualizer#README.md\",\"first_released\":\"2022-10-10T07:00:43.684882Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-input-visualizer\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/bauerdavid/napari-input-visualizer\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-10-10T07:00:43.684882Z\",\"report_issues\":\"https://github.com/bauerdavid/napari-input-visualizer/issues\",\"requirements\":[\"numpy\",\"magicgui\",\"qtpy\",\"napari\",\"imageio-ffmpeg\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\"],\"summary\":\"Visualize keyboard and mouse button presses\",\"support\":\"https://github.com/bauerdavid/napari-input-visualizer/issues\",\"twitter\":\"\",\"version\":\"0.0.1\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Anna Nadtochiy\"}],\"code_repository\":\"https://github.com/LemonJust/napari-vodex\",\"description\":\"# napari-vodex\\n\\n[![License BSD-3](https://img.shields.io/pypi/l/napari-vodex.svg?color=green)](https://github.com/LemonJust/napari-vodex/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-vodex.svg?color=green)](https://pypi.org/project/napari-vodex)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-vodex.svg?color=green)](https://python.org)\\n[![tests](https://github.com/LemonJust/napari-vodex/workflows/tests/badge.svg)](https://github.com/LemonJust/napari-vodex/actions)\\n[![codecov](https://codecov.io/gh/LemonJust/napari-vodex/branch/main/graph/badge.svg)](https://codecov.io/gh/LemonJust/napari-vodex)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-vodex)](https://napari-hub.org/plugins/napari-vodex)\\n\\nA plugin to load volumetric data based on experimental conditions.\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/stable/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-vodex` via [pip]:\\n\\n    pip install napari-vodex\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/LemonJust/napari-vodex.git\\n\\n## How-To Guide\\n\\nTo get started with napari_vodex, please see details and examples in [How-To Guide](https://lemonjust.github.io/vodex/napari/how-to/) .\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-vodex\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/LemonJust/napari-vodex/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-vodex\\n\\n\\n\\n\\n\\n\\nA plugin to load volumetric data based on experimental conditions.\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-vodex via pip:\\npip install napari-vodex\\n\\nTo install latest development version :\\npip install git+https://github.com/LemonJust/napari-vodex.git\\n\\nHow-To Guide\\nTo get started with napari_vodex, please see details and examples in How-To Guide .\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-vodex\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"VoDEx\",\"documentation\":\"https://lemonjust.github.io/vodex/napari/\",\"first_released\":\"2023-01-04T03:46:46.999763Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-vodex\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/LemonJust/napari-vodex\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2023-02-04T00:21:47.197155Z\",\"report_issues\":\"https://github.com/LemonJust/napari-vodex/issues\",\"requirements\":[\"vodex (>=1.0.7)\",\"numpy\",\"magicgui\",\"qtpy\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\"],\"summary\":\"A napari plugin for VoDEx : Volumetric Data and Experiment Manager. Allows to load volumetric data based on experimental conditions.\",\"support\":\"https://github.com/LemonJust/napari-vodex/issues\",\"twitter\":\"\",\"version\":\"1.0.11\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"John Fozard\"}],\"code_repository\":\"https://github.com/jfozard/napari-sift-registration\",\"conda\":[],\"description\":\"# skimage-sift-registration\\n\\n[![License BSD-3](https://img.shields.io/pypi/l/napari-sift-registration.svg?color=green)](https://github.com/jfozard/napari-sift-registration/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-sift-registration.svg?color=green)](https://pypi.org/project/napari-sift-registration)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-sift-registration.svg?color=green)](https://python.org)\\n[![tests](https://github.com/jfozard/napari-sift-registration/workflows/tests/badge.svg)](https://github.com/jfozard/napari-sift-registration/actions)\\n[![codecov](https://codecov.io/gh/jfozard/napari-sift-registration/branch/main/graph/badge.svg)](https://codecov.io/gh/jfozard/napari-sift-registration)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-sift-registration)](https://napari-hub.org/plugins/napari-sift-registration)\\n\\nSimple plugin for 2D keypoint detection, and affine registration with RANSAC.\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\nIt uses the [scikit-image] SIFT keypoint detection routines to find distinctive image points and generate local descriptions of the image near them.\\nCorrespondences between the two images are then found by looking for pairs of keypoints, one in each of the two images, with closely matching descriptors.\\n\\nFor typical images, many of these correspondances will be wrong. To reduce these false correspondences, the plugin applies the RANSAC algorithm. This randomly selects a small subset of the matching pairs of keypoints, estimates the affine transformation between these keypoints, and then evaluates how many of the other pairs of keypoints also closely agree with this affine transformation (\\\"inliers\\\"). A large number of random samples are tested, and the transformation with the most inliers retained.\\n\\nThe plugin outputs two points layers, one for each image, containing all the corresponding SIFT keypoints betwee the two images, highlighting (and labelling) those that were retained as inliers after RANSAC matching. It also uses the estimated affine transformation between the two images to deform the \\\"moving\\\" image layer onto the \\\"fixed\\\" image layer.\\n\\nThis approach is an attempt to provide similar functionality to the Stephan Saalfeld's Fiji \\\"Extract SIFT Correspondences\\\" plugin [extract], and more-or-less\\njust provides a napari interface to the existing routines in scikit-image. There are great examples in the scikit-image documentation (e.g. [SIFT-example] and [RANSAC-example]) that can be used if you would like to use these routines in your own analysis scripts.\\n\\n\\n## Installation\\n\\nYou can install `napari-sift-registration` via [pip]:\\n\\n    pip install napari-sift-registration\\n\\nTo install the latest development version :\\n\\n    pip install git+https://github.com/jfozard/napari-sift-registration.git\\n\\n## Usage\\n\\n### Basic usage\\n\\n- Load two 2D single channel images in Napari.\\n- Select the menu item Plugins > \\n- Select these two images as the \\\"Moving image layer\\\" and the \\\"Fixed image layer\\\". The moving image will be deformed by the transformation to look like the fixed image.\\n- The remaining parameters are the default settings from scikit-image; try these default values first.\\n\\n### Advanced usage\\n\\nThe parameter values for SIFT feature detection, keypoint matching and RANSAC are accessible from the plugin gui. For further information about their use, see the appropriate scikit-image documentation:\\n\\nUpsampling before feature detection, maximum number of octaves, maximum number of scales in every octave, blur level of seed image, feature descriptor size, feature descriptor orientation bins: see [scikit-image-SIFT].\\n\\nClosest/next closest ratio: see [scikit-image-match_descriptors]\\n\\nMinimum number of points sampled for each RANSAC model, distance for points to be inliers in RANSAC model, maximum number of trials in RANSAC model: see [scikit-image-RANSAC]\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-sift-registration\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[extract]: https://imagej.net/plugins/feature-extraction\\n[scikit-image]: https://scikit-image.org/\\n[SIFT-example]: https://scikit-image.org/docs/stable/auto_examples/features_detection/plot_sift.html\\n[RANSAC-example]: https://scikit-image.org/docs/stable/auto_examples/transform/plot_matching.html\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[scikit-image-SIFT]: https://scikit-image.org/docs/stable/api/skimage.feature.html#skimage.feature.SIFT\\n[scikit-image-match_descriptors]: https://scikit-image.org/docs/stable/api/skimage.feature.html#skimage.feature.match_descriptors\\n[scikit-image-RANSAC]: https://scikit-image.org/docs/stable/api/skimage.measure.html#skimage.measure.ransac\\n\\n[file an issue]: https://github.com/jfozard/napari-sift-registration/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"skimage-sift-registration\\n\\n\\n\\n\\n\\n\\nSimple plugin for 2D keypoint detection, and affine registration with RANSAC.\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\nIt uses the scikit-image SIFT keypoint detection routines to find distinctive image points and generate local descriptions of the image near them.\\nCorrespondences between the two images are then found by looking for pairs of keypoints, one in each of the two images, with closely matching descriptors.\\nFor typical images, many of these correspondances will be wrong. To reduce these false correspondences, the plugin applies the RANSAC algorithm. This randomly selects a small subset of the matching pairs of keypoints, estimates the affine transformation between these keypoints, and then evaluates how many of the other pairs of keypoints also closely agree with this affine transformation (\\\"inliers\\\"). A large number of random samples are tested, and the transformation with the most inliers retained.\\nThe plugin outputs two points layers, one for each image, containing all the corresponding SIFT keypoints betwee the two images, highlighting (and labelling) those that were retained as inliers after RANSAC matching. It also uses the estimated affine transformation between the two images to deform the \\\"moving\\\" image layer onto the \\\"fixed\\\" image layer.\\nThis approach is an attempt to provide similar functionality to the Stephan Saalfeld's Fiji \\\"Extract SIFT Correspondences\\\" plugin extract, and more-or-less\\njust provides a napari interface to the existing routines in scikit-image. There are great examples in the scikit-image documentation (e.g. SIFT-example and RANSAC-example) that can be used if you would like to use these routines in your own analysis scripts.\\nInstallation\\nYou can install napari-sift-registration via pip:\\npip install napari-sift-registration\\n\\nTo install the latest development version :\\npip install git+https://github.com/jfozard/napari-sift-registration.git\\n\\nUsage\\nBasic usage\\n\\nLoad two 2D single channel images in Napari.\\nSelect the menu item Plugins > \\nSelect these two images as the \\\"Moving image layer\\\" and the \\\"Fixed image layer\\\". The moving image will be deformed by the transformation to look like the fixed image.\\nThe remaining parameters are the default settings from scikit-image; try these default values first.\\n\\nAdvanced usage\\nThe parameter values for SIFT feature detection, keypoint matching and RANSAC are accessible from the plugin gui. For further information about their use, see the appropriate scikit-image documentation:\\nUpsampling before feature detection, maximum number of octaves, maximum number of scales in every octave, blur level of seed image, feature descriptor size, feature descriptor orientation bins: see scikit-image-SIFT.\\nClosest/next closest ratio: see scikit-image-match_descriptors\\nMinimum number of points sampled for each RANSAC model, distance for points to be inliers in RANSAC model, maximum number of trials in RANSAC model: see scikit-image-RANSAC\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-sift-registration\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 3 - Alpha\"],\"display_name\":\"SIFTReg\",\"documentation\":\"https://github.com/jfozard/napari-sift-registration#README.md\",\"first_released\":\"2022-07-27T09:51:31.420762Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-sift-registration\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/jfozard/napari-sift-registration\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-07-27T10:03:11.152347Z\",\"report_issues\":\"https://github.com/jfozard/napari-sift-registration/issues\",\"requirements\":[\"numpy\",\"magicgui\",\"qtpy\",\"scikit-image\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\"],\"summary\":\"Simple plugin for SIFT keypoint detection, and affine registration with RANSAC, based on scikit-image\",\"support\":\"https://github.com/jfozard/napari-sift-registration/issues\",\"twitter\":\"\",\"version\":\"0.1.1\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"ddoncila@gmail.com\",\"name\":\"Draga Doncila\"}],\"code_repository\":\"https://github.com/DragaDoncila/napari-compressed-labels-io\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-compressed-labels-io\"}],\"description\":\"# napari-compressed-labels-io\\n\\n[![License](https://img.shields.io/pypi/l/napari-compressed-labels-io.svg?color=green)](https://github.com/DragaDoncila/napari-compressed-labels-io/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-compressed-labels-io.svg?color=green)](https://pypi.org/project/napari-compressed-labels-io)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-compressed-labels-io.svg?color=green)](https://python.org)\\n[![tests](https://github.com/DragaDoncila/napari-compressed-labels-io/workflows/tests/badge.svg)](https://github.com/DragaDoncila/napari-compressed-labels-io/actions)\\n[![codecov](https://codecov.io/gh/DragaDoncila/napari-compressed-labels-io/branch/master/graph/badge.svg)](https://codecov.io/gh/DragaDoncila/napari-compressed-labels-io)\\n\\n\\n## Description\\n\\nThis napari plugin provides readers and writers for labels and their corresponding image layers into zarr format for compression and portability. Each reader/writer pair supports a round trip of saving and loading image and labels layers.\\n\\n## Writers\\nTwo writers are provided by this plugin, each with its own reader.\\n\\n### `labels_to_zarr`\\nThis writer is an alternative to napari's default label writer and will write an entire labels layer, regardless of its dimensions, into a single zarr file. This writer provides the best compression option and its associated reader `get_zarr_labels` will read the layer back into napari.\\n\\nThis writer will be called when the user tries to save a selected labels layer into a path ending with .zarr\\n\\n### `label_image_pairs_to_zarr`\\nThis writer will save 3-dimensional labels and image layers from the viewer into individual zarrs for portability and convenience. For example, given one labels and one image layer of the shape (10, 200, 200) saved to my_stacks.zarr, 10 subdirectories will be created, each with two zarrs inside of shape (200, 200) corresponding to the labels and image layer.\\n\\nThis writer allows users to load stacks of associated images, label them, and then quickly save these stacks out into individual slices for easy loading, viewing and interaction. Its associated reader supports the loading into napari of the whole stack, all layers at one slice of the stack, and an individual layer of a given slice of the stack.\\n\\nThe writer currently supports only 3D layers, with the exception of RGB images of the form (z, y, x, 3), which are also supported.\\n\\n\\n## Readers\\n\\nTwo readers are provided by this plugin for loading the formats saved by each writer. These are detailed below.\\n\\n### `get_zarr_labels`\\n\\nThis reader will open any zarr file with a .zarray at the top level in `path` as a labels layer. This is to be used in conjunction with `labels_to_zarr`.\\n\\n\\n### `get_label_image_stack`\\n\\nThis reader will open any zarr containing a `.zmeta` file as layers into napari. Depending on what is being opened, the reader will either load a full stack of labels and images, one slice of a stack of images and labels or an individual layer within a slice. This is to be used in conjunction with `label_image_pairs_to_zarr`.\\n\\n## .zmeta\\n\\nThis metadata file contains information about the layer types in the stack and in each individual slice, as well as the number of image/label slices. This allows the reader plugin to load the correct layer types with appropriate names both at a stack level and at the individual slice level.\\n\\n### An example .zmeta specification\\n\\n```json\\n{\\n    \\\"meta\\\": {\\n        \\\"stack\\\": 7                               # number of slices in the entire stack (1 for an individual slice, 0 for a layer within a slice)\\n    },\\n    \\\"data\\\": {\\n        \\\"image\\\" : [                              # all image layers must be listed here\\n            {\\n                \\\"name\\\": \\\"leaves_example_data\\\",\\n                \\\"shape\\\": [790, 790, 3],\\n                \\\"dtype\\\": \\\"uint8\\\",\\n                \\\"rgb\\\": true                      # where rgb is false the image will be loaded as greyscale (colormap support has not yet been implemented)\\n            }\\n        ],\\n        \\\"labels\\\" : [\\n            {\\n                \\\"name\\\": \\\"oak\\\",\\n                \\\"shape\\\": [790, 790],\\n                \\\"dtype\\\": \\\"int64\\\"\\n            },\\n            {\\n                \\\"name\\\": \\\"bg\\\",\\n                \\\"shape\\\": [790, 790],\\n                \\\"dtype\\\": \\\"int64\\\"\\n            }\\n        ]\\n    }\\n}\\n\\n```\\n\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-compressed-labels-io` via [pip]:\\n\\n    pip install napari-compressed-labels-io\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [MIT] license,\\n\\\"napari-compressed-labels-io\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/DragaDoncila/napari-compressed-labels-io/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-compressed-labels-io\\n\\n\\n\\n\\n\\nDescription\\nThis napari plugin provides readers and writers for labels and their corresponding image layers into zarr format for compression and portability. Each reader/writer pair supports a round trip of saving and loading image and labels layers.\\nWriters\\nTwo writers are provided by this plugin, each with its own reader.\\nlabels_to_zarr\\nThis writer is an alternative to napari's default label writer and will write an entire labels layer, regardless of its dimensions, into a single zarr file. This writer provides the best compression option and its associated reader get_zarr_labels will read the layer back into napari.\\nThis writer will be called when the user tries to save a selected labels layer into a path ending with .zarr\\nlabel_image_pairs_to_zarr\\nThis writer will save 3-dimensional labels and image layers from the viewer into individual zarrs for portability and convenience. For example, given one labels and one image layer of the shape (10, 200, 200) saved to my_stacks.zarr, 10 subdirectories will be created, each with two zarrs inside of shape (200, 200) corresponding to the labels and image layer.\\nThis writer allows users to load stacks of associated images, label them, and then quickly save these stacks out into individual slices for easy loading, viewing and interaction. Its associated reader supports the loading into napari of the whole stack, all layers at one slice of the stack, and an individual layer of a given slice of the stack.\\nThe writer currently supports only 3D layers, with the exception of RGB images of the form (z, y, x, 3), which are also supported.\\nReaders\\nTwo readers are provided by this plugin for loading the formats saved by each writer. These are detailed below.\\nget_zarr_labels\\nThis reader will open any zarr file with a .zarray at the top level in path as a labels layer. This is to be used in conjunction with labels_to_zarr.\\nget_label_image_stack\\nThis reader will open any zarr containing a .zmeta file as layers into napari. Depending on what is being opened, the reader will either load a full stack of labels and images, one slice of a stack of images and labels or an individual layer within a slice. This is to be used in conjunction with label_image_pairs_to_zarr.\\n.zmeta\\nThis metadata file contains information about the layer types in the stack and in each individual slice, as well as the number of image/label slices. This allows the reader plugin to load the correct layer types with appropriate names both at a stack level and at the individual slice level.\\nAn example .zmeta specification\\n```json\\n{\\n    \\\"meta\\\": {\\n        \\\"stack\\\": 7                               # number of slices in the entire stack (1 for an individual slice, 0 for a layer within a slice)\\n    },\\n    \\\"data\\\": {\\n        \\\"image\\\" : [                              # all image layers must be listed here\\n            {\\n                \\\"name\\\": \\\"leaves_example_data\\\",\\n                \\\"shape\\\": [790, 790, 3],\\n                \\\"dtype\\\": \\\"uint8\\\",\\n                \\\"rgb\\\": true                      # where rgb is false the image will be loaded as greyscale (colormap support has not yet been implemented)\\n            }\\n        ],\\n        \\\"labels\\\" : [\\n            {\\n                \\\"name\\\": \\\"oak\\\",\\n                \\\"shape\\\": [790, 790],\\n                \\\"dtype\\\": \\\"int64\\\"\\n            },\\n            {\\n                \\\"name\\\": \\\"bg\\\",\\n                \\\"shape\\\": [790, 790],\\n                \\\"dtype\\\": \\\"int64\\\"\\n            }\\n        ]\\n    }\\n}\\n```\\n\\nThis napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-compressed-labels-io via pip:\\npip install napari-compressed-labels-io\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the MIT license,\\n\\\"napari-compressed-labels-io\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"napari-compressed-labels-io\",\"documentation\":\"\",\"first_released\":\"2021-02-23T01:34:32.478790Z\",\"license\":\"MIT\",\"name\":\"napari-compressed-labels-io\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\",\"writer\"],\"project_site\":\"https://github.com/DragaDoncila/napari-compressed-labels-io\",\"python_version\":\">=3.6\",\"reader_file_extensions\":[\"*\"],\"release_date\":\"2021-03-03T20:41:59.257726Z\",\"report_issues\":\"\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"zarr\",\"dask[complete]\"],\"summary\":\"Plugin exploring different options for reading and writing compressed and portable labels layers in napari.\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.0.2\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[\"labels\"]}",
  "{\"authors\":[{\"email\":\"kpandit@nygenome.org\",\"name\":\"Kunal Pandit\"}],\"code_repository\":\"https://github.com/nygctech/PICASSO\",\"conda\":[],\"description\":\"# napari-PICASSO\\n\\n[![License](https://img.shields.io/pypi/l/napari-curtain.svg?color=green)](https://github.com/nygctech/PICASSO/blob/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-PICASSO.svg?color=green)](https://pypi.org/project/napari-PICASSO)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-PICASSO.svg?color=green)](https://python.org)\\n[![tests](https://github.com/nygctech/PICASSO/actions/workflows/test_and_deploy.yml/badge.svg?event=push)](https://github.com/nygctech/PICASSO/actions/workflows/test_and_deploy.yml)\\n[![codecov](https://codecov.io/gh/nygctech/napari-PICASSO/branch/main/graph/badge.svg)](https://codecov.io/gh/nygctech/napari-PICASSO)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-PICASSO)](https://napari-hub.org/plugins/napari-PICASSO)\\n\\nUnmix spectral spillover\\n\\n![](https://user-images.githubusercontent.com/72306584/176486552-50e1bca9-65fd-4466-8c92-a114e48d2278.gif)\\n\\n## Automatic Usage\\n\\nYou can find the `PICASSO` plugin in the menu `Plugins > napari-PICASSO: PICASSO`. Select sink images that have spectral spillover from corresponding source images, then click run to optimise the mixing parameters with PICASSO. \\n\\n## Manual Usage\\n\\n![](https://user-images.githubusercontent.com/72306584/176505151-572bd762-abe6-47b1-9821-4f3aaa4704c9.gif)\\n\\nSelect the manual button in options pop up window. Then select sink images that have spectral spillover from corresponding source images. In the source images window, sliders for each $source$ control the mixing spillover, $m$ (top), and background, $b$ (bottom, optional).\\n\\n## Mixing model\\n\\n$$ sink = \\\\sum_{i} m_i(source - b_i) $$\\n\\n## Installation\\n\\nYou can install `napari-PICASSO` via [pip]:\\n\\n    pip install napari-PICASSO\\n\\n## Details\\n\\nnapari-PICASSO is a napari widget to blindly unmix fluorescence images of known members using PICASSO<sup>1</sup>. \\n\\nFor example, if 2 fluorophores with overlapping spectra are imaged, spillover fluorescesce from a channel into an adjacent channel could be removed if you know which channel is the source of the spillover fluorescence and which channel is the sink of the spillover fluorescence. \\n\\nPICASSO is an algorithm to remove spillover fluorescence by minimizing the mutual information between sink and source images. The original algorithm described by Seo et al, minimized the mutual information between pairs of sink and source images using a Nelson-Mead simplex algorithm and computing the mutual information outright with custom written MATLAB code<sup>1</sup>. The napari plugin uses a neural net to estimate and minimize the mutual information (MINE<sup>2</sup>) between pairs of sink and source images using stochastic gradient descent with GPU acceleration.\\n\\n## References\\n\\n1. Seo, J. et al. PICASSO allows ultra-multiplexed fluorescence imaging of spatially overlapping proteins without reference spectra measurements. Nat Commun 13, 2475 (2022).\\n2. Belghazi, M. I. et al. MINE: Mutual Information Neural Estimation. arXiv:1801.04062 [cs, stat] (2018).\\n\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-PICASSO\\n\\n\\n\\n\\n\\n\\nUnmix spectral spillover\\n\\nAutomatic Usage\\nYou can find the PICASSO plugin in the menu Plugins > napari-PICASSO: PICASSO. Select sink images that have spectral spillover from corresponding source images, then click run to optimise the mixing parameters with PICASSO. \\nManual Usage\\n\\nSelect the manual button in options pop up window. Then select sink images that have spectral spillover from corresponding source images. In the source images window, sliders for each $source$ control the mixing spillover, $m$ (top), and background, $b$ (bottom, optional).\\nMixing model\\n$$ sink = \\\\sum_{i} m_i(source - b_i) $$\\nInstallation\\nYou can install napari-PICASSO via pip:\\npip install napari-PICASSO\\n\\nDetails\\nnapari-PICASSO is a napari widget to blindly unmix fluorescence images of known members using PICASSO1. \\nFor example, if 2 fluorophores with overlapping spectra are imaged, spillover fluorescesce from a channel into an adjacent channel could be removed if you know which channel is the source of the spillover fluorescence and which channel is the sink of the spillover fluorescence. \\nPICASSO is an algorithm to remove spillover fluorescence by minimizing the mutual information between sink and source images. The original algorithm described by Seo et al, minimized the mutual information between pairs of sink and source images using a Nelson-Mead simplex algorithm and computing the mutual information outright with custom written MATLAB code1. The napari plugin uses a neural net to estimate and minimize the mutual information (MINE2) between pairs of sink and source images using stochastic gradient descent with GPU acceleration.\\nReferences\\n\\nSeo, J. et al. PICASSO allows ultra-multiplexed fluorescence imaging of spatially overlapping proteins without reference spectra measurements. Nat Commun 13, 2475 (2022).\\nBelghazi, M. I. et al. MINE: Mutual Information Neural Estimation. arXiv:1801.04062 [cs, stat] (2018).\\n\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"napari-PICASSO\",\"documentation\":\"\",\"first_released\":\"2022-06-01T01:48:01.350779Z\",\"license\":\"GPL-3.0\",\"name\":\"napari-PICASSO\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/nygctech/PICASSO\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-06-29T18:36:43.599731Z\",\"report_issues\":\"https://github.com/nygctech/PICASSO/issues\",\"requirements\":[\"numpy\",\"magicgui\",\"qtpy\",\"dask\",\"psutil\",\"tox ; extra == 'testing'\",\"napari[all] ; extra == 'testing'\",\"torch ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"xarray ; extra == 'testing'\"],\"summary\":\"Blind fluorescence unmixing\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.3.0\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"rodriguezsantiago96@gmail.com\",\"name\":\"Santiago N. Rodriguez Alvarez\"}],\"code_repository\":\"https://github.com/santi-rodriguez/nfinder\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"nfinder\"}],\"description\":\"# Nfinder\\nAutomatic inference of neighboring cells based on their Delaunay triangulation.\\n\\n## Dependencies \\nnfinder was tested with:\\n\\n- python = 3.8.5\\n- napari = 0.4.12\\n- numpy = 1.21.2\\n- pandas = 1.3.4\\n- scikit-image = 0.18.3\\n- scipy = 1.7.1\\n- importlib-resources 5.4.0\\n\\n\\n## Installation\\n\\nIt can be installed with `pip` from PyPI:\\n\\n```\\npip install nfinder\\n```\\n\\n\\n## Usage\\nFor usage examples, please check out the [notebook](https://github.com/santi-rodriguez/nfinder/blob/main/examples.ipynb) in our GitHub repository.\",\"description_content_type\":\"text/markdown\",\"description_text\":\"Nfinder\\nAutomatic inference of neighboring cells based on their Delaunay triangulation.\\nDependencies\\nnfinder was tested with:\\n\\npython = 3.8.5\\nnapari = 0.4.12\\nnumpy = 1.21.2\\npandas = 1.3.4\\nscikit-image = 0.18.3\\nscipy = 1.7.1\\nimportlib-resources 5.4.0\\n\\nInstallation\\nIt can be installed with pip from PyPI:\\npip install nfinder\\nUsage\\nFor usage examples, please check out the notebook in our GitHub repository.\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"nfinder\",\"documentation\":\"\",\"first_released\":\"2021-12-06T20:31:59.553762Z\",\"license\":\"\",\"name\":\"nfinder\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/santi-rodriguez/nfinder\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-02-05T19:14:23.400655Z\",\"report_issues\":\"\",\"requirements\":null,\"summary\":\"Automatic inference of neighboring cells based on their Delaunay triangulation.\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.3\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"hectormz.git@gmail.com\",\"name\":\"Hector Munoz\"}],\"code_repository\":\"https://github.com/hectormz/napari-mat-images\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-mat-images\"}],\"description\":\"# napari-mat-images\\n\\n[![PyPI version](https://img.shields.io/pypi/v/napari-mat-images.svg)](https://pypi.org/project/napari-mat-images)\\n\\n[![Python versions](https://img.shields.io/pypi/pyversions/napari-mat-images.svg)](https://pypi.org/project/napari-mat-images)\\n\\n[![See Build Status on Azure Pipelines](https://dev.azure.com/hectormz-1/napari-mat-images/_apis/build/status/hectormz.napari-mat-images?branchName=main)](https://dev.azure.com/hectormz-1/napari-mat-images/_build/latest?definitionId=1&branchName=main)\\n\\n## Features\\n\\nThis plugin loads image variables stored in `MATLAB` `.mat` files into [napari](https://github.com/napari/napari).\\n\\nIt loads any variable that looks like an image.\\nPresently, that includes any array with more than two dimensions with size greater than 20 pixels (determined by `shape_is_image()`).\\n\\nIf loading a variable with 3 or more dimensions, the plugin assumes that it is a stack of images, and the dimension with greatest size is the axis of the stack.\\n\\n### Loading Large Files\\n\\nIf loading a large `.mat` file saved in `HDF5`/`v7.3` format, chunks of the images are loaded as needed, resulting in fast initial load, but potentially slower scrolling.\\n\\nSlices of the image stacks are randomly sampled to determine min/max contrast values.\\n\\n## Requirements\\n\\nThis plugin relies on `scipy` to load small `.mat` files and `h5py` (with `dask`) to load larger `HDF5`/`v7.3` `.mat` files.\\n\\nIt implicitly requires `napari` for use.\\n\\n## Installation\\n\\n`napari-mat-images` requires [napari](https://github.com/napari/napari) to be installed, although it is not listed as a requirement for installation.\\nThis plugin relies on plugin functionality found in `napari` version \\\\> `0.2.12`. This can be installed via [pip](https://pypi.org/project/pip/) from [PyPI](https://pypi.org/project):\\n\\n    $ pip install napari>0.2.12\\n\\nYou can install `napari-mat-images` via [pip](https://pypi.org/project/pip/) from [PyPI](https://pypi.org/project):\\n\\n    $ pip install napari-mat-images\\n\\n## Usage\\n\\nOnce installed, the plugin will be used whenever trying to load a `.mat` file.\\nThis can be done from the `napari` GUI or commandline:\\n\\n    $ napari my_file.mat\\n\\n## Contributing\\n\\nContributions are very welcome.\\nTests can be run with [pytest](https://docs.pytest.org/en/latest/),\\nplease ensure the coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3](http://opensource.org/licenses/BSD-3-Clause) license, `napari-mat-images` is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue](https://github.com/hectormz/napari-mat-images/issues) along with a detailed description.\\n\\n---\\n\\nThis [napari](https://github.com/napari/napari) plugin was generated with [Cookiecutter](https://github.com/audreyr/cookiecutter) along with [napari](https://github.com/napari/napari)\\\\'s [cookiecutter-napari-plugin](https://github.com/napari/cookiecutter-napari-plugin) template.\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-mat-images\\n\\n\\n\\nFeatures\\nThis plugin loads image variables stored in MATLAB .mat files into napari.\\nIt loads any variable that looks like an image.\\nPresently, that includes any array with more than two dimensions with size greater than 20 pixels (determined by shape_is_image()).\\nIf loading a variable with 3 or more dimensions, the plugin assumes that it is a stack of images, and the dimension with greatest size is the axis of the stack.\\nLoading Large Files\\nIf loading a large .mat file saved in HDF5/v7.3 format, chunks of the images are loaded as needed, resulting in fast initial load, but potentially slower scrolling.\\nSlices of the image stacks are randomly sampled to determine min/max contrast values.\\nRequirements\\nThis plugin relies on scipy to load small .mat files and h5py (with dask) to load larger HDF5/v7.3 .mat files.\\nIt implicitly requires napari for use.\\nInstallation\\nnapari-mat-images requires napari to be installed, although it is not listed as a requirement for installation.\\nThis plugin relies on plugin functionality found in napari version > 0.2.12. This can be installed via pip from PyPI:\\n$ pip install napari>0.2.12\\n\\nYou can install napari-mat-images via pip from PyPI:\\n$ pip install napari-mat-images\\n\\nUsage\\nOnce installed, the plugin will be used whenever trying to load a .mat file.\\nThis can be done from the napari GUI or commandline:\\n$ napari my_file.mat\\n\\nContributing\\nContributions are very welcome.\\nTests can be run with pytest,\\nplease ensure the coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license, napari-mat-images is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\\n\\nThis napari plugin was generated with Cookiecutter along with napari\\\\'s cookiecutter-napari-plugin template.\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"napari-mat-images\",\"documentation\":\"\",\"first_released\":\"2020-03-29T00:42:46.621697Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-mat-images\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[],\"project_site\":\"https://github.com/hectormz/napari-mat-images\",\"python_version\":\">=3.6\",\"reader_file_extensions\":[],\"release_date\":\"2021-06-30T14:03:05.689061Z\",\"report_issues\":\"\",\"requirements\":[\"dask[delayed]\",\"h5py\",\"numpy\",\"pluggy\",\"scipy\"],\"summary\":\"A plugin to load images stored in MATLAB .mat files with napari\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.1.3\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Pranjal Dhole\"},{\"name\":\"Duway Nicolas Lesmes Leon\"}],\"category\":{\"Workflow step\":[\"Image Segmentation\",\"Pixel classification\"]},\"category_hierarchy\":{\"Workflow step\":[[\"Image Segmentation\",\"Model-based segmentation\"],[\"Pixel classification\"]]},\"code_repository\":\"https://github.com/yapic/napari-yapic-prediction\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-yapic-prediction\"}],\"description\":\"# napari-yapic-prediction\\n\\n[![License](https://img.shields.io/pypi/l/napari-yapic-prediction.svg?color=green)](https://github.com/yapic/napari-yapic-prediction/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-yapic-prediction.svg?color=green)](https://pypi.org/project/napari-yapic-prediction)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-yapic-prediction.svg?color=green)](https://python.org)\\n[![tests](https://github.com/yapic/napari-yapic-prediction/workflows/tests/badge.svg)](https://github.com/yapic/napari-yapic-prediction/actions)\\n[![codecov](https://codecov.io/gh/yapic/napari-yapic-prediction/branch/master/graph/badge.svg?token=amah2YwOpx)](https://codecov.io/gh/yapic/napari-yapic-prediction)\\n\\nA napari widget plugin to perform YAPiC model segmentation prediction in the napari window. \\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Description\\n\\nThis napari plugin provides a widget to upload a [YAPiC] trained model and perform segmentation over all the present images in the napari window. The segmentation results are uploaded as napari layers into the viewer automatically with the name structure of *imgename_prediction*.\\n\\n## Installation\\n\\n1. Please install either GPU or CPU version of tensorflow that is compatible with your `cuda` and `cudnn` libraries before installing the plugin depending on your system.\\nOne of the plugin dependency is `yapic` that currently has sensitivity to tensorflow versions.\\n\\n2. You can install `napari-yapic-prediction` via [pip]:\\n\\n    ```pip install napari-yapic-prediction```\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [GNU GPL v3.0] license,\\n\\\"napari-yapic-prediction\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/yapic/napari-yapic-prediction/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n[YAPiC]: https://yapic.github.io/yapic/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-yapic-prediction\\n\\n\\n\\n\\n\\nA napari widget plugin to perform YAPiC model segmentation prediction in the napari window. \\n\\nThis napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.\\n\\nDescription\\nThis napari plugin provides a widget to upload a YAPiC trained model and perform segmentation over all the present images in the napari window. The segmentation results are uploaded as napari layers into the viewer automatically with the name structure of imgename_prediction.\\nInstallation\\n\\n\\nPlease install either GPU or CPU version of tensorflow that is compatible with your cuda and cudnn libraries before installing the plugin depending on your system.\\nOne of the plugin dependency is yapic that currently has sensitivity to tensorflow versions.\\n\\n\\nYou can install napari-yapic-prediction via pip:\\npip install napari-yapic-prediction\\n\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the GNU GPL v3.0 license,\\n\\\"napari-yapic-prediction\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"napari-yapic-prediction\",\"documentation\":\"https://yapic.github.io/napari-yapic-prediction/\",\"first_released\":\"2021-04-19T08:42:54.339457Z\",\"license\":\"GPL-3.0\",\"name\":\"napari-yapic-prediction\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://yapic.github.io/napari-yapic-prediction/\",\"python_version\":\">=3.6\",\"reader_file_extensions\":[],\"release_date\":\"2022-02-04T12:37:30.358543Z\",\"report_issues\":\"https://github.com/yapic/napari-yapic-prediction/issues\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"napari[all]\",\"yapic\",\"scikit-image\"],\"summary\":\"napari widget that performs image segmentation with yapic model in the napari window. Install TENSORFLOW to use this plugin.\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.2.0\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Niklas Netter\"}],\"code_repository\":\"https://github.com/gatoniel/napari-validate-random-label-predictions\",\"description\":\"# napari-validate-random-label-predictions\\n\\n[![License BSD-3](https://img.shields.io/pypi/l/napari-validate-random-label-predictions.svg?color=green)](https://github.com/gatoniel/napari-validate-random-label-predictions/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-validate-random-label-predictions.svg?color=green)](https://pypi.org/project/napari-validate-random-label-predictions)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-validate-random-label-predictions.svg?color=green)](https://python.org)\\n[![tests](https://github.com/gatoniel/napari-validate-random-label-predictions/workflows/tests/badge.svg)](https://github.com/gatoniel/napari-validate-random-label-predictions/actions)\\n[![codecov](https://codecov.io/gh/gatoniel/napari-validate-random-label-predictions/branch/main/graph/badge.svg)](https://codecov.io/gh/gatoniel/napari-validate-random-label-predictions)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-validate-random-label-predictions)](https://napari-hub.org/plugins/napari-validate-random-label-predictions)\\n\\nValidate separate instances of label predictions manually\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/stable/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-validate-random-label-predictions` via [pip]:\\n\\n    pip install napari-validate-random-label-predictions\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/gatoniel/napari-validate-random-label-predictions.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-validate-random-label-predictions\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/gatoniel/napari-validate-random-label-predictions/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-validate-random-label-predictions\\n\\n\\n\\n\\n\\n\\nValidate separate instances of label predictions manually\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-validate-random-label-predictions via pip:\\npip install napari-validate-random-label-predictions\\n\\nTo install latest development version :\\npip install git+https://github.com/gatoniel/napari-validate-random-label-predictions.git\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-validate-random-label-predictions\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari validate random label predictions\",\"documentation\":\"https://github.com/gatoniel/napari-validate-random-label-predictions#README.md\",\"first_released\":\"2022-11-23T15:54:46.355629Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-validate-random-label-predictions\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/gatoniel/napari-validate-random-label-predictions\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-11-23T15:54:46.355629Z\",\"report_issues\":\"https://github.com/gatoniel/napari-validate-random-label-predictions/issues\",\"requirements\":[\"numpy\",\"magicgui\",\"qtpy\",\"scikit-image\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\"],\"summary\":\"Validate separate instances of label predictions manually\",\"support\":\"https://github.com/gatoniel/napari-validate-random-label-predictions/issues\",\"twitter\":\"\",\"version\":\"0.0.1\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"lorenzo.gaifas@gmail.com\",\"name\":\"Lorenzo Gaifas\"}],\"category\":{\"Workflow step\":[\"Visualization\"]},\"category_hierarchy\":{\"Workflow step\":[[\"Visualization\"]]},\"code_repository\":\"https://github.com/brisvag/napari-properties-plotter\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-properties-plotter\"}],\"description\":\"# napari-properties-plotter\\n\\n[![License](https://img.shields.io/pypi/l/napari-properties-plotter.svg?color=green)](https://github.com/brisvag/napari-properties-plotter/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-properties-plotter.svg?color=green)](https://pypi.org/project/napari-properties-plotter)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-properties-plotter.svg?color=green)](https://python.org)\\n[![tests](https://github.com/brisvag/napari-properties-plotter/workflows/tests/badge.svg)](https://github.com/brisvag/napari-properties-plotter/actions)\\n[![codecov](https://codecov.io/gh/brisvag/napari-properties-plotter/branch/master/graph/badge.svg)](https://codecov.io/gh/brisvag/napari-properties-plotter)\\n\\nA napari plugin that automatically generates interactive plots based on layer properties.\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-properties-plotter` via [pip]:\\n\\n    pip install napari-properties-plotter\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-properties-plotter\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/brisvag/napari-properties-plotter/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-properties-plotter\\n\\n\\n\\n\\n\\nA napari plugin that automatically generates interactive plots based on layer properties.\\n\\nThis napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-properties-plotter via pip:\\npip install napari-properties-plotter\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-properties-plotter\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari-properties-plotter\",\"documentation\":\"https://github.com/brisvag/napari-properties-plotter#README.md\",\"first_released\":\"2021-05-26T13:31:54.874796Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-properties-plotter\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/brisvag/napari-properties-plotter\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2021-12-01T11:04:29.153379Z\",\"report_issues\":\"https://github.com/brisvag/napari-properties-plotter/issues\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"pandas\",\"pyqtgraph\",\"qtpy\"],\"summary\":\"A napari plugin that automatically generates interactive plots based on layer properties.\",\"support\":\"https://github.com/brisvag/napari-properties-plotter/issues\",\"twitter\":\"\",\"version\":\"0.2.2\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Adam Tyson\"},{\"name\":\"Horst Obenhaus\"}],\"category\":{\"Supported data\":[\"3D\"],\"Workflow step\":[\"Image Segmentation\",\"Image annotation\"]},\"category_hierarchy\":{\"Supported data\":[[\"3D\"]],\"Workflow step\":[[\"Image Segmentation\",\"Manual segmentation\"],[\"Image annotation\",\"Dense image annotation\",\"Manual segmentation\"],[\"Image Segmentation\"]]},\"code_repository\":\"https://github.com/brainglobe/brainreg-segment\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"brainreg-segment\"}],\"description\":\"[![Python Version](https://img.shields.io/pypi/pyversions/brainreg-segment.svg)](https://pypi.org/project/brainreg-segment)\\n[![PyPI](https://img.shields.io/pypi/v/brainreg-segment.svg)](https://pypi.org/project/brainreg-segment)\\n[![Wheel](https://img.shields.io/pypi/wheel/brainreg-segment.svg)](https://pypi.org/project/brainreg-segment)\\n[![Development Status](https://img.shields.io/pypi/status/brainreg-segment.svg)](https://github.com/brainglobe/brainreg-segment)\\n[![Tests](https://img.shields.io/github/workflow/status/brainglobe/brainreg-segment/tests)](\\n    https://github.com/brainglobe/brainreg-segment/actions)\\n[![Coverage Status](https://coveralls.io/repos/github/brainglobe/brainreg-segment/badge.svg?branch=master)](https://coveralls.io/github/brainglobe/brainreg-segment?branch=master)\\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/python/black)\\n[![Twitter](https://img.shields.io/twitter/follow/brain_globe?style=social)](https://twitter.com/brain_globe)\\n\\n# brainreg-segment\\nSegmentation of 1/2/3D brain structures in a common anatomical space\\n\\n`brainreg-segment` is a companion to [`brainreg`](https://github.com/brainglobe/brainreg) allowing manual segmentation of regions/objects within the brain (e.g. injection sites, probes etc.) allowing for automated analysis of brain region distribution, and visualisation (e.g. in [brainrender](https://github.com/BrancoLab/brainrender)).\\n\\n`brainreg-segment` and `brainreg` were developed by [Adam Tyson](https://github.com/adamltyson) and [Charly Rousseau](https://github.com/crousseau) in the [Margrie Lab](https://www.sainsburywellcome.org/web/groups/margrie-lab), based on [aMAP](https://doi.org/10.1038/ncomms11879) by [Christian Niedworok](https://github.com/cniedwor). The work was generously supported by the [Sainsbury Wellcome Centre](https://www.sainsburywellcome.org/web/).\\n\\n## Installation\\n\\nbrainreg-segment comes bundled with [`brainreg`](https://github.com/brainglobe/brainreg), so see the [brainreg installation instructions](https://docs.brainglobe.info/brainreg/installation).\\n\\nbrainreg-segment can be installed on it's own (`pip install brainreg-segment`), but you will need to register your data with brainreg first.\\n\\n## Usage\\n\\nSee [user guide](https://docs.brainglobe.info/brainreg-segment/user-guide).\\n\\nIf you have any questions, head over to the [image.sc forum](https://forum.image.sc/tag/brainglobe).\\n\\n## Contributing\\nContributions are very welcome. Please see the [contributing guide](https://github.com/brainglobe/.github/blob/main/CONTRIBUTING.md).\\n\\n### Citing brainreg-segment\\n\\nIf you find brainreg-segment useful, and use it in your research, please let us know and also cite the paper:\\n\\n> Tyson, A. L., V&eacute;lez-Fort, M.,  Rousseau, C. V., Cossell, L., Tsitoura, C., Lenzi, S. C., Obenhaus, H. A., Claudi, F., Branco, T.,  Margrie, T. W. (2022). Accurate determination of marker location within whole-brain microscopy images. Scientific Reports, 12, 867 [doi.org/10.1038/s41598-021-04676-9](https://doi.org/10.1038/s41598-021-04676-9)\\n\\n---\\nThe BrainGlobe project is generously supported by the Sainsbury Wellcome Centre and the Institute of Neuroscience, Technical University of Munich, with funding from Wellcome, the Gatsby Charitable Foundation and the Munich Cluster for Systems Neurology - Synergy.\\n\\n<img src='https://brainglobe.info/images/logos_combined.png' width=\\\"550\\\">\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"\\n\\n\\n\\n\\n\\n\\n\\nbrainreg-segment\\nSegmentation of 1/2/3D brain structures in a common anatomical space\\nbrainreg-segment is a companion to brainreg allowing manual segmentation of regions/objects within the brain (e.g. injection sites, probes etc.) allowing for automated analysis of brain region distribution, and visualisation (e.g. in brainrender).\\nbrainreg-segment and brainreg were developed by Adam Tyson and Charly Rousseau in the Margrie Lab, based on aMAP by Christian Niedworok. The work was generously supported by the Sainsbury Wellcome Centre.\\nInstallation\\nbrainreg-segment comes bundled with brainreg, so see the brainreg installation instructions.\\nbrainreg-segment can be installed on it's own (pip install brainreg-segment), but you will need to register your data with brainreg first.\\nUsage\\nSee user guide.\\nIf you have any questions, head over to the image.sc forum.\\nContributing\\nContributions are very welcome. Please see the contributing guide.\\nCiting brainreg-segment\\nIf you find brainreg-segment useful, and use it in your research, please let us know and also cite the paper:\\n\\nTyson, A. L., Vélez-Fort, M.,  Rousseau, C. V., Cossell, L., Tsitoura, C., Lenzi, S. C., Obenhaus, H. A., Claudi, F., Branco, T.,  Margrie, T. W. (2022). Accurate determination of marker location within whole-brain microscopy images. Scientific Reports, 12, 867 doi.org/10.1038/s41598-021-04676-9\\n\\n\\nThe BrainGlobe project is generously supported by the Sainsbury Wellcome Centre and the Institute of Neuroscience, Technical University of Munich, with funding from Wellcome, the Gatsby Charitable Foundation and the Munich Cluster for Systems Neurology - Synergy.\\n\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"brainreg-segment\",\"documentation\":\"https://docs.brainglobe.info/brainreg-segment\",\"first_released\":\"2020-08-26T09:04:27.878441Z\",\"license\":\"BSD-3-Clause\",\"name\":\"brainreg-segment\",\"npe2\":false,\"operating_system\":[\"Operating System :: Microsoft :: Windows :: Windows 10\",\"Operating System :: POSIX :: Linux\"],\"plugin_types\":[\"widget\",\"sample_data\"],\"project_site\":\"https://brainglobe.info/\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-06-15T12:30:32.795955Z\",\"report_issues\":\"https://github.com/brainglobe/brainreg-segment/issues\",\"requirements\":[\"numpy\",\"tables\",\"scikit-image\",\"pandas\",\"napari (>=0.4.5)\",\"napari-plugin-engine (>=0.1.4)\",\"imlib (>=0.0.26)\",\"dask (>=2.15.0)\",\"imio\",\"brainglobe-napari-io\",\"black ; extra == 'dev'\",\"pytest-cov ; extra == 'dev'\",\"pytest ; extra == 'dev'\",\"pytest-qt ; extra == 'dev'\",\"coverage ; extra == 'dev'\",\"bump2version ; extra == 'dev'\",\"pre-commit ; extra == 'dev'\",\"flake8 ; extra == 'dev'\"],\"summary\":\"Manual segmentation of 3D brain structures in a common anatomical space\",\"support\":\"https://forum.image.sc/tag/brainglobe\",\"twitter\":\"\",\"version\":\"0.2.16\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"alexis.japas@proton.me\",\"name\":\"Alexis Japas\"}],\"code_repository\":\"https://github.com/alexisjapas/napari-vesicles-segmentation\",\"conda\":[],\"description\":\"# napari-vesicles-segmentation\\n\\n[![License BSD-3](https://img.shields.io/pypi/l/napari-vesicles-segmentation.svg?color=green)](https://github.com/alexisjapas/napari-vesicles-segmentation/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-vesicles-segmentation.svg?color=green)](https://pypi.org/project/napari-vesicles-segmentation)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-vesicles-segmentation.svg?color=green)](https://python.org)\\n[![tests](https://github.com/alexisjapas/napari-vesicles-segmentation/workflows/tests/badge.svg)](https://github.com/alexisjapas/napari-vesicles-segmentation/actions)\\n[![codecov](https://codecov.io/gh/alexisjapas/napari-vesicles-segmentation/branch/main/graph/badge.svg)](https://codecov.io/gh/alexisjapas/napari-vesicles-segmentation)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-vesicles-segmentation)](https://napari-hub.org/plugins/napari-vesicles-segmentation)\\n\\nA simple plugin to detect vesicles in cells images.\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-vesicles-segmentation` via [pip]:\\n\\n    pip install napari-vesicles-segmentation\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/alexisjapas/napari-vesicles-segmentation.git\\n\\n## Usage\\n1. Open napari\\n2. Open your data\\n![usage-open-data](images/usage-open-data.png)\\n3. Launch the vesicles-segmentation plugin\\n4. Select the data you want to segment and set the parameters of the segmentation\\n![usage-setup](images/usage-setup.png)\\n    * **image**: The image to segment vesicles in. The image can be a 2D or 3D temporal stack of images.\\n    * **minimum vesicles size**: The minimum size of the vesicles to detect. Smaller detected vesicles are removed.\\n    * **membrane erosion**: The size of the disk radius used for eroding the cell. This is used to remove the external membrane. This parameter scales when downsizing the image, for more information see 'downsizing ratio' parameter.\\n    * **closing size**: The size of the disk radius used for closing the cell. This is used to fill holes in the cell. This parameter scales when downsizing the image, for more information see 'downsizing ratio' parameter.\\n    * **clip**: If set to zero, no standardization is performed. Otherwise, the standard deviation of the image is set to n_sigma * the standard deviation of the image, the image is standardized and its values are clipped to the range [-1, 1] in order to remove outliers. The higher the value of n_sigma, the less outliers are removed. This operation can lead to a better detection of the cell.\\n    * **downsampling ratio**: The downsampling ratio used for the downsampled image. This is used to speed up the computation. Downsampling the image have impact in reducing the resolution of erosion and closing e.g. for a downsize ratio of 2, setting the erosion size to 3 will result in an erosion size of 6.\\n    * **display cell detection**: If set to True, the cell detection is displayed in the viewer instead of the vesicle detection.\\n5. Click on the \\\"Segment\\\" button to start the segmentation. This can take few seconds or minutes depending on the size of the data. The result is added to the viewer as below.\\n![usage-segmentation](images/usage-segmentation.png)\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-vesicles-segmentation\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/alexisjapas/napari-vesicles-segmentation/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-vesicles-segmentation\\n\\n\\n\\n\\n\\n\\nA simple plugin to detect vesicles in cells images.\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-vesicles-segmentation via pip:\\npip install napari-vesicles-segmentation\\n\\nTo install latest development version :\\npip install git+https://github.com/alexisjapas/napari-vesicles-segmentation.git\\n\\nUsage\\n\\nOpen napari\\nOpen your data\\n\\nLaunch the vesicles-segmentation plugin\\nSelect the data you want to segment and set the parameters of the segmentation\\n\\nimage: The image to segment vesicles in. The image can be a 2D or 3D temporal stack of images.\\nminimum vesicles size: The minimum size of the vesicles to detect. Smaller detected vesicles are removed.\\nmembrane erosion: The size of the disk radius used for eroding the cell. This is used to remove the external membrane. This parameter scales when downsizing the image, for more information see 'downsizing ratio' parameter.\\nclosing size: The size of the disk radius used for closing the cell. This is used to fill holes in the cell. This parameter scales when downsizing the image, for more information see 'downsizing ratio' parameter.\\nclip: If set to zero, no standardization is performed. Otherwise, the standard deviation of the image is set to n_sigma * the standard deviation of the image, the image is standardized and its values are clipped to the range [-1, 1] in order to remove outliers. The higher the value of n_sigma, the less outliers are removed. This operation can lead to a better detection of the cell.\\ndownsampling ratio: The downsampling ratio used for the downsampled image. This is used to speed up the computation. Downsampling the image have impact in reducing the resolution of erosion and closing e.g. for a downsize ratio of 2, setting the erosion size to 3 will result in an erosion size of 6.\\ndisplay cell detection: If set to True, the cell detection is displayed in the viewer instead of the vesicle detection.\\n\\n\\nClick on the \\\"Segment\\\" button to start the segmentation. This can take few seconds or minutes depending on the size of the data. The result is added to the viewer as below.\\n\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-vesicles-segmentation\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"Vesicles Segmentation\",\"documentation\":\"https://github.com/alexisjapas/napari-vesicles-segmentation#README.md\",\"first_released\":\"2022-07-08T15:13:49.158166Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-vesicles-segmentation\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/alexisjapas/napari-vesicles-segmentation\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-07-08T15:13:49.158166Z\",\"report_issues\":\"https://github.com/alexisjapas/napari-vesicles-segmentation/issues\",\"requirements\":[\"numpy\",\"magicgui\",\"qtpy\",\"scikit-image\",\"scipy\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\",\"scikit-image ; extra == 'testing'\",\"scipy ; extra == 'testing'\"],\"summary\":\"A simple plugin to detect vesicles in cells images.\",\"support\":\"https://github.com/alexisjapas/napari-vesicles-segmentation/issues\",\"twitter\":\"\",\"version\":\"0.0.1\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"burke@mpi-cbg.de\",\"name\":\"Tom Burke\"}],\"code_repository\":\"https://github.com/Labelings/napari-labeling\",\"conda\":[],\"description\":\"# napari-labeling\\n\\n[![License](https://img.shields.io/pypi/l/napari-labeling.svg?color=green)](https://github.com/tomburke-rse/napari-labeling/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-labeling.svg?color=green)](https://pypi.org/project/napari-labeling)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-labeling.svg?color=green)](https://python.org)\\n[![tests](https://github.com/tomburke-rse/napari-labeling/workflows/tests/badge.svg)](https://github.com/tomburke-rse/napari-labeling/actions)\\n[![codecov](https://codecov.io/gh/tomburke-rse/napari-labeling/branch/main/graph/badge.svg)](https://codecov.io/gh/tomburke-rse/napari-labeling)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-labeling)](https://napari-hub.org/plugins/napari-labeling)\\n\\nThis is a napari-plugin based on the [labeling project].\\n\\nIt allows the generation of overlapping labels in one layer, save and load of this layer in a json-based file format and\\nit contains a widget to explore the overlapping labels layer and select specific segments with a mouse click .\\n\\nPlease note that currently, the widget part only works by adding it through code with:\\n\\n    from napari_labeling import edit_widget\\n    viewer = napari.Viewer()\\n    viewer.window.add_dock_widget(edit_widget)\\n\\nAn example on how to achieve this can be found in the [main.py] on GitHub.\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/plugins/stable/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-labeling` via [pip]:\\n\\n    pip install napari-labeling\\n\\n\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-labeling\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n[labeling project]: https://github.com/Labelings/Labeling\\n[main.py]: https://github.com/Labelings/Labeling/blob/main/main.py\\n[file an issue]: https://github.com/Labelings/napari-labeling/issues\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-labeling\\n\\n\\n\\n\\n\\n\\nThis is a napari-plugin based on the labeling project.\\nIt allows the generation of overlapping labels in one layer, save and load of this layer in a json-based file format and\\nit contains a widget to explore the overlapping labels layer and select specific segments with a mouse click .\\nPlease note that currently, the widget part only works by adding it through code with:\\nfrom napari_labeling import edit_widget\\nviewer = napari.Viewer()\\nviewer.window.add_dock_widget(edit_widget)\\n\\nAn example on how to achieve this can be found in the main.py on GitHub.\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-labeling via pip:\\npip install napari-labeling\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-labeling\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"napari Labeling\",\"documentation\":\"\",\"first_released\":\"2022-05-09T14:46:55.976557Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-labeling\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\",\"writer\",\"widget\"],\"project_site\":\"https://github.com/Labelings/napari-labeling\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[\"*.lbl.json\"],\"release_date\":\"2022-05-10T09:34:17.445511Z\",\"report_issues\":\"\",\"requirements\":[\"numpy\",\"labeling\"],\"summary\":\"A napari plugin for handling overlapping labeling data\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.1.2\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[\"labels\"]}",
  "{\"authors\":[{\"name\":\"Varun Kapoor\"}],\"category\":{\"Image modality\":[\"Medical imaging\"],\"Supported data\":[\"2D\",\"3D\",\"Time series\",\"Multi-channel\"],\"Workflow step\":[\"Image Segmentation\"]},\"category_hierarchy\":{\"Image modality\":[[\"Medical imaging\"]],\"Supported data\":[[\"2D\"],[\"3D\"],[\"Time series\"],[\"Multi-channel\"]],\"Workflow step\":[[\"Image Segmentation\",\"Cell segmentation\"],[\"Image Segmentation\",\"Model-based segmentation\"]]},\"code_repository\":\"https://github.com/kapoorlab/vollseg-napari\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"vollseg-napari\"}],\"description\":\"# VollSeg Napari Plugin\\n\\n\\n\\n[![PyPI version](https://img.shields.io/pypi/v/vollseg-napari.svg)](https://pypi.org/project/vollseg-napari)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/vollseg-napari)](https://napari-hub.org/plugins/vollseg-napari)\\n[![License](https://img.shields.io/pypi/l/napari-metroid.svg?color=green)](https://github.com/kapoorlab/napari-vollseg/raw/main/LICENSE)\\n[![codecov](https://codecov.io/gh/kapoorlab/napari-vollseg/branch/main/graph/badge.svg)](https://codecov.io/gh/kapoorlab/napari-vollseg)\\n[![Twitter Badge](https://badgen.net/badge/icon/twitter?icon=twitter&label)](https://twitter.com/entracod)\\n\\n\\nThis project provides the [napari](https://napari.org/) plugin for [VollSeg](https://github.com/kapoorlab/vollseg), a deep learning based 2D and 3D segmentation tool for irregular shaped cells. VollSeg has originally been developed (see [papers](http://conference.scipy.org/proceedings/scipy2021/varun_kapoor.html)) for the segmentation of densely packed membrane labelled cells in challenging images with low signal-to-noise ratios. The plugin allows to apply pretrained and custom trained models from within napari.\\nFor detailed demo of the plugin see these [videos](https://www.youtube.com/watch?v=W_gKrLWKNpQ) and a short video about the [parameter selection](https://www.youtube.com/watch?v=7tQMn_u8_7s&t=1s) \\n\\n\\n## Installation & Usage\\n\\nInstall the plugin with `pip install vollseg-napari` or from within napari via `Plugins > Install/Uninstall Package(s)…`. If you want GPU-accelerated prediction, please read the more detailed [installation instructions](https://github.com/kapoorlab/vollseg-napari#gpu_installation) for VollSeg.\\n\\nYou can activate the plugin in napari via `Plugins > VollSeg: VollSeg`. Example images for testing are provided via `File > Open Sample > VollSeg`.\\n\\nIf you use this plugin for your research, please [cite us](http://conference.scipy.org/proceedings/scipy2021/varun_kapoor.html).\\n\\n## GPU_Installation\\n\\nThis package is compatible with Python 3.6 - 3.9.\\n\\n1. Please first [install TensorFlow](https://www.tensorflow.org/install)\\n(TensorFlow 2) by following the official instructions.\\nFor [GPU support](https://www.tensorflow.org/install/gpu), it is very\\nimportant to install the specific versions of CUDA and cuDNN that are\\ncompatible with the respective version of TensorFlow. (If you need help and can use `conda`, take a look at [this](https://github.com/CSBDeep/CSBDeep/tree/master/extras#conda-environment).)\\n\\n2. *VollSeg* can then be installed with `pip`:\\n\\n    - If you installed TensorFlow 2 (version *2.x.x*):\\n\\n          pip install vollseg\\n\\n\\n## Examples\\n\\nVollSeg comes with different options to combine CARE based denoising with UNET, StarDist and segmentation in a region of interest (ROI). We present some examples which are represent optimal combination of these different modes for segmenting different cell types. We summarize this in the table below:\\n\\n| Example Image | Description | Training Data | Trained Model | GT image   | Optimal combination  | Notebook Code | Model Prediction | Metrics |\\n| --- | --- |--- | --- |--- | --- |--- | --- | --- |\\n| <img src=\\\"https://github.com/kapoorlab/vollseg-napari/blob/main/vollseg_napari/images/Ascadian_raw.png\\\"  title=\\\"Raw Ascadian Embryo\\\" width=\\\"200\\\">| Light sheet fused from four angles 3D single channel| [Training Data ~320 GB](https://figshare.com/articles/dataset/Astec-half-Pm1_Cut_at_2-cell_stage_half_Phallusia_mammillata_embryo_live_SPIM_imaging_stages_6-16_/11309570?backTo=/s/765d4361d1b073beedd5)| [UNET model](https://zenodo.org/record/6337699) |<img src=\\\"https://github.com/kapoorlab/vollseg-napari/blob/main/vollseg_napari/images/Ascadian_GT.png\\\" title=\\\"GT Ascadian Embryo\\\" width=\\\"200\\\"> | UNET model, slice_merge = False | [Colab Notebook](https://github.com/kapoorlab/VollSeg/blob/main/examples/Predict/Colab_VollSeg_Ascadian_Embryo.ipynb) |<img src=\\\"https://github.com/kapoorlab/vollseg-napari/blob/main/vollseg_napari/images/Ascadian_pred.png\\\" title=\\\"Prediction Ascadian Embryo\\\" width=\\\"200\\\" > | <img src=\\\"https://github.com/kapoorlab/vollseg-napari/blob/main/vollseg_napari/images/Metrics_Ascadian.png\\\" title=\\\"Metrics Ascadian Embryo\\\" width=\\\"200\\\" >  |\\n|  |  | | | | | | |  |\\n| <img src=\\\"https://github.com/kapoorlab/vollseg-napari/blob/main/vollseg_napari/images/Carcinoma_raw.png\\\"  title=\\\"Raw Carcinoma\\\" width=\\\"200\\\">| Confocal microscopy 3D single channel 8 bit| [Training Data](https://zenodo.org/record/5904082#.Yi8-BnrMJD8)| [Denoising Model](https://zenodo.org/record/5910645/) and [StarDist Model](https://zenodo.org/record/6354077/) |<img src=\\\"https://github.com/kapoorlab/vollseg-napari/blob/main/vollseg_napari/images/Carcinoma_GT.png\\\" title=\\\"GT Carcinoma\\\" width=\\\"200\\\"> | StarDist model + Denoising Model, dounet = False | [Colab Notebook](https://github.com/kapoorlab/VollSeg/blob/main/examples/Predict/Colab_VollSeg_Mamary_gland.ipynb) |<img src=\\\"https://github.com/kapoorlab/vollseg-napari/blob/main/vollseg_napari/images/Carcinoma_pred.png\\\" title=\\\"Prediction Carcinoma Cells\\\" width=\\\"200\\\" > | <img src=\\\"https://github.com/kapoorlab/vollseg-napari/blob/main/vollseg_napari/images/Metrics_carcinoma.png\\\" title=\\\"Metrics Carcinoma Cells\\\" width=\\\"200\\\" >  |\\n|  |  | | | | | | |  |\\n| <img src=\\\"https://github.com/kapoorlab/vollseg-napari/blob/main/vollseg_napari/images/Xenopus_tissue_raw.png\\\"  title=\\\"Raw Xenopus Tissue\\\" width=\\\"200\\\">| LaserScanningConfocalMicroscopy 2D single channel| [Dataset](https://zenodo.org/record/6076614#.YjBaNnrMJD8)| [UNET Model](https://zenodo.org/record/6060378/)  |<img src=\\\"https://github.com/kapoorlab/vollseg-napari/blob/main/vollseg_napari/images/Xenopus_tissue_GT.png\\\" title=\\\"GT Xenopus Tissue\\\" width=\\\"200\\\"> | UNET model| [Colab Notebook](https://github.com/kapoorlab/VollSeg/blob/main/examples/Predict/Colab_VollSeg_tissue_segmentation.ipynb) |<img src=\\\"https://github.com/kapoorlab/vollseg-napari/blob/main/vollseg_napari/images/Xenopus_tissue_pred.png\\\" title=\\\"Prediction Xenopus Tissue\\\" width=\\\"200\\\" > | No Metrics  |\\n|  |  | | | | | | |  |\\n| <img src=\\\"https://github.com/kapoorlab/vollseg-napari/blob/main/vollseg_napari/images/microtubule_kymo_raw.png\\\"  title=\\\"Raw Microtubule Kymograph\\\" width=\\\"200\\\">| TIRF + MultiKymograph Fiji tool 2D single channel| [Training Dataset](https://zenodo.org/record/6355705/files/Microtubule_edgedetector_training.zip)| [UNET Model](https://zenodo.org/record/6355705/)  |<img src=\\\"https://github.com/kapoorlab/vollseg-napari/blob/main/vollseg_napari/images/microtubule_kymo_GT.png\\\" title=\\\"GT Microtubule Kymograph\\\" width=\\\"200\\\"> | UNET model| [Colab Notebook](https://github.com/kapoorlab/VollSeg/blob/main/examples/Predict/Colab_Microtubule_kymo_segmentation.ipynb) |<img src=\\\"https://github.com/kapoorlab/vollseg-napari/blob/main/vollseg_napari/images/microtubule_kymo_pred.png\\\" title=\\\"Prediction Microtubule Kymographe\\\" width=\\\"200\\\" > | No Metrics  |\\n|  |  | | | | | | |  |\\n| <img src=\\\"https://github.com/kapoorlab/vollseg-napari/blob/main/vollseg_napari/images/lung_xray_raw.png\\\"  title=\\\"Raw Lung Xray\\\" width=\\\"200\\\">| XRay of Lung 2D single channel| [Training Dataset](https://www.kaggle.com/nikhilpandey360/lung-segmentation-from-chest-x-ray-dataset)| [UNET Model](https://zenodo.org/record/6060177/)  |<img src=\\\"https://github.com/kapoorlab/vollseg-napari/blob/main/vollseg_napari/images/lung_xray_GT.png\\\" title=\\\"GT Lung Xray\\\" width=\\\"200\\\"> | UNET model| [Colab Notebook](https://github.com/kapoorlab/VollSeg/blob/main/examples/Predict/Colab_Microtubule_kymo_segmentation.ipynb) |<img src=\\\"https://github.com/kapoorlab/vollseg-napari/blob/main/vollseg_napari/images/lung_xray_pred.png\\\" title=\\\"Prediction Lung Xray\\\" width=\\\"200\\\" > | <img src=\\\"https://github.com/kapoorlab/vollseg-napari/blob/main/vollseg_napari/images/Metrics_lung_xray.png\\\" title=\\\"Metrics Lung Xray\\\" width=\\\"200\\\" >   |\\n|  |  | | | | | | |  |\\n| <img src=\\\"https://github.com/kapoorlab/vollseg-napari/blob/main/vollseg_napari/images/nuclei_mask_raw.png\\\"  title=\\\"Raw Nuclei Mask\\\" width=\\\"200\\\">| LaserScanningConfocalMicroscopy 2D single channell| [Test Dataset](https://zenodo.org/record/6359349/)|Private |<img src=\\\"https://github.com/kapoorlab/vollseg-napari/blob/main/vollseg_napari/images/nuclei_mask_GT.png\\\" title=\\\"GT Nuclei Mask\\\" width=\\\"200\\\"> | UNET model| [Colab Notebook](https://github.com/kapoorlab/VollSeg/blob/main/examples/Predict/Colab_Microtubule_kymo_segmentation.ipynb) |<img src=\\\"https://github.com/kapoorlab/vollseg-napari/blob/main/vollseg_napari/images/nuclei_mask_pred.png\\\" title=\\\"Prediction Nuclei Mask\\\" width=\\\"200\\\" > | No metrics   |\\n|  |  | | | | | | |  |\\n| <img src=\\\"https://github.com/kapoorlab/vollseg-napari/blob/main/vollseg_napari/images/nuclei_raw.png\\\"  title=\\\"Raw Nuclei\\\" width=\\\"200\\\">| LaserScanningConfocalMicroscopy 3D single channell| [Test Dataset](https://zenodo.org/record/6359295/)|Private |<img src=\\\"https://github.com/kapoorlab/vollseg-napari/blob/main/vollseg_napari/images/nuclei_GT.png\\\" title=\\\"GT Nuclei\\\" width=\\\"200\\\"> | UNET model + StarDist model + ROI model| [Colab Notebook](https://github.com/kapoorlab/VollSeg/blob/main/examples/Predict/Colab_VollSeg_star_roi.ipynb) |<img src=\\\"https://github.com/kapoorlab/vollseg-napari/blob/main/vollseg_napari/images/nuclei_pred.png\\\" title=\\\"Prediction Nuclei\\\" width=\\\"200\\\" > |  <img src=\\\"https://github.com/kapoorlab/vollseg-napari/blob/main/vollseg_napari/images/Metrics_nuclei.png\\\" title=\\\"Metrics Nuclei\\\" width=\\\"200\\\" >   |\\n\\n\\n## Troubleshooting & Support\\n\\n- The [image.sc forum](https://forum.image.sc/tag/vollseg) is the best place to start getting help and support. Make sure to use the tag `vollseg`, since we are monitoring all questions with this tag.\\n- If you have technical questions or found a bug, feel free to [open an issue](https://github.com/kapoorlab/vollseg-napari/issues).\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"VollSeg Napari Plugin\\n\\n\\n\\n\\n\\nThis project provides the napari plugin for VollSeg, a deep learning based 2D and 3D segmentation tool for irregular shaped cells. VollSeg has originally been developed (see papers) for the segmentation of densely packed membrane labelled cells in challenging images with low signal-to-noise ratios. The plugin allows to apply pretrained and custom trained models from within napari.\\nFor detailed demo of the plugin see these videos and a short video about the parameter selection \\nInstallation & Usage\\nInstall the plugin with pip install vollseg-napari or from within napari via Plugins > Install/Uninstall Package(s)…. If you want GPU-accelerated prediction, please read the more detailed installation instructions for VollSeg.\\nYou can activate the plugin in napari via Plugins > VollSeg: VollSeg. Example images for testing are provided via File > Open Sample > VollSeg.\\nIf you use this plugin for your research, please cite us.\\nGPU_Installation\\nThis package is compatible with Python 3.6 - 3.9.\\n\\n\\nPlease first install TensorFlow\\n(TensorFlow 2) by following the official instructions.\\nFor GPU support, it is very\\nimportant to install the specific versions of CUDA and cuDNN that are\\ncompatible with the respective version of TensorFlow. (If you need help and can use conda, take a look at this.)\\n\\n\\nVollSeg can then be installed with pip:\\n\\n\\nIf you installed TensorFlow 2 (version 2.x.x):\\npip install vollseg\\n\\n\\n\\n\\nExamples\\nVollSeg comes with different options to combine CARE based denoising with UNET, StarDist and segmentation in a region of interest (ROI). We present some examples which are represent optimal combination of these different modes for segmenting different cell types. We summarize this in the table below:\\n| Example Image | Description | Training Data | Trained Model | GT image   | Optimal combination  | Notebook Code | Model Prediction | Metrics |\\n| --- | --- |--- | --- |--- | --- |--- | --- | --- |\\n| | Light sheet fused from four angles 3D single channel| Training Data ~320 GB| UNET model | | UNET model, slice_merge = False | Colab Notebook | |   |\\n|  |  | | | | | | |  |\\n| | Confocal microscopy 3D single channel 8 bit| Training Data| Denoising Model and StarDist Model | | StarDist model + Denoising Model, dounet = False | Colab Notebook | |   |\\n|  |  | | | | | | |  |\\n| | LaserScanningConfocalMicroscopy 2D single channel| Dataset| UNET Model  | | UNET model| Colab Notebook | | No Metrics  |\\n|  |  | | | | | | |  |\\n| | TIRF + MultiKymograph Fiji tool 2D single channel| Training Dataset| UNET Model  | | UNET model| Colab Notebook | | No Metrics  |\\n|  |  | | | | | | |  |\\n| | XRay of Lung 2D single channel| Training Dataset| UNET Model  | | UNET model| Colab Notebook | |    |\\n|  |  | | | | | | |  |\\n| | LaserScanningConfocalMicroscopy 2D single channell| Test Dataset|Private | | UNET model| Colab Notebook | | No metrics   |\\n|  |  | | | | | | |  |\\n| | LaserScanningConfocalMicroscopy 3D single channell| Test Dataset|Private | | UNET model + StarDist model + ROI model| Colab Notebook | |     |\\nTroubleshooting & Support\\n\\nThe image.sc forum is the best place to start getting help and support. Make sure to use the tag vollseg, since we are monitoring all questions with this tag.\\nIf you have technical questions or found a bug, feel free to open an issue.\\n\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"vollseg-napari\",\"documentation\":\"https://github.com/kapoorlab/vollseg-napari\",\"first_released\":\"2021-12-10T16:26:44.646909Z\",\"license\":\"BSD-3-Clause\",\"name\":\"vollseg-napari\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\",\"sample_data\"],\"project_site\":\"https://github.com/kapoorlab/vollseg-napari\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-10-03T17:46:52.271244Z\",\"report_issues\":\"https://github.com/kapoorlab/vollseg-napari/issues\",\"requirements\":[\"vollseg\",\"napari (>=0.4.13)\",\"magicgui (>=0.4.0)\",\"tensorflow ; platform_system != \\\"Darwin\\\" or platform_machine != \\\"arm64\\\"\",\"tensorflow-macos ; platform_system == \\\"Darwin\\\" and platform_machine == \\\"arm64\\\"\",\"pytest ; extra == 'test'\",\"pytest-qt ; extra == 'test'\",\"napari[pyqt] (>=0.4.13) ; extra == 'test'\"],\"summary\":\"Irregular cell shape segmentation using VollSeg\",\"support\":\"https://forum.image.sc/tag/vollseg-napari\",\"twitter\":\"https://twitter.com/entracod\",\"version\":\"2.3.7\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Hanjin Liu\"}],\"code_repository\":\"https://github.com/hanjinliu/napari-result-stack\",\"description\":\"# napari-result-stack\\n\\n[![License BSD-3](https://img.shields.io/pypi/l/napari-result-stack.svg?color=green)](https://github.com/hanjinliu/napari-result-stack/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-result-stack.svg?color=green)](https://pypi.org/project/napari-result-stack)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-result-stack.svg?color=green)](https://python.org)\\n[![tests](https://github.com/hanjinliu/napari-result-stack/workflows/tests/badge.svg)](https://github.com/hanjinliu/napari-result-stack/actions)\\n[![codecov](https://codecov.io/gh/hanjinliu/napari-result-stack/branch/main/graph/badge.svg)](https://codecov.io/gh/hanjinliu/napari-result-stack)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-result-stack)](https://napari-hub.org/plugins/napari-result-stack)\\n\\nWidgets and type annotations for storing function results of any types.\\n\\n## `Stored` type\\n\\nType `Stored[T]` is equivalent to `T` for the type checker, but `magicgui` is aware of this annotation and behaves as a \\\"storage\\\" for the `T` instances.\\n\\n```python\\nfrom pathlib import Path\\nimport pandas as pd\\nfrom magicgui import magicgui\\nfrom napari_result_stack import Stored\\n\\n# Returned values will be stored in a result stack.\\n@magicgui\\ndef provide_data(path: Path) -> Stored[pd.DataFrame]:\\n    return pd.read_csv(path)\\n\\n# You can choose one of the values stored in the result stack\\n# for the argument `df` from a ComboBox widget.\\n@magicgui\\ndef print_data(df: Stored[pd.DataFrame]):\\n    print(df)\\n```\\n\\n![](https://github.com/hanjinliu/napari-result-stack/blob/main/images/demo-0.gif)\\n\\n- Different types use different storage. e.g. `Stored[int]` and `Stored[str]` do not share the same place.\\n- Even for the same type, you can specify the second key to split the storage. e.g. `Stored[int]`, `Stored[int, 0]` and `Stored[int, \\\"my-plugin-name\\\"]` use the distinct storages.\\n\\n## Manually store variables\\n\\n`Stored.valuesof[T]` is a `list`-like object that returns a view of the values stored in `Stored[T]`. This value view is useful if you want to store values outside `@magicgui`.\\n\\n```python\\nfrom magicgui.widgets import PushButton\\nfrom datetime import datetime\\nfrom napari_result_stack import Stored\\n\\nbutton = PushButton(text=\\\"Click!\\\")\\n\\n@button.changed.connect\\ndef _record_now():\\n    Stored.valuesof[datetime].append(datetime.now())\\n\\n```\\n\\n## Result stack widget\\n\\n`napari-result-stack` provides a plugin widget that is helpful to inspect all the stored values.\\n\\n![](https://github.com/hanjinliu/napari-result-stack/blob/main/images/demo-1.gif)\\n\\n\\n<details><summary>Show code</summary><div>\\n\\n```python\\nfrom napari_result_stack import Stored\\nfrom magicgui import magicgui\\nimport numpy as np\\nimport pandas as pd\\n\\n@magicgui\\ndef f0() -> Stored[pd.DataFrame]:\\n    return pd.DataFrame(np.random.random((4, 3)))\\n\\n@magicgui\\ndef f1(x: Stored[pd.DataFrame]) -> Stored[float]:\\n    return np.mean(np.array(x))\\n\\nviewer.window.add_dock_widget(f0, name=\\\"returns a DataFrame\\\")\\nviewer.window.add_dock_widget(f1, name=\\\"mean of a DataFrame\\\")\\n```\\n\\n---\\n</div></details>\\n\\n\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/stable/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-result-stack` via [pip]:\\n\\n    pip install napari-result-stack\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/hanjinliu/napari-result-stack.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-result-stack\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/hanjinliu/napari-result-stack/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-result-stack\\n\\n\\n\\n\\n\\n\\nWidgets and type annotations for storing function results of any types.\\nStored type\\nType Stored[T] is equivalent to T for the type checker, but magicgui is aware of this annotation and behaves as a \\\"storage\\\" for the T instances.\\n```python\\nfrom pathlib import Path\\nimport pandas as pd\\nfrom magicgui import magicgui\\nfrom napari_result_stack import Stored\\nReturned values will be stored in a result stack.\\n@magicgui\\ndef provide_data(path: Path) -> Stored[pd.DataFrame]:\\n    return pd.read_csv(path)\\nYou can choose one of the values stored in the result stack\\nfor the argument df from a ComboBox widget.\\n@magicgui\\ndef print_data(df: Stored[pd.DataFrame]):\\n    print(df)\\n```\\n\\n\\nDifferent types use different storage. e.g. Stored[int] and Stored[str] do not share the same place.\\nEven for the same type, you can specify the second key to split the storage. e.g. Stored[int], Stored[int, 0] and Stored[int, \\\"my-plugin-name\\\"] use the distinct storages.\\n\\nManually store variables\\nStored.valuesof[T] is a list-like object that returns a view of the values stored in Stored[T]. This value view is useful if you want to store values outside @magicgui.\\n```python\\nfrom magicgui.widgets import PushButton\\nfrom datetime import datetime\\nfrom napari_result_stack import Stored\\nbutton = PushButton(text=\\\"Click!\\\")\\n@button.changed.connect\\ndef _record_now():\\n    Stored.valuesof[datetime].append(datetime.now())\\n```\\nResult stack widget\\nnapari-result-stack provides a plugin widget that is helpful to inspect all the stored values.\\n\\nShow code\\n\\n```python\\nfrom napari_result_stack import Stored\\nfrom magicgui import magicgui\\nimport numpy as np\\nimport pandas as pd\\n\\n@magicgui\\ndef f0() -> Stored[pd.DataFrame]:\\n    return pd.DataFrame(np.random.random((4, 3)))\\n\\n@magicgui\\ndef f1(x: Stored[pd.DataFrame]) -> Stored[float]:\\n    return np.mean(np.array(x))\\n\\nviewer.window.add_dock_widget(f0, name=\\\"returns a DataFrame\\\")\\nviewer.window.add_dock_widget(f1, name=\\\"mean of a DataFrame\\\")\\n```\\n\\n---\\n\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-result-stack via pip:\\npip install napari-result-stack\\n\\nTo install latest development version :\\npip install git+https://github.com/hanjinliu/napari-result-stack.git\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-result-stack\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"Result stack\",\"documentation\":\"https://github.com/hanjinliu/napari-result-stack#README.md\",\"first_released\":\"2023-01-27T12:34:21.726357Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-result-stack\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/hanjinliu/napari-result-stack\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2023-01-27T12:34:21.726357Z\",\"report_issues\":\"https://github.com/hanjinliu/napari-result-stack/issues\",\"requirements\":[\"numpy\",\"magicgui\",\"qtpy\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\"],\"summary\":\"Widgets and type annotations for storing function results of any types\",\"support\":\"https://github.com/hanjinliu/napari-result-stack/issues\",\"twitter\":\"\",\"version\":\"0.0.1\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Jun Ma\"}],\"code_repository\":\"https://github.com/JunMa11/napari-unicell\",\"description\":\"# napari-unicell\\n\\n[![License Apache Software License 2.0](https://img.shields.io/pypi/l/napari-unicell.svg?color=green)](https://github.com/JunMa11/napari-unicell/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-unicell.svg?color=green)](https://pypi.org/project/napari-unicell)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-unicell.svg?color=green)](https://python.org)\\n[![tests](https://github.com/JunMa11/napari-unicell/workflows/tests/badge.svg)](https://github.com/JunMa11/napari-unicell/actions)\\n[![codecov](https://codecov.io/gh/JunMa11/napari-unicell/branch/main/graph/badge.svg)](https://codecov.io/gh/JunMa11/napari-unicell)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-unicell)](https://napari-hub.org/plugins/napari-unicell)\\n\\nuniversal cell segmentation models\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-unicell` via [pip]:\\n\\n    pip install napari-unicell\\n\\n\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [Apache Software License 2.0] license,\\n\\\"napari-unicell\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/JunMa11/napari-unicell/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-unicell\\n\\n\\n\\n\\n\\n\\nuniversal cell segmentation models\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-unicell via pip:\\npip install napari-unicell\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the Apache Software License 2.0 license,\\n\\\"napari-unicell\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"unicell\",\"documentation\":\"https://github.com/JunMa11/napari-unicell#README.md\",\"first_released\":\"2022-11-12T22:55:14.228265Z\",\"license\":\"Apache-2.0\",\"name\":\"napari-unicell\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/JunMa11/napari-unicell\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-11-12T23:14:45.085988Z\",\"report_issues\":\"https://github.com/JunMa11/napari-unicell/issues\",\"requirements\":[\"torch\",\"imagecodecs\",\"scipy\",\"numpy\",\"magicgui\",\"qtpy\",\"scikit-image\",\"monai\",\"einops\",\"PyQt5\",\"napari\",\"napari-plugin-engine\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\"],\"summary\":\"universal cell segmentation models\",\"support\":\"https://github.com/JunMa11/napari-unicell/issues\",\"twitter\":\"\",\"version\":\"0.0.1.post3\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Mauro Silberberg\"},{\"name\":\"Hernán Edgardo Grecco\"}],\"citations\":{\"APA\":\"Silberberg M., Grecco H.E. Robust and unbiased estimation of the background distribution for automated quantitative imaging DOI: 10.1364/JOSAA.477468\\n\",\"BibTex\":\"@misc{YourReferenceHere,\\nauthor = {Silberberg, Mauro and Grecco, Hernán Edgardo},\\ndoi = {10.1364/JOSAA.477468},\\ntitle = {Robust and unbiased estimation of the background distribution for automated quantitative imaging}\\n}\\n\",\"RIS\":\"TY  - GEN\\nAB  - Background estimation is the first step in quantitative analysis of images. It has an impact on all subsequent analyses, in particular for segmentation and calculation of ratiometric quantities. Most methods recover only a single value such as the median or yield a biased estimation in non-trivial cases. We introduce, to our knowledge, the first method to recover an unbiased estimation of background distribution. It leverages the lack of local spatial correlation in background pixels to robustly select a subset that accurately represents the background. The resulting background distribution can be used to test for foreground membership of individual pixels or estimate confidence intervals in derived quantities.\\nAU  - Silberberg, Mauro\\nAU  - Grecco, Hernán Edgardo\\nDO  - 10.1364/JOSAA.477468\\nTI  - Robust and unbiased estimation of the background distribution for automated quantitative imaging\\nER\\n\",\"citation\":\"# This CITATION.cff file was generated with cffinit.\\n# Visit https://bit.ly/cffinit to generate yours today!\\n\\ncff-version: 1.2.0\\ntitle: >-\\n  Robust and unbiased estimation of the background\\n  distribution for automated quantitative imaging\\nmessage: >-\\n  If you use this software, please cite it using the\\n  metadata from this file.\\ntype: software\\nauthors:\\n  - given-names: Mauro\\n    family-names: Silberberg\\n    email: maurosilber@df.uba.ar\\n    orcid: 'https://orcid.org/0000-0002-2402-1100'\\n    affiliation: >-\\n      Department of Physics, FCEN, University of\\n      Buenos Aires and IFIBA, CONICET, Buenos Aires.\\n      C1428EHA, Argentina\\n  - given-names: Hernán Edgardo\\n    family-names: Grecco\\n    email: hgrecco@df.uba.ar\\n    affiliation: >-\\n      Department of Physics, FCEN, University of\\n      Buenos Aires and IFIBA, CONICET, Buenos Aires.\\n      C1428EHA, Argentina; and, Department of\\n      Systemic Cell Biology, Max Planck Institute of\\n      Molecular Physiology, Dortmund, 44227, Germany\\n    orcid: 'https://orcid.org/0000-0002-1165-4320'\\nidentifiers:\\n  - type: doi\\n    value: 10.1364/JOSAA.477468\\nabstract: >-\\n  Background estimation is the first step in quantitative analysis of images.\\n  It has an impact on all subsequent analyses,\\n  in particular for segmentation and calculation of ratiometric quantities.\\n  Most methods recover only a single value such as the median\\n  or yield a biased estimation in non-trivial cases.\\n  We introduce,\\n  to our knowledge,\\n  the first method to recover an unbiased estimation of background distribution.\\n  It leverages the lack of local spatial correlation in background pixels\\n  to robustly select a subset that accurately represents the background.\\n  The resulting background distribution can be used to test for foreground membership of individual pixels\\n  or estimate confidence intervals in derived quantities.\\nlicense: MIT\\n\"},\"code_repository\":\"https://github.com/maurosilber/smo\",\"description\":\"![PyPi](https://img.shields.io/pypi/pyversions/smo.svg)\\n[![License](https://img.shields.io/github/license/maurosilber/smo)](https://opensource.org/licenses/MIT)\\n[![PyPi](https://img.shields.io/pypi/v/smo.svg)](https://pypi.python.org/pypi/smo)\\n[![Conda](https://img.shields.io/conda/pn/conda-forge/smo)](https://anaconda.org/conda-forge/smo)\\n\\n# SMO\\n\\nSMO is a Python package that implements the Silver Mountain Operator (SMO), which allows to recover an unbiased estimation of the background intensity distribution in a robust way.\\n\\nWe provide an easy to use Python package and plugins for some of the major image processing softwares: [napari](https://napari.org), [CellProfiler](https://cellprofiler.org), and [ImageJ](https://imagej.net) / [FIJI](https://fiji.sc). See Plugins section below.\\n\\n## Citation\\n\\nTo learn more about the theory behind SMO, you can read the [pre-print in BioRxiv](https://doi.org/10.1101/2021.11.09.467975).\\n\\nIf you use this software, please cite that pre-print.\\n\\n## Usage\\n\\nTo obtain a background-corrected image, it is as straightforward as:\\n\\n```python\\nimport skimage.data\\nfrom smo import SMO\\n\\nimage = skimage.data.human_mitosis()\\nsmo = SMO(sigma=0, size=7, shape=(1024, 1024))\\nbackground_corrected_image = smo.bg_corrected(image)\\n```\\n\\nwhere we used a sample image from `scikit-image`.\\nBy default,\\nthe background correction subtracts the median value of the background distribution.\\nNote that the background regions will end up with negative values,\\nbut with a median value of 0.\\n\\nA notebook explaining in more detail the meaning of the parameters and other possible uses for SMO is available here: [smo/examples/usage.ipynb](https://github.com/maurosilber/SMO/blob/main/smo/examples/usage.ipynb) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/maurosilber/SMO/blob/main/smo/examples/usage.ipynb).\\n\\n## Installation\\n\\nIt can be installed with `pip` from PyPI:\\n\\n```\\npip install smo\\n```\\n\\nor with `conda` from the conda-forge channel:\\n\\n```\\nconda install -c conda-forge smo\\n```\\n\\n## Plugins\\n### Napari\\n\\nA [napari](https://napari.org) plugin is available.\\n\\nTo install:\\n\\n- Option 1: in napari, go to `Plugins > Install/Uninstall Plugins...` in the top menu, search for `smo` and click on the install button.\\n\\n- Option 2: just `pip` install this package in the napari environment.\\n\\nIt will appear in the `Plugins` menu.\\n\\n### CellProfiler\\n\\nA [CellProfiler](https://cellprofiler.org) plugin in available in the [smo/plugins/cellprofiler](smo/plugins/cellprofiler) folder.\\n\\n![](images/CellProfiler_SMO.png)\\n\\nTo install, save [this file](https://raw.githubusercontent.com/maurosilber/SMO/main/smo/plugins/cellprofiler/smo.py) into your CellProfiler plugins folder. You can find (or change) the location of your plugins directory in `File > Preferences > CellProfiler plugins directory`.\\n\\n### ImageJ / FIJI\\n\\nAn [ImageJ](https://imagej.net) / [FIJI](https://fiji.sc) plugin is available in the [smo/plugins/imagej](smo/plugins/imagej) folder.\\n\\n![](images/ImageJ_SMO.png)\\n\\nTo install, download [this file](https://raw.githubusercontent.com/maurosilber/SMO/main/smo/plugins/imagej/smo.py) and:\\n\\n- Option 1: in the ImageJ main window, click on `Plugins > Install... (Ctrl+Shift+M)`, which opens a file chooser dialog. Browse and select the downloaded file. It will prompt to restart ImageJ for changes to take effect.\\n\\n- Option 2: copy into your ImageJ plugins folder (`File > Show Folder > Plugins`).\\n\\nTo use the plugin, type `smo` on the bottom right search box:\\n\\n![](images/ImageJ_MainWindow.png)\\n\\nselect `smo` in the `Quick Search` window and click on the `Run` button.\\n\\n![](images/ImageJ_QuickSearch.png)\\n\\nNote: the ImageJ plugin does not check that saturated pixels are properly excluded.\\n\\n## Development\\n\\nCode style is enforced via pre-commit hooks. To set up a development environment, clone the repository, optionally create a virtual environment, install the [dev] extras and the pre-commit hooks:\\n\\n```\\ngit clone https://github.com/maurosilber/SMO\\ncd SMO\\nconda create -n smo python pip numpy scipy\\npip install -e .[dev]\\npre-commit install\\n```\\n\",\"description_content_type\":\"text/markdown; charset=UTF-8; variant=GFM\",\"description_text\":\"\\n\\n\\n\\nSMO\\nSMO is a Python package that implements the Silver Mountain Operator (SMO), which allows to recover an unbiased estimation of the background intensity distribution in a robust way.\\nWe provide an easy to use Python package and plugins for some of the major image processing softwares: napari, CellProfiler, and ImageJ / FIJI. See Plugins section below.\\nCitation\\nTo learn more about the theory behind SMO, you can read the pre-print in BioRxiv.\\nIf you use this software, please cite that pre-print.\\nUsage\\nTo obtain a background-corrected image, it is as straightforward as:\\n```python\\nimport skimage.data\\nfrom smo import SMO\\nimage = skimage.data.human_mitosis()\\nsmo = SMO(sigma=0, size=7, shape=(1024, 1024))\\nbackground_corrected_image = smo.bg_corrected(image)\\n```\\nwhere we used a sample image from scikit-image.\\nBy default,\\nthe background correction subtracts the median value of the background distribution.\\nNote that the background regions will end up with negative values,\\nbut with a median value of 0.\\nA notebook explaining in more detail the meaning of the parameters and other possible uses for SMO is available here: smo/examples/usage.ipynb .\\nInstallation\\nIt can be installed with pip from PyPI:\\npip install smo\\nor with conda from the conda-forge channel:\\nconda install -c conda-forge smo\\nPlugins\\nNapari\\nA napari plugin is available.\\nTo install:\\n\\n\\nOption 1: in napari, go to Plugins > Install/Uninstall Plugins... in the top menu, search for smo and click on the install button.\\n\\n\\nOption 2: just pip install this package in the napari environment.\\n\\n\\nIt will appear in the Plugins menu.\\nCellProfiler\\nA CellProfiler plugin in available in the smo/plugins/cellprofiler folder.\\n\\nTo install, save this file into your CellProfiler plugins folder. You can find (or change) the location of your plugins directory in File > Preferences > CellProfiler plugins directory.\\nImageJ / FIJI\\nAn ImageJ / FIJI plugin is available in the smo/plugins/imagej folder.\\n\\nTo install, download this file and:\\n\\n\\nOption 1: in the ImageJ main window, click on Plugins > Install... (Ctrl+Shift+M), which opens a file chooser dialog. Browse and select the downloaded file. It will prompt to restart ImageJ for changes to take effect.\\n\\n\\nOption 2: copy into your ImageJ plugins folder (File > Show Folder > Plugins).\\n\\n\\nTo use the plugin, type smo on the bottom right search box:\\n\\nselect smo in the Quick Search window and click on the Run button.\\n\\nNote: the ImageJ plugin does not check that saturated pixels are properly excluded.\\nDevelopment\\nCode style is enforced via pre-commit hooks. To set up a development environment, clone the repository, optionally create a virtual environment, install the [dev] extras and the pre-commit hooks:\\ngit clone https://github.com/maurosilber/SMO\\ncd SMO\\nconda create -n smo python pip numpy scipy\\npip install -e .[dev]\\npre-commit install\",\"development_status\":[],\"display_name\":\"smo\",\"documentation\":\"\",\"first_released\":\"2021-09-21T15:54:33.842241Z\",\"license\":\"MIT\",\"name\":\"smo\",\"npe2\":false,\"operating_system\":[\"Operating System :: MacOS :: MacOS X\",\"Operating System :: Microsoft :: Windows\",\"Operating System :: POSIX\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/maurosilber/smo\",\"python_version\":\"\",\"reader_file_extensions\":[],\"release_date\":\"2023-02-06T15:07:40.158799Z\",\"report_issues\":\"\",\"requirements\":[\"numpy\",\"scipy\",\"typing-extensions ; python_version < \\\"3.9\\\"\",\"pre-commit ; extra == 'dev'\",\"pytest ; extra == 'dev'\",\"tox ; extra == 'dev'\"],\"summary\":\"Implementation of the Silver Mountain Operator (SMO) for the estimation of background distributions.\",\"support\":\"\",\"twitter\":\"\",\"version\":\"2.0.2\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"meriadec.prigent@gmail.com\",\"name\":\"Sylvain Prigent\"}],\"code_repository\":\"https://github.com/sylvainprigent/napari-sdeconv\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-sdeconv\"}],\"description\":\"# Description\\n\\n`napari-sdeconv` is a suite of **Napari** plugins for the `sdeconv library\\n<https://sylvainprigent.github.io/sdeconv/>`_ dedicated to 2D and 3D images deconvolution. It contains multiple\\ndeconvolution algorithms and PSF Generators.\\n\\n* Example of deconvolution with Spitfire\\n\\n![img](https://raw.githubusercontent.com/sylvainprigent/napari-sdeconv/main/docs/images/spitfire3D.png)\\n\\n\\n* Example of how to generate a 3D PSF\\n\\n![img](https://raw.githubusercontent.com/sylvainprigent/napari-sdeconv/main/docs/images/gibson_lanni.png)\\n\\n\\n## Installation\\n\\nYou can install `napari-sdeconv` via [pip]:\\n\\n    pip install napari-sdeconv\\n    \\n\\nNote that the current version of the package only support python 3.9    \\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [GNU GPL v3.0] license,\\n\\\"napari-tracks-reader\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/sylvainprigent/napari-sdeconv/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"Description\\nnapari-sdeconv is a suite of Napari plugins for the sdeconv library\\n<https://sylvainprigent.github.io/sdeconv/>_ dedicated to 2D and 3D images deconvolution. It contains multiple\\ndeconvolution algorithms and PSF Generators.\\n\\nExample of deconvolution with Spitfire\\n\\n\\n\\nExample of how to generate a 3D PSF\\n\\n\\nInstallation\\nYou can install napari-sdeconv via pip:\\npip install napari-sdeconv\\n\\nNote that the current version of the package only support python 3.9    \\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the GNU GPL v3.0 license,\\n\\\"napari-tracks-reader\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari sdeconv\",\"documentation\":\"https://github.com/sylvainprigent/napari-sdeconv#README.md\",\"first_released\":\"2021-09-02T14:23:19.264744Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-sdeconv\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\",\"sample_data\"],\"project_site\":\"https://github.com/sylvainprigent/napari-sdeconv\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-07-06T18:51:42.845316Z\",\"report_issues\":\"https://github.com/sylvainprigent/napari-sdeconv/issues\",\"requirements\":[\"numpy\",\"magicgui\",\"qtpy\",\"sdeconv (>=1.0.1)\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\",\"sdeconv (>=1.0.1) ; extra == 'testing'\"],\"summary\":\"2D and 3D deconvolution\",\"support\":\"https://github.com/sylvainprigent/napari-sdeconv/issues\",\"twitter\":\"\",\"version\":\"1.0.1\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Sander van Otterdijk\"}],\"code_repository\":\"https://github.com/SanderSMFISH/napari-buds\",\"description\":\"# napari-buds\\n\\n[![License BSD-3](https://img.shields.io/pypi/l/napari-buds.svg?color=green)](https://github.com/SanderSMFISH/napari-buds/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-buds.svg?color=green)](https://pypi.org/project/napari-buds)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-buds.svg?color=green)](https://python.org)\\n[![tests](https://github.com/SanderSMFISH/napari-buds/workflows/tests/badge.svg)](https://github.com/SanderSMFISH/napari-buds/actions)\\n[![codecov](https://codecov.io/gh/SanderSMFISH/napari-buds/branch/main/graph/badge.svg)](https://codecov.io/gh/SanderSMFISH/napari-buds)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-buds)](https://napari-hub.org/plugins/napari-buds)\\n\\nRandom-forest automated bud annotation\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/stable/plugins/index.html\\n-->\\n\\n## Installation\\n\\nmake sure you already have installed napari. \\n\\nNext, You can install `napari-buds` via [pip]:\\n\\n    pip install napari-buds\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/SanderSMFISH/napari-buds.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## Documentation\\nNapari-Buds is a random forest based mother-bud annotation plugin for Napari devevoped by the TutucciLab (https://www.tutuccilab.com/) of the systems biology group at the Vrije Universiteit van Amsterdam. Mother-bud annotation requires single or multichannel 2D images of budding yeast and a fluorescent marker that localizes to the bud. In the example dataset provided smFISH DNA-probes were used as localized bud marker.The GUI layout for random forest based classification was inspired by ImageJ 'plugin Weka Segmentation' [1]. **Before installation make sure you have a working version of napari installed (pip install \\\"napari[all]\\\").** Napari-Buds is a random forest based mother-bud annotation plugin for Napari developed by the TutucciLab (https://www.tutuccilab.com/) of the systems biology group at the Vrije Universiteit van Amsterdam. Mother-bud annotation requires single or multichannel 2D images of budding yeast and a fluorescent marker that localizes to the bud. In the example dataset provided smFISH DNA-probes were used as localized bud marker.The GUI layout for random forest based classification was inspired by ImageJ 'plugin Weka Segmentation' [1]. \\n\\nPlease follow the workflow described underneath to perform mother-bud annotation:\\n\\n1. Open images in napari and create empty label layer. **Before starting the plugin it is required that a empty label layer is created**.\\nFor multichannel images each channel should be provided seperately to napari.\\nAn example (jupyter) notebook (Open Test Images Napari.ipynb) for loading test data in napari is provided in the notebooks folder. \\nExample dataset can be downloaded from https://zenodo.org/record/7004556#.YwM1_HZBztU. \\n    \\n2. If multichannel images are unaligned the  translate widget under Plugins>napari-buds>Translate can be used. \\nSelect which layer should be translated to align to the layers in widget menu. Then use the aswd keys to translate (move) the selected layer. \\nTo register changes and update coordinates of the translated image in napari press t. \\n    \\n### Random forest classification\\n3. To open the mother-bud annotation plugin go to Plugins>napari-buds>bud annotation. **Before starting the plugin it is required that a empty label layer is created**.\\n    \\n4. To train a random forest classifier, in the created label layer draw examples of cells, buds and background (see tutorial gif below). \\nIn the Define Label segment of the widget you define which label value (class #label_value) corresponds to cells, buds and background. \\nCurrently, cells and backgrounds and buds **have to be defined in the Define Label segment**  if you want to be able to segment the classification as well.\\nIn the segment **Layers to extract Features from** we can select which layers will be used in training the random forest classifier. \\nNext press **Train classifier**. After training is completed a result layer is added to layer list. \\nInspect the results carefully to asses classifier performance. The trained classifier can be saved using the **save classifier** button.\\nPreviously trained classifier can be loaded by pressing **Load classifier**. Loaded classifier can applied to new images by pressing **Classify**, resulting again in a results layer.\\nIt is possible to change the random forest parameters with **the Set random forest parameters** button and changing the values in the pop up menu.\\nPress **Run** to register changed settings. For an example of the parameters used see: \\nhttps://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html and \\nhttps://scikit-image.org/docs/stable/auto_examples/segmentation/plot_trainable_segmentation.html. \\n    \\n5. Next, we want to perfom watershed segmentation using the result layer. However, for watershed segmentation seeds (also called markers) are required\\n(for an explanation of watershed segmenation see: https://en.wikipedia.org/wiki/Watershed_(image_processing)). \\nTo define the seeds we can either simply threshold on one of the supplied image layers or we can use distance tranform (https://scikit-image.org/docs/stable/auto_examples/segmentation/plot_watershed.html#sphx-glr-auto   examples-segmentation-plot-watershed-py).The resulting seeds layer can be adjusted manually by editing in napari.\\nA good seeds layers correspond to each cell having a single seed (buds are not single cells). To perform watershed segmentation press the **Segment** button.\\n    \\n6. Carefully inspect the resulting cell mask and bud layer. Correct the mistakes in both layers. \\nBud label values should correspond to the label value of the cell mask of mother cell. To verify mother bud relations were drawn correctly\\npress **Draw Mother-Bud relations**. If Mother-Bud relations are correct, you can save both label layers. Mother and buds simply share the same label number.\\nThus, either the mother or bud layer can be manually corrected for mistakes. Corrections can be checked by clicking **Draw Mother-Bud relations** again. \\nmother and buds layer can be saved manually in napari. When using Jupyter notebook mother and bud layers can be saved as shown in Open Test Images Napari.ipynb.\\n\\n7. An example notebook for dataextraction of the created cell and bud masks can be found in the example notebooks folder (Extract_Mother_Buds_relations_from_Masks_and_intergrate_FQ_spot_data.ipynb).This notebooks relates RNA spots (smFISH data found on zenodo) to the mother or bud compartment. \\n\\n\\nSee video for clarification:\\n\\n![Watch the video](https://github.com/SanderSMFISH/napari-buds/blob/main/videos/Napari_bud_gif.gif)\\n\\n## Similar Napari plugins \\n\\n1-napari-accelerated-pixel-and-object-classification (APOC) by Robert Haase.\\n\\n2-napari-feature-classifier.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-buds\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n### Known Issues\\n\\nIf window geometry of the window is unable to be set, this might lead to issues in the display of the widget. For example, part of the widget might fall of the screen.\\nIn these cases, it might help to adjust in your display setting the display scaling to a lower setting. \\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/SanderSMFISH/napari-buds/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n## References\\n1. Arganda-Carreras, I., Kaynig, V., Rueden, C., Eliceiri, K. W., Schindelin, J., Cardona, A., & Sebastian Seung, H. (2017). Trainable Weka Segmentation: a machine learning tool for microscopy pixel classification. Bioinformatics, 33(15), 2424–2426. doi:10.1093/bioinformatics/btx180\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-buds\\n\\n\\n\\n\\n\\n\\nRandom-forest automated bud annotation\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nmake sure you already have installed napari. \\nNext, You can install napari-buds via pip:\\npip install napari-buds\\n\\nTo install latest development version :\\npip install git+https://github.com/SanderSMFISH/napari-buds.git\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nDocumentation\\nNapari-Buds is a random forest based mother-bud annotation plugin for Napari devevoped by the TutucciLab (https://www.tutuccilab.com/) of the systems biology group at the Vrije Universiteit van Amsterdam. Mother-bud annotation requires single or multichannel 2D images of budding yeast and a fluorescent marker that localizes to the bud. In the example dataset provided smFISH DNA-probes were used as localized bud marker.The GUI layout for random forest based classification was inspired by ImageJ 'plugin Weka Segmentation' [1]. Before installation make sure you have a working version of napari installed (pip install \\\"napari[all]\\\"). Napari-Buds is a random forest based mother-bud annotation plugin for Napari developed by the TutucciLab (https://www.tutuccilab.com/) of the systems biology group at the Vrije Universiteit van Amsterdam. Mother-bud annotation requires single or multichannel 2D images of budding yeast and a fluorescent marker that localizes to the bud. In the example dataset provided smFISH DNA-probes were used as localized bud marker.The GUI layout for random forest based classification was inspired by ImageJ 'plugin Weka Segmentation' [1]. \\nPlease follow the workflow described underneath to perform mother-bud annotation:\\n\\n\\nOpen images in napari and create empty label layer. Before starting the plugin it is required that a empty label layer is created.\\nFor multichannel images each channel should be provided seperately to napari.\\nAn example (jupyter) notebook (Open Test Images Napari.ipynb) for loading test data in napari is provided in the notebooks folder. \\nExample dataset can be downloaded from https://zenodo.org/record/7004556#.YwM1_HZBztU. \\n\\n\\nIf multichannel images are unaligned the  translate widget under Plugins>napari-buds>Translate can be used. \\nSelect which layer should be translated to align to the layers in widget menu. Then use the aswd keys to translate (move) the selected layer. \\nTo register changes and update coordinates of the translated image in napari press t. \\n\\n\\nRandom forest classification\\n\\n\\nTo open the mother-bud annotation plugin go to Plugins>napari-buds>bud annotation. Before starting the plugin it is required that a empty label layer is created.\\n\\n\\nTo train a random forest classifier, in the created label layer draw examples of cells, buds and background (see tutorial gif below). \\nIn the Define Label segment of the widget you define which label value (class #label_value) corresponds to cells, buds and background. \\nCurrently, cells and backgrounds and buds have to be defined in the Define Label segment  if you want to be able to segment the classification as well.\\nIn the segment Layers to extract Features from we can select which layers will be used in training the random forest classifier. \\nNext press Train classifier. After training is completed a result layer is added to layer list. \\nInspect the results carefully to asses classifier performance. The trained classifier can be saved using the save classifier button.\\nPreviously trained classifier can be loaded by pressing Load classifier. Loaded classifier can applied to new images by pressing Classify, resulting again in a results layer.\\nIt is possible to change the random forest parameters with the Set random forest parameters button and changing the values in the pop up menu.\\nPress Run to register changed settings. For an example of the parameters used see: \\nhttps://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html and \\nhttps://scikit-image.org/docs/stable/auto_examples/segmentation/plot_trainable_segmentation.html. \\n\\n\\nNext, we want to perfom watershed segmentation using the result layer. However, for watershed segmentation seeds (also called markers) are required\\n(for an explanation of watershed segmenation see: https://en.wikipedia.org/wiki/Watershed_(image_processing)). \\nTo define the seeds we can either simply threshold on one of the supplied image layers or we can use distance tranform (https://scikit-image.org/docs/stable/auto_examples/segmentation/plot_watershed.html#sphx-glr-auto   examples-segmentation-plot-watershed-py).The resulting seeds layer can be adjusted manually by editing in napari.\\nA good seeds layers correspond to each cell having a single seed (buds are not single cells). To perform watershed segmentation press the Segment button.\\n\\n\\nCarefully inspect the resulting cell mask and bud layer. Correct the mistakes in both layers. \\nBud label values should correspond to the label value of the cell mask of mother cell. To verify mother bud relations were drawn correctly\\npress Draw Mother-Bud relations. If Mother-Bud relations are correct, you can save both label layers. Mother and buds simply share the same label number.\\nThus, either the mother or bud layer can be manually corrected for mistakes. Corrections can be checked by clicking Draw Mother-Bud relations again. \\nmother and buds layer can be saved manually in napari. When using Jupyter notebook mother and bud layers can be saved as shown in Open Test Images Napari.ipynb.\\n\\n\\nAn example notebook for dataextraction of the created cell and bud masks can be found in the example notebooks folder (Extract_Mother_Buds_relations_from_Masks_and_intergrate_FQ_spot_data.ipynb).This notebooks relates RNA spots (smFISH data found on zenodo) to the mother or bud compartment. \\n\\n\\nSee video for clarification:\\n\\nSimilar Napari plugins\\n1-napari-accelerated-pixel-and-object-classification (APOC) by Robert Haase.\\n2-napari-feature-classifier.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-buds\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\\nKnown Issues\\nIf window geometry of the window is unable to be set, this might lead to issues in the display of the widget. For example, part of the widget might fall of the screen.\\nIn these cases, it might help to adjust in your display setting the display scaling to a lower setting. \\nReferences\\n\\nArganda-Carreras, I., Kaynig, V., Rueden, C., Eliceiri, K. W., Schindelin, J., Cardona, A., & Sebastian Seung, H. (2017). Trainable Weka Segmentation: a machine learning tool for microscopy pixel classification. Bioinformatics, 33(15), 2424–2426. doi:10.1093/bioinformatics/btx180\\n\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari BudAnnotation\",\"documentation\":\"https://github.com/SanderSMFISH/napari-buds#README.md\",\"first_released\":\"2022-08-16T13:14:29.716611Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-buds\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\",\"writer\",\"widget\",\"sample_data\"],\"project_site\":\"https://github.com/SanderSMFISH/napari-buds\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[\"*.npy\"],\"release_date\":\"2022-11-08T11:55:49.657916Z\",\"report_issues\":\"https://github.com/SanderSMFISH/napari-buds/issues\",\"requirements\":[\"numpy\",\"magicgui\",\"qtpy\",\"pandas\",\"napari\",\"magic-class\",\"scipy\",\"scikit-learn\",\"scikit-image\",\"matplotlib\",\"joblib\",\"imageio-ffmpeg\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\"],\"summary\":\"Random-forest automated bud annotation\",\"support\":\"https://github.com/SanderSMFISH/napari-buds/issues\",\"twitter\":\"\",\"version\":\"0.1.2\",\"visibility\":\"public\",\"writer_file_extensions\":[\".npy\"],\"writer_save_layers\":[\"image\",\"labels\"]}",
  "{\"authors\":[{\"email\":\"giorgiatortora2@gmail.com\",\"name\":\"Giorgia Tortora\"}],\"code_repository\":\"https://github.com/GiorgiaTortora/napari-power-spectrum\",\"conda\":[],\"description\":\"# napari-power-spectrum\\n\\n[![License](https://img.shields.io/pypi/l/napari-power-spectrum.svg?color=green)](https://github.com/GiorgiaTortora/napari-power-spectrum/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-power-spectrum.svg?color=green)](https://pypi.org/project/napari-power-spectrum)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-power-spectrum.svg?color=green)](https://python.org)\\n[![tests](https://github.com/GiorgiaTortora/napari-power-spectrum/workflows/tests/badge.svg)](https://github.com/GiorgiaTortora/napari-power-spectrum/actions)\\n[![codecov](https://codecov.io/gh/GiorgiaTortora/napari-power-spectrum/branch/main/graph/badge.svg)](https://codecov.io/gh/GiorgiaTortora/napari-power-spectrum)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-power-spectrum)](https://napari-hub.org/plugins/napari-power-spectrum)\\n\\nA simple plugin to get the power spectrum of frames of a stack image\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/plugins/stable/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-power-spectrum` via [pip]:\\n\\n    pip install napari-power-spectrum\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/GiorgiaTortora/napari-power-spectrum.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-power-spectrum\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/GiorgiaTortora/napari-power-spectrum/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-power-spectrum\\n\\n\\n\\n\\n\\n\\nA simple plugin to get the power spectrum of frames of a stack image\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-power-spectrum via pip:\\npip install napari-power-spectrum\\n\\nTo install latest development version :\\npip install git+https://github.com/GiorgiaTortora/napari-power-spectrum.git\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-power-spectrum\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"Power Spectrum\",\"documentation\":\"https://github.com/GiorgiaTortora/napari-power-spectrum#README.md\",\"first_released\":\"2022-04-08T10:06:35.233809Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-power-spectrum\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/GiorgiaTortora/napari-power-spectrum\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-04-22T15:54:51.976065Z\",\"report_issues\":\"https://github.com/GiorgiaTortora/napari-power-spectrum/issues\",\"requirements\":[\"numpy\",\"magicgui\",\"qtpy\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\"],\"summary\":\"A simple plugin to get the power spectrum of frames of a stack image\",\"support\":\"https://github.com/GiorgiaTortora/napari-power-spectrum/issues\",\"twitter\":\"\",\"version\":\"0.0.6\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Robert Haase\"}],\"category\":{\"Supported data\":[\"2D\",\"3D\"],\"Workflow step\":[\"Image Segmentation\",\"Image correction\",\"Image reconstruction\",\"Image enhancement\",\"Morphological operations\",\"Image feature detection\"]},\"category_hierarchy\":{\"Supported data\":[[\"2D\"],[\"3D\"]],\"Workflow step\":[[\"Image Segmentation\",\"Connected-component analysis\"],[\"Image correction\",\"Illumination correction\"],[\"Image correction\"],[\"Image reconstruction\",\"Image denoising\"],[\"Image enhancement\",\"Image denoising\"],[\"Image Segmentation\",\"Image thresholding\"],[\"Image Segmentation\"],[\"Image enhancement\",\"Smoothing\"],[\"Morphological operations\"],[\"Image Segmentation\",\"Semi-automatic segmentation\"],[\"Image feature detection\",\"Edge detection\"],[\"Morphological operations\",\"Top-hat transform\"],[\"Morphological operations\",\"Closing\"],[\"Morphological operations\",\"Dilation\"],[\"Morphological operations\",\"Opening\"],[\"Morphological operations\",\"Erosion\"],[\"Image enhancement\"]]},\"code_repository\":\"https://github.com/haesleinhuepf/napari-cupy-image-processing\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-cupy-image-processing\"}],\"description\":\"# napari-cupy-image-processing\\n\\n[![License](https://img.shields.io/pypi/l/napari-cupy-image-processing.svg?color=green)](https://github.com/haesleinhuepf/napari-cupy-image-processing/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-cupy-image-processing.svg?color=green)](https://pypi.org/project/napari-cupy-image-processing)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-cupy-image-processing.svg?color=green)](https://python.org)\\n[![tests](https://github.com/haesleinhuepf/napari-cupy-image-processing/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-cupy-image-processing/actions)\\n[![codecov](https://codecov.io/gh/haesleinhuepf/napari-cupy-image-processing/branch/master/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-cupy-image-processing)\\n[![Development Status](https://img.shields.io/pypi/status/napari-cupy-image-processing.svg)](https://en.wikipedia.org/wiki/Software_release_life_cycle#Alpha)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-cupy-image-processing)](https://napari-hub.org/plugins/napari-cupy-image-processing)\\n\\n\\nGPU-accelerated image processing using [cupy](https://cupy.dev) and [CUDA](https://en.wikipedia.org/wiki/CUDA)\\n\\n## Usage\\n\\nThis napari plugin adds some menu entries to the Tools menu. You can recognize them with their suffix `(n-cupy)` in brackets.\\nFurthermore, it can be used from the [napari-assistant](https://www.napari-hub.org/plugins/napari-assistant) graphical user interface. \\nTherefore, just click the menu `Tools > Utilities > Assistant (na)` or run `naparia` from the command line.\\n\\n![img.png](https://github.com/haesleinhuepf/napari-cupy-image-processing/raw/main/docs/screenshot-with-tools-menu.png)\\n\\nYou can also call operations from python, e.g. as shown in this [demo notebook](https://github.com/haesleinhuepf/napari-cupy-image-processing/raw/main/docs/demo.ipynb).\\n\\n## Installation\\n\\nFollow the [instructions for installing cupy](https://docs.cupy.dev/en/stable/install.html#installing-cupy-from-conda-forge) on your computer first.\\n\\n    conda install -c conda-forge cupy\\n\\nAfterwards, you can install `napari-cupy-image-processing` via [pip]:\\n\\n    pip install napari-cupy-image-processing\\n\\nA more detailed example for installation (change 11.2 to your desired CUDA version):\\n```\\nconda create --name cupy_p39 python=3.9\\nconda activate cupy_p39\\nconda install -c conda-forge cupy cudatoolkit=11.2 napari\\npip install napari-cupy-image-processing\\n```\\n\\n## Contributing\\n\\nContributions are very welcome. Adding [cupy ndimage](https://docs.cupy.dev/en/stable/reference/ndimage.html) functions is quite easy as you can see in the \\n[implementation of the current operations](https://github.com/haesleinhuepf/napari-cupy-image-processing/blob/main/napari_cupy_image_processing/_cupy_image_processing.py#L48). \\nIf you need another function in napari, just send a PR. Please make sure the tests pass locally before submitting a PR.\\n\\n```\\npip install pytest-cov pytest-qt\\npytest --cov=napari_cupy_image_processing\\n```\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n## License\\n\\nDistributed under the terms of the [MIT] license,\\n\\\"napari-cupy-image-processing\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/haesleinhuepf/napari-cupy-image-processing/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-cupy-image-processing\\n\\n\\n\\n\\n\\n\\n\\nGPU-accelerated image processing using cupy and CUDA\\nUsage\\nThis napari plugin adds some menu entries to the Tools menu. You can recognize them with their suffix (n-cupy) in brackets.\\nFurthermore, it can be used from the napari-assistant graphical user interface. \\nTherefore, just click the menu Tools > Utilities > Assistant (na) or run naparia from the command line.\\n\\nYou can also call operations from python, e.g. as shown in this demo notebook.\\nInstallation\\nFollow the instructions for installing cupy on your computer first.\\nconda install -c conda-forge cupy\\n\\nAfterwards, you can install napari-cupy-image-processing via pip:\\npip install napari-cupy-image-processing\\n\\nA more detailed example for installation (change 11.2 to your desired CUDA version):\\nconda create --name cupy_p39 python=3.9\\nconda activate cupy_p39\\nconda install -c conda-forge cupy cudatoolkit=11.2 napari\\npip install napari-cupy-image-processing\\nContributing\\nContributions are very welcome. Adding cupy ndimage functions is quite easy as you can see in the \\nimplementation of the current operations. \\nIf you need another function in napari, just send a PR. Please make sure the tests pass locally before submitting a PR.\\npip install pytest-cov pytest-qt\\npytest --cov=napari_cupy_image_processing\\nThis napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.\\nLicense\\nDistributed under the terms of the MIT license,\\n\\\"napari-cupy-image-processing\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 3 - Alpha\"],\"display_name\":\"napari-cupy-image-processing\",\"documentation\":\"https://github.com/haesleinhuepf/napari-cupy-image-processing#README.md\",\"first_released\":\"2021-10-22T19:41:19.679750Z\",\"license\":\"MIT\",\"name\":\"napari-cupy-image-processing\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/haesleinhuepf/napari-cupy-image-processing\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-08-27T12:39:35.723835Z\",\"report_issues\":\"https://github.com/haesleinhuepf/napari-cupy-image-processing/issues\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"toolz\",\"cupy\",\"napari-tools-menu\",\"scikit-image\",\"napari-time-slicer (>=0.4.8)\",\"napari-skimage-regionprops\",\"napari-assistant\",\"stackview (>=0.3.2)\"],\"summary\":\"GPU-accelerated image processing using CUDA\",\"support\":\"https://github.com/haesleinhuepf/napari-cupy-image-processing/issues\",\"twitter\":\"\",\"version\":\"0.3.1\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Laura Zigutyte\"},{\"name\":\"Ryan Savill\"},{\"name\":\"Johannes Müller\"},{\"name\":\"Marcelo Zoccoler\"},{\"name\":\"Robert Haase\"}],\"category\":{\"Supported data\":[\"2D\",\"3D\"],\"Workflow step\":[\"Clustering\",\"Visualization\",\"Object feature extraction\"]},\"category_hierarchy\":{\"Supported data\":[[\"2D\"],[\"3D\"]],\"Workflow step\":[[\"Clustering\"],[\"Clustering\",\"Centroid-based clustering\"],[\"Clustering\",\"Hierarchical clustering\"],[\"Visualization\"],[\"Visualization\",\"Plotting\"],[\"Object feature extraction\"],[\"Object feature extraction\",\"Shape features extraction\"]]},\"citations\":{\"APA\":\"Zigutyte L., Savill R., Müller J., Zoccoler M., Haase R. (2021). napari-clusters-plotter (version 0.5.1). DOI: 10.5281/zenodo.5884657 URL: https://github.com/BiAPoL/napari-clusters-plotter\\n\",\"BibTex\":\"@misc{YourReferenceHere,\\nauthor = {Zigutyte, Laura and Savill, Ryan and Müller, Johannes and Zoccoler, Marcelo and Haase, Robert},\\ndoi = {10.5281/zenodo.5884657},\\nmonth = {11},\\ntitle = {napari-clusters-plotter},\\nurl = {https://github.com/BiAPoL/napari-clusters-plotter},\\nyear = {2021}\\n}\\n\",\"RIS\":\"TY  - GEN\\nAB  - A plugin to use with napari for clustering objects according to their properties.\\nAU  - Zigutyte, Laura\\nAU  - Savill, Ryan\\nAU  - Müller, Johannes\\nAU  - Zoccoler, Marcelo\\nAU  - Haase, Robert\\nDA  - 2021-11-15\\nDO  - 10.5281/zenodo.5884657\\nPY  - 2021\\nTI  - napari-clusters-plotter\\nUR  - https://github.com/BiAPoL/napari-clusters-plotter\\nER\\n\",\"citation\":\"cff-version: 1.2.0\\ntitle: napari-clusters-plotter\\nmessage: \\\"If you use this software, please cite it using the metadata from this file.\\\"\\nabstract: \\\"A plugin to use with napari for clustering objects according to their properties.\\\"\\ntype: software\\nauthors:\\n  - given-names: Laura\\n    family-names: Zigutyte\\n    email: zigutyte@gmail.com\\n  - given-names: Ryan\\n    family-names: Savill\\n  - given-names: Johannes\\n    family-names: Müller\\n  - given-names: Marcelo\\n    family-names: Zoccoler\\n  - given-names: Robert\\n    family-names: Haase\\n    email: robert.haase@tu-dresden.de\\nversion: 0.5.1\\ndate-released: 2021-11-15\\nidentifiers:\\n  - description: This is the collection of archived snapshots of all versions of napari-clusters-plotter\\n    type: doi\\n    value: \\\"10.5281/zenodo.5884657\\\"\\n  - description: This is the archived snapshot of version 0.5.1 of napari-clusters-plotter\\n    type: doi\\n    value: \\\"10.5281/zenodo.6620442\\\"\\n  - description: This is the archived snapshot of version 0.5.0 of napari-clusters-plotter\\n    type: doi\\n    value: \\\"10.5281/zenodo.6520216\\\"\\n  - description: This is the archived snapshot of version 0.4.0 of napari-clusters-plotter\\n    type: doi\\n    value: \\\"10.5281/zenodo.6483425\\\"\\n  - description: This is the archived snapshot of version 0.3.0 of napari-clusters-plotter\\n    type: doi\\n    value: \\\"10.5281/zenodo.6412049\\\"\\n  - description: This is the archived snapshot of version 0.2.2 of napari-clusters-plotter\\n    type: doi\\n    value: \\\"10.5281/zenodo.5884658\\\"\\nlicense: BSD-3-Clause\\nrepository-code: https://github.com/BiAPoL/napari-clusters-plotter\\n\"},\"code_repository\":\"https://github.com/BiAPoL/napari-clusters-plotter\",\"description\":\"# napari-clusters-plotter\\r\\n\\r\\n[![License](https://img.shields.io/pypi/l/napari-clusters-plotter.svg?color=green)](https://github.com/lazigu/napari-clusters-plotter/raw/master/LICENSE)\\r\\n[![PyPI](https://img.shields.io/pypi/v/napari-clusters-plotter.svg?color=green)](https://pypi.org/project/napari-clusters-plotter)\\r\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-clusters-plotter.svg?color=green)](https://python.org)\\r\\n[![Anaconda-Server Badge](https://anaconda.org/conda-forge/napari-clusters-plotter/badges/version.svg)](https://anaconda.org/conda-forge/napari-clusters-plotter)\\r\\n[![tests](https://github.com/lazigu/napari-clusters-plotter/workflows/tests/badge.svg)](https://github.com/lazigu/napari-clusters-plotter/actions)\\r\\n[![codecov](https://codecov.io/gh/BiAPoL/napari-clusters-plotter/branch/main/graph/badge.svg?token=R6W2KO1NJ8)](https://codecov.io/gh/BiAPoL/napari-clusters-plotter)\\r\\n[![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&logoColor=white)](https://github.com/pre-commit/pre-commit)\\r\\n[![Anaconda-Server Badge](https://anaconda.org/conda-forge/napari-clusters-plotter/badges/downloads.svg)](https://anaconda.org/conda-forge/napari-clusters-plotter)\\r\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-clusters-plotter)](https://www.napari-hub.org/plugins/napari-clusters-plotter)\\r\\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.7011471.svg)](https://doi.org/10.5281/zenodo.7011471)\\r\\n\\r\\nA plugin to use with napari for clustering objects according to their properties.\\r\\n\\r\\n----------------------------------\\r\\n\\r\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\\r\\n\\r\\n<img src=\\\"https://github.com/BiAPoL/napari-clusters-plotter/raw/main/images/screencast.gif\\\" width=\\\"700\\\"/>\\r\\n\\r\\nDemonstration of handling 3D time-lapse data:\\r\\n\\r\\n<img src=\\\"https://github.com/BiAPoL/napari-clusters-plotter/raw/main/images/screencast2_timelapse.gif\\\" width=\\\"700\\\"/>\\r\\n\\r\\n----------------------------------\\r\\n\\r\\nJump to:\\r\\n- [Usage](#usage)\\r\\n  - [Starting point](#starting-point)\\r\\n  - [Measurements](#measurements)\\r\\n  - [Time-Lapse Measurements](#time-lapse-measurements)\\r\\n  - [Plotting](#plotting)\\r\\n  - [Time-Lapse Plotting](#time-lapse-plotting)\\r\\n  - [Dimensionality reduction: UMAP, t-SNE or PCA](#dimensionality-reduction-umap-t-sne-or-pca)\\r\\n  - [Clustering](#clustering)\\r\\n  - [Plotting clustering results](#plotting-clustering-results)\\r\\n- [Installation](#installation)\\r\\n- [Troubleshooting installation](#troubleshooting-installation)\\r\\n- [Contributing](#contributing)\\r\\n- [License](#license)\\r\\n- [Acknowledgements](#acknowledgements)\\r\\n\\r\\n\\r\\n## Usage\\r\\n\\r\\n### Starting point\\r\\nFor clustering objects according to their properties, the starting point is a [grey-value image](example_data/blobs.tif) and a label image\\r\\nrepresenting a segmentation of objects. For segmenting objects, you can for example use the\\r\\n[Voronoi-Otsu-labelling approach](https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes#voronoi-otsu-labelling)\\r\\nin the napari plugin [napari-segment-blobs-and-things-with-membranes](https://www.napari-hub.org/plugins/napari-segment-blobs-and-things-with-membranes).\\r\\n\\r\\n![](https://github.com/BiAPoL/napari-clusters-plotter/raw/main/images/starting_point.png)\\r\\n\\r\\n### Measurements\\r\\nThe first step is deriving measurements from the labelled image and the corresponding pixels in the grey-value image.\\r\\nSince the 0.6.0 release measurements widget is no longer part of this plugin and you will have to use other napari plugins to measure your data.\\r\\nOne way is to use the measurement functions in [napari-skimage-regionprops](https://www.napari-hub.org/plugins/napari-skimage-regionprops), which comes pre-installed with the napari cluster plotter.\\r\\nUse the menu `Tools > Measurement > Regionprops (scikit-image, nsr)` to get to the measurement widget.\\r\\nJust select the image, the corresponding label image and the measurements to analyse and click on `Run`.\\r\\n\\r\\nIn the previous napari-cluster-plotter release a GPU dependant measurement function was implemented which you can find in the [napari-pyclesperanto-assistant](https://www.napari-hub.org/plugins/napari-pyclesperanto-assistant).\\r\\nTo use this function you will need to install this library (see optional installation steps) and you can find the widget under the menu `Tools > Measurement > Label statistics (clEsperanto)`. As before, select the image, the corresponding label image and the measurements to analyse and click on `Run`.\\r\\n\\r\\nA table with the measurements will open and afterwards, you can save and/or close the measurement table. Also, close the Measure widget.\\r\\n\\r\\nIf you want to upload your own measurements you can do this using [napari-skimage-regionprops](https://www.napari-hub.org/plugins/napari-skimage-regionprops).\\r\\nUnder the menu `Tools > Measurement > Load from CSV (nsr)` you can find a widget to upload your own csv file.\\r\\nMake sure that there is a column that specifies the which measurement belongs to which label by adding a column with the name \\\"label\\\".\\r\\nIf you don't specify this column it will be assumed that measurements start at 1 and each\\r\\ncolumn describes the next label.\\r\\n\\r\\nNote that tables for time-lapse data need to include an **additional column named \\\"frame\\\"**, which indicates which slice in\\r\\ntime the given row refers to.\\r\\n\\r\\n**For the correct visualisation of clusters IDs in space**, it is **important** that label images/time-points of the time-lapse\\r\\nare either **labelled sequentially** or missing labels still exist in the loaded csv file (i.e., missing label exists in the\\r\\n\\\"label\\\" column with `NaN` values for other measurements in the same row). If you perform measurements using before mentioned\\r\\nplugins, the obtained dataframe is already in the correct form.\\r\\n\\r\\n#### Time-Lapse Measurements\\r\\nIn case you have 2D time-lapse data you need to convert it into a suitable shape using the function: `Tools > Utilities > Convert 3D stack to 2D time-lapse (time-slicer)`,\\r\\nwhich can be found in the [napari time slicer](https://www.napari-hub.org/plugins/napari-time-slicer).\\r\\n\\r\\nNote that tables for time-lapse data will include an additional column named \\\"frame\\\", which indicates which slice in\\r\\ntime the given row refers to. If you want to import your own csv files for time-lapse data make sure to include this column!\\r\\nIf you have tracking data where each column specifies measurements for a track instead of a label at a specific time point,\\r\\nthis column must not be added.\\r\\n\\r\\nBoth [napari-skimage-regionprops](https://www.napari-hub.org/plugins/napari-skimage-regionprops) and [napari-pyclesperanto-assistant](https://www.napari-hub.org/plugins/napari-pyclesperanto-assistant) include measuring widgets for timelapse data.\\r\\n\\r\\n### Plotting\\r\\n\\r\\nOnce measurements were made or uploaded, these measurements were saved in the `properties/features` of the labels layer which was\\r\\nanalysed. You can then plot these measurements using the menu `Tools > Measurement > Plot measurement (ncp)`.\\r\\n\\r\\nIn this widget, you can select the labels layer which was analysed and the measurements which should be plotted\\r\\non the X- and Y-axis. If you cannot see any options in axes selection boxes, but you have performed measurements, click\\r\\non `Update Axes/Clustering Selection Boxes` to refresh them. Click on `Run` to draw the data points in the plot area.\\r\\n\\r\\n![](https://github.com/BiAPoL/napari-clusters-plotter/raw/main/images/plot_plain.png)\\r\\n\\r\\nYou can also manually select a region in the plot. To use lasso (freehand) tool use left mouse click, and to use a\\r\\nrectangle - right click. The resulting manual clustering will also be visualized in the original image. To optimize\\r\\nvisualization in the image, turn off the visibility of the analysed labels layer.\\r\\n\\r\\n![](https://github.com/BiAPoL/napari-clusters-plotter/raw/main/images/plot_interactive.png)\\r\\n\\r\\nHold down the SHIFT key while annotating regions in the plot to manually select multiple clusters.\\r\\n\\r\\n![](https://github.com/BiAPoL/napari-clusters-plotter/raw/main/images/multi-select-manual-clustering.gif)\\r\\n\\r\\n#### Time-Lapse Plotting\\r\\nWhen you plot your time-lapse datasets you will notice that the plots look slightly different.\\r\\nDatapoints of the current time frame are highlighted in white and you can see the datapoints move through the plot if you press play:\\r\\n\\r\\n![](https://github.com/BiAPoL/napari-clusters-plotter/raw/main/images/plotting_time-lapse_data_as_movie.gif)\\r\\n\\r\\nYou can also manually select groups using the lasso tool and plot a measurement per frame and see how the group behaves in time.\\r\\nFurthermore, you could also select a group in time and see where the datapoints lie in a different feature space:\\r\\n\\r\\n![](https://github.com/BiAPoL/napari-clusters-plotter/raw/main/images/timelapse_manual_clustering_tips.gif)\\r\\n\\r\\n### Dimensionality reduction: UMAP, t-SNE or PCA\\r\\n\\r\\nFor getting more insights into your data, you can reduce the dimensionality of the measurements, e.g.\\r\\nusing the [UMAP algorithm](https://umap-learn.readthedocs.io/en/latest/), [t-SNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)\\r\\nor [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html).\\r\\nTo apply it to your data use the menu `Tools > Measurement > Dimensionality reduction (ncp)`.\\r\\nSelect the label image that was analysed and in the list below, select all measurements that should be\\r\\ndimensionality reduced. By default, all measurements are selected in the box. If you cannot see any measurements, but\\r\\nyou have performed them, click on `Update Measurements` to refresh the box. You can read more about parameters of both\\r\\nalgorithms by hovering over question marks or by clicking on them. When you are done with the selection, click on `Run`\\r\\nand after a moment, the table of measurements will re-appear with two additional columns representing the reduced\\r\\ndimensions of the dataset. These columns are automatically saved in the `properties` of the labels layer so there is no\\r\\nneed to save them for usage in other widgets unless you wish to do so.\\r\\n\\r\\n![](https://github.com/BiAPoL/napari-clusters-plotter/raw/main/images/umap.png)\\r\\n\\r\\nAfterwards, you can again save and/or close the table. Also, close the Dimensionality Reduction widget.\\r\\n\\r\\n### Clustering\\r\\nIf manual clustering, as shown above, is not an option, you can automatically cluster your data, using these implemented algorithms:\\r\\n* [k-means clustering (KMEANS)](https://towardsdatascience.com/k-means-clustering-algorithm-applications-evaluation-methods-and-drawbacks-aa03e644b48a)\\r\\n* [Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN)](https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html)\\r\\n* [Gaussian Mixture Model (GMM)](https://scikit-learn.org/stable/modules/mixture.html)\\r\\n* [Mean Shift (MS)](https://scikit-learn.org/stable/auto_examples/cluster/plot_mean_shift.html#sphx-glr-auto-examples-cluster-plot-mean-shift-py)\\r\\n* [Agglomerative clustering (AC)](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html)\\r\\n\\r\\nTherefore, click the menu `Tools > Measurement > Clustering (ncp)`,\\r\\nagain, select the analysed labels layer.\\r\\nThis time select the measurements for clustering, e.g. select _only_ the `UMAP` measurements.\\r\\nSelect the clustering method `KMeans` and click on `Run`.\\r\\nThe table of measurements will reappear with an additional column `ALGORITHM_NAME_CLUSTERING_ID` containing the cluster\\r\\nID of each datapoint.\\r\\n\\r\\n![](https://github.com/BiAPoL/napari-clusters-plotter/raw/main/images/clustering.png)\\r\\n\\r\\nAfterwards, you can again save and/or close the table. Also, close the clustering widget.\\r\\n\\r\\n### Plotting clustering results\\r\\nReturn to the Plotter widget using the menu `Tools > Measurement > Plot measurement (ncp)`.\\r\\nSelect `UMAP_0` and `UMAP_1` as X- and Y-axis and the `ALGORITHM_NAME_CLUSTERING_ID` as `Clustering`, and click on `Run`.\\r\\n\\r\\n![](https://github.com/BiAPoL/napari-clusters-plotter/raw/main/images/hdbscan_clusters_plot.png)\\r\\n\\r\\nExample of k-means clustering results:\\r\\n\\r\\n![](https://github.com/BiAPoL/napari-clusters-plotter/raw/main/images/kmeans_clusters_plot.png)\\r\\n\\r\\n## Installation\\r\\n### Devbio-napari installation\\r\\nThe easiest way to install this plugin is to install the [devbio-napari](https://github.com/haesleinhuepf/devbio-napari) library.\\r\\nThis library installs napari alongside many other useful plugins, including the napari-clusters-plotter.\\r\\nWe recommend this library as it is not only the easiest way to install the napari-cluster-plotter, but it includes plugins for segmentation and measurement, which we don't provide.\\r\\nThere are detailed installation instructions on the [napari-hub-page](https://www.napari-hub.org/plugins/devbio-napari) if you have any problems installing it.\\r\\nIn case you want to have a minimal installation of our plugin you can find other installation options below.\\r\\n\\r\\n### Minimal installation\\r\\n* Get a python environment, e.g. via [mini-conda](https://docs.conda.io/en/latest/miniconda.html).\\r\\n  If you never used python/conda environments before, please follow the instructions\\r\\n  [here](https://mpicbg-scicomp.github.io/ipf_howtoguides/guides/Python_Conda_Environments) first. It is recommended to\\r\\n  install python 3.9 to your new conda environment from the start. The plugin is not yet supported with Python 3.10.\\r\\n  Create a new environment, for example, like this:\\r\\n\\r\\n```\\r\\nconda create --name ncp-env python=3.9\\r\\n```\\r\\n\\r\\n* Activate the new environment via conda:\\r\\n\\r\\n```\\r\\nconda activate ncp-env\\r\\n```\\r\\n\\r\\n* Install [napari], e.g. via [pip]:\\r\\n\\r\\n```\\r\\npython -m pip install \\\"napari[all]\\\"\\r\\n```\\r\\n\\r\\nAfterwards, you can install `napari-clusters-plotter` via [pip]:\\r\\n\\r\\n```\\r\\npip install napari-clusters-plotter\\r\\n```\\r\\n\\r\\n### Optional installation\\r\\nFollow these steps instead of the regular installation to include the [napari-pyclesperanto-assistant](https://www.napari-hub.org/plugins/napari-pyclesperanto-assistant).\\r\\nCreating the environment like this will allow you to use your GPU to render your cluster results.\\r\\nFurthermore, you can access the deprecated measurement functions of the napari-cluster-plotter in releases < 0.6.0.\\r\\nIf you have trouble with this library you can use the regular installation above.\\r\\n\\r\\n```\\r\\nconda create --name ncp-env python==3.9\\r\\n```\\r\\n\\r\\n\\r\\n* Activate the new environment via conda:\\r\\n\\r\\n```\\r\\nconda activate ncp-env\\r\\n```\\r\\n\\r\\n* Install napari-pyclesperanto-assistant, e.g. with pip:\\r\\n\\r\\n´´´\\r\\npip install napari-pyclesperanto-assistant\\r\\n´´´\\r\\n\\r\\n* Mac-users please also install this:\\r\\n\\r\\n´´´\\r\\nconda install -c conda-forge ocl_icd_wrapper_apple\\r\\n´´´\\r\\n\\r\\n* Linux users please also install this:\\r\\n\\r\\n´´´\\r\\nconda install -c conda-forge ocl-icd-system\\r\\n´´´\\r\\n\\r\\n* Install [napari], e.g. via [pip]:\\r\\n\\r\\n```\\r\\npython -m pip install \\\"napari[all]\\\"\\r\\n```\\r\\n\\r\\nAfterwards, you can install `napari-clusters-plotter` via [pip]:\\r\\n\\r\\n```\\r\\npip install napari-clusters-plotter\\r\\n```\\r\\n\\r\\n## Troubleshooting installation\\r\\n\\r\\n- If the plugin does not appear in napari 'Plugins' menu, and in 'Plugin errors...' you can see such an error:\\r\\n\\r\\n```\\r\\nImportError: DLL load failed while importing _cl\\r\\n```\\r\\n\\r\\nTry downloading and installing a pyopencl with a lower cl version, e.g. cl12 : pyopencl=2020.1. However, in this case,\\r\\nyou will need an environment with a lower python version (python=3.8).\\r\\n\\r\\n- `Error: Could not build wheels for hdbscan which use PEP 517 and cannot be installed directly`\\r\\n\\r\\nInstall hdbscan via conda before installing the plugin:\\r\\n\\r\\n```\\r\\nconda install -c conda-forge hdbscan\\r\\n```\\r\\n\\r\\n- `ValueError: numpy.ndarray size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject`\\r\\n\\r\\nSimilar to the above-described error, this error can occur when importing hdbscan through pip or in the wrong order. This can be fixed by installing packages separately through conda and in the following order:\\r\\n```bash\\r\\nconda install -c conda-forge napari pyopencl hdbscan\\r\\npip install napari-clusters-plotter\\r\\n```\\r\\n\\r\\n- `WARNING: No ICDs were found` or `LogicError: clGetPlatformIDs failed: PLATFORM_NOT_FOUND_KHR`\\r\\n\\r\\nMake your system-wide implementation visible by installing either of the following conda packages:\\r\\n\\r\\n```\\r\\nconda install -c conda-forge ocl-icd-system\\r\\nconda install -c conda-forge ocl_icd_wrapper_apple\\r\\n```\\r\\n\\r\\n## Contributing\\r\\n\\r\\nContributions are very welcome. Tests can be run with [pytest], please ensure\\r\\nthe coverage at least stays the same before you submit a pull request.\\r\\n\\r\\n## License\\r\\n\\r\\nDistributed under the terms of the [BSD-3] license,\\r\\n\\\"napari-clusters-plotter\\\" is free and open source software\\r\\n\\r\\n## Acknowledgements\\r\\nThis project was supported by the Deutsche Forschungsgemeinschaft under Germany’s Excellence Strategy – EXC2068 - Cluster of Excellence \\\"Physics of Life\\\" of TU Dresden.\\r\\nThis project has been made possible in part by grant number [2021-240341 (Napari plugin accelerator grant)](https://chanzuckerberg.com/science/programs-resources/imaging/napari/improving-image-processing/) from the Chan Zuckerberg Initiative DAF, an advised fund of the Silicon Valley Community Foundation.\\r\\n\\r\\n## Issues\\r\\n\\r\\nIf you encounter any problems, please [file an issue](https://github.com/BiAPoL/napari-clusters-plotter/issues) along\\r\\nwith a detailed description.\\r\\n\\r\\n[napari]: https://github.com/napari/napari\\r\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\r\\n[@napari]: https://github.com/napari\\r\\n[MIT]: http://opensource.org/licenses/MIT\\r\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\r\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\r\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\r\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\r\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\r\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\r\\n\\r\\n[napari]: https://github.com/napari/napari\\r\\n[pytest]: https://docs.pytest.org/en/7.0.x/\\r\\n[pip]: https://pypi.org/project/pip/\\r\\n[PyPI]: https://pypi.org/\\r\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-clusters-plotter\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nA plugin to use with napari for clustering objects according to their properties.\\n\\nThis napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.\\n\\nDemonstration of handling 3D time-lapse data:\\n\\n\\nJump to:\\n- Usage\\n  - Starting point\\n  - Measurements\\n  - Time-Lapse Measurements\\n  - Plotting\\n  - Time-Lapse Plotting\\n  - Dimensionality reduction: UMAP, t-SNE or PCA\\n  - Clustering\\n  - Plotting clustering results\\n- Installation\\n- Troubleshooting installation\\n- Contributing\\n- License\\n- Acknowledgements\\nUsage\\nStarting point\\nFor clustering objects according to their properties, the starting point is a grey-value image and a label image\\nrepresenting a segmentation of objects. For segmenting objects, you can for example use the\\nVoronoi-Otsu-labelling approach\\nin the napari plugin napari-segment-blobs-and-things-with-membranes.\\n\\nMeasurements\\nThe first step is deriving measurements from the labelled image and the corresponding pixels in the grey-value image.\\nSince the 0.6.0 release measurements widget is no longer part of this plugin and you will have to use other napari plugins to measure your data.\\nOne way is to use the measurement functions in napari-skimage-regionprops, which comes pre-installed with the napari cluster plotter.\\nUse the menu Tools > Measurement > Regionprops (scikit-image, nsr) to get to the measurement widget.\\nJust select the image, the corresponding label image and the measurements to analyse and click on Run.\\nIn the previous napari-cluster-plotter release a GPU dependant measurement function was implemented which you can find in the napari-pyclesperanto-assistant.\\nTo use this function you will need to install this library (see optional installation steps) and you can find the widget under the menu Tools > Measurement > Label statistics (clEsperanto). As before, select the image, the corresponding label image and the measurements to analyse and click on Run.\\nA table with the measurements will open and afterwards, you can save and/or close the measurement table. Also, close the Measure widget.\\nIf you want to upload your own measurements you can do this using napari-skimage-regionprops.\\nUnder the menu Tools > Measurement > Load from CSV (nsr) you can find a widget to upload your own csv file.\\nMake sure that there is a column that specifies the which measurement belongs to which label by adding a column with the name \\\"label\\\".\\nIf you don't specify this column it will be assumed that measurements start at 1 and each\\ncolumn describes the next label.\\nNote that tables for time-lapse data need to include an additional column named \\\"frame\\\", which indicates which slice in\\ntime the given row refers to.\\nFor the correct visualisation of clusters IDs in space, it is important that label images/time-points of the time-lapse\\nare either labelled sequentially or missing labels still exist in the loaded csv file (i.e., missing label exists in the\\n\\\"label\\\" column with NaN values for other measurements in the same row). If you perform measurements using before mentioned\\nplugins, the obtained dataframe is already in the correct form.\\nTime-Lapse Measurements\\nIn case you have 2D time-lapse data you need to convert it into a suitable shape using the function: Tools > Utilities > Convert 3D stack to 2D time-lapse (time-slicer),\\nwhich can be found in the napari time slicer.\\nNote that tables for time-lapse data will include an additional column named \\\"frame\\\", which indicates which slice in\\ntime the given row refers to. If you want to import your own csv files for time-lapse data make sure to include this column!\\nIf you have tracking data where each column specifies measurements for a track instead of a label at a specific time point,\\nthis column must not be added.\\nBoth napari-skimage-regionprops and napari-pyclesperanto-assistant include measuring widgets for timelapse data.\\nPlotting\\nOnce measurements were made or uploaded, these measurements were saved in the properties/features of the labels layer which was\\nanalysed. You can then plot these measurements using the menu Tools > Measurement > Plot measurement (ncp).\\nIn this widget, you can select the labels layer which was analysed and the measurements which should be plotted\\non the X- and Y-axis. If you cannot see any options in axes selection boxes, but you have performed measurements, click\\non Update Axes/Clustering Selection Boxes to refresh them. Click on Run to draw the data points in the plot area.\\n\\nYou can also manually select a region in the plot. To use lasso (freehand) tool use left mouse click, and to use a\\nrectangle - right click. The resulting manual clustering will also be visualized in the original image. To optimize\\nvisualization in the image, turn off the visibility of the analysed labels layer.\\n\\nHold down the SHIFT key while annotating regions in the plot to manually select multiple clusters.\\n\\nTime-Lapse Plotting\\nWhen you plot your time-lapse datasets you will notice that the plots look slightly different.\\nDatapoints of the current time frame are highlighted in white and you can see the datapoints move through the plot if you press play:\\n\\nYou can also manually select groups using the lasso tool and plot a measurement per frame and see how the group behaves in time.\\nFurthermore, you could also select a group in time and see where the datapoints lie in a different feature space:\\n\\nDimensionality reduction: UMAP, t-SNE or PCA\\nFor getting more insights into your data, you can reduce the dimensionality of the measurements, e.g.\\nusing the UMAP algorithm, t-SNE\\nor PCA.\\nTo apply it to your data use the menu Tools > Measurement > Dimensionality reduction (ncp).\\nSelect the label image that was analysed and in the list below, select all measurements that should be\\ndimensionality reduced. By default, all measurements are selected in the box. If you cannot see any measurements, but\\nyou have performed them, click on Update Measurements to refresh the box. You can read more about parameters of both\\nalgorithms by hovering over question marks or by clicking on them. When you are done with the selection, click on Run\\nand after a moment, the table of measurements will re-appear with two additional columns representing the reduced\\ndimensions of the dataset. These columns are automatically saved in the properties of the labels layer so there is no\\nneed to save them for usage in other widgets unless you wish to do so.\\n\\nAfterwards, you can again save and/or close the table. Also, close the Dimensionality Reduction widget.\\nClustering\\nIf manual clustering, as shown above, is not an option, you can automatically cluster your data, using these implemented algorithms:\\n* k-means clustering (KMEANS)\\n* Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN)\\n* Gaussian Mixture Model (GMM)\\n* Mean Shift (MS)\\n* Agglomerative clustering (AC)\\nTherefore, click the menu Tools > Measurement > Clustering (ncp),\\nagain, select the analysed labels layer.\\nThis time select the measurements for clustering, e.g. select only the UMAP measurements.\\nSelect the clustering method KMeans and click on Run.\\nThe table of measurements will reappear with an additional column ALGORITHM_NAME_CLUSTERING_ID containing the cluster\\nID of each datapoint.\\n\\nAfterwards, you can again save and/or close the table. Also, close the clustering widget.\\nPlotting clustering results\\nReturn to the Plotter widget using the menu Tools > Measurement > Plot measurement (ncp).\\nSelect UMAP_0 and UMAP_1 as X- and Y-axis and the ALGORITHM_NAME_CLUSTERING_ID as Clustering, and click on Run.\\n\\nExample of k-means clustering results:\\n\\nInstallation\\nDevbio-napari installation\\nThe easiest way to install this plugin is to install the devbio-napari library.\\nThis library installs napari alongside many other useful plugins, including the napari-clusters-plotter.\\nWe recommend this library as it is not only the easiest way to install the napari-cluster-plotter, but it includes plugins for segmentation and measurement, which we don't provide.\\nThere are detailed installation instructions on the napari-hub-page if you have any problems installing it.\\nIn case you want to have a minimal installation of our plugin you can find other installation options below.\\nMinimal installation\\n\\nGet a python environment, e.g. via mini-conda.\\n  If you never used python/conda environments before, please follow the instructions\\n  here first. It is recommended to\\n  install python 3.9 to your new conda environment from the start. The plugin is not yet supported with Python 3.10.\\n  Create a new environment, for example, like this:\\n\\nconda create --name ncp-env python=3.9\\n\\nActivate the new environment via conda:\\n\\nconda activate ncp-env\\n\\nInstall napari, e.g. via pip:\\n\\npython -m pip install \\\"napari[all]\\\"\\nAfterwards, you can install napari-clusters-plotter via pip:\\npip install napari-clusters-plotter\\nOptional installation\\nFollow these steps instead of the regular installation to include the napari-pyclesperanto-assistant.\\nCreating the environment like this will allow you to use your GPU to render your cluster results.\\nFurthermore, you can access the deprecated measurement functions of the napari-cluster-plotter in releases < 0.6.0.\\nIf you have trouble with this library you can use the regular installation above.\\nconda create --name ncp-env python==3.9\\n\\nActivate the new environment via conda:\\n\\nconda activate ncp-env\\n\\nInstall napari-pyclesperanto-assistant, e.g. with pip:\\n\\n´´´\\npip install napari-pyclesperanto-assistant\\n´´´\\n\\nMac-users please also install this:\\n\\n´´´\\nconda install -c conda-forge ocl_icd_wrapper_apple\\n´´´\\n\\nLinux users please also install this:\\n\\n´´´\\nconda install -c conda-forge ocl-icd-system\\n´´´\\n\\nInstall napari, e.g. via pip:\\n\\npython -m pip install \\\"napari[all]\\\"\\nAfterwards, you can install napari-clusters-plotter via pip:\\npip install napari-clusters-plotter\\nTroubleshooting installation\\n\\nIf the plugin does not appear in napari 'Plugins' menu, and in 'Plugin errors...' you can see such an error:\\n\\nImportError: DLL load failed while importing _cl\\nTry downloading and installing a pyopencl with a lower cl version, e.g. cl12 : pyopencl=2020.1. However, in this case,\\nyou will need an environment with a lower python version (python=3.8).\\n\\nError: Could not build wheels for hdbscan which use PEP 517 and cannot be installed directly\\n\\nInstall hdbscan via conda before installing the plugin:\\nconda install -c conda-forge hdbscan\\n\\nValueError: numpy.ndarray size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\\n\\nSimilar to the above-described error, this error can occur when importing hdbscan through pip or in the wrong order. This can be fixed by installing packages separately through conda and in the following order:\\nbash\\nconda install -c conda-forge napari pyopencl hdbscan\\npip install napari-clusters-plotter\\n\\nWARNING: No ICDs were found or LogicError: clGetPlatformIDs failed: PLATFORM_NOT_FOUND_KHR\\n\\nMake your system-wide implementation visible by installing either of the following conda packages:\\nconda install -c conda-forge ocl-icd-system\\nconda install -c conda-forge ocl_icd_wrapper_apple\\nContributing\\nContributions are very welcome. Tests can be run with pytest, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-clusters-plotter\\\" is free and open source software\\nAcknowledgements\\nThis project was supported by the Deutsche Forschungsgemeinschaft under Germany’s Excellence Strategy – EXC2068 - Cluster of Excellence \\\"Physics of Life\\\" of TU Dresden.\\nThis project has been made possible in part by grant number 2021-240341 (Napari plugin accelerator grant) from the Chan Zuckerberg Initiative DAF, an advised fund of the Silicon Valley Community Foundation.\\nIssues\\nIf you encounter any problems, please file an issue along\\nwith a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari-clusters-plotter\",\"documentation\":\"https://github.com/BiAPoL/napari-clusters-plotter\",\"first_released\":\"2021-11-15T13:51:45.035968Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-clusters-plotter\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/BiAPoL/napari-clusters-plotter\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-12-13T19:14:18.849635Z\",\"report_issues\":\"https://github.com/BiAPoL/napari-clusters-plotter/issues\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy (>=1.21)\",\"scikit-learn\",\"matplotlib\",\"pandas\",\"umap-learn\",\"napari-tools-menu\",\"napari-skimage-regionprops (>=0.3.1)\",\"hdbscan\",\"joblib (==1.1.0)\"],\"summary\":\"A plugin to use with napari for clustering objects according to their properties\",\"support\":\"https://github.com/BiAPoL/napari-clusters-plotter/issues\",\"twitter\":\"\",\"version\":\"0.6.0\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Computational Microscopy Platform\"},{\"name\":\"CZ Biohub\"}],\"code_repository\":\"https://github.com/mehta-lab/recOrder/tree/main/recOrder\",\"description\":\"# recOrder\\n![PyPI - Python Version](https://img.shields.io/pypi/pyversions/recOrder-napari)\\n[![Downloads](https://pepy.tech/badge/recOrder-napari)](https://pepy.tech/project/recOrder-napari)\\n[![Python package index](https://img.shields.io/pypi/v/recOrder-napari.svg)](https://pypi.org/project/recOrder-napari)\\n[![Development Status](https://img.shields.io/pypi/status/napari.svg)](https://en.wikipedia.org/wiki/Software_release_life_cycle#Alpha)\\n\\nThis package provides a napari plugin and a command line interface for quantitative label-free microscopy.\\n\\nIn this repository you will find python tools and a napari plugin that allow the user to calibrate microscope hardware, acquire multi-modal data, reconstruct density and anisotropy, and visualize the results.\\n\\nThe acquisition, calibration, background correction, reconstruction, and applications of QLIPP (quantitative label-free imaging with phase and polarization)  are described in the following [E-Life Paper](https://elifesciences.org/articles/55502):\\n\\n```bibtex\\nSyuan-Ming Guo, Li-Hao Yeh, Jenny Folkesson, Ivan E Ivanov, Anitha P Krishnan, Matthew G Keefe, Ezzat Hashemi, David Shin, Bryant B Chhun, Nathan H Cho, Manuel D Leonetti, May H Han, Tomasz J Nowakowski, Shalin B Mehta, \\\"Revealing architectural order with quantitative label-free imaging and deep learning,\\\" eLife 2020;9:e55502 DOI: 10.7554/eLife.55502 (2020).\\n```\\n\\n`recOrder` is to be used alongside a conventional widefield microscope fitted with a universal polarizer (Panel A below).  The universal polarizer allows for the collection of label-free information including the intrinsic anisotropy of the sample and its relative phase (density). These measurements are collected by acquiring data under calibrated, polarization-diverse illumination followed by a computational reconstruction.  The overall structure of `recOrder` is shown in Panel B, highlighting the two different usage modes and their features: graphical user interface (GUI) through a napari plugin, and a command line interface (CLI).\\n\\n![Flow Chart](https://github.com/mehta-lab/recOrder/blob/main/docs/images/recOrder_Fig1_Overview.png?raw=true)\\n\\n## Dataset\\n\\n[Slides](https://doi.org/10.5281/zenodo.5135889) and a [dataset](https://doi.org/10.5281/zenodo.5178487) shared during a workshop on QLIPP and recOrder can be found on Zenodo.\\n\\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.5178487.svg)](https://doi.org/10.5281/zenodo.5178487)\\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.5135889.svg)](https://doi.org/10.5281/zenodo.5135889)\\n\\n## Quick Start\\n\\n(Optional but recommended) install [anaconda](https://www.anaconda.com/products/distribution) and create a virtual environment:\\n\\n```sh\\nconda create -y -n recOrder python=3.9\\nconda activate recOrder\\n```\\n\\n> *Apple Silicon users please use*:\\n>\\n> ```sh\\n> CONDA_SUBDIR=osx-64 conda create -y -n recOrder python=3.9\\n> conda activate recOrder\\n> ```\\n\\nInstall `recOrder-napari`:\\n\\n```sh\\npip install recOrder-napari\\n```\\n\\nOpen `napari` with `recOrder-napari`:\\n\\n```sh\\nnapari -w recOrder-napari\\n```\\n\\nView command-line help by running\\n\\n```sh\\nrecOrder.help\\n```\\n\\nFor more help, see [`recOrder`s documentation](./docs).\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"recOrder\\n\\n\\n\\n\\nThis package provides a napari plugin and a command line interface for quantitative label-free microscopy.\\nIn this repository you will find python tools and a napari plugin that allow the user to calibrate microscope hardware, acquire multi-modal data, reconstruct density and anisotropy, and visualize the results.\\nThe acquisition, calibration, background correction, reconstruction, and applications of QLIPP (quantitative label-free imaging with phase and polarization)  are described in the following E-Life Paper:\\nbibtex\\nSyuan-Ming Guo, Li-Hao Yeh, Jenny Folkesson, Ivan E Ivanov, Anitha P Krishnan, Matthew G Keefe, Ezzat Hashemi, David Shin, Bryant B Chhun, Nathan H Cho, Manuel D Leonetti, May H Han, Tomasz J Nowakowski, Shalin B Mehta, \\\"Revealing architectural order with quantitative label-free imaging and deep learning,\\\" eLife 2020;9:e55502 DOI: 10.7554/eLife.55502 (2020).\\nrecOrder is to be used alongside a conventional widefield microscope fitted with a universal polarizer (Panel A below).  The universal polarizer allows for the collection of label-free information including the intrinsic anisotropy of the sample and its relative phase (density). These measurements are collected by acquiring data under calibrated, polarization-diverse illumination followed by a computational reconstruction.  The overall structure of recOrder is shown in Panel B, highlighting the two different usage modes and their features: graphical user interface (GUI) through a napari plugin, and a command line interface (CLI).\\n\\nDataset\\nSlides and a dataset shared during a workshop on QLIPP and recOrder can be found on Zenodo.\\n\\n\\nQuick Start\\n(Optional but recommended) install anaconda and create a virtual environment:\\nsh\\nconda create -y -n recOrder python=3.9\\nconda activate recOrder\\n\\nApple Silicon users please use:\\nsh\\nCONDA_SUBDIR=osx-64 conda create -y -n recOrder python=3.9\\nconda activate recOrder\\n\\nInstall recOrder-napari:\\nsh\\npip install recOrder-napari\\nOpen napari with recOrder-napari:\\nsh\\nnapari -w recOrder-napari\\nView command-line help by running\\nsh\\nrecOrder.help\\nFor more help, see recOrders documentation.\",\"development_status\":[],\"display_name\":\"recOrder-napari\",\"documentation\":\"https://github.com/mehta-lab/recOrder/wiki\",\"first_released\":\"2022-04-19T19:14:03.116915Z\",\"license\":\"BSD 3-Clause License\",\"name\":\"recOrder-napari\",\"npe2\":true,\"operating_system\":[\"Operating System :: MacOS\",\"Operating System :: Microsoft :: Windows\",\"Operating System :: POSIX\",\"Operating System :: Unix\"],\"plugin_types\":[\"reader\",\"widget\"],\"project_site\":\"https://github.com/mehta-lab/recOrder\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[\"*.tif\",\"*.zarr\"],\"release_date\":\"2023-01-12T18:13:17.555878Z\",\"report_issues\":\"https://github.com/mehta-lab/recOrder/issues\",\"requirements\":[\"waveorder (==1.0.0rc0)\",\"pycromanager (==0.19.2)\",\"click (>=8.0.1)\",\"pyyaml (>=5.4.1)\",\"tqdm (>=4.61.1)\",\"opencv-python (>=4.5.3.56)\",\"natsort (>=7.1.1)\",\"colorspacious (>=1.1.2)\",\"pyqtgraph (>=0.12.3)\",\"superqt (>=0.2.4)\",\"napari-ome-zarr (>=0.3.2)\",\"qtpy\",\"napari[all]\",\"imageio (!=2.11.0,!=2.22.1)\",\"importlib-metadata\",\"numpy (==1.23.5)\",\"pytest (>=5.0.0) ; extra == 'dev'\",\"pytest-cov ; extra == 'dev'\",\"pytest-qt ; extra == 'dev'\",\"wget (>=3.2) ; extra == 'dev'\",\"tox ; extra == 'dev'\",\"pre-commit ; extra == 'dev'\",\"black ; extra == 'dev'\"],\"summary\":\"Computational microscopy toolkit for label-free imaging\",\"support\":\"https://github.com/mehta-lab/recOrder/issues\",\"twitter\":\"\",\"version\":\"0.2.1\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Ashley Anderson\"}],\"code_repository\":\"https://github.com/aganders3/napari-solarized\",\"description\":\"# napari-solarized\\n\\nSolarized (-ish) themes for napari, based on [solarized](https://ethanschoonover.com/solarized/).\\n\\n![solarized dark screenshot](https://raw.githubusercontent.com/aganders3/napari-solarized/main/screenshot_dark.png)\\n![solarized light screenshot](https://raw.githubusercontent.com/aganders3/napari-solarized/main/screenshot_light.png)\\n\\n[![License MIT](https://img.shields.io/pypi/l/napari-solarized.svg?color=green)](https://github.com/aganders3/napari-solarized/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-solarized.svg?color=green)](https://pypi.org/project/napari-solarized)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-solarized.svg?color=green)](https://python.org)\\n[![tests](https://github.com/aganders3/napari-solarized/workflows/tests/badge.svg)](https://github.com/aganders3/napari-solarized/actions)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-solarized)](https://napari-hub.org/plugins/napari-solarized)\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n## Installation\\n\\nYou can install `napari-solarized` via [pip]:\\n\\n    pip install napari-solarized\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/aganders3/napari-solarized.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome.\\n\\n## License\\n\\nDistributed under the terms of the [MIT] license,\\n\\\"napari-solarized\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/aganders3/napari-solarized/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-solarized\\nSolarized (-ish) themes for napari, based on solarized.\\n\\n\\n\\n\\n\\n\\n\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\nInstallation\\nYou can install napari-solarized via pip:\\npip install napari-solarized\\n\\nTo install latest development version :\\npip install git+https://github.com/aganders3/napari-solarized.git\\n\\nContributing\\nContributions are very welcome.\\nLicense\\nDistributed under the terms of the MIT license,\\n\\\"napari-solarized\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"Napari Solarized\",\"documentation\":\"https://github.com/aganders3/napari-solarized#README.md\",\"first_released\":\"2023-01-23T16:33:42.578696Z\",\"license\":\"MIT\",\"name\":\"napari-solarized\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"theme\"],\"project_site\":\"https://github.com/aganders3/napari-solarized\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2023-01-23T16:43:59.563555Z\",\"report_issues\":\"https://github.com/aganders3/napari-solarized/issues\",\"requirements\":[\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"npe2 ; extra == 'testing'\",\"typer ; extra == 'testing'\",\"importlib-resources ; extra == 'testing'\"],\"summary\":\"Solarized themes for napari\",\"support\":\"https://github.com/aganders3/napari-solarized/issues\",\"twitter\":\"\",\"version\":\"0.1.1\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"lukas.migas@yahoo.com\",\"name\":\"Lukasz G. Migas\"}],\"code_repository\":\"https://github.com/lukasz-migas/napari-1d\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-plot\"}],\"description\":\"# napari-plot\\n\\n[![License](https://img.shields.io/pypi/l/napari-plot.svg?color=green)](https://github.com/lukasz-migas/napari-1d/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-plot.svg?color=green)](https://pypi.org/project/napari-plot)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-plot.svg?color=green)](https://python.org)\\n[![tests](https://github.com/lukasz-migas/napari-1d/workflows/tests/badge.svg)](https://github.com/lukasz-migas/napari-1d/actions)\\n[![codecov](https://codecov.io/gh/lukasz-migas/napari-1d/branch/main/graph/badge.svg)](https://codecov.io/gh/lukasz-migas/napari-1d)\\n\\nPlugin providing support for 1d plotting in napari.\\n\\nThis plugin is in very early stages of development and many things are still in a state of disarray. New features and bug fixes\\nwill be coming over the coming months. \\n\\n## Note\\n\\n`napari-plot` provides several custom icons and stylesheets to take advantage of the `Qt` backend. Since it would be a bit busy to add multiple layer lists,\\nI opted to include a toolbar that quickly pulls the layer list whenever requested. Simple use the toolbar to access several commonly accessed elements.\\n\\n## Usage\\n\\nYou can use `napari-plot` alongside `napari` where it is embedded as a dock widget. If using this option, controls are relegated to toolbar\\nwhere you can adjust layer properties like you would do in `napari`.\\n\\n![embedded](https://github.com/lukasz-migas/napari-1d/blob/main/misc/embedded.png)\\n\\nOr as a standalone app where only one-dimensional plotting is enabled. In this mode, controls take central stage and reflect `napari's` own\\nbehaviour where layer controls are embedded in the main application.\\n\\n![live-view](https://github.com/lukasz-migas/napari-1d/blob/main/misc/napariplot-live-line.gif)\\n\\n## Roadmap:\\n\\nThis is only provisional list of features that I would like to see implemented. It barely scratches the surface of what plotting tool should cover so as soon as the basics are covered,\\nfocus will be put towards adding more exotic features. If there are features that you certainly wish to be included,\\nplease modify the list below or create a [new issue](https://github.com/lukasz-migas/napari-1d/issues/new)\\n\\n- [ ] Support for new layer types. Layers are based on `napari's` `Layer`, albeit in a two-dimensional setting. Supported and planned layers:\\n  - [x] Line Layer - simple line plot.\\n  - [x] Scatter Layer - scatter plot (similar to `napari's Points` layer).\\n  - [x] Centroids/Segments Layer - horizontal or vertical line segments.\\n  - [x] InfLine Layer - infinite horizontal or vertical lines that span over very broad range. Useful for defining regions of interest.\\n  - [x] Region Layer - infinite horizontal or vertical rectangular boxes that span over very broad range. Useful for defining regions of interest.\\n  - [x] Shapes Layer - `napari's` own `Shapes` layer\\n  - [x] Points Layer - `napari's` own `Points` layer\\n  - [x] Multi-line Layer - more efficient implementation of `Line` layer when multiple lines are necessary.\\n  - [ ] Bar - horizontal and vertical barchart (TODO)\\n- [x] Proper interactivity of each layer type (e.g. moving `Region` or `InfLine`, adding points, etc...)\\n- [x] Intuitive interactivity. `napari-plot` will provide excellent level of interactivity with the plotted data. We plan to support several types of `Tools` that permit efficient interrogation of the data. We currently provide several `zoom` and `select` tools and hope to add few extras in the future.\\n  - [x] Box-zoom - standard zooming rectangle. Simply `left-mouse + drag/release` in the canvas on region of interest\\n  - [x] Horizontal span - zoom-in only in the y-axis by `Ctrl + left-mouse + drag/release` in the canvas.\\n  - [x] Vertical span - span-in only in the x-axis by `Shift + left-mouse + drag/release` in the canvas.\\n  - [x] Rectangle select - rectangle tool allowing sub-selection of data in the canvas. Similar to the `Box-zoom` but without the zooming part.\\n  - [x] Polygon select - polygon tool allowing sub-selection of data in the canvas.\\n  - [x] Lasso select - lasso tool allowing sub-selection of data in the canvas.\\n- [ ] Interactive plot legend\\n- [ ] Customizable axis visuals.\\n  - [x] Plot axis enabling customization of tick/label size and color\\n  - [ ] Support for non-linear scale\\n- [ ] Add convenient plotting interface:\\n  - [ ] Add `.plot` functionality\\n  - [ ] Add `.scatter` functionality\\n  - [ ] Add `.hbar` and `.vbar` functionality\\n  - [ ] Add `.imshow` functionality\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-plot` directly from PyPI via:\\n\\n```python\\npip install napari-plot\\n```\\n\\nor from the git repo:\\n\\n```python\\ngit clone https://github.com/lukasz-migas/napari-1d.git\\ncd napari-1d\\npip install -e '.[all]'\\n```\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-plot\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/lukasz-migas/napari-1d/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-plot\\n\\n\\n\\n\\n\\nPlugin providing support for 1d plotting in napari.\\nThis plugin is in very early stages of development and many things are still in a state of disarray. New features and bug fixes\\nwill be coming over the coming months. \\nNote\\nnapari-plot provides several custom icons and stylesheets to take advantage of the Qt backend. Since it would be a bit busy to add multiple layer lists,\\nI opted to include a toolbar that quickly pulls the layer list whenever requested. Simple use the toolbar to access several commonly accessed elements.\\nUsage\\nYou can use napari-plot alongside napari where it is embedded as a dock widget. If using this option, controls are relegated to toolbar\\nwhere you can adjust layer properties like you would do in napari.\\n\\nOr as a standalone app where only one-dimensional plotting is enabled. In this mode, controls take central stage and reflect napari's own\\nbehaviour where layer controls are embedded in the main application.\\n\\nRoadmap:\\nThis is only provisional list of features that I would like to see implemented. It barely scratches the surface of what plotting tool should cover so as soon as the basics are covered,\\nfocus will be put towards adding more exotic features. If there are features that you certainly wish to be included,\\nplease modify the list below or create a new issue\\n\\n[ ] Support for new layer types. Layers are based on napari's Layer, albeit in a two-dimensional setting. Supported and planned layers:\\n[x] Line Layer - simple line plot.\\n[x] Scatter Layer - scatter plot (similar to napari's Points layer).\\n[x] Centroids/Segments Layer - horizontal or vertical line segments.\\n[x] InfLine Layer - infinite horizontal or vertical lines that span over very broad range. Useful for defining regions of interest.\\n[x] Region Layer - infinite horizontal or vertical rectangular boxes that span over very broad range. Useful for defining regions of interest.\\n[x] Shapes Layer - napari's own Shapes layer\\n[x] Points Layer - napari's own Points layer\\n[x] Multi-line Layer - more efficient implementation of Line layer when multiple lines are necessary.\\n[ ] Bar - horizontal and vertical barchart (TODO)\\n[x] Proper interactivity of each layer type (e.g. moving Region or InfLine, adding points, etc...)\\n[x] Intuitive interactivity. napari-plot will provide excellent level of interactivity with the plotted data. We plan to support several types of Tools that permit efficient interrogation of the data. We currently provide several zoom and select tools and hope to add few extras in the future.\\n[x] Box-zoom - standard zooming rectangle. Simply left-mouse + drag/release in the canvas on region of interest\\n[x] Horizontal span - zoom-in only in the y-axis by Ctrl + left-mouse + drag/release in the canvas.\\n[x] Vertical span - span-in only in the x-axis by Shift + left-mouse + drag/release in the canvas.\\n[x] Rectangle select - rectangle tool allowing sub-selection of data in the canvas. Similar to the Box-zoom but without the zooming part.\\n[x] Polygon select - polygon tool allowing sub-selection of data in the canvas.\\n[x] Lasso select - lasso tool allowing sub-selection of data in the canvas.\\n[ ] Interactive plot legend\\n[ ] Customizable axis visuals.\\n[x] Plot axis enabling customization of tick/label size and color\\n[ ] Support for non-linear scale\\n[ ] Add convenient plotting interface:\\n[ ] Add .plot functionality\\n[ ] Add .scatter functionality\\n[ ] Add .hbar and .vbar functionality\\n[ ] Add .imshow functionality\\n\\n\\nThis napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-plot directly from PyPI via:\\npython\\npip install napari-plot\\nor from the git repo:\\npython\\ngit clone https://github.com/lukasz-migas/napari-1d.git\\ncd napari-1d\\npip install -e '.[all]'\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-plot\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari-plot\",\"documentation\":\"https://github.com/lukasz-migas/napari-1d#README.md\",\"first_released\":\"2022-01-09T18:20:25.848886Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-plot\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/lukasz-migas/napari-1d\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-03-22T15:01:04.475629Z\",\"report_issues\":\"https://github.com/lukasz-migas/napari-1d/issues\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"qtpy\",\"qtawesome\",\"napari (<0.4.15,>=0.4.13)\",\"matplotlib\",\"vispy (>=0.9.6)\",\"PySide2 (!=5.15.0,>=5.13.2) ; extra == 'all'\",\"pre-commit (>=2.9.0) ; extra == 'dev'\",\"black (==22.1.0) ; extra == 'dev'\",\"flake8 (==4.0.1) ; extra == 'dev'\",\"PySide2 (!=5.15.0,>=5.13.2) ; extra == 'dev'\",\"pytest ; extra == 'dev'\",\"pytest-qt ; extra == 'dev'\",\"scikit-image ; extra == 'dev'\",\"PyQt5 (!=5.15.0,>=5.12.3) ; extra == 'pyqt'\",\"PyQt5 (!=5.15.0,>=5.12.3) ; extra == 'pyqt5'\",\"PySide2 (!=5.15.0,>=5.13.2) ; extra == 'pyside'\",\"PySide2 (!=5.15.0,>=5.13.2) ; extra == 'pyside2'\",\"PyQt5 (!=5.15.0,>=5.12.3) ; extra == 'qt'\",\"pytest ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"scikit-image ; extra == 'testing'\"],\"summary\":\"Plugin providing support for 1d plotting in napari.\",\"support\":\"https://github.com/lukasz-migas/napari-1d/issues\",\"twitter\":\"\",\"version\":\"0.1.5\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"liuhanjin-sc@g.ecc.u-tokyo.ac.jp\",\"name\":\"Hanjin Liu\"}],\"code_repository\":\"https://github.com/hanjinliu/napari-text-layer\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-text-layer\"}],\"description\":\"# napari-text-layer\\n\\nNapari text layer for bio-image annotation.\\n\\n![](https://github.com/hanjinliu/napari-text-layer/raw/main/GIFs/annot.gif)\\n\\n![](https://github.com/hanjinliu/napari-text-layer/raw/main/GIFs/age.gif)\\n\\n### Installation\\n\\nYou can install using pip:\\n\\n```\\npip install napari-text-layer\\n```\\n\\n### Keybindings and mouse callbacks\\n\\n- \\\"&rarr;\\\", \\\"&larr;\\\", \\\"&uarr;\\\", \\\"&darr;\\\" ... Move selected shapes by 1 pixel.\\n- \\\"F2\\\" ... Enter edit mode at the selected shape (or the last one if no shape is selected).\\n- \\\"Enter\\\" ... Finish edit mode or add a new shape at the same interval.\\n- \\\"Ctrl\\\" + \\\"Shift\\\" + \\\"<\\\" or \\\">\\\" ... Change font size.\\n- double click ... Enter edit mode at the clicked shape.\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-text-layer\\nNapari text layer for bio-image annotation.\\n\\n\\nInstallation\\nYou can install using pip:\\npip install napari-text-layer\\nKeybindings and mouse callbacks\\n\\n\\\"→\\\", \\\"←\\\", \\\"↑\\\", \\\"↓\\\" ... Move selected shapes by 1 pixel.\\n\\\"F2\\\" ... Enter edit mode at the selected shape (or the last one if no shape is selected).\\n\\\"Enter\\\" ... Finish edit mode or add a new shape at the same interval.\\n\\\"Ctrl\\\" + \\\"Shift\\\" + \\\"<\\\" or \\\">\\\" ... Change font size.\\ndouble click ... Enter edit mode at the clicked shape.\\n\",\"development_status\":[],\"display_name\":\"napari-text-layer\",\"documentation\":\"\",\"first_released\":\"2021-10-30T02:46:10.282131Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-text-layer\",\"npe2\":false,\"operating_system\":[],\"plugin_types\":[\"widget\"],\"project_site\":\"\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2021-12-13T07:12:56.138020Z\",\"report_issues\":\"\",\"requirements\":null,\"summary\":\"Text layer for bio-image annotation.\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.1.2\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"sebgoti8@gmail.com\",\"name\":\"Sebastian Gonzalez-Tirado\"}],\"code_repository\":\"https://github.com/sebgoti/napari-spatial-omics\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-spatial-omics\"}],\"description\":\"# napari-spatial-omics\\n\\n[![License](https://img.shields.io/pypi/l/napari-spatial-omics.svg?color=green)](https://github.com/sebgoti/napari-spatial-omics/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-spatial-omics.svg?color=green)](https://pypi.org/project/napari-spatial-omics)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-spatial-omics.svg?color=green)](https://python.org)\\n[![tests](https://github.com/sebgoti/napari-spatial-omics/workflows/tests/badge.svg)](https://github.com/sebgoti/napari-spatial-omics/actions)\\n[![codecov](https://codecov.io/gh/sebgoti/napari-spatial-omics/branch/main/graph/badge.svg)](https://codecov.io/gh/sebgoti/napari-spatial-omics)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-spatial-omics)](https://napari-hub.org/plugins/napari-spatial-omics)\\n\\nA simple plugin to visualize spatial omic data stored in CSV format\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/plugins/stable/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-spatial-omics` via [pip]:\\n\\n    pip install napari-spatial-omics\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/sebgoti/napari-spatial-omics.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-spatial-omics\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/sebgoti/napari-spatial-omics/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-spatial-omics\\n\\n\\n\\n\\n\\n\\nA simple plugin to visualize spatial omic data stored in CSV format\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-spatial-omics via pip:\\npip install napari-spatial-omics\\n\\nTo install latest development version :\\npip install git+https://github.com/sebgoti/napari-spatial-omics.git\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-spatial-omics\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari-spatial-omics\",\"documentation\":\"https://github.com/sebgoti/napari-spatial-omics#README.md\",\"first_released\":\"2021-12-10T12:06:23.898797Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-spatial-omics\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\",\"widget\"],\"project_site\":\"https://github.com/sebgoti/napari-spatial-omics\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[\"*\"],\"release_date\":\"2021-12-12T11:24:57.465896Z\",\"report_issues\":\"https://github.com/sebgoti/napari-spatial-omics/issues\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"pandas\"],\"summary\":\"A simple plugin to visualize spatial omic data stored in CSV format\",\"support\":\"https://github.com/sebgoti/napari-spatial-omics/issues\",\"twitter\":\"\",\"version\":\"0.0.8\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"robert.haase@tu-dresden.de\",\"name\":\"Robert Haase\"}],\"code_repository\":\"https://github.com/haesleinhuepf/natari\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"natari\"}],\"description\":\"# natari\\n\\n[![License](https://img.shields.io/pypi/l/natari.svg?color=green)](https://github.com/haesleinhuepf/natari/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/natari.svg?color=green)](https://pypi.org/project/natari)\\n[![Python Version](https://img.shields.io/pypi/pyversions/natari.svg?color=green)](https://python.org)\\n\\nNapari gaming\\n\\n## Sliding puzzle\\n\\nRestore the image by reordering the superpixels using the `W`, `A`, `S`, `D` keys! \\n\\n![](https://github.com/haesleinhuepf/natari/raw/master/images/sliding_puzzle.gif)\\n\\n## Cell counting arcade\\nCommander! Cells are intruding our dish! Control your tiny space ship using `1` and `2` keys to move it left/right.\\nUse the `9` key to shoot a anti-cell bullet.\\n\\n![](https://github.com/haesleinhuepf/natari/raw/master/images/cell_counting_arcade.gif)\\n\\nThe image originates from [BBBC022v1](https://bbbc.broadinstitute.org/BBBC022) (Gustafsdottir et al., PLOS ONE, 2013), available from the Broad Bioimage Benchmark Collection (Ljosa et al., Nature Methods, 2012).\\n\\n## Snake\\nTwo mitochondria navigating in a cell searching for stress granules. \\nThe two players can control their mito using the `W`, `A`, `S`, `D` and `I`, `J`, `K`, `L`  keys, respectively.\\n\\n![](https://github.com/haesleinhuepf/natari/raw/master/images/snake.gif)\\n\\n## Ping pong\\nDon't drop the organoid! Use your racket and hit it back to your colleague!\\nThe two players can use `W`, `S` and `I`, `K` to control their racket, respectively.\\n\\n![](https://github.com/haesleinhuepf/natari/raw/master/images/ping_pong.gif)\\n\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n## Installation\\n\\nYou can install `natari` via [pip]:\\n\\n    pip install natari\\n\\n## Known issues\\n\\n* To make the keyboard buttons work, you sometimes have to click within the image after starting the game.\\n\\n## Contributing\\n\\nContributions are very welcome. \\n\\n## License\\n\\n\\\"natari\\\" is free and open source software. The code is in the public domain.\\n\\n[See also: unlicense.org](https://unlicense.org)\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/haesleinhuepf/natari/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"natari\\n\\n\\n\\nNapari gaming\\nSliding puzzle\\nRestore the image by reordering the superpixels using the W, A, S, D keys! \\n\\nCell counting arcade\\nCommander! Cells are intruding our dish! Control your tiny space ship using 1 and 2 keys to move it left/right.\\nUse the 9 key to shoot a anti-cell bullet.\\n\\nThe image originates from BBBC022v1 (Gustafsdottir et al., PLOS ONE, 2013), available from the Broad Bioimage Benchmark Collection (Ljosa et al., Nature Methods, 2012).\\nSnake\\nTwo mitochondria navigating in a cell searching for stress granules. \\nThe two players can control their mito using the W, A, S, D and I, J, K, L  keys, respectively.\\n\\nPing pong\\nDon't drop the organoid! Use your racket and hit it back to your colleague!\\nThe two players can use W, S and I, K to control their racket, respectively.\\n\\nThis napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.\\nInstallation\\nYou can install natari via pip:\\npip install natari\\n\\nKnown issues\\n\\nTo make the keyboard buttons work, you sometimes have to click within the image after starting the game.\\n\\nContributing\\nContributions are very welcome. \\nLicense\\n\\\"natari\\\" is free and open source software. The code is in the public domain.\\nSee also: unlicense.org\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 3 - Alpha\"],\"display_name\":\"natari\",\"documentation\":\"https://github.com/haesleinhuepf/natari#README.md\",\"first_released\":\"2021-10-17T09:32:48.750117Z\",\"license\":\"Unlicense\",\"name\":\"natari\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/haesleinhuepf/natari\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2021-12-22T14:04:54.166677Z\",\"report_issues\":\"https://github.com/haesleinhuepf/natari/issues\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"napari\",\"scipy\",\"napari-tools-menu\"],\"summary\":\"Napari gaming\",\"support\":\"https://github.com/haesleinhuepf/natari/issues\",\"twitter\":\"\",\"version\":\"0.2.7\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Jules Scholler\"}],\"code_repository\":null,\"description\":\"Plugin for annotating TissueScope data.\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"Plugin for annotating TissueScope data.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari-annotate\",\"documentation\":\"\",\"first_released\":\"2022-11-03T14:24:57.004714Z\",\"license\":\"MPL-2.0\",\"name\":\"napari-annotate\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\",\"widget\"],\"project_site\":\"https://github.com/WyssCenter\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[\"*_pbm\"],\"release_date\":\"2022-11-03T14:24:57.004714Z\",\"report_issues\":\"\",\"requirements\":[\"numpy\",\"napari[all]\",\"napari-tools-menu\",\"magic-class\",\"napari-plugin-engine (>=0.1.4)\"],\"summary\":\"Annotate large 2D slides\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.0.1\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Herearii Metuarea\"}],\"code_repository\":\"https://github.com/hereariim/napari-conidie\",\"description\":\"# napari-conidie\\r\\n\\r\\n[![License BSD-3](https://img.shields.io/pypi/l/napari-conidie.svg?color=green)](https://github.com/hereariim/napari-conidie/raw/main/LICENSE)\\r\\n[![PyPI](https://img.shields.io/pypi/v/napari-conidie.svg?color=green)](https://pypi.org/project/napari-conidie)\\r\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-conidie.svg?color=green)](https://python.org)\\r\\n[![tests](https://github.com/hereariim/napari-conidie/workflows/tests/badge.svg)](https://github.com/hereariim/napari-conidie/actions)\\r\\n[![codecov](https://codecov.io/gh/hereariim/napari-conidie/branch/main/graph/badge.svg)](https://codecov.io/gh/hereariim/napari-conidie)\\r\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-conidie)](https://napari-hub.org/plugins/napari-conidie)\\r\\n\\r\\nA segmentation tool to get conidie and hyphe\\r\\n\\r\\n----------------------------------\\r\\n\\r\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\r\\n\\r\\n<!--\\r\\nDon't miss the full getting started guide to set up your new package:\\r\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\r\\n\\r\\nand review the napari docs for plugin developers:\\r\\nhttps://napari.org/stable/plugins/index.html\\r\\n-->\\r\\n\\r\\nThis plugin is a use case for obtaining conidia and hyphae surface from images. This plugin is a private tool dedicated exclusively to the work of the QUASAV team.\\r\\n\\r\\n## Installation\\r\\n\\r\\nThis private tool cannot be found in the built-in napari. The installation therefore follows two steps:\\r\\n\\r\\n1 - Install latest development version :\\r\\n\\r\\n    git clone https://github.com/hereariim/napari-conidie.git\\r\\n\\r\\nOr:\\r\\n\\r\\n    Download napari-conidie zip file\\r\\n\\r\\n2 - Drag and drop napari-conidie file into the built-in\\r\\n\\r\\n## Contributing\\r\\n\\r\\nContributions are very welcome. Tests can be run with [tox], please ensure\\r\\nthe coverage at least stays the same before you submit a pull request.\\r\\n\\r\\n## License\\r\\n\\r\\nDistributed under the terms of the [BSD-3] license,\\r\\n\\\"napari-conidie\\\" is free and open source software\\r\\n\\r\\n## Issues\\r\\n\\r\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\r\\n\\r\\n[napari]: https://github.com/napari/napari\\r\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\r\\n[@napari]: https://github.com/napari\\r\\n[MIT]: http://opensource.org/licenses/MIT\\r\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\r\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\r\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\r\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\r\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\r\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\r\\n\\r\\n[file an issue]: https://github.com/hereariim/napari-conidie/issues\\r\\n\\r\\n[napari]: https://github.com/napari/napari\\r\\n[tox]: https://tox.readthedocs.io/en/latest/\\r\\n[pip]: https://pypi.org/project/pip/\\r\\n[PyPI]: https://pypi.org/\\r\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-conidie\\n\\n\\n\\n\\n\\n\\nA segmentation tool to get conidie and hyphe\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nThis plugin is a use case for obtaining conidia and hyphae surface from images. This plugin is a private tool dedicated exclusively to the work of the QUASAV team.\\nInstallation\\nThis private tool cannot be found in the built-in napari. The installation therefore follows two steps:\\n1 - Install latest development version :\\ngit clone https://github.com/hereariim/napari-conidie.git\\n\\nOr:\\nDownload napari-conidie zip file\\n\\n2 - Drag and drop napari-conidie file into the built-in\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-conidie\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"Conidie\",\"documentation\":\"https://github.com/hereariim/napari-conidie#README.md\",\"first_released\":\"2022-12-15T13:02:30.925644Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-conidie\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/hereariim/napari-conidie\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-12-15T13:02:30.925644Z\",\"report_issues\":\"https://github.com/hereariim/napari-conidie/issues\",\"requirements\":[\"numpy\",\"magicgui\",\"qtpy\",\"pandas\",\"h5py\",\"scikit-image\",\"napari\",\"matplotlib\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\"],\"summary\":\"A segmentation tool to get conidie and hyphe\",\"support\":\"https://github.com/hereariim/napari-conidie/issues\",\"twitter\":\"\",\"version\":\"0.0.1\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Martin Weigert\"}],\"code_repository\":\"https://github.com/maweigert/napari-nlm\",\"conda\":[],\"description\":\"# napari-nlm\\n\\n[![License BSD-3](https://img.shields.io/pypi/l/napari-nlm.svg)](https://github.com/maweigert/napari-nlm/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-nlm.svg)](https://pypi.org/project/napari-nlm)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-nlm.svg)](https://python.org)\\n[![tests](https://github.com/maweigert/napari-nlm/workflows/tests/badge.svg)](https://github.com/maweigert/napari-nlm/actions)\\n[![codecov](https://codecov.io/gh/maweigert/napari-nlm/branch/main/graph/badge.svg)](https://codecov.io/gh/maweigert/napari-nlm)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-nlm)](https://napari-hub.org/plugins/napari-nlm)\\n\\n----------------------------------\\n\\n\\nGPU accelerated non local means (NLM) denoising plugin for napari (WIP)\\n\\n* currently only supports single-channel 2D or 3D images\\n* requires a OpenCL capable GPU\\n\\n![Screenshot](images/screenshot.jpg)\\n\\n\\n## Installation\\n\\nYou can install `napari-nlm` via [pip]:\\n\\n    pip install napari-nlm\\n\\n## Usage\\n\\n1. Open example image `Open Sample > napari-nlm: noisy bricks`\\n2. Adjust parameters \\n   * `sigma`: denoising strength (the larger sigma, the greater the smoothing)\\n   * `patch_radius`: size of local patches, 2 or 3 is a good default\\n   * `search_radius`: size of search area around each pixel to find similar patches, 7-11 is a good default\\n3. Denoise by pressing `run`\\n\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-nlm\\\" is free and open source software\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-nlm\\n\\n\\n\\n\\n\\n\\n\\nGPU accelerated non local means (NLM) denoising plugin for napari (WIP)\\n\\ncurrently only supports single-channel 2D or 3D images\\nrequires a OpenCL capable GPU\\n\\n\\nInstallation\\nYou can install napari-nlm via [pip]:\\npip install napari-nlm\\n\\nUsage\\n\\nOpen example image Open Sample > napari-nlm: noisy bricks\\nAdjust parameters \\nsigma: denoising strength (the larger sigma, the greater the smoothing)\\npatch_radius: size of local patches, 2 or 3 is a good default\\nsearch_radius: size of search area around each pixel to find similar patches, 7-11 is a good default\\nDenoise by pressing run\\n\\nLicense\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-nlm\\\" is free and open source software\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari NLM\",\"documentation\":\"https://github.com/maweigert/napari-nlm#README.md\",\"first_released\":\"2022-07-25T22:00:24.500417Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-nlm\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\",\"sample_data\"],\"project_site\":\"https://github.com/maweigert/napari-nlm\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-07-26T23:46:26.367059Z\",\"report_issues\":\"https://github.com/maweigert/napari-nlm/issues\",\"requirements\":[\"numpy\",\"magicgui\",\"qtpy\",\"pyopencl (==2022.1.5)\",\"gputools\",\"scikit-image\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\"],\"summary\":\"NLM (non local means) denoising\",\"support\":\"https://github.com/maweigert/napari-nlm/issues\",\"twitter\":\"\",\"version\":\"0.0.4\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Luis Perdigao\"}],\"code_repository\":\"https://github.com/rosalindfranklininstitute/RedLionfish\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"redlionfish\"}],\"description\":\"# Red lionfish (RL) deconvolution\\n\\n*Richardson-Lucy deconvolution for fishes, scientists and engineers.*\\n\\n\\nThis software is for filtering 3D data using the Richardson-Lucy deconvolution algorithm.\\n\\nRichardson-Lucy is an iterative deconvolution algorithm that is used to remove\\npoint spread function (PSF) or optical transfer function (OTF) artifacts from experimental images.\\n\\nThe method was originally developed for astronomy to remove optical effects and simultaneously reduce poisson noise in 2D images.\\n\\n[Lucy, L. B. An iterative technique for the rectification of observed distributions. The Astronomical Journal 79, 745 (1974). DOI: 10.1086/111605](https://ui.adsabs.harvard.edu/abs/1974AJ.....79..745L/abstract)\\n\\nThe method can also be applied to 3D data. Nowadays this filtering technique is also widely used by microscopists.\\n\\nThe Richardson-Lucy deconvolution algorigthm is iterative. Each iteration involves the calculation of 2 convolutions, one element-wise multiplication and one element-wise division.\\n\\nWhen dealing with 3D data, the Richardson-Lucy algorithm is quite computional intensive primarly due to the calculation of the convolution, and can take a while to complete depending on the resources available. Convolution is significantly sped up using FFT compared to raw convolution.\\n\\nThis software was developed with the aim to make the R-L computation faster by exploiting GPU resources, and with the use of FFT convolution.\\n\\nTo make RedLionfish easily accessible, it is available through PyPi and anaconda (conda-forge channel). A useful plugin for Napari is also available.\\n\\nPlease note that this software only works with 3D data. For 2D data there are many alternatives such as the DeconvolutionLab2 in Fiji (ImageJ) and sckikit-image.\\n\\n## Napari plugin\\n\\nYou can now use the Napari's plugin installation in *Menu -> Plugins -> Install/Uninstall Plugins...*.\\nHowever, if you chose to use this method, GPU acceleration may not be available and it will use the CPU backend. Better check.\\n\\n![](resources\\\\imag1.jpg)\\n\\nAlternatively, if you follow the installation instructions below, and install the napari in the same python environment\\nthen the plugin should be immediately available in the *Menu -> Plugins -> RedLionfish*.\\n\\n\\n## Installation\\n\\nPreviously there was a problem in installing using `pip`, because no PyOpenCL wheels for windows were avaiable. It is now avaialble. This package can be installed using pip or conda.\\n\\n### Conda install\\n\\nThis package is available in conda-forge channel.\\nIt contains the precompiled libraries and it will install all the requirments for GPU-accelerated RL calculations.\\n\\n`conda install redlionfish -c conda-forge`\\n\\n### Install from PyPi\\n\\n```\\npip install redlionfish\\n```\\n\\nIn Linux , the package `ocl-icd-system` may also be useful.\\n\\n```\\nconda install reikna pyopencl ocl-icd-system -c conda-forge\\n```\\n\\n\\n#### Manual installation using the conda package file.\\n\\nDownload the appropriate conda package .bz2 at [https://github.com/rosalindfranklininstitute/RedLionfish/releases](https://github.com/rosalindfranklininstitute/RedLionfish/releases)\\n\\nIn the command line, successively run:\\n```\\nconda install <filename.bz2>\\nconda update --all -c conda-forge\\n```\\nThe second line is needed because you are installing from a local file, conda installer will not install dependencies. Right after this you should run the update command given.\\n\\n\\n### Manual installation (advanced and for developers)\\n\\nPlease note that in order to use OpenCL GPU accelerations, PyOpenCL must be installed.\\nThe best way to get it working is to install it under a conda environment.\\n\\nThe installation is similar to the previously described for PyPi.\\n\\n`conda install reikna pyopencl`\\n\\nor\\n`conda install reikna pyopencl ocl-icd-system -c conda-forge` (Linux)\\n\\nClone/download from source [https://github.com/rosalindfranklininstitute/RedLionfish/](https://github.com/rosalindfranklininstitute/RedLionfish/)\\n\\nand run\\n\\n`python setup.py install`\\n\\n\\n### Debug installation\\nIf you want to test and modify the code then you should probably install in debug mode using:\\n\\n`python setup.py develop`\\n\\nor\\n\\n`pip install -e .`\\n\\n\\n## More information\\n\\nThe software has algorithms for Richardson-Lucy deconvolution that use either CPU and GPU.\\n\\nThe CPU version is very similar to the [skimage.restoration.richardson_lucy](https://scikit-image.org/docs/dev/api/skimage.restoration.html#skimage.restoration.richardson_lucy) code, with improvments in speed.\\nmajor differences are:\\n\\n- the convolution steps use FFT only.\\n- PSF and PSF-flipped FFTs are precalculated before starting iterations.\\n\\nThe GPU version, was written in to use Reikna package, which does FFT using OpenCL, via PyOpenCL.\\n\\nUnfortunately, a major limitation in RAM usage exists with PyOpenCL.\\nLarge 3D data volumes with cause out-of-memory error when trying to upload data to the GPU for FFT calculations.\\nAs such, to overcome this problem, a block algorithm is used, which splits data into blocks with padded data.\\nThe results are then combined together to give the final result.\\nThis affects the perfomance of the calculation rather significantly, but with the advantage of being possible to handle large data volumes.\\n\\nIf Richardson-Lucy deconvolution using the GPU method fails, RedLionfish will falls back to CPU calculation. Check console output for messages.\\n\\nIf you are using the RedLionfish in your code, note that, by default, `def doRLDeconvolutionFromNpArrays()` method it uses the GPU OpenCL version.\\n\\n## Testing\\n\\nMany examples can be found in `/test' folder.\\n\\nA quick and benchmarking installation can be run from the proect root using the command:\\n\\n'python test\\\\test_and_benchm.py'\\n\\nor (*nix)\\n\\n'python test/test_and_benchm.py'\\n\\nThis will print out information about your GPU device (if available) and run some deconvolutions.\\nIt initially creates some data programatically, convolutes with a gaussian PSF, and add Poisson noise.\\nThen it executes executes the\\nRichardson-Lucy deconvolution calculation using CPU and GPU methods, for 10 iterations.\\nDuring the calculation it will print some information to the console/terminal, including the time it takes to run the calculation.\\n\\n\\nComputer generated data and an experimental PSF can be found in `test\\\\testdata`\\n\\n### Testing Redlionfish in napari\\n\\nHere is an example testing the Redlionfish plugin in napari:\\n\\n1. load data `test\\\\testdata\\\\gendata_psfconv_poiss_large.tif` (can use draga and drop)\\n2. load psf data `test\\\\testdata\\\\PSF_RFI_8bit.tif`\\n3. In the RedLionfish side window ensure that 'gendata_psfconv_poiss_large' is selected in data dropdown widget, and `PSF_RFI_8bit` is selected in psfdata widget.\\n4. Choose number of iterations (default=10)\\n5. Click 'Go' button and wait until result shows as a new data layer.\\n6. Use controls of the left panel to compare before and after RL deconvolution: select 'RL-deconvolution' layer and set colormap to red. Hide PSF_RFI_8bit. Make sure that both 'RL-deconvolution' and 'gendata-psfconv' are visible. Now, hide/unhide RL-deconvolution layer to see before and after deconvolution. Adjust contrast limits of each layer as desired.\\n\\n\\n## GPU vs CPU\\n\\nYou may notice that choosing GPU does not make RL-calculation much faster compared with CPU, and sometimes is slower.\\n\\nWhich method runs the R-L deconvolution faster. That depends on the computer configuration/architecture.\\n\\nGPU calculations will be generally faster than CPU with bigger data volumes.\\n\\nGPU calculation will be significantly faster if using a dedicated GPU card.\\n\\nPlease see benchmark values that highlights significant variability in calculation speeds.\\n\\n\\n## Coding\\n\\nPlease feel free to browse /test folder for examples.\\n\\nIn your code, add the import.\\n\\n`import RedLionfishDeconv`\\n\\nin order to use the functions.\\n\\nThe most useful function is perhaps the following.\\n\\n`def doRLDeconvolutionFromNpArrays(data_np , psf_np ,*, niter=10, method='gpu', useBlockAlgorithm=False, callbkTickFunc=None, resAsUint8 = False) `\\n\\nThis will do the Richardson-Lucy deconvolution on the data_np (numpy, 3 dimensional data volume) using the provided PSF data volume, for 10 iterations. GPU method is generally faster but it may fail. If it does fail, the program will automatically use the CPU version that uses the scipy fft package.\\n\\n\\n\\n## Manually building the conda package\\n\\nFor this installation, ensure that the conda-build package is installed\\n\\n`conda install conda-build`\\n\\nIn windows, simply execute\\n\\n`conda-create-package.bat`\\n\\n\\nOr, execute the following command-line to create the installation package.\\n\\n`conda-build --output-folder ./conda-built-packages -c conda-forge conda-recipe`\\n\\nand the conda package will be created in folder *conda-built-packages*.\\n\\nOtherwise, navigate to `conda-recipe`, and execute on the command-line `conda build .`\\n\\nIt will take a while to complete.\\n\\n## Contact\\n\\nReport issues and questions in project's github page, please. Please don't try to send emails as they may be igored or spam-filtered.\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"Red lionfish (RL) deconvolution\\nRichardson-Lucy deconvolution for fishes, scientists and engineers.\\nThis software is for filtering 3D data using the Richardson-Lucy deconvolution algorithm.\\nRichardson-Lucy is an iterative deconvolution algorithm that is used to remove\\npoint spread function (PSF) or optical transfer function (OTF) artifacts from experimental images.\\nThe method was originally developed for astronomy to remove optical effects and simultaneously reduce poisson noise in 2D images.\\nLucy, L. B. An iterative technique for the rectification of observed distributions. The Astronomical Journal 79, 745 (1974). DOI: 10.1086/111605\\nThe method can also be applied to 3D data. Nowadays this filtering technique is also widely used by microscopists.\\nThe Richardson-Lucy deconvolution algorigthm is iterative. Each iteration involves the calculation of 2 convolutions, one element-wise multiplication and one element-wise division.\\nWhen dealing with 3D data, the Richardson-Lucy algorithm is quite computional intensive primarly due to the calculation of the convolution, and can take a while to complete depending on the resources available. Convolution is significantly sped up using FFT compared to raw convolution.\\nThis software was developed with the aim to make the R-L computation faster by exploiting GPU resources, and with the use of FFT convolution.\\nTo make RedLionfish easily accessible, it is available through PyPi and anaconda (conda-forge channel). A useful plugin for Napari is also available.\\nPlease note that this software only works with 3D data. For 2D data there are many alternatives such as the DeconvolutionLab2 in Fiji (ImageJ) and sckikit-image.\\nNapari plugin\\nYou can now use the Napari's plugin installation in Menu -> Plugins -> Install/Uninstall Plugins....\\nHowever, if you chose to use this method, GPU acceleration may not be available and it will use the CPU backend. Better check.\\n\\nAlternatively, if you follow the installation instructions below, and install the napari in the same python environment\\nthen the plugin should be immediately available in the Menu -> Plugins -> RedLionfish.\\nInstallation\\nPreviously there was a problem in installing using pip, because no PyOpenCL wheels for windows were avaiable. It is now avaialble. This package can be installed using pip or conda.\\nConda install\\nThis package is available in conda-forge channel.\\nIt contains the precompiled libraries and it will install all the requirments for GPU-accelerated RL calculations.\\nconda install redlionfish -c conda-forge\\nInstall from PyPi\\npip install redlionfish\\nIn Linux , the package ocl-icd-system may also be useful.\\nconda install reikna pyopencl ocl-icd-system -c conda-forge\\nManual installation using the conda package file.\\nDownload the appropriate conda package .bz2 at https://github.com/rosalindfranklininstitute/RedLionfish/releases\\nIn the command line, successively run:\\nconda install <filename.bz2>\\nconda update --all -c conda-forge\\nThe second line is needed because you are installing from a local file, conda installer will not install dependencies. Right after this you should run the update command given.\\nManual installation (advanced and for developers)\\nPlease note that in order to use OpenCL GPU accelerations, PyOpenCL must be installed.\\nThe best way to get it working is to install it under a conda environment.\\nThe installation is similar to the previously described for PyPi.\\nconda install reikna pyopencl\\nor\\nconda install reikna pyopencl ocl-icd-system -c conda-forge (Linux)\\nClone/download from source https://github.com/rosalindfranklininstitute/RedLionfish/\\nand run\\npython setup.py install\\nDebug installation\\nIf you want to test and modify the code then you should probably install in debug mode using:\\npython setup.py develop\\nor\\npip install -e .\\nMore information\\nThe software has algorithms for Richardson-Lucy deconvolution that use either CPU and GPU.\\nThe CPU version is very similar to the skimage.restoration.richardson_lucy code, with improvments in speed.\\nmajor differences are:\\n\\nthe convolution steps use FFT only.\\nPSF and PSF-flipped FFTs are precalculated before starting iterations.\\n\\nThe GPU version, was written in to use Reikna package, which does FFT using OpenCL, via PyOpenCL.\\nUnfortunately, a major limitation in RAM usage exists with PyOpenCL.\\nLarge 3D data volumes with cause out-of-memory error when trying to upload data to the GPU for FFT calculations.\\nAs such, to overcome this problem, a block algorithm is used, which splits data into blocks with padded data.\\nThe results are then combined together to give the final result.\\nThis affects the perfomance of the calculation rather significantly, but with the advantage of being possible to handle large data volumes.\\nIf Richardson-Lucy deconvolution using the GPU method fails, RedLionfish will falls back to CPU calculation. Check console output for messages.\\nIf you are using the RedLionfish in your code, note that, by default, def doRLDeconvolutionFromNpArrays() method it uses the GPU OpenCL version.\\nTesting\\nMany examples can be found in `/test' folder.\\nA quick and benchmarking installation can be run from the proect root using the command:\\n'python test\\\\test_and_benchm.py'\\nor (*nix)\\n'python test/test_and_benchm.py'\\nThis will print out information about your GPU device (if available) and run some deconvolutions.\\nIt initially creates some data programatically, convolutes with a gaussian PSF, and add Poisson noise.\\nThen it executes executes the\\nRichardson-Lucy deconvolution calculation using CPU and GPU methods, for 10 iterations.\\nDuring the calculation it will print some information to the console/terminal, including the time it takes to run the calculation.\\nComputer generated data and an experimental PSF can be found in test\\\\testdata\\nTesting Redlionfish in napari\\nHere is an example testing the Redlionfish plugin in napari:\\n\\nload data test\\\\testdata\\\\gendata_psfconv_poiss_large.tif (can use draga and drop)\\nload psf data test\\\\testdata\\\\PSF_RFI_8bit.tif\\nIn the RedLionfish side window ensure that 'gendata_psfconv_poiss_large' is selected in data dropdown widget, and PSF_RFI_8bit is selected in psfdata widget.\\nChoose number of iterations (default=10)\\nClick 'Go' button and wait until result shows as a new data layer.\\nUse controls of the left panel to compare before and after RL deconvolution: select 'RL-deconvolution' layer and set colormap to red. Hide PSF_RFI_8bit. Make sure that both 'RL-deconvolution' and 'gendata-psfconv' are visible. Now, hide/unhide RL-deconvolution layer to see before and after deconvolution. Adjust contrast limits of each layer as desired.\\n\\nGPU vs CPU\\nYou may notice that choosing GPU does not make RL-calculation much faster compared with CPU, and sometimes is slower.\\nWhich method runs the R-L deconvolution faster. That depends on the computer configuration/architecture.\\nGPU calculations will be generally faster than CPU with bigger data volumes.\\nGPU calculation will be significantly faster if using a dedicated GPU card.\\nPlease see benchmark values that highlights significant variability in calculation speeds.\\nCoding\\nPlease feel free to browse /test folder for examples.\\nIn your code, add the import.\\nimport RedLionfishDeconv\\nin order to use the functions.\\nThe most useful function is perhaps the following.\\ndef doRLDeconvolutionFromNpArrays(data_np , psf_np ,*, niter=10, method='gpu', useBlockAlgorithm=False, callbkTickFunc=None, resAsUint8 = False)\\nThis will do the Richardson-Lucy deconvolution on the data_np (numpy, 3 dimensional data volume) using the provided PSF data volume, for 10 iterations. GPU method is generally faster but it may fail. If it does fail, the program will automatically use the CPU version that uses the scipy fft package.\\nManually building the conda package\\nFor this installation, ensure that the conda-build package is installed\\nconda install conda-build\\nIn windows, simply execute\\nconda-create-package.bat\\nOr, execute the following command-line to create the installation package.\\nconda-build --output-folder ./conda-built-packages -c conda-forge conda-recipe\\nand the conda package will be created in folder conda-built-packages.\\nOtherwise, navigate to conda-recipe, and execute on the command-line conda build .\\nIt will take a while to complete.\\nContact\\nReport issues and questions in project's github page, please. Please don't try to send emails as they may be igored or spam-filtered.\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"RedLionfish\",\"documentation\":\"\",\"first_released\":\"2021-11-19T14:55:32.357000Z\",\"license\":\"Apache-2.0\",\"name\":\"RedLionfish\",\"npe2\":false,\"operating_system\":[\"Operating System :: MacOS\",\"Operating System :: Microsoft :: Windows\",\"Operating System :: POSIX :: Linux\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/rosalindfranklininstitute/RedLionfish\",\"python_version\":\"\",\"reader_file_extensions\":[],\"release_date\":\"2022-09-30T20:40:24.914946Z\",\"report_issues\":\"\",\"requirements\":[\"numpy\",\"scipy\",\"pyopencl\",\"reikna\"],\"summary\":\"Fast Richardson-Lucy deconvolution of 3D volume data using GPU or CPU with napari plugin.\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.8\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Robin Koch\"},{\"name\":\"Marc Boucsein\"}],\"code_repository\":\"https://github.com/RobAnKo/napari-image-stacker\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-image-stacker\"}],\"description\":\"# napari-image-stacker\\n\\n[![License](https://img.shields.io/pypi/l/napari-image-stacker.svg?color=green)](https://github.com/RobAnKo/napari-image-stacker/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-image-stacker.svg?color=green)](https://pypi.org/project/napari-image-stacker)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-image-stacker.svg?color=green)](https://python.org)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-image-stacker)](https://napari-hub.org/plugins/napari-image-stacker)\\n\\nA plugin designed to convert multiple open layers into a stack or vice versa\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n\\n## Installation\\n\\nYou can install `napari-image-stacker` via [pip]:\\n\\n    pip install napari-image-stacker\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/RobAnKo/napari-image-stacker.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox].\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-image-stacker\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/RobAnKo/napari-image-stacker/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-image-stacker\\n\\n\\n\\n\\nA plugin designed to convert multiple open layers into a stack or vice versa\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\nInstallation\\nYou can install napari-image-stacker via pip:\\npip install napari-image-stacker\\n\\nTo install latest development version :\\npip install git+https://github.com/RobAnKo/napari-image-stacker.git\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-image-stacker\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari-image-stacker\",\"documentation\":\"https://github.com/RobAnKo/napari-image-stacker#README.md\",\"first_released\":\"2022-01-11T16:09:57.895727Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-image-stacker\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/RobAnKo/napari-image-stacker\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-02-04T13:59:14.581478Z\",\"report_issues\":\"https://github.com/RobAnKo/napari-image-stacker/issues\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\"],\"summary\":\"A plugin designed to convert multiple open layers into a stack or vice versa\",\"support\":\"https://github.com/RobAnKo/napari-image-stacker/issues\",\"twitter\":\"\",\"version\":\"0.1.10\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Hanjin Liu\"}],\"code_repository\":\"https://github.com/hanjinliu/napari-spreadsheet\",\"description\":\"# napari-spreadsheet\\n\\n[![License BSD-3](https://img.shields.io/pypi/l/napari-spreadsheet.svg?color=green)](https://github.com/hanjinliu/napari-spreadsheet/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-spreadsheet.svg?color=green)](https://pypi.org/project/napari-spreadsheet)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-spreadsheet.svg?color=green)](https://python.org)\\n[![tests](https://github.com/hanjinliu/napari-spreadsheet/workflows/tests/badge.svg)](https://github.com/hanjinliu/napari-spreadsheet/actions)\\n[![codecov](https://codecov.io/gh/hanjinliu/napari-spreadsheet/branch/main/graph/badge.svg)](https://codecov.io/gh/hanjinliu/napari-spreadsheet)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-spreadsheet)](https://napari-hub.org/plugins/napari-spreadsheet)\\n\\nLet's replace Microsoft Excel or Google Spreadsheet with `napari-spreadsheet` for your daily image analysis.\\n\\n### Highlights\\n\\n- Convert layer features to a spreadsheet.\\n- Update layer features from a spreadsheet.\\n- Send spreadsheet data to the namespace of napari's console directly.\\n\\n![](images/image.png)\\n\\nThis plugin is largely dependent on [tabulous](https://github.com/hanjinliu/tabulous).\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/stable/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-spreadsheet` via [pip]:\\n\\n    pip install napari-spreadsheet\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/hanjinliu/napari-spreadsheet.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-spreadsheet\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/hanjinliu/napari-spreadsheet/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-spreadsheet\\n\\n\\n\\n\\n\\n\\nLet's replace Microsoft Excel or Google Spreadsheet with napari-spreadsheet for your daily image analysis.\\nHighlights\\n\\nConvert layer features to a spreadsheet.\\nUpdate layer features from a spreadsheet.\\nSend spreadsheet data to the namespace of napari's console directly.\\n\\n\\nThis plugin is largely dependent on tabulous.\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-spreadsheet via pip:\\npip install napari-spreadsheet\\n\\nTo install latest development version :\\npip install git+https://github.com/hanjinliu/napari-spreadsheet.git\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-spreadsheet\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"Spreadsheet\",\"documentation\":\"https://github.com/hanjinliu/napari-spreadsheet#README.md\",\"first_released\":\"2022-08-28T02:06:43.203186Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-spreadsheet\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\",\"widget\"],\"project_site\":\"https://github.com/hanjinliu/napari-spreadsheet\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[\"*.csv\",\"*.xlsx\",\"*.dat\",\"*.txt\"],\"release_date\":\"2023-01-15T05:53:34.460280Z\",\"report_issues\":\"https://github.com/hanjinliu/napari-spreadsheet/issues\",\"requirements\":[\"magicgui\",\"napari\",\"numpy\",\"pandas\",\"qtpy\",\"tabulous (>=0.2.0)\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"tox ; extra == 'testing'\"],\"summary\":\"A spreadsheet widget for napari\",\"support\":\"https://github.com/hanjinliu/napari-spreadsheet/issues\",\"twitter\":\"\",\"version\":\"0.0.2\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"benjamin.graedel@unibe.ch\",\"name\":\"Benjamin Grädel\"}],\"code_repository\":\"https://github.com/bgraedel/arcos-gui\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"arcos-gui\"}],\"description\":\"# arcos-gui\\n\\n[![License](https://img.shields.io/pypi/l/arcos-gui.svg?color=green)](https://github.com/bgraedel/arcos-gui/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/arcos-gui.svg?color=green)](https://pypi.org/project/arcos-gui)\\n[![Python Version](https://img.shields.io/pypi/pyversions/arcos-gui.svg?color=green)](https://python.org)\\n[![tests](https://github.com/bgraedel/arcos-gui/workflows/tests/badge.svg)](https://github.com/bgraedel/arcos-gui/actions)\\n[![codecov](https://codecov.io/gh/bgraedel/arcos-gui/branch/main/graph/badge.svg)](https://codecov.io/gh/bgraedel/arcos-gui)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/arcos-gui)](https://napari-hub.org/plugins/arcos-gui)\\n\\nA napari plugin to detect and visualize collective signaling events\\n\\n----------------------------------\\n- Package specific Documentation: <https://bgraedel.github.io/arcos-gui>\\n- ARCOS documentation: <https://arcos.gitbook.io>\\n\\n**A**utomated **R**ecognition of **C**ollective **S**ignalling (ARCOS) is an algorithm to identify collective spatial events in time series data,\\nthat was written by Maciej Dobrzynski (https://github.com/dmattek). It is available as an R (ARCOS) and python (arcos4py) package.\\nARCOS can identify and visualize collective protein activation in 2- and 3D cell cultures over time.\\n\\nThis plugin integrates ARCOS into napari. Users can import tracked time-series data in CSV format. The plugin\\nprovides GUI elements to process this data with ARCOS. Layers containing the detected collective events are subsequently added to the viewer.\\n\\nFollowing analysis, the user can export the output as a CSV file with the detected collective events or as a sequence of images to generate a movie.\\n\\n\\n![](https://user-images.githubusercontent.com/100028238/177808096-399abe6c-37a7-473b-96d2-afda5042a51e.gif)\\n\\n[Watch full demo on youtube](https://www.youtube.com/watch?v=hG_z_BFcAiQ)\\n\\n\\n# Installation\\n\\nYou can install `arcos-gui` via [pip]:\\n\\n    pip install arcos-gui\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"arcos-gui\\\" is free and open-source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/bgraedel/arcos-gui/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/arcos-gui/\\n[PyPI]: https://pypi.org/\\n\\n# Credits\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"arcos-gui\\n\\n\\n\\n\\n\\n\\nA napari plugin to detect and visualize collective signaling events\\n\\n\\nPackage specific Documentation: https://bgraedel.github.io/arcos-gui\\nARCOS documentation: https://arcos.gitbook.io\\n\\nAutomated Recognition of Collective Signalling (ARCOS) is an algorithm to identify collective spatial events in time series data,\\nthat was written by Maciej Dobrzynski (https://github.com/dmattek). It is available as an R (ARCOS) and python (arcos4py) package.\\nARCOS can identify and visualize collective protein activation in 2- and 3D cell cultures over time.\\nThis plugin integrates ARCOS into napari. Users can import tracked time-series data in CSV format. The plugin\\nprovides GUI elements to process this data with ARCOS. Layers containing the detected collective events are subsequently added to the viewer.\\nFollowing analysis, the user can export the output as a CSV file with the detected collective events or as a sequence of images to generate a movie.\\n\\nWatch full demo on youtube\\nInstallation\\nYou can install arcos-gui via pip:\\npip install arcos-gui\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"arcos-gui\\\" is free and open-source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\\nCredits\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"napari ARCOS\",\"documentation\":\"https://bgraedel.github.io/arcos-gui/\",\"first_released\":\"2022-02-24T16:00:50.612224Z\",\"license\":\"BSD-3-Clause\",\"name\":\"arcos-gui\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\",\"sample_data\"],\"project_site\":\"https://github.com/bgraedel/arcos-gui\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-07-07T15:33:53.627299Z\",\"report_issues\":\"https://github.com/bgraedel/arcos-gui/issues\",\"requirements\":[\"arcos4py (>=0.1.4)\",\"magicgui (>=0.3.0)\",\"matplotlib (>=3.3.4)\",\"napari (>=0.4.14)\",\"numpy (>=1.21.5)\",\"pandas (>=1.3.5)\",\"scikit-image (>=0.18.1)\",\"scipy (>=1.7.3)\",\"mkdocs ; extra == 'doc'\",\"mkdocs-include-markdown-plugin ; extra == 'doc'\",\"mkdocs-material ; extra == 'doc'\",\"mkdocs-material-extensions ; extra == 'doc'\",\"pyqt5 ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\"],\"summary\":\"A napari plugin to detect and visualize collective signaling events\",\"support\":\"https://github.com/bgraedel/arcos-gui/issues\",\"twitter\":\"\",\"version\":\"0.0.6\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"cqzhang@g.ecc.u-tokyo.ac.jp\",\"name\":\"Chenqi Zhang\"}],\"code_repository\":\"https://github.com/zcqwh/disease-classifier\",\"conda\":[],\"description\":\"# disease-classifier\\n\\n[![License](https://img.shields.io/pypi/l/disease-classifier.svg?color=green)](https://github.com/zcqwh/disease-classifier/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/disease-classifier.svg?color=green)](https://pypi.org/project/disease-classifier)\\n[![Python Version](https://img.shields.io/pypi/pyversions/disease-classifier.svg?color=green)](https://python.org)\\n[![tests](https://github.com/zcqwh/disease-classifier/workflows/tests/badge.svg)](https://github.com/zcqwh/disease-classifier/actions)\\n[![codecov](https://codecov.io/gh/zcqwh/disease-classifier/branch/main/graph/badge.svg)](https://codecov.io/gh/zcqwh/disease-classifier)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/disease-classifier)](https://napari-hub.org/plugins/disease-classifier)\\n\\nA napari plugin for disease classification based on iPAC images.\\n\\n\\n\\n## Installation\\n\\nYou can install `disease-classifier` via [pip]:\\n\\n    pip install disease-classifier\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/zcqwh/disease-classifier.git\\n\\n## Introduction\\n#### Load data (.rtdc or .bin)\\n* Drag and drop the data in .rtdc or .bin into the files table.\\n* Click eye button to preview images.\\n![](https://github.com/zcqwh/disease-classifier/blob/main/Tutorial/Gif/01_Load_preview.gif?raw=true)\\n\\n\\n#### Choose model and classify\\n\\n* Choose the model folder including CNN and RF/PLDA.\\n* Check the data.\\n* Click classify.\\n![](https://github.com/zcqwh/disease-classifier/blob/main/Tutorial/Gif/02_model_classify.gif?raw=true)\\n\\n#### Preview classification results\\n* Click the eye button to preview the result.\\n* Click the header to show all.\\n![](https://github.com/zcqwh/disease-classifier/blob/main/Tutorial/Gif/03_preview_result.gif?raw=true)\\n\\n\\n#### Save results\\n* Click “Add classification to .rtdc file” button to save results.\\n![](https://github.com/zcqwh/disease-classifier/blob/main/Tutorial/Gif/04_save.gif?raw=true)\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"disease-classifier\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/plugins/stable/index.html\\n-->\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/zcqwh/disease-classifier/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"disease-classifier\\n\\n\\n\\n\\n\\n\\nA napari plugin for disease classification based on iPAC images.\\nInstallation\\nYou can install disease-classifier via pip:\\npip install disease-classifier\\n\\nTo install latest development version :\\npip install git+https://github.com/zcqwh/disease-classifier.git\\n\\nIntroduction\\nLoad data (.rtdc or .bin)\\n\\nDrag and drop the data in .rtdc or .bin into the files table.\\nClick eye button to preview images.\\n\\n\\nChoose model and classify\\n\\nChoose the model folder including CNN and RF/PLDA.\\nCheck the data.\\nClick classify.\\n\\n\\nPreview classification results\\n\\nClick the eye button to preview the result.\\nClick the header to show all.\\n\\n\\nSave results\\n\\nClick “Add classification to .rtdc file” button to save results.\\n\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"disease-classifier\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"Disease classifier\",\"documentation\":\"https://github.com/zcqwh/disease-classifier#README.md\",\"first_released\":\"2022-06-07T06:07:03.112636Z\",\"license\":\"BSD-3-Clause\",\"name\":\"disease-classifier\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/zcqwh/disease-classifier\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-06-07T06:07:03.112636Z\",\"report_issues\":\"https://github.com/zcqwh/disease-classifier/issues\",\"requirements\":[\"numpy\",\"magicgui\",\"qtpy\",\"matplotlib\",\"h5py (>=3.6.0)\",\"napari (>=0.4.15)\",\"numpy (>=1.22.4)\",\"opencv-contrib-python-headless (>=4.5.5.64)\",\"pytranskit (>=0.2.3)\",\"statsmodels (>=0.13.2)\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\"],\"summary\":\"A disease classifier based on iPAC images.\",\"support\":\"https://github.com/zcqwh/disease-classifier/issues\",\"twitter\":\"\",\"version\":\"0.0.1\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Allen Institute for Cell Science\"}],\"code_repository\":\"https://github.com/aics-int/napari-allencell-annotator\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-allencell-annotator\"}],\"description\":\"# napari-allencell-annotator\\n\\n[![License BSD-3](https://img.shields.io/pypi/l/napari-allencell-annotator.svg?color=green)](https://github.com/bbridge0200/napari-allencell-annotator/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-allencell-annotator.svg?color=green)](https://pypi.org/project/napari-allencell-annotator)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-allencell-annotator.svg?color=green)](https://python.org)\\n[![tests](https://github.com/bbridge0200/napari-allencell-annotator/workflows/tests/badge.svg)](https://github.com/bbridge0200/napari-allencell-annotator/actions)\\n[![codecov](https://codecov.io/gh/bbridge0200/napari-allencell-annotator/branch/main/graph/badge.svg)](https://codecov.io/gh/bbridge0200/napari-allencell-annotator)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-allencell-annotator)](https://napari-hub.org/plugins/napari-allencell-annotator)\\n\\nA plugin that enables image annotation/scoring and writes annotations to a .csv file. \\nPlugin provided by the Allen Institute for Cell Science.\\n\\nThe Allen Cell Image Annotator plugin for napari provides an intuitive\\ngraphical user interface to create annotation templates, annotate large \\nimage sets using these templates, and save image annotations to a csv file. \\nThe Allen Cell Image Annotator is a Python-based open source toolkit \\ndeveloped at the Allen Institute for Cell Science for both blind, unbiased and un-blind \\nmicroscope image annotating. This toolkit supports easy image set selection\\nfrom a file finder and creation of annotation templates (text, checkbox, drop-down, and spinbox).\\nWith napari's multi-dimensional image viewing capabilities, the plugin seamlessly allows users to\\nview each image and write annotations into the custom template.\\nAnnotation templates can be written to a json file for sharing or re-using. After annotating,\\nthe annotation template, image file list, and the annotation values \\nare conveniently saved to csv file, which can be re-opened for further annotating. \\n\\n-   Supports the following image types:\\n    - `OME-TIFF`\\n    - `TIFF`\\n    - `CZI` \\n    - `PNG` \\n    -   `JPEG` \\n\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to files up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/plugins/index.html\\n-->\\n\\n## Installation\\n### 1. Prerequisites\\n\\nThe plugin requires [Conda](https://docs.anaconda.com/anaconda/install/).\\n- [Installing on Windows ](https://docs.anaconda.com/anaconda/install/windows/) \\n  - Follow the steps linked above except\\n  - On step 8, check top the box to add to PATH\\n  - ![Alt text](napari_allencell_annotator/assets/windowsstep8.png)\\n- [Installing on Mac ](https://docs.anaconda.com/anaconda/install/mac-os/) \\n\\n### 2. Install the plugin\\nClick the link corresponding to your OS.\\n#### [Windows](https://alleninstitute-my.sharepoint.com/:u:/g/personal/beatrice_bridge_alleninstitute_org/EVOKZ8-PZB5AvO6z6OAjZ_YB2EHbaU9XRc_Z281oM0ctOg?e=skbKzh)\\n- From the link above, click the three dots on the top menu bar and select download. \\n- Open a file explorer and go to the Downloads folder. Use **Option 1** below. A prompt window should open and start installing. If this fails use **Option 2**. \\n  - **Option 1**: Double-click the file _install_napari.sh_\\n  - **Option 2**: Search the file finder for Anaconda Prompt. Open version 3. Run the following commands one line at a time. \\n    - conda create -n napari_annotator python=3.9 anaconda\\n    - conda activate napari_annotator\\n    - python -m pip install --upgrade pip\\n    - python -m pip install \\\"napari[all]\\\"\\n    - python -m pip install napari-allencell-annotator\\n    - napari\\n  - **Still not working?** Try using conda forge instead of pip. \\n    - Ex: conda install -c conda-forge napari instead of python -m pip install \\\"napari[all]\\\"\\n#### [MacOS/Unix](https://alleninstitute-my.sharepoint.com/:u:/g/personal/beatrice_bridge_alleninstitute_org/EaeV_RPXZz9DijxYy7qfoeMB3Hbq4vMpmJERqDyhL97KAg?e=HuKY2k)\\n- From the link above, download the file. \\n- Open terminal. \\n- Run _chmod +x ./Downloads/install_napari.command_ \\n  - If you get a file not found error try adjusting the path to match where install_napari.command was downloaded.\\n- Open finder, navigate to the file, double-click _install_napari.command_ . \\n  - A terminal window should open and start installing. \\n  \\n\\n### 3. Launch the Plugin\\n\\nOnce the napari window opens, go to **Plugins**.\\n- If **napari-allencell-annotator: Annotator** is listed click it to launch. \\n- If it is not listed \\n- **Install/Uninstall Plugins** ⇨ check the box next to **napari-allencell-annotator** ⇨ **close** ⇨ **Plugins** ⇨ **napari-allencell-annotator: Annotator** .\\n\\n### 4. Re-opening the Plugin After Installing\\n- Windows\\n  - Search for anaconda navigator in file finder\\n  - Click on navigator version 3\\n  - Once the navigator opens, click **Environments** on the left side\\n  - Click on the annotator environment and wait for it to load\\n  - Press the play button\\n  - Type _napari_ in the prompt that opens\\n  - Click **Plugins** ⇨ **napari-allencell-annotator: Annotator**\\n- MacOS\\n  - Open terminal\\n  - Run these commands one line at a time\\n    - conda activate napari_annotator\\n    - napari\\n  - Click **Plugins** ⇨ **napari-allencell-annotator: Annotator**\\n\\n## Quick Start\\n\\n1. Open napari\\n2. Start the plugin \\n   - Open napari, go to \\\"Plugins\\\" ⇨ \\\"napari-allencell-annotator\\\".\\n3. Create or import annotations and add images to annotate.\\n\\nFor more detailed usage instructions, check out this [document](napari_allencell_annotator/assets/AnnotatorInstructions.pdf) \\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-allencell-annotator\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/bbridge0200/napari-allencell-annotator/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-allencell-annotator\\n\\n\\n\\n\\n\\n\\nA plugin that enables image annotation/scoring and writes annotations to a .csv file. \\nPlugin provided by the Allen Institute for Cell Science.\\nThe Allen Cell Image Annotator plugin for napari provides an intuitive\\ngraphical user interface to create annotation templates, annotate large \\nimage sets using these templates, and save image annotations to a csv file. \\nThe Allen Cell Image Annotator is a Python-based open source toolkit \\ndeveloped at the Allen Institute for Cell Science for both blind, unbiased and un-blind \\nmicroscope image annotating. This toolkit supports easy image set selection\\nfrom a file finder and creation of annotation templates (text, checkbox, drop-down, and spinbox).\\nWith napari's multi-dimensional image viewing capabilities, the plugin seamlessly allows users to\\nview each image and write annotations into the custom template.\\nAnnotation templates can be written to a json file for sharing or re-using. After annotating,\\nthe annotation template, image file list, and the annotation values \\nare conveniently saved to csv file, which can be re-opened for further annotating. \\n\\nSupports the following image types:\\nOME-TIFF\\nTIFF\\nCZI \\nPNG \\nJPEG \\n\\n\\n\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\n1. Prerequisites\\nThe plugin requires Conda.\\n- Installing on Windows  \\n  - Follow the steps linked above except\\n  - On step 8, check top the box to add to PATH\\n  - \\n- Installing on Mac  \\n2. Install the plugin\\nClick the link corresponding to your OS.\\nWindows\\n\\nFrom the link above, click the three dots on the top menu bar and select download. \\nOpen a file explorer and go to the Downloads folder. Use Option 1 below. A prompt window should open and start installing. If this fails use Option 2. \\nOption 1: Double-click the file install_napari.sh\\nOption 2: Search the file finder for Anaconda Prompt. Open version 3. Run the following commands one line at a time. \\nconda create -n napari_annotator python=3.9 anaconda\\nconda activate napari_annotator\\npython -m pip install --upgrade pip\\npython -m pip install \\\"napari[all]\\\"\\npython -m pip install napari-allencell-annotator\\nnapari\\n\\n\\nStill not working? Try using conda forge instead of pip. \\nEx: conda install -c conda-forge napari instead of python -m pip install \\\"napari[all]\\\"\\n\\n\\n\\nMacOS/Unix\\n\\nFrom the link above, download the file. \\nOpen terminal. \\nRun chmod +x ./Downloads/install_napari.command \\nIf you get a file not found error try adjusting the path to match where install_napari.command was downloaded.\\nOpen finder, navigate to the file, double-click install_napari.command . \\nA terminal window should open and start installing. \\n\\n3. Launch the Plugin\\nOnce the napari window opens, go to Plugins.\\n- If napari-allencell-annotator: Annotator is listed click it to launch. \\n- If it is not listed \\n- Install/Uninstall Plugins ⇨ check the box next to napari-allencell-annotator ⇨ close ⇨ Plugins ⇨ napari-allencell-annotator: Annotator .\\n4. Re-opening the Plugin After Installing\\n\\nWindows\\nSearch for anaconda navigator in file finder\\nClick on navigator version 3\\nOnce the navigator opens, click Environments on the left side\\nClick on the annotator environment and wait for it to load\\nPress the play button\\nType napari in the prompt that opens\\nClick Plugins ⇨ napari-allencell-annotator: Annotator\\nMacOS\\nOpen terminal\\nRun these commands one line at a time\\nconda activate napari_annotator\\nnapari\\n\\n\\nClick Plugins ⇨ napari-allencell-annotator: Annotator\\n\\nQuick Start\\n\\nOpen napari\\nStart the plugin \\nOpen napari, go to \\\"Plugins\\\" ⇨ \\\"napari-allencell-annotator\\\".\\nCreate or import annotations and add images to annotate.\\n\\nFor more detailed usage instructions, check out this document \\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-allencell-annotator\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[],\"display_name\":\"napari-allencell-annotator\",\"documentation\":\"\",\"first_released\":\"2022-07-27T20:13:22.558984Z\",\"license\":\"\",\"name\":\"napari-allencell-annotator\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/aics-int/napari-allencell-annotator/\",\"python_version\":\">=3.9\",\"reader_file_extensions\":[],\"release_date\":\"2022-09-16T23:36:53.473442Z\",\"report_issues\":\"\",\"requirements\":[\"napari (>=0.4.9)\",\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"aicsimageio (>=4.9.1)\",\"xarray (>=2022.6.0)\",\"magicgui (>=0.3.7)\",\"aicspylibczi (>=3.0.5)\",\"fsspec (>=2022.8.2)\",\"bioformats-jar\",\"bfio\",\"qtpy\",\"napari (>=0.4.9) ; extra == 'all'\",\"napari-plugin-engine (>=0.1.4) ; extra == 'all'\",\"numpy ; extra == 'all'\",\"aicsimageio (>=4.9.1) ; extra == 'all'\",\"xarray (>=2022.6.0) ; extra == 'all'\",\"magicgui (>=0.3.7) ; extra == 'all'\",\"aicspylibczi (>=3.0.5) ; extra == 'all'\",\"fsspec (>=2022.8.2) ; extra == 'all'\",\"bioformats-jar ; extra == 'all'\",\"bfio ; extra == 'all'\",\"qtpy ; extra == 'all'\",\"black (>=19.10b0) ; extra == 'all'\",\"codecov (>=2.0.22) ; extra == 'all'\",\"docutils (<0.16,>=0.10) ; extra == 'all'\",\"flake8 (>=3.7.7) ; extra == 'all'\",\"psutil (>=5.7.0) ; extra == 'all'\",\"pytest (>=4.3.0) ; extra == 'all'\",\"pytest-cov (==2.6.1) ; extra == 'all'\",\"pytest-raises (>=0.10) ; extra == 'all'\",\"pytest-qt (>=3.3.0) ; extra == 'all'\",\"quilt3 (>=3.1.12) ; extra == 'all'\",\"pytest-runner ; extra == 'all'\",\"bumpversion (>=0.5.3) ; extra == 'all'\",\"gitchangelog (>=3.0.4) ; extra == 'all'\",\"ipython (>=7.5.0) ; extra == 'all'\",\"m2r (>=0.2.1) ; extra == 'all'\",\"pytest-runner (>=4.4) ; extra == 'all'\",\"Sphinx (<3,>=2.0.0b1) ; extra == 'all'\",\"sphinx-rtd-theme (>=0.1.2) ; extra == 'all'\",\"tox (>=3.5.2) ; extra == 'all'\",\"twine (>=1.13.0) ; extra == 'all'\",\"wheel (>=0.33.1) ; extra == 'all'\",\"black (>=19.10b0) ; extra == 'dev'\",\"bumpversion (>=0.5.3) ; extra == 'dev'\",\"docutils (<0.16,>=0.10) ; extra == 'dev'\",\"flake8 (>=3.7.7) ; extra == 'dev'\",\"gitchangelog (>=3.0.4) ; extra == 'dev'\",\"ipython (>=7.5.0) ; extra == 'dev'\",\"m2r (>=0.2.1) ; extra == 'dev'\",\"pytest (>=4.3.0) ; extra == 'dev'\",\"pytest-cov (==2.6.1) ; extra == 'dev'\",\"pytest-raises (>=0.10) ; extra == 'dev'\",\"pytest-runner (>=4.4) ; extra == 'dev'\",\"pytest-qt (>=3.3.0) ; extra == 'dev'\",\"quilt3 (>=3.1.12) ; extra == 'dev'\",\"Sphinx (<3,>=2.0.0b1) ; extra == 'dev'\",\"sphinx-rtd-theme (>=0.1.2) ; extra == 'dev'\",\"tox (>=3.5.2) ; extra == 'dev'\",\"twine (>=1.13.0) ; extra == 'dev'\",\"wheel (>=0.33.1) ; extra == 'dev'\",\"pytest-runner ; extra == 'setup'\",\"black (>=19.10b0) ; extra == 'test'\",\"codecov (>=2.0.22) ; extra == 'test'\",\"docutils (<0.16,>=0.10) ; extra == 'test'\",\"flake8 (>=3.7.7) ; extra == 'test'\",\"psutil (>=5.7.0) ; extra == 'test'\",\"pytest (>=4.3.0) ; extra == 'test'\",\"pytest-cov (==2.6.1) ; extra == 'test'\",\"pytest-raises (>=0.10) ; extra == 'test'\",\"pytest-qt (>=3.3.0) ; extra == 'test'\",\"quilt3 (>=3.1.12) ; extra == 'test'\"],\"summary\":\"A plugin that enables annotations provided by Allen Institute for Cell Science\",\"support\":\"\",\"twitter\":\"\",\"version\":\"1.0.7\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Ruben Lopez\"}],\"code_repository\":\"https://github.com/rjlopez2/napari-omaas\",\"conda\":[],\"description\":\"# napari-omaas\\n\\n[![License BSD-3](https://img.shields.io/pypi/l/napari-omaas.svg?color=green)](https://github.com/rjlopez2/napari-omaas/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-omaas.svg?color=green)](https://pypi.org/project/napari-omaas)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-omaas.svg?color=green)](https://python.org)\\n[![tests](https://github.com/rjlopez2/napari-omaas/workflows/tests/badge.svg)](https://github.com/rjlopez2/napari-omaas/actions)\\n[![codecov](https://codecov.io/gh/rjlopez2/napari-omaas/branch/main/graph/badge.svg)](https://codecov.io/gh/rjlopez2/napari-omaas)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-omaas)](https://napari-hub.org/plugins/napari-omaas)\\n\\nnapari-OMAAS stands for Optical Mapping Acquisition and Analysis Software\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/stable/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-omaas` via [pip]:\\n\\n    pip install napari-omaas\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/rjlopez2/napari-omaas.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-omaas\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/rjlopez2/napari-omaas/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-omaas\\n\\n\\n\\n\\n\\n\\nnapari-OMAAS stands for Optical Mapping Acquisition and Analysis Software\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-omaas via pip:\\npip install napari-omaas\\n\\nTo install latest development version :\\npip install git+https://github.com/rjlopez2/napari-omaas.git\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-omaas\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari OMAAS\",\"documentation\":\"https://github.com/rjlopez2/napari-omaas#README.md\",\"first_released\":\"2022-08-09T21:49:57.410742Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-omaas\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\",\"writer\",\"widget\",\"sample_data\"],\"project_site\":\"https://github.com/rjlopez2/napari-omaas\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[\"*.sif\"],\"release_date\":\"2022-08-09T21:49:57.410742Z\",\"report_issues\":\"https://github.com/rjlopez2/napari-omaas/issues\",\"requirements\":[\"numpy\",\"magicgui\",\"qtpy\",\"sif-parser\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\"],\"summary\":\"napari-OMAAS stands for Optical Mapping Acquisition and Analysis Software\",\"support\":\"https://github.com/rjlopez2/napari-omaas/issues\",\"twitter\":\"\",\"version\":\"0.1.0\",\"visibility\":\"public\",\"writer_file_extensions\":[\".npy\"],\"writer_save_layers\":[\"image\",\"labels\"]}",
  "{\"authors\":[{\"name\":\"Tim-Oliver Buchholz\"}],\"code_repository\":\"https://github.com/fmi-faim/napari-psf-analysis\",\"description\":\"# napari-psf-analysis\\n\\n[![License](https://img.shields.io/badge/License-BSD_3--Clause-blue.svg)](https://opensource.org/licenses/BSD-3-Clause)\\n[![PyPI](https://img.shields.io/pypi/v/napari-psf-analysis.svg?color=green)](https://pypi.org/project/napari-psf-analysis)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-psf-analysis.svg?color=green)](https://python.org)\\n[![tests](https://github.com/fmi-faim/napari-psf-analysis/workflows/tests/badge.svg)](https://github.com/fmi-faim/napari-psf-analysis/actions)\\n[![codecov](https://codecov.io/gh/fmi-faim/napari-psf-analysis/branch/main/graph/badge.svg)](https://codecov.io/gh/fmi-faim/napari-psf-analysis)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-psf-analysis)](https://napari-hub.org/plugins/napari-psf-analysis)\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n---\\n![application_screenshot](./figs/napari-psf-analysis_demo.gif)\\nA plugin to analyse point spread funcitons (PSFs) of optical systems.\\n\\n## Usage\\n### Starting Point\\nTo run a PSF analysis open an image of acquired beads. Add a point-layer\\nand indicate the beads you want to measure by adding a point.\\n\\n### Run Analyis\\nOpen the plugin (Plugins > napari-psf-analysis > PSF Analysis) and ensure\\nthat your bead image and point layers are select in the `Basic` tab under\\n`Image` and `Points` respectively.\\nIn the `Advanced` tab further information can be provided. Only the filled\\nin fields of the `Advanced` tab are saved in the output.\\n\\nAfter verifying all input fields click `Extract PSFs`.\\n\\n### Discard and Save Measurement\\nOnce the PSF extraction has finished a new layer (`Analyzed Beads`) appears,\\nholding a summary\\nimage for every selected bead.\\nIndividual summaries can be discarded by clicking the `Delete Displayed\\nMeasurement` button.\\n\\nResults are saved to the selected `Save Dir` by clicking the `Save\\nMeasurements` button.\\n\\nNote: Beads for which the bounding box does not fit within the image are\\nautomatically excluded from the analysis and no output is generated.\\n\\n\\n### Saved Data\\nEvery image of the `Analyzed Beads` layer is saved as `{source_image_name}_X\\n{bead-centroid-x}_Y{bead-centroid-y}_Z{bead-centroid-z}.png` file.\\nAdditionally a `PSFMeasurement_{source_image_acquisition_date}_\\n{source_image_name}_{microscope_name}_{magnification}_{NA}.csv` file is\\nstored containing the measured values and all filled in fields.\\n\\nFor the demo gif above the following table is saved:\\n\\n|ImageName               |Date      |Microscope|Magnification|NA |Amplitude        |Amplitude_2D      |Background        |Background_2D     |X                 |Y                 |Z                 |X_2D             |Y_2D              |FWHM_X            |FWHM_Y            |FWHM_Z           |FWHM_X_2D         |FWHM_Y_2D         |PrincipalAxis_1  |PrincipalAxis_2   |PrincipalAxis_3   |PrincipalAxis_1_2D|PrincipalAxis_2_2D|SignalToBG        |SignalToBG_2D     |XYpixelsize|Zspacing|cov_xx           |cov_xy            |cov_xz            |cov_yy           |cov_yz            |cov_zz           |cov_xx_2D        |cov_xy_2D         |cov_yy_2D        |sde_peak         |sde_background      |sde_X               |sde_Y              |sde_Z              |sde_cov_xx        |sde_cov_xy        |sde_cov_xz        |sde_cov_yy        |sde_cov_yz        |sde_cov_zz        |sde_peak_2D      |sde_background_2D   |sde_X_2D            |sde_Y_2D           |sde_cov_xx_2D      |sde_cov_xy_2D     |sde_cov_yy_2D     |version                      |Objective_id|PSF_path                                                   |\\n|------------------------|----------|----------|-------------|---|-----------------|------------------|------------------|------------------|------------------|------------------|------------------|-----------------|------------------|------------------|------------------|-----------------|------------------|------------------|-----------------|------------------|------------------|------------------|------------------|------------------|------------------|-----------|--------|-----------------|------------------|------------------|-----------------|------------------|-----------------|-----------------|------------------|-----------------|-----------------|--------------------|--------------------|-------------------|-------------------|------------------|------------------|------------------|------------------|------------------|------------------|-----------------|--------------------|--------------------|-------------------|-------------------|------------------|------------------|-----------------------------|------------|-----------------------------------------------------------|\\n|100x_1_conf488Virtex.tif|2022-03-03|Microscope|100          |1.4|4969.668887708917|5337.819377008636 |108.68118565810401|112.06260978398689|3304.2501743562757|3250.4033761925416|3065.7405670931444|3298.302439467146|3250.4113027135386|200.8139074557401 |197.03885411472754|674.9482557309917|183.72294307475494|176.22103947242965|675.8518951846926|199.23103476771948|195.54256297143823|184.93322114936828|174.9505023717054 |45.72703966759075 |47.63247426860641 |65.0       |200.0   |7272.305680278791|199.51201664296516|3544.234164883923 |7001.454943429444|1986.6819006579824|82153.39409342635|6087.112657801992|213.67205054291261|5600.155281533672|4.505314027916024|0.020623059963186995|0.06914746933299017 |0.06784783324982839|0.23241058486654181|11.794695129256654|8.185734859446441 |28.322224373959276|11.355300093289646|27.596443942593737|133.24416529759324|4.505314027916024|0.020623059963186995|0.06914746933299017 |0.06784783324982839|0.23241058486654181|11.794695129256654|8.185734859446441 |0.2.2.dev0+g1cb747a.d20221017|obj_1       |./100x_1_conf488Virtex.tif_Bead_X3304.3_Y3250.4_Z3065.7.png|\\n|100x_1_conf488Virtex.tif|2022-03-03|Microscope|100          |1.4|6131.783156459655|7007.7128858909955|108.97903673830632|112.03049813071806|3283.1815179892947|3369.970476029713 |3032.554247929097 |3276.851481699453|3370.657390994046 |210.4996203597178 |197.86302004157108|689.3745507955736|190.91529553428666|174.67418701487333|689.6867779885374|209.54607959880923|197.78706972073468|190.9553524915874 |174.63039550165533|56.265712562536564|62.55183189236865 |65.0       |200.0   |7990.743418986118|71.21282284528938 |2449.8383466578707|7060.148226446074|195.46702412069595|85702.80681598671|6573.035837695339|54.41831030118964 |5502.271462869397|4.621842915212504|0.02204379426143539 |0.060262260154465876|0.05664426296450384|0.19735347823753954|10.774935265711402|7.1611338777206415|25.058014334542662|9.520360761496262 |23.451758416706767|115.56192359712198|4.621842915212504|0.02204379426143539 |0.060262260154465876|0.05664426296450384|0.19735347823753954|10.774935265711402|7.1611338777206415|0.2.2.dev0+g1cb747a.d20221017|obj_1       |./100x_1_conf488Virtex.tif_Bead_X3283.2_Y3370.0_Z3032.6.png|\\n|100x_1_conf488Virtex.tif|2022-03-03|Microscope|100          |1.4|5619.498212394354|5796.371864919072 |108.65622592515462|111.58266064326322|3091.108753635228 |3279.722510310466 |3076.2986816645853|3084.318731921503|3278.6653904046307|214.82526951473534|210.45093794302085|708.1707538388272|205.59236614714763|202.64724874262387|709.557918556187 |211.60340602370195|209.0379608329753 |206.12587984384623|202.10455239686695|51.718142835783915|51.946887011866814|65.0       |200.0   |8322.52833820903 |217.27766177392047|4878.624412287828 |7987.047795051033|2326.0083470459012|90439.99432189878|7622.519106230307|100.7865902930646 |7405.697623587783|4.830591872545995|0.0241438047271005  |0.07013922523759801 |0.06871098346239177|0.23121554192278806|12.798834896781521|8.867905979672457 |30.29851519735856 |12.282968059322448|29.332165284940636|139.08589313352223|4.830591872545995|0.0241438047271005  |0.07013922523759801 |0.06871098346239177|0.23121554192278806|12.798834896781521|8.867905979672457 |0.2.2.dev0+g1cb747a.d20221017|obj_1       |./100x_1_conf488Virtex.tif_Bead_X3091.1_Y3279.7_Z3076.3.png|\\n\\n\\nWith the three summary images:\\n\\n![summaries](figs/summaries.png)\\n\\n---\\n\\n## Installation\\nWe recommend installation into a fresh conda environment.\\n\\n### 1. Install napari\\n```shell\\nconda create -y -n psf-analysis -c conda-forge python=3.9\\n\\nconda activate psf-analysis\\n\\npython -m pip install \\\"napari[all]\\\"\\n```\\n\\n### 2. Install napari-aicsimageio and bioformats\\nRequired if you want to open other files than `.tif` e.g. `.stk. `.\\n\\n__Note:__ See [napari-aicsimageio](https://www.napari-hub.org/plugins/napari-aicsimageio) for more information about opening images.\\n```shell\\nconda install -c conda-forge openjdk\\n\\nconda deactivate\\nconda activate psf-analysis\\n\\npython -m pip install \\\"bfio[bioformats]\\\"\\npython -m pip install \\\"aicsimageio[all]\\\"\\npython -m pip install napari-aicsimageio\\n```\\n\\n### 3. Install napari-psf-analysis\\nYou can install `napari-psf-analysis` via [pip]:\\n\\n```shell\\npython -m pip install napari-psf-analysis\\n```\\n\\n### 4. Optional `Set Config`\\nYou can provide a config yaml file with the available microscope names and a default save directory.\\nThis will change the `Microscope` text field to a drop down menu and change the default save directory.\\n\\n`example_config.yaml`\\n```yaml\\nmicroscopes:\\n  - TIRF\\n  - Zeiss Z1\\noutput_path: \\\"D:\\\\\\\\psf_analysis\\\\\\\\measurements\\\"\\n```\\n\\nTo use this config navigate to `Plugins > napari-psf-analysis > Set Config` and select the config file.\\n\\n__Note:__ The save path is OS specific.\\n\\n### 5. Desktop Icon for Windows\\nFollow [this](https://twitter.com/haesleinhuepf/status/1537030855843094529)\\nthread by Robert Haase.\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-psf-analysis\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue](https://github.com/fmi-faim/napari-psf-analysis/issues) along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[@napari]: https://github.com/napari\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-psf-analysis\\n\\n\\n\\n\\n\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\n\\nA plugin to analyse point spread funcitons (PSFs) of optical systems.\\nUsage\\nStarting Point\\nTo run a PSF analysis open an image of acquired beads. Add a point-layer\\nand indicate the beads you want to measure by adding a point.\\nRun Analyis\\nOpen the plugin (Plugins > napari-psf-analysis > PSF Analysis) and ensure\\nthat your bead image and point layers are select in the Basic tab under\\nImage and Points respectively.\\nIn the Advanced tab further information can be provided. Only the filled\\nin fields of the Advanced tab are saved in the output.\\nAfter verifying all input fields click Extract PSFs.\\nDiscard and Save Measurement\\nOnce the PSF extraction has finished a new layer (Analyzed Beads) appears,\\nholding a summary\\nimage for every selected bead.\\nIndividual summaries can be discarded by clicking the Delete Displayed\\nMeasurement button.\\nResults are saved to the selected Save Dir by clicking the Save\\nMeasurements button.\\nNote: Beads for which the bounding box does not fit within the image are\\nautomatically excluded from the analysis and no output is generated.\\nSaved Data\\nEvery image of the Analyzed Beads layer is saved as {source_image_name}_X\\n{bead-centroid-x}_Y{bead-centroid-y}_Z{bead-centroid-z}.png file.\\nAdditionally a PSFMeasurement_{source_image_acquisition_date}_\\n{source_image_name}_{microscope_name}_{magnification}_{NA}.csv file is\\nstored containing the measured values and all filled in fields.\\nFor the demo gif above the following table is saved:\\n|ImageName               |Date      |Microscope|Magnification|NA |Amplitude        |Amplitude_2D      |Background        |Background_2D     |X                 |Y                 |Z                 |X_2D             |Y_2D              |FWHM_X            |FWHM_Y            |FWHM_Z           |FWHM_X_2D         |FWHM_Y_2D         |PrincipalAxis_1  |PrincipalAxis_2   |PrincipalAxis_3   |PrincipalAxis_1_2D|PrincipalAxis_2_2D|SignalToBG        |SignalToBG_2D     |XYpixelsize|Zspacing|cov_xx           |cov_xy            |cov_xz            |cov_yy           |cov_yz            |cov_zz           |cov_xx_2D        |cov_xy_2D         |cov_yy_2D        |sde_peak         |sde_background      |sde_X               |sde_Y              |sde_Z              |sde_cov_xx        |sde_cov_xy        |sde_cov_xz        |sde_cov_yy        |sde_cov_yz        |sde_cov_zz        |sde_peak_2D      |sde_background_2D   |sde_X_2D            |sde_Y_2D           |sde_cov_xx_2D      |sde_cov_xy_2D     |sde_cov_yy_2D     |version                      |Objective_id|PSF_path                                                   |\\n|------------------------|----------|----------|-------------|---|-----------------|------------------|------------------|------------------|------------------|------------------|------------------|-----------------|------------------|------------------|------------------|-----------------|------------------|------------------|-----------------|------------------|------------------|------------------|------------------|------------------|------------------|-----------|--------|-----------------|------------------|------------------|-----------------|------------------|-----------------|-----------------|------------------|-----------------|-----------------|--------------------|--------------------|-------------------|-------------------|------------------|------------------|------------------|------------------|------------------|------------------|-----------------|--------------------|--------------------|-------------------|-------------------|------------------|------------------|-----------------------------|------------|-----------------------------------------------------------|\\n|100x_1_conf488Virtex.tif|2022-03-03|Microscope|100          |1.4|4969.668887708917|5337.819377008636 |108.68118565810401|112.06260978398689|3304.2501743562757|3250.4033761925416|3065.7405670931444|3298.302439467146|3250.4113027135386|200.8139074557401 |197.03885411472754|674.9482557309917|183.72294307475494|176.22103947242965|675.8518951846926|199.23103476771948|195.54256297143823|184.93322114936828|174.9505023717054 |45.72703966759075 |47.63247426860641 |65.0       |200.0   |7272.305680278791|199.51201664296516|3544.234164883923 |7001.454943429444|1986.6819006579824|82153.39409342635|6087.112657801992|213.67205054291261|5600.155281533672|4.505314027916024|0.020623059963186995|0.06914746933299017 |0.06784783324982839|0.23241058486654181|11.794695129256654|8.185734859446441 |28.322224373959276|11.355300093289646|27.596443942593737|133.24416529759324|4.505314027916024|0.020623059963186995|0.06914746933299017 |0.06784783324982839|0.23241058486654181|11.794695129256654|8.185734859446441 |0.2.2.dev0+g1cb747a.d20221017|obj_1       |./100x_1_conf488Virtex.tif_Bead_X3304.3_Y3250.4_Z3065.7.png|\\n|100x_1_conf488Virtex.tif|2022-03-03|Microscope|100          |1.4|6131.783156459655|7007.7128858909955|108.97903673830632|112.03049813071806|3283.1815179892947|3369.970476029713 |3032.554247929097 |3276.851481699453|3370.657390994046 |210.4996203597178 |197.86302004157108|689.3745507955736|190.91529553428666|174.67418701487333|689.6867779885374|209.54607959880923|197.78706972073468|190.9553524915874 |174.63039550165533|56.265712562536564|62.55183189236865 |65.0       |200.0   |7990.743418986118|71.21282284528938 |2449.8383466578707|7060.148226446074|195.46702412069595|85702.80681598671|6573.035837695339|54.41831030118964 |5502.271462869397|4.621842915212504|0.02204379426143539 |0.060262260154465876|0.05664426296450384|0.19735347823753954|10.774935265711402|7.1611338777206415|25.058014334542662|9.520360761496262 |23.451758416706767|115.56192359712198|4.621842915212504|0.02204379426143539 |0.060262260154465876|0.05664426296450384|0.19735347823753954|10.774935265711402|7.1611338777206415|0.2.2.dev0+g1cb747a.d20221017|obj_1       |./100x_1_conf488Virtex.tif_Bead_X3283.2_Y3370.0_Z3032.6.png|\\n|100x_1_conf488Virtex.tif|2022-03-03|Microscope|100          |1.4|5619.498212394354|5796.371864919072 |108.65622592515462|111.58266064326322|3091.108753635228 |3279.722510310466 |3076.2986816645853|3084.318731921503|3278.6653904046307|214.82526951473534|210.45093794302085|708.1707538388272|205.59236614714763|202.64724874262387|709.557918556187 |211.60340602370195|209.0379608329753 |206.12587984384623|202.10455239686695|51.718142835783915|51.946887011866814|65.0       |200.0   |8322.52833820903 |217.27766177392047|4878.624412287828 |7987.047795051033|2326.0083470459012|90439.99432189878|7622.519106230307|100.7865902930646 |7405.697623587783|4.830591872545995|0.0241438047271005  |0.07013922523759801 |0.06871098346239177|0.23121554192278806|12.798834896781521|8.867905979672457 |30.29851519735856 |12.282968059322448|29.332165284940636|139.08589313352223|4.830591872545995|0.0241438047271005  |0.07013922523759801 |0.06871098346239177|0.23121554192278806|12.798834896781521|8.867905979672457 |0.2.2.dev0+g1cb747a.d20221017|obj_1       |./100x_1_conf488Virtex.tif_Bead_X3091.1_Y3279.7_Z3076.3.png|\\nWith the three summary images:\\n\\n\\nInstallation\\nWe recommend installation into a fresh conda environment.\\n1. Install napari\\n```shell\\nconda create -y -n psf-analysis -c conda-forge python=3.9\\nconda activate psf-analysis\\npython -m pip install \\\"napari[all]\\\"\\n```\\n2. Install napari-aicsimageio and bioformats\\nRequired if you want to open other files than .tif e.g. .stk..\\nNote: See napari-aicsimageio for more information about opening images.\\n```shell\\nconda install -c conda-forge openjdk\\nconda deactivate\\nconda activate psf-analysis\\npython -m pip install \\\"bfio[bioformats]\\\"\\npython -m pip install \\\"aicsimageio[all]\\\"\\npython -m pip install napari-aicsimageio\\n```\\n3. Install napari-psf-analysis\\nYou can install napari-psf-analysis via pip:\\nshell\\npython -m pip install napari-psf-analysis\\n4. Optional Set Config\\nYou can provide a config yaml file with the available microscope names and a default save directory.\\nThis will change the Microscope text field to a drop down menu and change the default save directory.\\nexample_config.yaml\\nyaml\\nmicroscopes:\\n  - TIRF\\n  - Zeiss Z1\\noutput_path: \\\"D:\\\\\\\\psf_analysis\\\\\\\\measurements\\\"\\nTo use this config navigate to Plugins > napari-psf-analysis > Set Config and select the config file.\\nNote: The save path is OS specific.\\n5. Desktop Icon for Windows\\nFollow this\\nthread by Robert Haase.\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-psf-analysis\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 3 - Alpha\"],\"display_name\":\"napari-psf-analysis\",\"documentation\":\"https://github.com/fmi-faim/napari-psf-analysis#README.md\",\"first_released\":\"2022-03-30T15:58:04.217762Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-psf-analysis\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/fmi-faim/napari-psf-analysis.git\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-11-23T15:12:40.589495Z\",\"report_issues\":\"https://github.com/fmi-faim/napari-psf-analysis/issues\",\"requirements\":[\"bfio\",\"matplotlib\",\"matplotlib-scalebar\",\"napari\",\"numpy\",\"pandas\",\"scikit-image\"],\"summary\":\"A plugin to analyse point spread functions (PSFs).\",\"support\":\"https://github.com/fmi-faim/napari-psf-analysis/issues\",\"twitter\":\"\",\"version\":\"0.2.5\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Hippolyte Verdier\"}],\"code_repository\":\"https://github.com/hippover/palmari\",\"description\":\"\\n\\n<!-- This file is designed to provide you with a starting template for documenting\\nthe functionality of your plugin. Its content will be rendered on your plugin's\\nnapari hub page.\\n\\nThe sections below are given as a guide for the flow of information only, and\\nare in no way prescriptive. You should feel free to merge, remove, add and \\nrename sections at will to make this document work best for your plugin. \\n\\n## Description\\n\\nThis should be a detailed description of the context of your plugin and its \\nintended purpose.\\n\\nIf you have videos or screenshots of your plugin in action, you should include them\\nhere as well, to make them front and center for new users. \\n\\nYou should use absolute links to these assets, so that we can easily display them \\non the hub. The easiest way to include a video is to use a GIF, for example hosted\\non imgur. You can then reference this GIF as an image.\\n\\n![Example GIF hosted on Imgur](https://i.imgur.com/A5phCX4.gif)\\n\\nNote that GIFs larger than 5MB won't be rendered by GitHub - we will however,\\nrender them on the napari hub.\\n\\nThe other alternative, if you prefer to keep a video, is to use GitHub's video\\nembedding feature.\\n\\n1. Push your `DESCRIPTION.md` to GitHub on your repository (this can also be done\\nas part of a Pull Request)\\n2. Edit `.napari/DESCRIPTION.md` **on GitHub**.\\n3. Drag and drop your video into its desired location. It will be uploaded and\\nhosted on GitHub for you, but will not be placed in your repository.\\n4. We will take the resolved link to the video and render it on the hub.\\n\\nHere is an example of an mp4 video embedded this way.\\n\\nhttps://user-images.githubusercontent.com/17995243/120088305-6c093380-c132-11eb-822d-620e81eb5f0e.mp4\\n\\n## Intended Audience & Supported Data\\n\\nThis section should describe the target audience for this plugin (any knowledge,\\nskills and experience required), as well as a description of the types of data\\nsupported by this plugin.\\n\\nTry to make the data description as explicit as possible, so that users know the\\nformat your plugin expects. This applies both to reader plugins reading file formats\\nand to function/dock widget plugins accepting layers and/or layer data.\\nFor example, if you know your plugin only works with 3D integer data in \\\"tyx\\\" order,\\nmake sure to mention this.\\n\\nIf you know of researchers, groups or labs using your plugin, or if it has been cited\\nanywhere, feel free to also include this information here.\\n\\n## Quickstart\\n\\nThis section should go through step-by-step examples of how your plugin should be used.\\nWhere your plugin provides multiple dock widgets or functions, you should split these\\nout into separate subsections for easy browsing. Include screenshots and videos\\nwherever possible to elucidate your descriptions. \\n\\nIdeally, this section should start with minimal examples for those who just want a\\nquick overview of the plugin's functionality, but you should definitely link out to\\nmore complex and in-depth tutorials highlighting any intricacies of your plugin, and\\nmore detailed documentation if you have it.\\n\\n## Additional Install Steps (uncommon)\\nWe will be providing installation instructions on the hub, which will be sufficient\\nfor the majority of plugins. They will include instructions to pip install, and\\nto install via napari itself.\\n\\nMost plugins can be installed out-of-the-box by just specifying the package requirements\\nover in `setup.cfg`. However, if your plugin has any more complex dependencies, or \\nrequires any additional preparation before (or after) installation, you should add \\nthis information here.\\n\\n## How to Cite\\n\\nMany plugins may be used in the course of published (or publishable) research, as well as\\nduring conference talks and other public facing events. If you'd like to be cited in\\na particular format, or have a DOI you'd like used, you should provide that information here. -->\\n\\nProcess your SPT-PALM recordings with PALMari's all-inclusive pipeline: spot detection, sub-pixel localization, tracking & more.\\nStart quickly with default parameters or customize your pipeline, and run it on entire folder of microscope recordings.\\n\\n## Getting Help\\n\\nEmail Hippolyte Verdier : hverdier@pasteur.fr\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"\\nProcess your SPT-PALM recordings with PALMari's all-inclusive pipeline: spot detection, sub-pixel localization, tracking & more.\\nStart quickly with default parameters or customize your pipeline, and run it on entire folder of microscope recordings.\\nGetting Help\\nEmail Hippolyte Verdier : hverdier@pasteur.fr\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"PALMari\",\"documentation\":\"https://palmari.readthedocs.io/en/latest/\",\"first_released\":\"2022-05-06T17:58:23.787615Z\",\"license\":\"\\\"CeCILL\\\"\",\"name\":\"palmari\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/hippover/palmari\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2023-01-13T15:13:29.922314Z\",\"report_issues\":\"https://github.com/hippover/palmari/issues\",\"requirements\":[\"click\",\"dask (>=2022.1.0)\",\"dask-image (>=2021.12.0)\",\"imageio-ffmpeg\",\"magicgui (>=0.5.0)\",\"matplotlib (>=3.5)\",\"munkres\",\"napari\",\"nd2reader\",\"numpy\",\"pandas\",\"pyyaml\",\"qtpy\",\"scikit-image (>=0.18.3)\",\"scikit-learn\",\"tiffile\",\"toml\",\"tqdm\",\"trackpy (>=0.5.0)\",\"tox ; extra == 'testing'\",\"PyQt5 ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\"],\"summary\":\"A pipeline for PALM movies analysis (pre-processing, localization, drifft correction, tracking)\",\"support\":\"https://github.com/hippover/palmari/issues\",\"twitter\":\"\",\"version\":\"0.2.18\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"robert.haase@tu-dresden.de\",\"name\":\"Robert Haase\"}],\"code_repository\":\"https://github.com/haesleinhuepf/napari-workflow-optimizer\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-workflow-optimizer\"}],\"description\":\"# napari-workflow-optimizer\\n\\n[![License](https://img.shields.io/pypi/l/napari-workflow-optimizer.svg?color=green)](https://github.com/haesleinhuepf/napari-workflow-optimizer/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-workflow-optimizer.svg?color=green)](https://pypi.org/project/napari-workflow-optimizer)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-workflow-optimizer.svg?color=green)](https://python.org)\\n[![tests](https://github.com/haesleinhuepf/napari-workflow-optimizer/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-workflow-optimizer/actions)\\n[![codecov](https://codecov.io/gh/haesleinhuepf/napari-workflow-optimizer/branch/main/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-workflow-optimizer)\\n[![Development Status](https://img.shields.io/pypi/status/napari-workflow-optimizer.svg)](https://en.wikipedia.org/wiki/Software_release_life_cycle#Alpha)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-workflow-optimizer)](https://napari-hub.org/plugins/napari-workflow-optimizer)\\n\\nOptimize image processing workflows in napari for segmentation quality\\n\\n![img.png](https://github.com/haesleinhuepf/napari-workflow-optimizer/raw/main/docs/napari-workflow-optimizer.gif)\\n\\n## Usage\\n\\nThe starting point for workflow optimization is a workflow and some reference (\\\"ground truth\\\") labels image. \\nThe label image can be a sparse annotation, which means only some objects and also parts of objets are annotated (see [hints](https://github.com/haesleinhuepf/napari-workflow-optimizer#optimization-hints)). \\nThese datasets should be ready. You can reproduce the following procedure by downloading an \\n[examle raw image](https://github.com/haesleinhuepf/napari-workflow-optimizer/blob/main/demo/membranes_2d.tif) (derived from the \\n[scikit-image cells3d example data set](https://scikit-image.org/docs/dev/api/skimage.data.html#skimage.data.cells3d)) and a corresponding \\n[sparse annotation label image](https://github.com/haesleinhuepf/napari-workflow-optimizer/blob/main/demo/membranes_2d_sparse_labels.tif).\\nFor reproducing the following procedure, also follow the [installation instructions](https://github.com/haesleinhuepf/napari-workflow-optimizer#optimization-hints) below.\\nThe whole procedure is [also shown in this video](https://github.com/haesleinhuepf/napari-workflow-optimizer/raw/main/docs/napari-workflow-optimizer.mp4), an extended version of the trailer above.\\n\\n### Step 0: Loading data and setting up the workflow\\n\\nLoad the \\\"membranes_2d.tif\\\" data set, e.g. by drag&drop on napari and start the Assistant from the `Tools > Utilities > Assistant (clEsperanto)` menu.\\n\\n![](https://github.com/haesleinhuepf/napari-workflow-optimizer/raw/main/docs/screenshot1_start_raw.png)\\n\\nClick the `Label` button and select as operation \\\"Seeded watershed using local minima as seeds and an intensity threshold (nsbatwm)\\\".\\n\\n![](https://github.com/haesleinhuepf/napari-workflow-optimizer/raw/main/docs/screenshot2_labeled_beginning.png)\\n\\nDraw an annotation in a new labels layer or load the example spare annotation \\\"membranes_2d_sparse_labels.tif\\\". \\n\\n![](https://github.com/haesleinhuepf/napari-workflow-optimizer/raw/main/docs/screenshot4_loaded_manual_annotation.png)\\n\\nIn case the image is not displayed as label image, convert it to a label image by right-clicking on the entry in the layers list:\\n\\n![](https://github.com/haesleinhuepf/napari-workflow-optimizer/raw/main/docs/screenshot3_load_manual_annotation.png)\\n\\n### Step 1: The Workflow Optimizer\\n\\nStart the Workflow Optimizer from the `Tools > Utilities > Workflow optimizer (Labels)` menu. \\nConfigure the target layer, showing the label image that should be optimized.\\nSelect the manual annotation as reference layer for the optimization. \\nConsider increasing the number of iterations. This number depends on your segmenation problem. \\nIn the present example, 100 iterations should be enough.\\n\\n![](https://github.com/haesleinhuepf/napari-workflow-optimizer/raw/main/docs/screenshot5_start_optimization.png)\\n\\nThe optimizer will plot quality over the number of iterations to show the progress of optimization. \\nTo determine the quality, the optimizer will measure the maximum overlap ([Jaccard index](https://en.wikipedia.org/wiki/Jaccard_index)) \\nof any labeled object over the manually annotated objects and calculate the mean of this value over all annotated objects.\\nAfter a moment, optimization will finish and update the labeled image. \\nIf your starting point for the optimization was already good, the result may now look better than before.\\n\\n![](https://github.com/haesleinhuepf/napari-workflow-optimizer/raw/main/docs/screenshot6_finished_optimization.png)\\n\\n### Step 2: Manual parameter space plotting\\n\\nIn case the result is not perfect yet (as the fringed segmentation above suggests), consider manual plotting of the \\nindividual parameters and their relation to segmentation quality to get an idea about the surrounding parameter space.\\nTherefore, click the `Plot` button next to one of the workflow parameters.\\nSelect the range in which the labeling quality should be determined (green arrows). In our example, the optimizer was setting the parameter to 2.34. \\nThus, to demonstrate the procedure we plot the parameter space beween 0 and 10. \\nThe quality plotted over this parameter obviously has a local maxium at 2.34, which was detected by the optimizer.\\nHowever, it also has another local maxium at 8 and actually a plateau in the quality plot (orange arrows).\\n\\n![](https://github.com/haesleinhuepf/napari-workflow-optimizer/raw/main/docs/screenshot7_parameter_quality_plot.png)\\n\\nFor further optimization, we re-configure the algorithm and set a new starting point for optimization of the parameter to 8.\\nAfterwards, we restart the optimization. It will then optimize the settings again from the new starting point.\\n\\n![](https://github.com/haesleinhuepf/napari-workflow-optimizer/raw/main/docs/screenshot8_start_optimization_again.png)\\n\\nAfter another moment, optimization will finish again, potentially leading to an even better result.\\n\\n![](https://github.com/haesleinhuepf/napari-workflow-optimizer/raw/main/docs/screenshot9_finished_optimization_again.png)\\n\\n### Step 3: Visualization of results\\n\\nMake sure the segmentation has high quality by inspecting the result visually. Use the `contour` setting of the labels layer\\nand hide/show the outlines of the labeled layer:\\n\\n![](https://github.com/haesleinhuepf/napari-workflow-optimizer/raw/main/docs/screenshot10_contours_on.png)\\n\\n![](https://github.com/haesleinhuepf/napari-workflow-optimizer/raw/main/docs/screenshot11_contours_off.jpg)\\n\\n### Optimization Hints\\n\\nThe Workflow Optimizer uses the [Nelder-Mead simplex method](https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method)\\nfor optimizing parameters. This algorithm varies individual parameters and makes steps in the parameter space ideally following a gradient \\nto a local optimum. Hence, this algorithm may not be capable of determining a global optimum in parameter space. \\nParameter optimization is no magic. If it does not immediately work on your data, plot the parameters as introduced in Step 2 \\nand identify parameters with a clear gradient and those with many local maxima. \\nConsider optimizing the parameters with many local maxima manually and de-selecting their checkboxes for the optimization.\\nThe optimizer will then only optimize the parameters showing the clear gradient. \\nRepeat these steps a couple of times to get a feeling for your parameter space. \\n\\nFurthermore, parameter optimization works well if\\n* the initial settings are close to a good segmentation,\\n* a small number of parameters (a short workflow) are optimized,\\n* the reference annotation is prepared carefully and\\n* the dataset is small. Consider using a small representative crop in case of bigger datasets.\\n\\n### Workflow optimization scripting\\n\\nFor optimizing workflows from within a jupyter notebook, check out our [example notebook for optimization using spare labels](https://github.com/haesleinhuepf/napari-workflow-optimizer/blob/main/demo/sparse_label_image_optimizer.ipynb). \\nThe examples are more flexible than the graphical user interface and allow for example [optimizing intensity images](https://github.com/haesleinhuepf/napari-workflow-optimizer/blob/main/demo/intensity_image_optimizer.ipynb)\\nand [binary images](https://github.com/haesleinhuepf/napari-workflow-optimizer/blob/main/demo/binary_image_optimizer.ipynb).\\nThe membrane segmentation workflow optimization similar to the one shown above is also available as [jupyter notebook](https://github.com/haesleinhuepf/napari-workflow-optimizer/blob/main/demo/membrane_segmentation.ipynb).\\n\\n### Known issues\\n\\nIf you change the workflow architecture after the optimizer window was opened, please re-open it\\nto select the parameters that should be optimized. Changing parameters is ok and re-opening is not necessary.\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n## Installation\\n\\nFurthermore, to reproduce the procedure above, please download and install \\n[napari](https://napari.org/),\\n[pyopencl](https://documen.tician.de/pyopencl/),\\nthe [napari-pyclesperanto-assistant](https://www.napari-hub.org/plugins/napari-pyclesperanto-assistant) and\\nthe [napari-segment-blobs-and-things-with-membranes](https://www.napari-hub.org/plugins/napari-segment-blobs-and-things-with-membranes) plugin. E.g. using \\n[conda](https://docs.conda.io/en/latest/) and [pip](https://pypi.org/project/pip/):\\n\\n```\\nconda create --name napari-opti python=3.8\\nconda activate napari-opti\\n\\nconda install pyopencl napari\\npip install napari-pyclesperanto-assistant napari-segment-blobs-and-things-with-membranes\\npip install napari-workflow-optimizer\\n```\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-workflow-optimizer\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/haesleinhuepf/napari-workflow-optimizer/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-workflow-optimizer\\n\\n\\n\\n\\n\\n\\n\\nOptimize image processing workflows in napari for segmentation quality\\n\\nUsage\\nThe starting point for workflow optimization is a workflow and some reference (\\\"ground truth\\\") labels image. \\nThe label image can be a sparse annotation, which means only some objects and also parts of objets are annotated (see hints). \\nThese datasets should be ready. You can reproduce the following procedure by downloading an \\nexamle raw image (derived from the \\nscikit-image cells3d example data set) and a corresponding \\nsparse annotation label image.\\nFor reproducing the following procedure, also follow the installation instructions below.\\nThe whole procedure is also shown in this video, an extended version of the trailer above.\\nStep 0: Loading data and setting up the workflow\\nLoad the \\\"membranes_2d.tif\\\" data set, e.g. by drag&drop on napari and start the Assistant from the Tools > Utilities > Assistant (clEsperanto) menu.\\n\\nClick the Label button and select as operation \\\"Seeded watershed using local minima as seeds and an intensity threshold (nsbatwm)\\\".\\n\\nDraw an annotation in a new labels layer or load the example spare annotation \\\"membranes_2d_sparse_labels.tif\\\". \\n\\nIn case the image is not displayed as label image, convert it to a label image by right-clicking on the entry in the layers list:\\n\\nStep 1: The Workflow Optimizer\\nStart the Workflow Optimizer from the Tools > Utilities > Workflow optimizer (Labels) menu. \\nConfigure the target layer, showing the label image that should be optimized.\\nSelect the manual annotation as reference layer for the optimization. \\nConsider increasing the number of iterations. This number depends on your segmenation problem. \\nIn the present example, 100 iterations should be enough.\\n\\nThe optimizer will plot quality over the number of iterations to show the progress of optimization. \\nTo determine the quality, the optimizer will measure the maximum overlap (Jaccard index) \\nof any labeled object over the manually annotated objects and calculate the mean of this value over all annotated objects.\\nAfter a moment, optimization will finish and update the labeled image. \\nIf your starting point for the optimization was already good, the result may now look better than before.\\n\\nStep 2: Manual parameter space plotting\\nIn case the result is not perfect yet (as the fringed segmentation above suggests), consider manual plotting of the \\nindividual parameters and their relation to segmentation quality to get an idea about the surrounding parameter space.\\nTherefore, click the Plot button next to one of the workflow parameters.\\nSelect the range in which the labeling quality should be determined (green arrows). In our example, the optimizer was setting the parameter to 2.34. \\nThus, to demonstrate the procedure we plot the parameter space beween 0 and 10. \\nThe quality plotted over this parameter obviously has a local maxium at 2.34, which was detected by the optimizer.\\nHowever, it also has another local maxium at 8 and actually a plateau in the quality plot (orange arrows).\\n\\nFor further optimization, we re-configure the algorithm and set a new starting point for optimization of the parameter to 8.\\nAfterwards, we restart the optimization. It will then optimize the settings again from the new starting point.\\n\\nAfter another moment, optimization will finish again, potentially leading to an even better result.\\n\\nStep 3: Visualization of results\\nMake sure the segmentation has high quality by inspecting the result visually. Use the contour setting of the labels layer\\nand hide/show the outlines of the labeled layer:\\n\\n\\nOptimization Hints\\nThe Workflow Optimizer uses the Nelder-Mead simplex method\\nfor optimizing parameters. This algorithm varies individual parameters and makes steps in the parameter space ideally following a gradient \\nto a local optimum. Hence, this algorithm may not be capable of determining a global optimum in parameter space. \\nParameter optimization is no magic. If it does not immediately work on your data, plot the parameters as introduced in Step 2 \\nand identify parameters with a clear gradient and those with many local maxima. \\nConsider optimizing the parameters with many local maxima manually and de-selecting their checkboxes for the optimization.\\nThe optimizer will then only optimize the parameters showing the clear gradient. \\nRepeat these steps a couple of times to get a feeling for your parameter space. \\nFurthermore, parameter optimization works well if\\n* the initial settings are close to a good segmentation,\\n* a small number of parameters (a short workflow) are optimized,\\n* the reference annotation is prepared carefully and\\n* the dataset is small. Consider using a small representative crop in case of bigger datasets.\\nWorkflow optimization scripting\\nFor optimizing workflows from within a jupyter notebook, check out our example notebook for optimization using spare labels. \\nThe examples are more flexible than the graphical user interface and allow for example optimizing intensity images\\nand binary images.\\nThe membrane segmentation workflow optimization similar to the one shown above is also available as jupyter notebook.\\nKnown issues\\nIf you change the workflow architecture after the optimizer window was opened, please re-open it\\nto select the parameters that should be optimized. Changing parameters is ok and re-opening is not necessary.\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\nInstallation\\nFurthermore, to reproduce the procedure above, please download and install \\nnapari,\\npyopencl,\\nthe napari-pyclesperanto-assistant and\\nthe napari-segment-blobs-and-things-with-membranes plugin. E.g. using \\nconda and pip:\\n```\\nconda create --name napari-opti python=3.8\\nconda activate napari-opti\\nconda install pyopencl napari\\npip install napari-pyclesperanto-assistant napari-segment-blobs-and-things-with-membranes\\npip install napari-workflow-optimizer\\n```\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-workflow-optimizer\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 3 - Alpha\"],\"display_name\":\"napari-workflow-optimizer\",\"documentation\":\"https://github.com/haesleinhuepf/napari-workflow-optimizer#README.md\",\"first_released\":\"2021-12-24T17:11:46.811500Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-workflow-optimizer\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/haesleinhuepf/napari-workflow-optimizer\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-04-15T16:35:08.144452Z\",\"report_issues\":\"https://github.com/haesleinhuepf/napari-workflow-optimizer/issues\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"pyclesperanto-prototype\",\"scikit-learn\",\"napari-time-slicer\",\"matplotlib\",\"scipy\",\"napari-workflows\",\"napari-assistant (>=0.1.9)\"],\"summary\":\"Optimize image processing workflows in napari for segmentation quality\",\"support\":\"https://github.com/haesleinhuepf/napari-workflow-optimizer/issues\",\"twitter\":\"\",\"version\":\"0.1.4\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"\",\"name\":\"Tee Li\"}],\"code_repository\":\"https://github.com/teeli8/skeleton-finder\",\"conda\":[],\"description\":\"# Skeleton_Finder\\n\\n## Description\\nA Napari plugin for skeleton computation on 2D gray-scale images. Currently support:\\n\\n1. Skeleton finding and drawing\\n2. A customizable segmentation parameter for thresholding the area of interest\\n3. A customizable pruning parameter to keep a part of the skeleton\\n4. Skeleton colored by distance to the boundary\\n5. Only support single png or jpg images\\n\\n![example](../imgs/horse.png)\\n\\n\\n## Usage\\nInstall with\\n\\n```\\npip install skeleton-finder\\n```\\n\\nOpen the plugin widget and select an image layer.\\n\\nAdjust parameters and find skeleton.\\n\\nRe-adjusting parameters after computing a skeleton is also supported.\\n\\nBe sure to use the Reset button after finishing the computation for an image\\n\\n## License\\nThe plugin is distributed under the terms of [BSD-3](https://opensource.org/licenses/BSD-3-Clause) license.\\n\\n## Issues\\nIf you encounter any issues, please file an issue along with a detailed description\\n\",\"description_content_type\":\"\",\"description_text\":\"Skeleton_Finder\\nDescription\\nA Napari plugin for skeleton computation on 2D gray-scale images. Currently support:\\n\\nSkeleton finding and drawing\\nA customizable segmentation parameter for thresholding the area of interest\\nA customizable pruning parameter to keep a part of the skeleton\\nSkeleton colored by distance to the boundary\\nOnly support single png or jpg images\\n\\n\\nUsage\\nInstall with\\npip install skeleton-finder\\nOpen the plugin widget and select an image layer.\\nAdjust parameters and find skeleton.\\nRe-adjusting parameters after computing a skeleton is also supported.\\nBe sure to use the Reset button after finishing the computation for an image\\nLicense\\nThe plugin is distributed under the terms of BSD-3 license.\\nIssues\\nIf you encounter any issues, please file an issue along with a detailed description\",\"development_status\":[],\"display_name\":\"\",\"documentation\":\"\",\"first_released\":\"2022-05-25T18:47:44.045175Z\",\"license\":\"BSD-3-Clause\",\"name\":\"skeleton-finder\",\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[],\"project_site\":\"https://github.com/teeli8/skeleton-finder\",\"python_version\":\"\",\"reader_file_extensions\":[],\"release_date\":\"2022-05-25T20:11:26.199968Z\",\"report_issues\":\"\",\"requirements\":[\"napari\",\"plyfile\"],\"summary\":\"\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.0.2\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Austin E. Y. T. Lefebvre\"}],\"code_repository\":\"https://github.com/aelefebv/snouty-viewer\",\"conda\":[],\"description\":\"\\n\\n<!-- This file is designed to provide you with a starting template for documenting\\nthe functionality of your plugin. Its content will be rendered on your plugin's\\nnapari hub page.\\n\\nThe sections below are given as a guide for the flow of information only, and\\nare in no way prescriptive. You should feel free to merge, remove, add and\\nrename sections at will to make this document work best for your plugin.\\n\\n## Description\\n\\nThis should be a detailed description of the context of your plugin and its\\nintended purpose.\\n\\nIf you have videos or screenshots of your plugin in action, you should include them\\nhere as well, to make them front and center for new users.\\n\\nYou should use absolute links to these assets, so that we can easily display them\\non the hub. The easiest way to include a video is to use a GIF, for example hosted\\non imgur. You can then reference this GIF as an image.\\n\\n![Example GIF hosted on Imgur](https://i.imgur.com/A5phCX4.gif)\\n\\nNote that GIFs larger than 5MB won't be rendered by GitHub - we will however,\\nrender them on the napari hub.\\n\\nThe other alternative, if you prefer to keep a video, is to use GitHub's video\\nembedding feature.\\n\\n1. Push your `DESCRIPTION.md` to GitHub on your repository (this can also be done\\nas part of a Pull Request)\\n2. Edit `.napari/DESCRIPTION.md` **on GitH ub**.\\n3. Drag and drop your video into its desired location. It will be uploaded and\\nhosted on GitHub for you, but will not be placed in your repository.\\n4. We will take the resolved link to the video and render it on the hub.\\n\\nHere is an example of an mp4 video embedded this way.\\n\\nhttps://user-images.githubusercontent.com/17995243/120088305-6c093380-c132-11eb-822d-620e81eb5f0e.mp4\\n\\n## Intended Audience & Supported Data\\n\\nThis section should describe the target audience for this plugin (any knowledge,\\nskills and experience required), as well as a description of the types of data\\nsupported by this plugin.\\n\\nTry to make the data description as explicit as possible, so that users know the\\nformat your plugin expects. This applies both to reader plugins reading file formats\\nand to function/dock widget plugins accepting layers and/or layer data.\\nFor example, if you know your plugin only works with 3D integer data in \\\"tyx\\\" order,\\nmake sure to mention this.\\n\\nIf you know of researchers, groups or labs using your plugin, or if it has been cited\\nanywhere, feel free to also include this information here.\\n\\n## Quickstart\\n\\nThis section should go through step-by-step examples of how your plugin should be used.\\nWhere your plugin provides multiple dock widgets or functions, you should split these\\nout into separate subsections for easy browsing. Include screenshots and videos\\nwherever possible to elucidate your descriptions.\\n\\nIdeally, this section should start with minimal examples for those who just want a\\nquick overview of the plugin's functionality, but you should definitely link out to\\nmore complex and in-depth tutorials highlighting any intricacies of your plugin, and\\nmore detailed documentation if you have it.\\n\\n## Additional Install Steps (uncommon)\\nWe will be providing installation instructions on the hub, which will be sufficient\\nfor the majority of plugins. They will include instructions to pip install, and\\nto install via napari itself.\\n\\nMost plugins can be installed out-of-the-box by just specifying the package requirements\\nover in `setup.cfg`. However, if your plugin has any more complex dependencies, or\\nrequires any additional preparation before (or after) installation, you should add\\nthis information here.\\n\\n## Getting Help\\n\\nThis section should point users to your preferred support tools, whether this be raising\\nan issue on GitHub, asking a question on image.sc, or using some other method of contact.\\nIf you distinguish between usage support and bug/feature support, you should state that\\nhere.\\n\\n## How to Cite\\n\\nMany plugins may be used in the course of published (or publishable) research, as well as\\nduring conference talks and other public facing events. If you'd like to be cited in\\na particular format, or have a DOI you'd like used, you should provide that information here. -->\\n\\n## Description\\nEasy to use plugin for opening raw Snouty files and converting them to native view.\\n\\n![Example](https://i.imgur.com/VirE5DM.gif)\\n\\n## Intended Audience & Supported Data\\nThis plugin is intended for those using a SOLS (Snouty) microscope collected via\\n[Alfred Millett-Sikking's code](https://github.com/amsikking/SOLS_microscope).\\n\\nThis plugin accepts a folder with at least subdirectories of data and metadata as an input.\\nThe metadata must have a 000000.txt file for the metadata to be properly parsed.\\n\\n## Quickstart\\n\\n### Getting the plugin working\\n1. pip install snouty-viewer (within a virtual environment of Python 3.8, 3.9, or 3.10 recommended)\\n2. Open up napari\\n\\n### Viewing raw Snouty data\\n- Drag and drop a root folder of your Snouty data. This is the folder that includes the data and metadata subfolders.\\n- Select \\\"Snouty Viewer\\\" for opening.\\n\\n### Converting raw Snouty data to its native view\\n1. Click plugins, snouty-viewer: Native View\\n2. Select the file you want to convert\\n3. Press Run\\n\\n### Saving your native view file\\n1. Select the file you want to save\\n2. File > Save Selected Layer(s)...\\n3. Select where you want to save your file\\n4. Write your file name (recommended to end in .tif)\\n5. Save\\n6. Wait (this could take a few minutes depending on your file's size)\\n\\n## Getting Help\\n- Open up an issue on [GitHub](https://github.com/aelefebv/snouty-viewer/issues).\\n- Start a thread on [image.sc](https://forum.image.sc/)\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"\\nDescription\\nEasy to use plugin for opening raw Snouty files and converting them to native view.\\n\\nIntended Audience & Supported Data\\nThis plugin is intended for those using a SOLS (Snouty) microscope collected via\\nAlfred Millett-Sikking's code.\\nThis plugin accepts a folder with at least subdirectories of data and metadata as an input.\\nThe metadata must have a 000000.txt file for the metadata to be properly parsed.\\nQuickstart\\nGetting the plugin working\\n\\npip install snouty-viewer (within a virtual environment of Python 3.8, 3.9, or 3.10 recommended)\\nOpen up napari\\n\\nViewing raw Snouty data\\n\\nDrag and drop a root folder of your Snouty data. This is the folder that includes the data and metadata subfolders.\\nSelect \\\"Snouty Viewer\\\" for opening.\\n\\nConverting raw Snouty data to its native view\\n\\nClick plugins, snouty-viewer: Native View\\nSelect the file you want to convert\\nPress Run\\n\\nSaving your native view file\\n\\nSelect the file you want to save\\nFile > Save Selected Layer(s)...\\nSelect where you want to save your file\\nWrite your file name (recommended to end in .tif)\\nSave\\nWait (this could take a few minutes depending on your file's size)\\n\\nGetting Help\\n\\nOpen up an issue on GitHub.\\nStart a thread on image.sc\\n\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"Snouty Viewer\",\"documentation\":\"https://github.com/aelefebv/snouty-viewer#README.md\",\"first_released\":\"2022-08-31T23:36:04.300528Z\",\"license\":\"MIT\",\"name\":\"snouty-viewer\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\",\"widget\"],\"project_site\":\"https://github.com/aelefebv/snouty-viewer\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-09-08T22:33:29.469083Z\",\"report_issues\":\"https://github.com/aelefebv/snouty-viewer/issues\",\"requirements\":[\"magicgui\",\"napari\",\"numpy\",\"tifffile\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"tox ; extra == 'testing'\"],\"summary\":\"A plugin to visualize and convert Snouty data.\",\"support\":\"https://github.com/aelefebv/snouty-viewer/issues\",\"twitter\":\"\",\"version\":\"0.0.4\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Robert Haase\"}],\"code_repository\":\"https://github.com/haesleinhuepf/napari-plugin-search\",\"description\":\"# napari-plugin-search\\r\\n\\r\\n[![License](https://img.shields.io/pypi/l/napari-plugin-search.svg?color=green)](https://github.com/haesleinhuepf/napari-plugin-search/raw/master/LICENSE)\\r\\n[![PyPI](https://img.shields.io/pypi/v/napari-plugin-search.svg?color=green)](https://pypi.org/project/napari-plugin-search)\\r\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-plugin-search.svg?color=green)](https://python.org)\\r\\n[![tests](https://github.com/haesleinhuepf/napari-plugin-search/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-plugin-search/actions)\\r\\n[![codecov](https://codecov.io/gh/haesleinhuepf/napari-plugin-search/branch/master/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-plugin-search)\\r\\n\\r\\nFind napari plugins\\r\\n![img.png](https://github.com/haesleinhuepf/napari-plugin-search/raw/main/docs/napari-plugin-search-screencast.gif)\\r\\n\\r\\n## Usage\\r\\nEnter the name of the plugin you are searching for and use the `up` and `down` arrow keys to navigate between them. \\r\\nHit `Enter` to start a plugin.\\r\\n\\r\\n----------------------------------\\r\\n\\r\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\\r\\n\\r\\n## Installation\\r\\n\\r\\nYou can install `napari-plugin-search` via [pip]:\\r\\n\\r\\n    pip install napari-plugin-search\\r\\n\\r\\n## Contributing\\r\\n\\r\\nContributions are very welcome. Tests can be run with [tox], please ensure\\r\\nthe coverage at least stays the same before you submit a pull request.\\r\\n\\r\\n## License\\r\\n\\r\\nDistributed under the terms of the [BSD-3] license,\\r\\n\\\"napari-plugin-search\\\" is free and open source software\\r\\n\\r\\n## Issues\\r\\n\\r\\nIf you encounter any problems, please create a thread on [image.sc] along with a detailed description and tag [@haesleinhuepf].\\r\\n\\r\\n[napari]: https://github.com/napari/napari\\r\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\r\\n[@napari]: https://github.com/napari\\r\\n[MIT]: http://opensource.org/licenses/MIT\\r\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\r\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\r\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\r\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\r\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\r\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\r\\n\\r\\n[file an issue]: https://github.com/haesleinhuepf/napari-plugin-search/issues\\r\\n\\r\\n[napari]: https://github.com/napari/napari\\r\\n[tox]: https://tox.readthedocs.io/en/latest/\\r\\n[pip]: https://pypi.org/project/pip/\\r\\n[PyPI]: https://pypi.org/\\r\\n[image.sc]: https://image.sc\\r\\n[@haesleinhuepf]: https://twitter.com/haesleinhuepf\\r\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-plugin-search\\n\\n\\n\\n\\n\\nFind napari plugins\\n\\nUsage\\nEnter the name of the plugin you are searching for and use the up and down arrow keys to navigate between them. \\nHit Enter to start a plugin.\\n\\nThis napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.\\nInstallation\\nYou can install napari-plugin-search via pip:\\npip install napari-plugin-search\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-plugin-search\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please create a thread on image.sc along with a detailed description and tag @haesleinhuepf.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari-plugin-search\",\"documentation\":\"https://github.com/haesleinhuepf/napari-plugin-search#README.md\",\"first_released\":\"2021-08-21T17:18:51.077813Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-plugin-search\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/haesleinhuepf/napari-plugin-search\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2023-01-04T13:55:45.617813Z\",\"report_issues\":\"https://github.com/haesleinhuepf/napari-plugin-search/issues\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"napari-tools-menu\"],\"summary\":\"Find napari plugins\",\"support\":\"https://github.com/haesleinhuepf/napari-plugin-search/issues\",\"twitter\":\"\",\"version\":\"0.1.4\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Jonas Windhager\"}],\"code_repository\":\"https://github.com/BodenmillerGroup/napari-roi\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-roi\"}],\"description\":\"# napari-roi\\n\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-roi)](https://napari-hub.org/plugins/napari-roi)\\n[![PyPI](https://img.shields.io/pypi/v/napari-roi.svg?color=green)](https://pypi.org/project/napari-roi)\\n[![License](https://img.shields.io/pypi/l/napari-roi.svg?color=green)](https://github.com/BodenmillerGroup/napari-roi/raw/main/LICENSE)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-roi.svg?color=green)](https://python.org)\\n[![Issues](https://img.shields.io/github/issues/BodenmillerGroup/napari-roi)](https://github.com/BodenmillerGroup/napari-roi/issues)\\n[![Pull requests](https://img.shields.io/github/issues-pr/BodenmillerGroup/napari-roi)](https://github.com/BodenmillerGroup/napari-roi/pulls)\\n\\nSelect regions of interest (ROIs) using napari\\n\\n## Installation\\n\\nYou can install napari-roi via [pip](https://pypi.org/project/pip/):\\n\\n    pip install napari-roi\\n\\nAlternatively, you can install napari-roi via [conda](https://conda.io/):\\n\\n    conda install -c conda-forge napari-roi\\n\\n## Usage\\n\\nThe *napari-roi* plugin can be opened from within napari (`napari -> napari-roi: regions of interest`) and operates on napari *Shapes* layers.\\n\\nROIs can be added to any napari *Shapes* layer, either by drawing a standard napari shape (e.g. rectangle), or by adding a rectangular ROI of specified size using the `Add ROI` functionality in the *napari-roi* widget. Each ROI is associated with a name, a position (X/Y origin), and a size (width/height). The location of the X/Y origin of all ROIs can be chosen in the *napari-roi* widget. Note that any shape supported by napari (e.g. ellipse, rectangle, polygon, line, path) can serve as an ROI; for non-rectangular shapes, *napari-roi* computes rectangular bounding boxes aligned with the napari coordinate system to determine their positions and sizes. ROIs can be edited or deleted by modifying the corresponding shapes in napari, or by editing the corresponding row in the *napari-roi* widget.\\n\\nAll ROIs in the current *Shapes* layer can be saved to a comma-separated values (CSV) file using the `Save` functionality in the *napari-roi* widget. When the `Autosave` option is checked, the file is automatically updated on every ROI change. Note that the selected file is specific to the current *Shapes* layer; ROIs from different *Shapes* layers cannot be saved to the same file. ROIs can be loaded from a previously saved file and added to the current *Shapes* layer by opening the file in the *napari-roi* widget.\\n\\nCSV files saved using *napari-roi* adhere to the following format:\\n\\n| Columns | Description |\\n| --- | --- |\\n| `Name` | ROI name |\\n| `X`, `Y` | Position (X/Y origin) |\\n| `W`, `H` | Size (width/height) |\\n\\n## Authors\\n\\nCreated and maintained by Jonas Windhager [jonas.windhager@uzh.ch](mailto:jonas.windhager@uzh.ch)\\n\\n## Contributing\\n\\n[Contributing](https://github.com/BodenmillerGroup/napari-roi/blob/main/CONTRIBUTING.md)\\n\\n## Changelog\\n\\n[Changelog](https://github.com/BodenmillerGroup/napari-roi/blob/main/CHANGELOG.md)\\n\\n## License\\n\\n[MIT](https://github.com/BodenmillerGroup/napari-roi/blob/main/LICENSE)\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-roi\\n\\n\\n\\n\\n\\n\\nSelect regions of interest (ROIs) using napari\\nInstallation\\nYou can install napari-roi via pip:\\npip install napari-roi\\n\\nAlternatively, you can install napari-roi via conda:\\nconda install -c conda-forge napari-roi\\n\\nUsage\\nThe napari-roi plugin can be opened from within napari (napari -> napari-roi: regions of interest) and operates on napari Shapes layers.\\nROIs can be added to any napari Shapes layer, either by drawing a standard napari shape (e.g. rectangle), or by adding a rectangular ROI of specified size using the Add ROI functionality in the napari-roi widget. Each ROI is associated with a name, a position (X/Y origin), and a size (width/height). The location of the X/Y origin of all ROIs can be chosen in the napari-roi widget. Note that any shape supported by napari (e.g. ellipse, rectangle, polygon, line, path) can serve as an ROI; for non-rectangular shapes, napari-roi computes rectangular bounding boxes aligned with the napari coordinate system to determine their positions and sizes. ROIs can be edited or deleted by modifying the corresponding shapes in napari, or by editing the corresponding row in the napari-roi widget.\\nAll ROIs in the current Shapes layer can be saved to a comma-separated values (CSV) file using the Save functionality in the napari-roi widget. When the Autosave option is checked, the file is automatically updated on every ROI change. Note that the selected file is specific to the current Shapes layer; ROIs from different Shapes layers cannot be saved to the same file. ROIs can be loaded from a previously saved file and added to the current Shapes layer by opening the file in the napari-roi widget.\\nCSV files saved using napari-roi adhere to the following format:\\n| Columns | Description |\\n| --- | --- |\\n| Name | ROI name |\\n| X, Y | Position (X/Y origin) |\\n| W, H | Size (width/height) |\\nAuthors\\nCreated and maintained by Jonas Windhager jonas.windhager@uzh.ch\\nContributing\\nContributing\\nChangelog\\nChangelog\\nLicense\\nMIT\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"napari-roi\",\"documentation\":\"https://github.com/BodenmillerGroup/napari-roi#README.md\",\"first_released\":\"2021-11-19T19:28:29.299053Z\",\"license\":\"MIT\",\"name\":\"napari-roi\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/BodenmillerGroup/napari-roi\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-08-10T14:46:04.023324Z\",\"report_issues\":\"https://github.com/BodenmillerGroup/napari-roi/issues\",\"requirements\":[\"numpy\",\"pandas\",\"qtpy\"],\"summary\":\"Select regions of interest (ROIs) using napari\",\"support\":\"https://github.com/BodenmillerGroup/napari-roi/issues\",\"twitter\":\"\",\"version\":\"0.1.7\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Jacopo Abramo\"}],\"code_repository\":\"https://github.com/jacopoabramo/napari-trait2d\",\"conda\":[],\"description\":\"# napari-trait2d\\n\\n[![License BSD-3](https://img.shields.io/pypi/l/napari-trait2d.svg?color=green)](https://github.com/jacopoabramo/napari-trait2d/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-trait2d.svg?color=green)](https://pypi.org/project/napari-trait2d)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-trait2d.svg?color=green)](https://python.org)\\n[![tests](https://github.com/jacopoabramo/napari-trait2d/workflows/tests/badge.svg)](https://github.com/jacopoabramo/napari-trait2d/actions)\\n[![codecov](https://codecov.io/gh/jacopoabramo/napari-trait2d/branch/main/graph/badge.svg)](https://codecov.io/gh/jacopoabramo/napari-trait2d)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-trait2d)](https://napari-hub.org/plugins/napari-trait2d)\\n\\nA napari plugin for TRAIT2D, a software for quantitative analysis of single particle diffusion data\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-trait2d` via [pip]:\\n\\n    pip install napari-trait2d\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/jacopoabramo/napari-trait2d.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-trait2d\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/jacopoabramo/napari-trait2d/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-trait2d\\n\\n\\n\\n\\n\\n\\nA napari plugin for TRAIT2D, a software for quantitative analysis of single particle diffusion data\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-trait2d via pip:\\npip install napari-trait2d\\n\\nTo install latest development version :\\npip install git+https://github.com/jacopoabramo/napari-trait2d.git\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-trait2d\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari TRAIT2D\",\"documentation\":\"https://github.com/jacopoabramo/napari-trait2d#README.md\",\"first_released\":\"2022-07-09T19:06:50.319717Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-trait2d\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/jacopoabramo/napari-trait2d\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-07-21T16:41:08.060506Z\",\"report_issues\":\"https://github.com/jacopoabramo/napari-trait2d/issues\",\"requirements\":[\"numpy\",\"qtpy\",\"napari[pyqt5]\",\"dacite\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\"],\"summary\":\"A napari plugin for TRAIT2D, a software for quantitative analysis of single particle diffusion data\",\"support\":\"https://github.com/jacopoabramo/napari-trait2d/issues\",\"twitter\":\"\",\"version\":\"0.1.4\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Jan Clemens\"}],\"category\":{\"Supported data\":[\"Time series\"]},\"category_hierarchy\":{\"Supported data\":[[\"Time series\"]]},\"code_repository\":\"https://github.com/janclemenslab/napari-video\",\"conda\":[],\"description\":\"# napari-video\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari_video)](https://napari-hub.org/plugins/napari_video)\\n\\nNapari plugin for working with videos.\\n\\nRelies on [pyvideoreader](https://pypi.org/project/pyvideoreader/) as a backend which itself uses [opencv](https://opencv.org) for reading videos.\\n\\n## Installation\\n```shell\\npip install napari[all] napari_video\\n```\\n\\n## Usage\\nFrom a terminal:\\n```shell\\nnapari video.avi\\n```\\n\\nOr from within python:\\n```shell\\nimport napari\\nfrom napari_video.napari_video import VideoReaderNP\\n\\npath='video.mp4'\\nvr = VideoReaderNP(path)\\nwith napari.gui_qt():\\n    viewer = napari.view_image(vr, name=path)\\n```\\n\\n## Internals\\n`napari_video.napari_video.VideoReaderNP` exposes a video with a numpy-like interface, using opencv as a backend.\\n\\nFor instance, open a video:\\n```python\\nvr = VideoReaderNP('video.avi')\\nprint(vr)\\n```\\n```\\nvideo.avi with 60932 frames of size (920, 912, 3) at 100.00 fps\\n```\\nThen\\n\\n- `vr[100]` will return the 100th frame as a numpy array with shape `(902, 912, 3)`.\\n- `vr[100:200:10]` will return 10 frames evenly spaced between frame number 100 and 200 (shape `(10, 902, 912, 3)`).\\n- Note that by default, single-frame and slice indexing return 3D and 4D arrays, respectively. To consistently return 4D arrays, open the video with `remove_leading_singleton=False`. `vr[100]` will then return a `(1, 902, 912, 3)` array.\\n- We can also request specific ROIs and channels. For instance, `vr[100:200:10,100:400,800:850,1]` will return an array with shape `(10, 300, 50, 1)`.\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-video\\n\\nNapari plugin for working with videos.\\nRelies on pyvideoreader as a backend which itself uses opencv for reading videos.\\nInstallation\\nshell\\npip install napari[all] napari_video\\nUsage\\nFrom a terminal:\\nshell\\nnapari video.avi\\nOr from within python:\\n```shell\\nimport napari\\nfrom napari_video.napari_video import VideoReaderNP\\npath='video.mp4'\\nvr = VideoReaderNP(path)\\nwith napari.gui_qt():\\n    viewer = napari.view_image(vr, name=path)\\n```\\nInternals\\nnapari_video.napari_video.VideoReaderNP exposes a video with a numpy-like interface, using opencv as a backend.\\nFor instance, open a video:\\npython\\nvr = VideoReaderNP('video.avi')\\nprint(vr)\\nvideo.avi with 60932 frames of size (920, 912, 3) at 100.00 fps\\nThen\\n\\nvr[100] will return the 100th frame as a numpy array with shape (902, 912, 3).\\nvr[100:200:10] will return 10 frames evenly spaced between frame number 100 and 200 (shape (10, 902, 912, 3)).\\nNote that by default, single-frame and slice indexing return 3D and 4D arrays, respectively. To consistently return 4D arrays, open the video with remove_leading_singleton=False. vr[100] will then return a (1, 902, 912, 3) array.\\nWe can also request specific ROIs and channels. For instance, vr[100:200:10,100:400,800:850,1] will return an array with shape (10, 300, 50, 1).\\n\",\"development_status\":[],\"display_name\":\"napari_video\",\"documentation\":\"https://github.com/janclemenslab/napari-video/blob/main/README.md\",\"first_released\":\"2021-02-27T15:40:14.736684Z\",\"license\":\"MIT\",\"name\":\"napari_video\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\"],\"project_site\":\"https://github.com/janclemenslab/napari-video\",\"python_version\":\">=3.6\",\"reader_file_extensions\":[\"*\"],\"release_date\":\"2022-08-23T12:13:46.443797Z\",\"report_issues\":\"https://github.com/janclemenslab/napari-video/issues\",\"requirements\":[\"numpy\",\"pyvideoreader\"],\"summary\":\"napari plugin for reading videos.\",\"support\":\"https://github.com/janclemenslab/napari-video/issues\",\"twitter\":\"\",\"version\":\"0.2.9\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Guillaume Witz\"}],\"code_repository\":\"https://github.com/guiwitz/napari-serialcellpose\",\"conda\":[],\"description\":\"# napari-serialcellpose\\n\\n[![License](https://img.shields.io/pypi/l/napari-serialcellpose.svg?color=green)](https://github.com/guiwitz/napari-serialcellpose/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-serialcellpose.svg?color=green)](https://pypi.org/project/napari-serialcellpose)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-serialcellpose.svg?color=green)](https://python.org)\\n[![tests](https://github.com/guiwitz/napari-serialcellpose/workflows/tests/badge.svg)](https://github.com/guiwitz/napari-serialcellpose/actions)\\n[![codecov](https://codecov.io/gh/guiwitz/napari-serialcellpose/branch/main/graph/badge.svg)](https://codecov.io/gh/guiwitz/napari-serialcellpose)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-serialcellpose)](https://napari-hub.org/plugins/napari-serialcellpose)\\n\\nThis napari plugin allows you to segment single images or series of images using built-in or custom Cellpose models as well as to analyze the properties of these segmented regions (\\\"region properties\\\"). Properties can be visualized for a single image or a complete experiment in the form of histograms that can also be filtered (e.g. based on area size, mean intensity etc.) Thanks to the [napari-skimage-regionprops](https://github.com/haesleinhuepf/napari-skimage-regionprops) plugin, properties of segmented objects can be interactively explored at a single object level.\\n\\n## Main goal\\n\\nThe main goal of this plugin is to simplify the classical image processing pipeline of image segmentation followed by region analysis via Cellpose. It allows to quickly get a quantification of a set of images without the need for any scripting.\\n\\n## Installation\\n\\nIn order to use this plugin, whe highly recommend to create a specific environment and to install the required software in it. You can create a conda environment using:\\n\\n    conda create -n serialcellpose python=3.8.5 napari -c conda-forge\\n\\nThen activate it and install the plugin:\\n    \\n    conda activate serialcellpose\\n    pip install napari-serialcellpose\\n\\n### Potential issue with PyTorch\\n\\nCellpose and therefore the plugin and napari can crash without warning in some cases with ```torch==1.12.0```. This can be fixed by reverting to an earlier version using:\\n    \\n    pip install torch==1.11.0\\n\\n### GPU\\n\\nIn order to use a GPU:\\n\\n1. Uninstall the PyTorch version that gets installed by default with Cellpose:\\n\\n        pip uninstall torch\\n\\n2. Make sure your have up-to-date drivers for your NVIDIA card installed.\\n\\n3. Re-install a GPU version of PyTorch via conda using a command that you can find [here](https://pytorch.org/get-started/locally/) (this takes care of the cuda toolkit, cudnn etc. so **no need to install manually anything more than the driver**). The command will look like this:\\n\\n        conda install pytorch torchvision cudatoolkit=11.3 -c pytorch\\n\\n### Plugin Updates\\n\\nTo update the plugin, you only need to activate the existing environment and install the new version:\\n\\n    conda activate serialcellpose\\n    pip install napari-serialcellpose -U\\n\\n## Usage: segmentation\\n\\nThe main interface is shown below. The sequence of events should be the following:\\n\\n1. Select a folder containing images. The list of files within that folder will appear in the area above. You can also just drag and drop a folder or an image in that area. When selecting an image, it gets displayed in the viewer. Images are opened via [aicsimageio](https://allencellmodeling.github.io/aicsimageio/). You can use grayscale images, RGB images or multi-channel images. In the latter case, **make sure each channel opens as a separate layer when you open them using the napari-aicsimagio importer**.\\n2. If you want to save the segmentation and tables with properties, select a folder that will contain the output.\\n3. Select the type of cellpose model.\\n4. If you use a custom model, select its location.\\n5. Run the analysis on the currently selected image or on all files in the folder.\\n### Options\\n\\n6. Select if you want to use a GPU or not.\\n7. If you are using multi-channel images, you can specify which channel to segment and optionally which to use as \\\"nuclei\\\" channel to help cell segmentation.\\n8. In case you are using one of the built-in models, you can set the estimated diameter of your objects.\\n9. In the Options tab you will find a few more options for segmentation, including the two thresholds ```flow_threshold``` and ```cellprob_threshold```. You can also decide to discard objects touching the border. Using the ```Select options yml file``` you can select a ```.yml``` file which contains a list of additional options to pass to the ```eval``` method of the Cellpose model. **Note that options specified in the yml file will override options set in the GUI**. The file [my_options.yml](https://raw.githubusercontent.com/guiwitz/napari-serialcellpose/main/src/napari_serialcellpose/_tests/my_options.yml) is an example of such a file where for example the ```diameter``` (also available in the GUI) and ```resample``` (not available in the GUI) options are set. \\n\\n<img src=\\\"https://github.com/guiwitz/napari-serialcellpose/raw/main/illustrations/napari_serialcellpose_gui1.png\\\" alt=\\\"image\\\" width=\\\"500\\\">\\n<img src=\\\"https://github.com/guiwitz/napari-serialcellpose/raw/main/illustrations/napari_serialcellpose_gui1b.png\\\" alt=\\\"image\\\" width=\\\"500\\\">\\n\\n### Properties\\n\\n10. After segmentation, properties of the objects can automatically be computed. You can select which properties should be computed in the Options tab. As defined in ```napari-skimage-regionprops``` properties are grouped by types. If you want to measure intensity properties such as mean intensity, you have to specify which channel (```Analysis channel```) you want to perform the measurement on.\\n\\n### Output\\n\\nThe results of the analysis are saved in the folder chosen in #2. The segmentation mask is saved with the same name as the original image with the suffix ```_mask.tif```. A table with properties is saved in the subfolder ```tables``` also with the same name as the image with the suffix ```props.csv```. If you run the plugin on multiple files in a folder, a ```summary.csv``` file is also generated which compiles all the data.\\n## Usage: post-processing\\n\\nAfter the analysis is done, when you select an image, the corresponding segmentation mask is shown on top of the image as shown below. This also works for saved segmentations: in that case you just select a folder with data and the corresponding output folder.\\n\\n<img src=\\\"https://github.com/guiwitz/napari-serialcellpose/raw/main/illustrations/napari_serialcellpose_gui2.png\\\" alt=\\\"image\\\" width=\\\"500\\\">\\n\\n### Properties\\n\\nIf you head to the **Properties** tab, you will find there two histograms showing the distribution of two properties that you can choose from a list at the top of the window. Below the plot you find the table containing information for each cell (each line is a cell).\\n\\nAs shown below, if you select the box ```show selected```, you can select items in the properties table and it will highlight the corresponding cell in the viewer. If you select the pipet tool, you can also select a cell and see the corresponding line in the table highlighted.\\n\\n<img src=\\\"https://github.com/guiwitz/napari-serialcellpose/raw/main/illustrations/napari_serialcellpose_gui3.png\\\" alt=\\\"image\\\" width=\\\"500\\\">\\n\\n### Summary\\n\\nFinally if you select the **Summary** tab, and click on ```Load summary```, it will load all data of the current output folder and create histograms of two properties that can be selected. An additional property can be used for filtering the data. Using the sliders, one can set a minimum and maximum threshold on the \\\"filtering property\\\", which will create a sub-selection of the data.\\n\\n<img src=\\\"https://github.com/guiwitz/napari-serialcellpose/raw/main/illustrations/napari_serialcellpose_gui4.png\\\" alt=\\\"image\\\" width=\\\"500\\\">\\n\\n## Data\\n\\nSample data were acquired by Fabian Blank at the DBMR, University of Bern.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-serialcellpose\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/guiwitz/napari-serialcellpose/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-serialcellpose\\n\\n\\n\\n\\n\\n\\nThis napari plugin allows you to segment single images or series of images using built-in or custom Cellpose models as well as to analyze the properties of these segmented regions (\\\"region properties\\\"). Properties can be visualized for a single image or a complete experiment in the form of histograms that can also be filtered (e.g. based on area size, mean intensity etc.) Thanks to the napari-skimage-regionprops plugin, properties of segmented objects can be interactively explored at a single object level.\\nMain goal\\nThe main goal of this plugin is to simplify the classical image processing pipeline of image segmentation followed by region analysis via Cellpose. It allows to quickly get a quantification of a set of images without the need for any scripting.\\nInstallation\\nIn order to use this plugin, whe highly recommend to create a specific environment and to install the required software in it. You can create a conda environment using:\\nconda create -n serialcellpose python=3.8.5 napari -c conda-forge\\n\\nThen activate it and install the plugin:\\nconda activate serialcellpose\\npip install napari-serialcellpose\\n\\nPotential issue with PyTorch\\nCellpose and therefore the plugin and napari can crash without warning in some cases with torch==1.12.0. This can be fixed by reverting to an earlier version using:\\npip install torch==1.11.0\\n\\nGPU\\nIn order to use a GPU:\\n\\n\\nUninstall the PyTorch version that gets installed by default with Cellpose:\\npip uninstall torch\\n\\n\\n\\nMake sure your have up-to-date drivers for your NVIDIA card installed.\\n\\n\\nRe-install a GPU version of PyTorch via conda using a command that you can find here (this takes care of the cuda toolkit, cudnn etc. so no need to install manually anything more than the driver). The command will look like this:\\nconda install pytorch torchvision cudatoolkit=11.3 -c pytorch\\n\\n\\n\\nPlugin Updates\\nTo update the plugin, you only need to activate the existing environment and install the new version:\\nconda activate serialcellpose\\npip install napari-serialcellpose -U\\n\\nUsage: segmentation\\nThe main interface is shown below. The sequence of events should be the following:\\n\\nSelect a folder containing images. The list of files within that folder will appear in the area above. You can also just drag and drop a folder or an image in that area. When selecting an image, it gets displayed in the viewer. Images are opened via aicsimageio. You can use grayscale images, RGB images or multi-channel images. In the latter case, make sure each channel opens as a separate layer when you open them using the napari-aicsimagio importer.\\nIf you want to save the segmentation and tables with properties, select a folder that will contain the output.\\nSelect the type of cellpose model.\\nIf you use a custom model, select its location.\\nRun the analysis on the currently selected image or on all files in the folder.\\n\\nOptions\\n\\nSelect if you want to use a GPU or not.\\nIf you are using multi-channel images, you can specify which channel to segment and optionally which to use as \\\"nuclei\\\" channel to help cell segmentation.\\nIn case you are using one of the built-in models, you can set the estimated diameter of your objects.\\nIn the Options tab you will find a few more options for segmentation, including the two thresholds flow_threshold and cellprob_threshold. You can also decide to discard objects touching the border. Using the Select options yml file you can select a .yml file which contains a list of additional options to pass to the eval method of the Cellpose model. Note that options specified in the yml file will override options set in the GUI. The file my_options.yml is an example of such a file where for example the diameter (also available in the GUI) and resample (not available in the GUI) options are set. \\n\\n\\n\\nProperties\\n\\nAfter segmentation, properties of the objects can automatically be computed. You can select which properties should be computed in the Options tab. As defined in napari-skimage-regionprops properties are grouped by types. If you want to measure intensity properties such as mean intensity, you have to specify which channel (Analysis channel) you want to perform the measurement on.\\n\\nOutput\\nThe results of the analysis are saved in the folder chosen in #2. The segmentation mask is saved with the same name as the original image with the suffix _mask.tif. A table with properties is saved in the subfolder tables also with the same name as the image with the suffix props.csv. If you run the plugin on multiple files in a folder, a summary.csv file is also generated which compiles all the data.\\nUsage: post-processing\\nAfter the analysis is done, when you select an image, the corresponding segmentation mask is shown on top of the image as shown below. This also works for saved segmentations: in that case you just select a folder with data and the corresponding output folder.\\n\\nProperties\\nIf you head to the Properties tab, you will find there two histograms showing the distribution of two properties that you can choose from a list at the top of the window. Below the plot you find the table containing information for each cell (each line is a cell).\\nAs shown below, if you select the box show selected, you can select items in the properties table and it will highlight the corresponding cell in the viewer. If you select the pipet tool, you can also select a cell and see the corresponding line in the table highlighted.\\n\\nSummary\\nFinally if you select the Summary tab, and click on Load summary, it will load all data of the current output folder and create histograms of two properties that can be selected. An additional property can be used for filtering the data. Using the sliders, one can set a minimum and maximum threshold on the \\\"filtering property\\\", which will create a sub-selection of the data.\\n\\nData\\nSample data were acquired by Fabian Blank at the DBMR, University of Bern.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-serialcellpose\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"serialcellpose\",\"documentation\":\"https://github.com/guiwitz/napari-serialcellpose#README.md\",\"first_released\":\"2022-07-28T14:16:35.456233Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-serialcellpose\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/guiwitz/napari-serialcellpose\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-08-08T20:39:23.176539Z\",\"report_issues\":\"https://github.com/guiwitz/napari-serialcellpose/issues\",\"requirements\":[\"cellpose\",\"numpy\",\"magicgui\",\"qtpy\",\"matplotlib\",\"napari-skimage-regionprops\",\"napari-aicsimageio\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\"],\"summary\":\"A simple plugin to batch segment cells with cellpose\",\"support\":\"https://github.com/guiwitz/napari-serialcellpose/issues\",\"twitter\":\"\",\"version\":\"0.2.2\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Philipp Plewa\"}],\"code_repository\":\"https://github.com/AstraZeneca/napari-wsi\",\"description\":\"# Description\\n\\nThis [napari] plugin provides a reader for various whole slide image formats.\\n\\nBy default, any of the following formats is read using the [tifffile] library.\\nIf the image file contains a tag `GDAL_METADATA`, the [rasterio] library is used\\ninstead.\\n\\n- .bif\\n- .ndpi\\n- .qptiff\\n- .scn\\n- .svs\\n- .tif\\n- .tiff\\n\\n# Quickstart\\n\\nFrom the terminal:\\n\\n```bash\\nnapari CMU-1.svs\\n```\\n\\nFrom python:\\n\\n```python\\nimport napari\\n\\nviewer = napari.Viewer()\\nviewer.open(\\\"CMU-1.svs\\\")\\n```\\n\\n[napari]: https://github.com/napari/napari\\n[rasterio]: https://github.com/rasterio/rasterio\\n[tifffile]: https://github.com/cgohlke/tifffile\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"Description\\nThis napari plugin provides a reader for various whole slide image formats.\\nBy default, any of the following formats is read using the tifffile library.\\nIf the image file contains a tag GDAL_METADATA, the rasterio library is used\\ninstead.\\n\\n.bif\\n.ndpi\\n.qptiff\\n.scn\\n.svs\\n.tif\\n.tiff\\n\\nQuickstart\\nFrom the terminal:\\nbash\\nnapari CMU-1.svs\\nFrom python:\\n```python\\nimport napari\\nviewer = napari.Viewer()\\nviewer.open(\\\"CMU-1.svs\\\")\\n```\",\"development_status\":[],\"display_name\":\"WSI Reader\",\"documentation\":\"\",\"first_released\":\"2023-02-01T13:23:05.934764Z\",\"license\":\"Apache-2.0\",\"name\":\"napari-wsi\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\",\"widget\"],\"project_site\":\"https://github.com/AstraZeneca/napari-wsi\",\"python_version\":\">=3.8,<3.11\",\"reader_file_extensions\":[\"*.ndpi\",\"*.svs\",\"*.tiff\",\"*.tif\",\"*.bif\",\"*.scn\",\"*.qptiff\"],\"release_date\":\"2023-02-01T13:23:05.934764Z\",\"report_issues\":\"\",\"requirements\":[\"dask (>=2022)\",\"imagecodecs (>=2022)\",\"magicgui (<1.0.0)\",\"matplotlib (>=3.0.0,<4.0.0)\",\"napari (>=0.4.0,<0.5.0)\",\"numpy (>=1.0.0,<2.0.0)\",\"pydantic (!=1.10.0)\",\"rasterio (>=1.0.0,<2.0.0)\",\"tifffile (>=2022)\",\"zarr (>=2.0.0,<3.0.0)\"],\"summary\":\"A plugin to read whole slide images within napari.\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.1.0\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Jonas Windhager\"}],\"code_repository\":\"https://github.com/BodenmillerGroup/napari-hierarchical\",\"description\":\"# napari-hierarchical\\n\\n[![License MIT](https://img.shields.io/pypi/l/napari-hierarchical.svg?color=green)](https://github.com/BodenmillerGroup/napari-hierarchical/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-hierarchical.svg?color=green)](https://pypi.org/project/napari-hierarchical)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-hierarchical.svg?color=green)](https://python.org)\\n[![tests](https://github.com/BodenmillerGroup/napari-hierarchical/workflows/tests/badge.svg)](https://github.com/BodenmillerGroup/napari-hierarchical/actions)\\n[![codecov](https://codecov.io/gh/BodenmillerGroup/napari-hierarchical/branch/main/graph/badge.svg)](https://codecov.io/gh/BodenmillerGroup/napari-hierarchical)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-hierarchical)](https://napari-hub.org/plugins/napari-hierarchical)\\n\\nHierarchical file format support for napari\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n\\n## Installation\\n\\nYou can install `napari-hierarchical` via [pip]:\\n\\n    pip install \\\"napari-hierarchical[all]\\\"\\n\\nTo install latest development version :\\n\\n    pip install \\\"git+https://github.com/BodenmillerGroup/napari-hierarchical.git#egg=napari-hierarchical[all]\\\"\\n\\n## Usage\\n\\nThe plugin enables the reading, editing and writing of container formats. In the plugin, *groups* represent hierarchically structured collections of *arrays*. Each group can hold zero or more arrays and can have zero or more child groups (hierarchical structure). An array is a logical representation of (image) data on disk and directly corresponds to a napari layer when loaded.\\n\\nFiles can be opened through napari (e.g. `File -> Open File(s)` menu, `Viewer.open(...)` function), as the plugin implements napari's file reader hook. Upon opening a hierarchically structured file, the *Groups* and *Arrays* widgets are displayed. The *Groups* widget allows to browse and restructure the groups tree, while the *Arrays* widget groups arrays from the selected groups by file format-specific metadata (e.g. channel name for MCD files). Selecting arrays also selects the corresponding napari layers, allowing to adjust their properties.\\n\\nArrays can be loaded individually by toggling their *loaded* state (circular button), which will add napari layers for the corresponding arrays. Similarly, loaded arrays can be shown or hidden by toggling their *visible* state (eye button), which will toggle the visibility of the associated napari layers. The loaded/visible states of groups (collections of arrays) can be toggled in a similar fashion. Arrays are always loaded into memory (no memory mapping), to allow for editing the tree structure. Loaded root groups can be exported to supported hierarchical file formats.\\n\\nCurrently, reading/writing of HDF5 and Zarr (not: OME-NGFF) files are supported out of the box, as well as reading imaging mass cytometry (IMC) data (i.e., MCD files). For these file formats, sample data is available through the plugin. Additional readers/writers can be implemented using a pluggy-based interface, similar to the first generation `napari-plugin-engine`.\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n\\n## License\\n\\nDistributed under the terms of the [MIT] license,\\n\\\"napari-hierarchical\\\" is free and open source software\\n\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/BodenmillerGroup/napari-hierarchical/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-hierarchical\\n\\n\\n\\n\\n\\n\\nHierarchical file format support for napari\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\nInstallation\\nYou can install napari-hierarchical via pip:\\npip install \\\"napari-hierarchical[all]\\\"\\n\\nTo install latest development version :\\npip install \\\"git+https://github.com/BodenmillerGroup/napari-hierarchical.git#egg=napari-hierarchical[all]\\\"\\n\\nUsage\\nThe plugin enables the reading, editing and writing of container formats. In the plugin, groups represent hierarchically structured collections of arrays. Each group can hold zero or more arrays and can have zero or more child groups (hierarchical structure). An array is a logical representation of (image) data on disk and directly corresponds to a napari layer when loaded.\\nFiles can be opened through napari (e.g. File -> Open File(s) menu, Viewer.open(...) function), as the plugin implements napari's file reader hook. Upon opening a hierarchically structured file, the Groups and Arrays widgets are displayed. The Groups widget allows to browse and restructure the groups tree, while the Arrays widget groups arrays from the selected groups by file format-specific metadata (e.g. channel name for MCD files). Selecting arrays also selects the corresponding napari layers, allowing to adjust their properties.\\nArrays can be loaded individually by toggling their loaded state (circular button), which will add napari layers for the corresponding arrays. Similarly, loaded arrays can be shown or hidden by toggling their visible state (eye button), which will toggle the visibility of the associated napari layers. The loaded/visible states of groups (collections of arrays) can be toggled in a similar fashion. Arrays are always loaded into memory (no memory mapping), to allow for editing the tree structure. Loaded root groups can be exported to supported hierarchical file formats.\\nCurrently, reading/writing of HDF5 and Zarr (not: OME-NGFF) files are supported out of the box, as well as reading imaging mass cytometry (IMC) data (i.e., MCD files). For these file formats, sample data is available through the plugin. Additional readers/writers can be implemented using a pluggy-based interface, similar to the first generation napari-plugin-engine.\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the MIT license,\\n\\\"napari-hierarchical\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[],\"display_name\":\"napari-hierarchical\",\"documentation\":\"https://github.com/BodenmillerGroup/napari-hierarchical#README.md\",\"first_released\":\"2022-12-21T16:24:46.716911Z\",\"license\":\"MIT\",\"name\":\"napari-hierarchical\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\",\"widget\",\"sample_data\"],\"project_site\":\"https://github.com/BodenmillerGroup/napari-hierarchical\",\"python_version\":\"<3.11,>=3.8\",\"reader_file_extensions\":[\"*\"],\"release_date\":\"2022-12-21T16:24:46.716911Z\",\"report_issues\":\"https://github.com/BodenmillerGroup/napari-hierarchical/issues\",\"requirements\":[\"napari (<0.4.18,>=0.4.17)\",\"pluggy\",\"qtpy\",\"dask ; extra == 'all'\",\"h5py ; extra == 'all'\",\"readimc ; extra == 'all'\",\"s3fs ; extra == 'all'\",\"zarr ; extra == 'all'\",\"dask ; extra == 'hdf5'\",\"h5py ; extra == 'hdf5'\",\"dask ; extra == 'imc'\",\"readimc ; extra == 'imc'\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\",\"dask ; extra == 'zarr'\",\"s3fs ; extra == 'zarr'\",\"zarr ; extra == 'zarr'\"],\"summary\":\"Hierarchical file format support for napari\",\"support\":\"https://github.com/BodenmillerGroup/napari-hierarchical/issues\",\"twitter\":\"\",\"version\":\"0.1.0\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"David Bauer\"},{\"name\":\"Jozsef Molnar\"},{\"name\":\"Dominik Hirling\"}],\"code_repository\":\"https://github.com/bauerdavid/napari-nD-annotator\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-nd-annotator\"}],\"description\":\"# napari-nD-annotator\\n\\n[![License BSD-3](https://img.shields.io/pypi/l/napari-nD-annotator.svg?color=green)](https://github.com/bauerdavid/napari-nD-annotator/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-nD-annotator.svg?color=green)](https://pypi.org/project/napari-nD-annotator)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-nD-annotator.svg?color=green)](https://python.org)\\n[![tests](https://github.com/bauerdavid/napari-nD-annotator/workflows/tests/badge.svg)](https://github.com/bauerdavid/napari-nD-annotator/actions)\\n[![codecov](https://codecov.io/gh/bauerdavid/napari-nD-annotator/branch/main/graph/badge.svg)](https://codecov.io/gh/bauerdavid/napari-nD-annotator)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-nD-annotator)](https://napari-hub.org/plugins/napari-nD-annotator)\\n\\nA toolbox for annotating objects one by one in nD\\n\\nThis plugin contains some tools to make 2D/3D, but basically any dimensional annotation easier.\\nMain features:\\n * nD bounding box layer\\n * object list from bounding boxes\\n * visualizing selected objects from different projections\\n * auto-filling labels\\n * label slice interpolation (geometric mean, RSPV representation)\\n * minimal contour segmentation\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-nD-annotator` via [pip]:\\n\\n    pip install napari-nD-annotator\\n\\nThe plugin is also available in napari-hub, to install it directly from napari, please refer to\\n[plugin installation instructions] at the official [napari] website.\\n\\n## Usage\\nYou can start napari with the plugin's widgets already opened as:\\n\\n    napari -w napari-nD-annotator \\\"Object List\\\" \\\"Annotation Toolbox\\\"\\n\\n\\n### Bounding boxes\\nThe main idea is to create bounding boxes around objects we want to annotate, crop them, and annotate them one by one. This has mainly two advantages when visualizing in 3D:\\n\\n1. We don't have to load the whole data into memory\\n2. The surrounding objects won't occlude the annotated ones, making it easier to check the annotation.\\n\\nBounding boxes can be created from the `Object list` widget. The dimensionality of the bounding box layer will be determined from the image layer. As bounding boxes are created, a small thumbnail will be displayed.\\n\\nThe proposed pipeline goes as follows:\\n\\n 1. Create a bounding box layer\\n 2. Select data parts using the bounding boxes\\n 3. Select an object from the object list\\n 4. Annotate the object\\n 5. Repeat from 3.\\n\\n### Slice interpolation\\nThe `Interpolation` tab contains tools for estimating missing annotation slices from existing ones. Two methods are implemented:\\n * Geometric: the interpolation will be determined by calculating the average of the corresponding contour points.\\n * RSPV: A more sophisticated average contour calculation, see the preprint [here](https://arxiv.org/pdf/1901.02823.pdf).\\n\\nhttps://user-images.githubusercontent.com/36735863/188876826-1771acee-93ba-4905-982e-bfb459329659.mp4\\n\\n### Minimal contour\\nThis plugin can estimate a minimal contour, which is calculated from a point set on the edges of the object, which are provided by the user. This contour will follow some image feature (pixels with high gradient or high/low intensity).\\nFeatures:\\n * With a single click a new point can be added to the set. This will also extend the contour with the curve shown in red\\n * A double click will close the curve by adding both the red and gray curves to the minimal contour\\n * When holding `Shift`, the gray and red highlight will be swapped, so the other curve can be added to the contour\\n * With the `Ctrl` button down a straight line can be added instead of the minimal path\\n * If the anchor points were misplaced, the last point can be removed by right-clicking, or the whole point set can be cleared by pressing `Esc`\\n * The `Param` value at the widget will decide, how strongly should the contour follow edges on the image. Higher value means higher sensitivity to image data, while a lower value will be closer to straight lines.\\n * Different features can be used, like image gradient or pixel intensities, and also user-defined features (using Python)\\n   * the image is accessed as the `image` variable, and the features should be stored in the `features` variable in the small code editor widget\\n\\nThis functionality can be used by selecting the `Minimal Contour` tab in the `Annotation Toolbox` widget, which will create a new layer called `Anchors`.\\n\\n**Important note: Do not remove or modify this layer directly!**\\n\\n*Note: if any layer is created before opening the `Annotation Toolbox` widget, some \\\"temporary\\\" layers appear in the layer list. This is not intended, but currently there is no way to hide these. __Do not remove or modify these, as this could break the plugin!__ Whenever possible, open the toolbox first, in order to prevent these from appearing.*\\n\\n#### Intensity-based:\\n\\nhttps://user-images.githubusercontent.com/36735863/191023482-0dfafb5c-003a-47f6-a21b-8582a4e3930f.mp4\\n\\n#### Gradient-based:\\n\\nhttps://user-images.githubusercontent.com/36735863/191024941-f20f63a0-8281-47d2-be22-d1ec34fe1f5d.mp4\\n\\n#### Custom feature:\\n\\nhttps://user-images.githubusercontent.com/36735863/191025028-3f807bd2-1f2e-40d2-800b-48af820a7dbe.mp4\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-nD-annotator\\\" is free and open source software\\n\\n## Issues\\n\\n### Known issues\\n * When using the `Annotation Toolbox`:\\n   * When deleting a single layer from the layer list, some other layers' names might be overwritten by some \\\"invisible\\\" utility layers. Selecting and unselecting these will restore the original layer.\\n   * When deleting multiple layers, some strange behavior can happen (layer duplicates appear, only in the layer list; napari breaks etc.). Until fixed, layers should be removed one by one.\\n   * If any layer is created before opening the `Annotation Toolbox` widget, some \\\"temporary\\\" layers appear in the layer list. For further information see the [Minimal contour](https://github.com/bauerdavid/napari-nD-annotator/edit/mean_contour/README.md#minimal-contour) section\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n[plugin installation instructions]: https://napari.org/plugins/find_and_install_plugin.html\\n[file an issue]: https://github.com/bauerdavid/napari-nD-annotator/issues/new/choose\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-nD-annotator\\n\\n\\n\\n\\n\\n\\nA toolbox for annotating objects one by one in nD\\nThis plugin contains some tools to make 2D/3D, but basically any dimensional annotation easier.\\nMain features:\\n * nD bounding box layer\\n * object list from bounding boxes\\n * visualizing selected objects from different projections\\n * auto-filling labels\\n * label slice interpolation (geometric mean, RSPV representation)\\n * minimal contour segmentation\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-nD-annotator via pip:\\npip install napari-nD-annotator\\n\\nThe plugin is also available in napari-hub, to install it directly from napari, please refer to\\nplugin installation instructions at the official napari website.\\nUsage\\nYou can start napari with the plugin's widgets already opened as:\\nnapari -w napari-nD-annotator \\\"Object List\\\" \\\"Annotation Toolbox\\\"\\n\\nBounding boxes\\nThe main idea is to create bounding boxes around objects we want to annotate, crop them, and annotate them one by one. This has mainly two advantages when visualizing in 3D:\\n\\nWe don't have to load the whole data into memory\\nThe surrounding objects won't occlude the annotated ones, making it easier to check the annotation.\\n\\nBounding boxes can be created from the Object list widget. The dimensionality of the bounding box layer will be determined from the image layer. As bounding boxes are created, a small thumbnail will be displayed.\\nThe proposed pipeline goes as follows:\\n\\nCreate a bounding box layer\\nSelect data parts using the bounding boxes\\nSelect an object from the object list\\nAnnotate the object\\nRepeat from 3.\\n\\nSlice interpolation\\nThe Interpolation tab contains tools for estimating missing annotation slices from existing ones. Two methods are implemented:\\n * Geometric: the interpolation will be determined by calculating the average of the corresponding contour points.\\n * RSPV: A more sophisticated average contour calculation, see the preprint here.\\nhttps://user-images.githubusercontent.com/36735863/188876826-1771acee-93ba-4905-982e-bfb459329659.mp4\\nMinimal contour\\nThis plugin can estimate a minimal contour, which is calculated from a point set on the edges of the object, which are provided by the user. This contour will follow some image feature (pixels with high gradient or high/low intensity).\\nFeatures:\\n * With a single click a new point can be added to the set. This will also extend the contour with the curve shown in red\\n * A double click will close the curve by adding both the red and gray curves to the minimal contour\\n * When holding Shift, the gray and red highlight will be swapped, so the other curve can be added to the contour\\n * With the Ctrl button down a straight line can be added instead of the minimal path\\n * If the anchor points were misplaced, the last point can be removed by right-clicking, or the whole point set can be cleared by pressing Esc\\n * The Param value at the widget will decide, how strongly should the contour follow edges on the image. Higher value means higher sensitivity to image data, while a lower value will be closer to straight lines.\\n * Different features can be used, like image gradient or pixel intensities, and also user-defined features (using Python)\\n   * the image is accessed as the image variable, and the features should be stored in the features variable in the small code editor widget\\nThis functionality can be used by selecting the Minimal Contour tab in the Annotation Toolbox widget, which will create a new layer called Anchors.\\nImportant note: Do not remove or modify this layer directly!\\nNote: if any layer is created before opening the Annotation Toolbox widget, some \\\"temporary\\\" layers appear in the layer list. This is not intended, but currently there is no way to hide these. Do not remove or modify these, as this could break the plugin! Whenever possible, open the toolbox first, in order to prevent these from appearing.\\nIntensity-based:\\nhttps://user-images.githubusercontent.com/36735863/191023482-0dfafb5c-003a-47f6-a21b-8582a4e3930f.mp4\\nGradient-based:\\nhttps://user-images.githubusercontent.com/36735863/191024941-f20f63a0-8281-47d2-be22-d1ec34fe1f5d.mp4\\nCustom feature:\\nhttps://user-images.githubusercontent.com/36735863/191025028-3f807bd2-1f2e-40d2-800b-48af820a7dbe.mp4\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-nD-annotator\\\" is free and open source software\\nIssues\\nKnown issues\\n\\nWhen using the Annotation Toolbox:\\nWhen deleting a single layer from the layer list, some other layers' names might be overwritten by some \\\"invisible\\\" utility layers. Selecting and unselecting these will restore the original layer.\\nWhen deleting multiple layers, some strange behavior can happen (layer duplicates appear, only in the layer list; napari breaks etc.). Until fixed, layers should be removed one by one.\\nIf any layer is created before opening the Annotation Toolbox widget, some \\\"temporary\\\" layers appear in the layer list. For further information see the Minimal contour section\\n\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"Annotation Toolbox\",\"documentation\":\"https://github.com/bauerdavid/napari-nD-annotator/blob/main/README.md\",\"first_released\":\"2022-06-01T13:44:46.234050Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-nD-annotator\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-09-29T13:55:42.851940Z\",\"report_issues\":\"https://github.com/bauerdavid/napari-nD-annotator/issues\",\"requirements\":[\"numpy\",\"magicgui\",\"qtpy\",\"opencv-python\",\"matplotlib\",\"napari (>=0.4.15)\",\"scikit-image (>=0.19)\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\"],\"summary\":\"A toolbox for annotating objects one by one in nD\",\"support\":\"https://github.com/bauerdavid/napari-nD-annotator/issues\",\"twitter\":\"\",\"version\":\"0.0.8\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Juan Nunez-Iglesias\"},{\"name\":\"Abigail McGovern\"}],\"category\":{\"Workflow step\":[\"Image Segmentation\"]},\"category_hierarchy\":{\"Workflow step\":[[\"Image Segmentation\",\"Cell segmentation\"],[\"Image Segmentation\",\"Region growing\",\"Watershed segmentation\"]]},\"code_repository\":\"https://github.com/jni/platelet-unet-watershed\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"platelet-unet-watershed\"}],\"description\":\"# platelet-unet-watershed\\n\\n[![License](https://img.shields.io/pypi/l/platelet-unet-watershed.svg?color=green)](https://github.com/jni/platelet-unet-watershed/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/platelet-unet-watershed.svg?color=green)](https://pypi.org/project/platelet-unet-watershed)\\n[![Python Version](https://img.shields.io/pypi/pyversions/platelet-unet-watershed.svg?color=green)](https://python.org)\\n[![tests](https://github.com/jni/platelet-unet-watershed/workflows/tests/badge.svg)](https://github.com/jni/platelet-unet-watershed/actions)\\n[![codecov](https://codecov.io/gh/jni/platelet-unet-watershed/branch/master/graph/badge.svg)](https://codecov.io/gh/jni/platelet-unet-watershed)\\n\\nSegment platelets with pretrained unet and affinity watershed\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `platelet-unet-watershed` via [pip]:\\n\\n    pip install platelet-unet-watershed\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"platelet-unet-watershed\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/jni/platelet-unet-watershed/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"platelet-unet-watershed\\n\\n\\n\\n\\n\\nSegment platelets with pretrained unet and affinity watershed\\n\\nThis napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install platelet-unet-watershed via pip:\\npip install platelet-unet-watershed\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"platelet-unet-watershed\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"platelet-unet-watershed\",\"documentation\":\"https://github.com/jni/platelet-unet-watershed#README.md\",\"first_released\":\"2021-07-01T06:29:19.185077Z\",\"license\":\"BSD-3-Clause\",\"name\":\"platelet-unet-watershed\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/jni/platelet-unet-watershed\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2021-09-20T07:40:31.740406Z\",\"report_issues\":\"https://github.com/jni/platelet-unet-watershed/issues\",\"requirements\":null,\"summary\":\"Segment platelets with pretrained unet and affinity watershed\",\"support\":\"https://github.com/jni/platelet-unet-watershed/issues\",\"twitter\":\"\",\"version\":\"0.0.3\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"jhnnsrs\"}],\"code_repository\":\"https://github.com/jhnnsrs/mikro-napari\",\"conda\":[],\"description\":\"# mikro-napari\\n\\n[![codecov](https://codecov.io/gh/jhnnsrs/mikro-napari/branch/master/graph/badge.svg?token=UGXEA2THBV)](https://codecov.io/gh/jhnnsrs/mikro-napari)\\n[![PyPI version](https://badge.fury.io/py/mikro-napari.svg)](https://pypi.org/project/mikro-napari/)\\n[![Maintenance](https://img.shields.io/badge/Maintained%3F-yes-green.svg)](https://pypi.org/project/mikro-napari/)\\n![Maintainer](https://img.shields.io/badge/maintainer-jhnnsrs-blue)\\n[![PyPI pyversions](https://img.shields.io/pypi/pyversions/mikro-napari.svg)](https://pypi.python.org/pypi/mikro-napari/)\\n[![PyPI status](https://img.shields.io/pypi/status/mikro-napari.svg)](https://pypi.python.org/pypi/mikro-napari/)\\n\\nmikro napari enables napari on the mikro/arkitekt platform\\n\\n# DEVELOPMENT\\n\\n## Idea\\n\\nThis is a napari plugin, that provides a simple user interface to use napari with mikro you can view and annotate\\ndata on the mikro platform (synchronised between all of your napari instances) and use napari within arkitekt workflows\\n(can be extended with other plugins)\\n\\n## Install\\n\\nSimple install this plugin via naparis plugin-manager and enable it. \\nLogin with your local mikro/arkitekt platform and start using it in workflows\\n\\nYou can also install mikro-napari directly in your enviroment \\n\\n```bash\\npip install mikro-napari napari[pyqt5]\\n```\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"mikro-napari\\n\\n\\n\\n\\n\\n\\nmikro napari enables napari on the mikro/arkitekt platform\\nDEVELOPMENT\\nIdea\\nThis is a napari plugin, that provides a simple user interface to use napari with mikro you can view and annotate\\ndata on the mikro platform (synchronised between all of your napari instances) and use napari within arkitekt workflows\\n(can be extended with other plugins)\\nInstall\\nSimple install this plugin via naparis plugin-manager and enable it. \\nLogin with your local mikro/arkitekt platform and start using it in workflows\\nYou can also install mikro-napari directly in your enviroment \\nbash\\npip install mikro-napari napari[pyqt5]\",\"development_status\":[],\"display_name\":\"mikro-napari\",\"documentation\":\"https://jhnnsrs.github.io/doks/\",\"first_released\":\"2021-09-30T09:39:08.169115Z\",\"license\":\"CC BY-NC 3.0\",\"name\":\"mikro-napari\",\"npe2\":true,\"operating_system\":[],\"plugin_types\":[\"widget\"],\"project_site\":\"https://jhnnsrs.github.io/doks/\",\"python_version\":\">=3.8,<4.0\",\"reader_file_extensions\":[],\"release_date\":\"2022-10-22T17:04:38.835694Z\",\"report_issues\":\"https://github.com/jhnnsrs/mikro-napari/issues\",\"requirements\":[\"arkitekt (==0.3.16)\"],\"summary\":\"A napari plugin to interact with and provide functionality for a connected arkitekt server\",\"support\":\"https://jhnnsrs.github.io/doks/\",\"twitter\":\"https://twitter.com/jhnnsrs\",\"version\":\"0.1.49\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Tom Burke\"},{\"name\":\"Joran Deschamps\"}],\"code_repository\":\"https://github.com/juglab/napari_denoiseg\",\"description\":\"# napari-denoiseg\\n\\n[![License](https://img.shields.io/pypi/l/napari-denoiseg.svg?color=green)](https://github.com/juglab/napari-denoiseg/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-denoiseg.svg?color=green)](https://pypi.org/project/napari-denoiseg)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-denoiseg.svg?color=green)](https://python.org)\\n[![tests](https://github.com/juglab/napari-denoiseg/workflows/build/badge.svg)](https://github.com/juglab/napari-denoiseg/actions)\\n[![codecov](https://codecov.io/gh/juglab/napari-denoiseg/branch/main/graph/badge.svg)](https://codecov.io/gh/juglab/napari-denoiseg)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-denoiseg)](https://napari-hub.org/plugins/napari-denoiseg)\\n\\nA napari plugin performing joint denoising and segmentation of microscopy images using [DenoiSeg](https://github.com/juglab/DenoiSeg).\\n\\n<img src=\\\"https://raw.githubusercontent.com/juglab/napari-denoiseg/master/docs/images/example.png\\\" width=\\\"800\\\" />\\n----------------------------------\\n\\n## Installation\\n\\nCheck out the [documentation](https://juglab.github.io/napari-denoiseg/installation.html) for more detailed installation \\ninstructions. \\n\\n\\n## Quick demo\\n\\nYou can try out a demo by loading the `DenoiSeg Demo prediction` plugin and directly clicking on `Predict`.\\n\\n\\n<img src=\\\"https://raw.githubusercontent.com/juglab/napari-denoiseg/master/docs/images/prediction.gif\\\" width=\\\"800\\\" />\\n\\n\\n## Documentation\\n\\nDocumentation is available on the [project website](https://juglab.github.io/napari-denoiseg/).\\n\\n\\n## Contributing and feedback\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request. You can also \\nhelp us improve by [filing an issue] along with a detailed description or contact us\\nthrough the [image.sc](https://forum.image.sc/) forum (tag @jdeschamps).\\n\\n\\n## Cite us\\n\\n\\nTim-Oliver Buchholz, Mangal Prakash, Alexander Krull and Florian Jug, \\\"[DenoiSeg: Joint Denoising and Segmentation](https://arxiv.org/abs/2005.02987)\\\" _arxiv_ (2020)\\n\\n\\n## Acknowledgements\\n\\nThis plugin was developed thanks to the support of the Silicon Valley Community Foundation (SCVF) and the \\nChan-Zuckerberg Initiative (CZI) with the napari Plugin Accelerator grant _2021-239867_.\\n\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-denoiseg\\\" is a free and open source software.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[filing an issue]: https://github.com/juglab/napari-denoiseg/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-denoiseg\\n\\n\\n\\n\\n\\n\\nA napari plugin performing joint denoising and segmentation of microscopy images using DenoiSeg.\\n\\nInstallation\\nCheck out the documentation for more detailed installation \\ninstructions. \\nQuick demo\\nYou can try out a demo by loading the DenoiSeg Demo prediction plugin and directly clicking on Predict.\\n\\nDocumentation\\nDocumentation is available on the project website.\\nContributing and feedback\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request. You can also \\nhelp us improve by filing an issue along with a detailed description or contact us\\nthrough the image.sc forum (tag @jdeschamps).\\nCite us\\nTim-Oliver Buchholz, Mangal Prakash, Alexander Krull and Florian Jug, \\\"DenoiSeg: Joint Denoising and Segmentation\\\" arxiv (2020)\\nAcknowledgements\\nThis plugin was developed thanks to the support of the Silicon Valley Community Foundation (SCVF) and the \\nChan-Zuckerberg Initiative (CZI) with the napari Plugin Accelerator grant 2021-239867.\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-denoiseg\\\" is a free and open source software.\",\"development_status\":[\"Development Status :: 3 - Alpha\"],\"display_name\":\"DenoiSeg\",\"documentation\":\"https://juglab.github.io/napari-denoiseg/\",\"first_released\":\"2022-10-31T21:39:50.344176Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-denoiseg\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\",\"sample_data\"],\"project_site\":\"https://github.com/juglab/napari_denoiseg\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-11-01T16:23:33.870728Z\",\"report_issues\":\"https://github.com/juglab/napari_denoiseg/issues\",\"requirements\":[\"numpy\",\"pyqtgraph\",\"denoiseg (>=0.3.0)\",\"bioimageio.core\",\"magicgui\",\"qtpy\",\"napari-time-slicer (>=0.4.9)\",\"napari (<=0.4.15)\",\"vispy (<=0.9.6)\",\"imageio (!=2.11.0,!=2.22.1,>=2.5.0)\",\"tensorflow ; platform_system != \\\"Darwin\\\" or platform_machine != \\\"arm64\\\"\",\"tensorflow-macos ; platform_system == \\\"Darwin\\\" and platform_machine == \\\"arm64\\\"\",\"tensorflow-metal ; platform_system == \\\"Darwin\\\" and platform_machine == \\\"arm64\\\"\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\"],\"summary\":\"A napari plugin performing joint denoising and segmentation of microscopy images using DenoiSeg.\",\"support\":\"https://github.com/juglab/napari_denoiseg/issues\",\"twitter\":\"\",\"version\":\"0.0.1rc2\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Johannes Müller\"},{\"name\":\"Ben J. Gross\"},{\"name\":\"Robert Haase\"},{\"name\":\"Elijah Shelton\"},{\"name\":\"Carlos Gomez\"},{\"name\":\"Otger Campas\"}],\"code_repository\":\"https://github.com/campaslab/napari-stress\",\"description\":\"[![License](https://img.shields.io/pypi/l/napari-stress.svg?color=green)](https://github.com/campaslab/napari-stress/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-stress.svg?color=green)](https://pypi.org/project/napari-stress)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-stress.svg?color=green)](https://python.org)\\n[![tests](https://github.com/campaslab/napari-stress/actions/workflows/test_and_deploy.yml/badge.svg)](https://github.com/campaslab/napari-stress/actions/workflows/test_and_deploy.yml)\\n[![codecov](https://codecov.io/gh/campaslab/napari-stress/branch/main/graph/badge.svg?token=ZXQGREJAT9)](https://codecov.io/gh/campaslab/napari-stress)\\n[![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&logoColor=white)](https://github.com/pre-commit/pre-commit)\\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/napari-stress.svg)](https://pypistats.org/packages/napari-stress)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-stress)](https://www.napari-hub.org/plugins/napari-stress)\\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.6607329.svg)](https://doi.org/10.5281/zenodo.6607329)\\n\\n# napari-stress\\n\\nThis plugin provides tools for the analysis of surfaces in Napari, such as utilities to determine and refine the surface-representation of objects using a ray-casting approach and calculate the curvature of surfaces. \\nIt re-implements code in Napari that was written for [Gross et al. (2021): STRESS, an automated geometrical characterization of deformable particles for in vivo measurements of cell and tissue mechanical stresses](https://www.biorxiv.org/content/10.1101/2021.03.26.437148v1) \\nand has been made open source in [this repository](https://github.com/campaslab/STRESS).\\n\\n![](https://github.com/campaslab/napari-stress/raw/main/docs/imgs/function_gifs/spherical_harmonics.gif)\\n\\n## Usage\\n\\nFor documentation on how to use napari-stress both interactively from the napari-viewer or from code, please visit the [**documentation**](https://campaslab.github.io/napari-stress/intro.html)\\n\\n\\n## Installation\\n\\nCreate a new conda environment with the following command. \\nIf you have never used conda before, please [read this guide first](https://campaslab.github.io/blog/johannes_mueller/anaconda_getting_started/).\\n\\n```\\nconda create -n napari-stress Python=3.9 napari jupyterlab -c conda-forge\\nconda activate napari-stress\\n```\\n\\nYou can then install napari-stress using pip:\\n\\n```\\npip install napari-stress\\n```\\n\\n## Issues\\n\\nTo report bugs, request new features or get in touch, please [open an issue](https://github.com/campaslab/napari-stress/issues) or tag `@EL_Pollo_Diablo` on [image.sc](https://forum.image.sc/).\\n\\n## See also\\n\\nThere are other napari plugins with similar / overlapping functionality\\n\\n* [morphometrics](https://www.napari-hub.org/plugins/morphometrics)\\n* [napari-pymeshlab](https://www.napari-hub.org/plugins/napari-pymeshlab)\\n* [napari-process-points-and-surfaces](https://www.napari-hub.org/plugins/napari-process-points-and-surfaces)\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [pytest], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-stress\\\" is free and open source software\\n\\n## Acknowledgements\\nThis project was supported by the Deutsche Forschungsgemeinschaft under Germany’s Excellence Strategy – EXC2068 - Cluster of Excellence \\\"Physics of Life\\\" of TU Dresden.\\n\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[pytest]: https://docs.pytest.org/en/7.0.x/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"\\n\\n\\n\\n\\n\\n\\n\\n\\nnapari-stress\\nThis plugin provides tools for the analysis of surfaces in Napari, such as utilities to determine and refine the surface-representation of objects using a ray-casting approach and calculate the curvature of surfaces. \\nIt re-implements code in Napari that was written for Gross et al. (2021): STRESS, an automated geometrical characterization of deformable particles for in vivo measurements of cell and tissue mechanical stresses \\nand has been made open source in this repository.\\n\\nUsage\\nFor documentation on how to use napari-stress both interactively from the napari-viewer or from code, please visit the documentation\\nInstallation\\nCreate a new conda environment with the following command. \\nIf you have never used conda before, please read this guide first.\\nconda create -n napari-stress Python=3.9 napari jupyterlab -c conda-forge\\nconda activate napari-stress\\nYou can then install napari-stress using pip:\\npip install napari-stress\\nIssues\\nTo report bugs, request new features or get in touch, please open an issue or tag @EL_Pollo_Diablo on image.sc.\\nSee also\\nThere are other napari plugins with similar / overlapping functionality\\n\\nmorphometrics\\nnapari-pymeshlab\\nnapari-process-points-and-surfaces\\n\\nContributing\\nContributions are very welcome. Tests can be run with pytest, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-stress\\\" is free and open source software\\nAcknowledgements\\nThis project was supported by the Deutsche Forschungsgemeinschaft under Germany’s Excellence Strategy – EXC2068 - Cluster of Excellence \\\"Physics of Life\\\" of TU Dresden.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari-stress\",\"documentation\":\"https://campaslab.github.io/napari-stress\",\"first_released\":\"2022-06-02T08:39:03.961427Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-stress\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"sample_data\"],\"project_site\":\"\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2023-01-30T23:10:50.955460Z\",\"report_issues\":\"https://github.com/campaslab/napari-stress/issues\",\"requirements\":[\"numpy (<1.24.0)\",\"vedo (>=2023.4.3)\",\"napari\",\"vispy\",\"matplotlib\",\"dask\",\"distributed\",\"tqdm\",\"scipy\",\"pandas\",\"seaborn\",\"scikit-image\",\"napari-tools-menu (>=0.1.15)\",\"napari-process-points-and-surfaces (>=0.3.0)\",\"aicsimageio\",\"napari-segment-blobs-and-things-with-membranes\",\"mpmath\",\"pyshtools (<=4.10.0)\",\"napari-matplotlib\",\"pygeodesic\"],\"summary\":\"Interactive surface analysis in napari for measuring mechanical stresses in biological tissues\",\"support\":\"https://github.com/campaslab/napari-stress/issues\",\"twitter\":\"\",\"version\":\"0.1.5\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Robert Haase\"}],\"category\":{\"Workflow step\":[\"Object feature extraction\"]},\"category_hierarchy\":{\"Workflow step\":[[\"Object feature extraction\",\"Shape features extraction\"],[\"Object feature extraction\"]]},\"code_repository\":\"https://github.com/haesleinhuepf/napari-skimage-regionprops\",\"description\":\"# napari-skimage-regionprops (nsr)\\r\\n\\r\\n\\r\\n\\r\\n[![License](https://img.shields.io/pypi/l/napari-skimage-regionprops.svg?color=green)](https://github.com/haesleinhuepf/napari-skimage-regionprops/raw/master/LICENSE)\\r\\n\\r\\n[![PyPI](https://img.shields.io/pypi/v/napari-skimage-regionprops.svg?color=green)](https://pypi.org/project/napari-skimage-regionprops)\\r\\n\\r\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-skimage-regionprops.svg?color=green)](https://python.org)\\r\\n\\r\\n[![tests](https://github.com/haesleinhuepf/napari-skimage-regionprops/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-skimage-regionprops/actions)\\r\\n\\r\\n[![codecov](https://codecov.io/gh/haesleinhuepf/napari-skimage-regionprops/branch/master/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-skimage-regionprops)\\r\\n\\r\\n[![Development Status](https://img.shields.io/pypi/status/napari-skimage-regionprops.svg)](https://en.wikipedia.org/wiki/Software_release_life_cycle#Alpha)\\r\\n\\r\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-skimage-regionprops)](https://napari-hub.org/plugins/napari-skimage-regionprops)\\r\\n\\r\\n\\r\\n\\r\\n \\r\\n\\r\\nA [napari] plugin for measuring properties of labeled objects based on [scikit-image]\\r\\n\\r\\n\\r\\n\\r\\n![](https://github.com/haesleinhuepf/napari-skimage-regionprops/raw/master/images/interactive.gif)\\r\\n\\r\\n\\r\\n\\r\\n## Usage: measure region properties\\r\\n\\r\\n\\r\\n\\r\\nFrom the menu `Tools > Measurement > Regionprops (nsr)` you can open a dialog where you can choose an intensity image, a corresponding label image and the features you want to measure:\\r\\n\\r\\n\\r\\n\\r\\n![img.png](https://github.com/haesleinhuepf/napari-skimage-regionprops/raw/master/images/dialog.png)\\r\\n\\r\\n\\r\\n\\r\\nIf you want to interface with the labels and see which table row corresponds to which labeled object, use the label picker and\\r\\n\\r\\nactivate the `show selected` checkbox.\\r\\n\\r\\n\\r\\n\\r\\n![](https://github.com/haesleinhuepf/napari-skimage-regionprops/raw/master/images/interactive.png)\\r\\n\\r\\n\\r\\n\\r\\nIf you closed a table and want to reopen it, you can use the menu `Tools > Measurements > Show table (nsr)` to reopen it. \\r\\n\\r\\nYou just need to select the labels layer the properties are associated with.\\r\\n\\r\\n\\r\\n\\r\\nFor visualizing measurements with different grey values, as parametric images, you can double-click table headers.\\r\\n\\r\\n\\r\\n\\r\\n![img.png](https://github.com/haesleinhuepf/napari-skimage-regionprops/raw/master/images/label_value_visualization.gif)\\r\\n\\r\\n\\r\\n\\r\\n## Usage: measure point intensities\\r\\n\\r\\n\\r\\n\\r\\nAnalogously, also the intensity and coordinates of point layers can be measured using the menu `Tools > Measurement > Measure intensity at point coordinates (nsr)`. \\r\\n\\r\\nAlso these measurements can be visualized by double-clicking table headers:\\r\\n\\r\\n\\r\\n\\r\\n![img.png](measure_point_intensity.png)\\r\\n\\r\\n\\r\\n\\r\\n![img_1.png](measure_point_coordinate.png)\\r\\n\\r\\n\\r\\n\\r\\n## Working with time-lapse and tracking data\\r\\n\\r\\n\\r\\n\\r\\nNote that tables for time-lapse data should include a column named \\\"frame\\\", which indicates which slice in\\r\\n\\r\\ntime the given row refers to. If you want to import your own csv files for time-lapse data make sure to include this column.\\r\\n\\r\\nIf you have tracking data where each column specifies measurements for a track instead of a label at a specific time point,\\r\\n\\r\\nthis column must not be added.\\r\\n\\r\\n\\r\\n\\r\\nIn case you have 2D time-lapse data you need to convert it into a suitable shape using the function: `Tools > Utilities > Convert 3D stack to 2D time-lapse (time-slicer)`,\\r\\n\\r\\nwhich can be found in the [napari time slicer](https://www.napari-hub.org/plugins/napari-time-slicer).\\r\\n\\r\\n\\r\\n\\r\\nLast but not least, make sure that in case of time-lapse data the label image has labels that are subsquently labeled per timepoint.\\r\\n\\r\\nE.g. a dataset where label 5 is missing at timepoint 4 may be visualized incorrectly.\\r\\n\\r\\n\\r\\n\\r\\n## Usage, programmatically\\r\\n\\r\\n\\r\\n\\r\\nYou can also control the tables programmatically. See this \\r\\n\\r\\n[example notebook](https://github.com/haesleinhuepf/napari-skimage-regionprops/blob/master/demo/tables.ipynb) for details on regionprops and\\r\\n\\r\\n[this example notebook](https://github.com/haesleinhuepf/napari-skimage-regionprops/blob/master/demo/measure_points.ipynb) for details on measuring intensity at point coordinates.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n## Features\\r\\n\\r\\nThe user can select categories of features for feature extraction in the user interface. These categories contain measurements from the scikit-image [regionprops list of measurements](https://scikit-image.org/docs/dev/api/skimage.measure.html#skimage.measure.regionprops) library:\\r\\n\\r\\n* size:\\r\\n\\r\\n  * area\\r\\n\\r\\n  * bbox_area\\r\\n\\r\\n  * convex_area\\r\\n\\r\\n  * equivalent_diameter\\r\\n\\r\\n* intensity:\\r\\n\\r\\n  * max_intensity \\r\\n\\r\\n  * mean_intensity\\r\\n\\r\\n  * min_intensity\\r\\n\\r\\n  * standard_deviation_intensity (`extra_properties` implementation using numpy)\\r\\n\\r\\n* perimeter:\\r\\n\\r\\n  * perimeter\\r\\n\\r\\n  * perimeter_crofton\\r\\n\\r\\n* shape\\r\\n\\r\\n  * major_axis_length\\r\\n\\r\\n  * minor_axis_length\\r\\n\\r\\n  * orientation\\r\\n\\r\\n  * solidity\\r\\n\\r\\n  * eccentricity\\r\\n\\r\\n  * extent\\r\\n\\r\\n  * feret_diameter_max\\r\\n\\r\\n  * local_centroid\\r\\n\\r\\n  * roundness as defined for 2D labels [by ImageJ](https://imagej.nih.gov/ij/docs/menus/analyze.html#set)\\r\\n\\r\\n  * circularity as defined for 2D labels  [by ImageJ](https://imagej.nih.gov/ij/docs/menus/analyze.html#set)\\r\\n\\r\\n  * aspect_ratio as defined for 2D labels [by ImageJ](https://imagej.nih.gov/ij/docs/menus/analyze.html#set)\\r\\n\\r\\n* position:\\r\\n\\r\\n  * centroid\\r\\n\\r\\n  * bbox\\r\\n\\r\\n  * weighted_centroid\\r\\n\\r\\n* moments:\\r\\n\\r\\n  * moments\\r\\n\\r\\n  * moments_central\\r\\n\\r\\n  * moments_hu\\r\\n\\r\\n  * moments_normalized\\r\\n\\r\\n\\r\\n\\r\\n \\r\\n\\r\\n\\r\\n\\r\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\\r\\n\\r\\n\\r\\n\\r\\n## See also\\r\\n\\r\\n\\r\\n\\r\\nThere are other napari plugins with similar functionality for extracting features:\\r\\n\\r\\n* [morphometrics](https://www.napari-hub.org/plugins/morphometrics)\\r\\n\\r\\n* [PartSeg](https://www.napari-hub.org/plugins/PartSeg)\\r\\n\\r\\n* [napari-simpleitk-image-processing](https://www.napari-hub.org/plugins/napari-simpleitk-image-processing)\\r\\n\\r\\n* [napari-cupy-image-processing](https://www.napari-hub.org/plugins/napari-cupy-image-processing)\\r\\n\\r\\n* [napari-pyclesperanto-assistant](https://www.napari-hub.org/plugins/napari-pyclesperanto-assistant)\\r\\n\\r\\n\\r\\n\\r\\nFurthermore, there are plugins for postprocessing extracted measurements\\r\\n\\r\\n* [napari-feature-classifier](https://www.napari-hub.org/plugins/napari-feature-classifier)\\r\\n\\r\\n* [napari-clusters-plotter](https://www.napari-hub.org/plugins/napari-clusters-plotter)\\r\\n\\r\\n* [napari-accelerated-pixel-and-object-classification](https://www.napari-hub.org/plugins/napari-accelerated-pixel-and-object-classification)\\r\\n\\r\\n\\r\\n\\r\\n## Installation\\r\\n\\r\\n\\r\\n\\r\\nYou can install `napari-skimage-regionprops` via [pip]:\\r\\n\\r\\n\\r\\n\\r\\n    pip install napari-skimage-regionprops\\r\\n\\r\\n\\r\\n\\r\\nOr if you plan to develop it:\\r\\n\\r\\n\\r\\n\\r\\n    git clone https://github.com/haesleinhuepf/napari-skimage-regionprops\\r\\n\\r\\n    cd napari-skimage-regionprops\\r\\n\\r\\n    pip install -e .\\r\\n\\r\\n\\r\\n\\r\\nIf there is an error message suggesting that git is not installed, run `conda install git`.\\r\\n\\r\\n\\r\\n\\r\\n## Contributing\\r\\n\\r\\n\\r\\n\\r\\nContributions are very welcome. Tests can be run with [tox], please ensure\\r\\n\\r\\nthe coverage at least stays the same before you submit a pull request.\\r\\n\\r\\n\\r\\n\\r\\n## License\\r\\n\\r\\n\\r\\n\\r\\nDistributed under the terms of the [BSD-3] license,\\r\\n\\r\\n\\\"napari-skimage-regionprops\\\" is free and open source software\\r\\n\\r\\n\\r\\n\\r\\n## Issues\\r\\n\\r\\n\\r\\n\\r\\nIf you encounter any problems, please create a thread on [image.sc] along with a detailed description and tag [@haesleinhuepf].\\r\\n\\r\\n\\r\\n\\r\\n[napari]: https://github.com/napari/napari\\r\\n\\r\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\r\\n\\r\\n[@napari]: https://github.com/napari\\r\\n\\r\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\r\\n\\r\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\r\\n\\r\\n[image.sc]: https://image.sc\\r\\n\\r\\n[napari]: https://github.com/napari/napari\\r\\n\\r\\n[tox]: https://tox.readthedocs.io/en/latest/\\r\\n\\r\\n[pip]: https://pypi.org/project/pip/\\r\\n\\r\\n[PyPI]: https://pypi.org/\\r\\n\\r\\n[scikit-image]: https://scikit-image.org/\\r\\n\\r\\n[@haesleinhuepf]: https://twitter.com/haesleinhuepf\\r\\n\\r\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-skimage-regionprops (nsr)\\n\\n\\n\\n\\n\\n\\n\\nA napari plugin for measuring properties of labeled objects based on scikit-image\\n\\nUsage: measure region properties\\nFrom the menu Tools > Measurement > Regionprops (nsr) you can open a dialog where you can choose an intensity image, a corresponding label image and the features you want to measure:\\n\\nIf you want to interface with the labels and see which table row corresponds to which labeled object, use the label picker and\\nactivate the show selected checkbox.\\n\\nIf you closed a table and want to reopen it, you can use the menu Tools > Measurements > Show table (nsr) to reopen it. \\nYou just need to select the labels layer the properties are associated with.\\nFor visualizing measurements with different grey values, as parametric images, you can double-click table headers.\\n\\nUsage: measure point intensities\\nAnalogously, also the intensity and coordinates of point layers can be measured using the menu Tools > Measurement > Measure intensity at point coordinates (nsr). \\nAlso these measurements can be visualized by double-clicking table headers:\\n\\n\\nWorking with time-lapse and tracking data\\nNote that tables for time-lapse data should include a column named \\\"frame\\\", which indicates which slice in\\ntime the given row refers to. If you want to import your own csv files for time-lapse data make sure to include this column.\\nIf you have tracking data where each column specifies measurements for a track instead of a label at a specific time point,\\nthis column must not be added.\\nIn case you have 2D time-lapse data you need to convert it into a suitable shape using the function: Tools > Utilities > Convert 3D stack to 2D time-lapse (time-slicer),\\nwhich can be found in the napari time slicer.\\nLast but not least, make sure that in case of time-lapse data the label image has labels that are subsquently labeled per timepoint.\\nE.g. a dataset where label 5 is missing at timepoint 4 may be visualized incorrectly.\\nUsage, programmatically\\nYou can also control the tables programmatically. See this \\nexample notebook for details on regionprops and\\nthis example notebook for details on measuring intensity at point coordinates.\\nFeatures\\nThe user can select categories of features for feature extraction in the user interface. These categories contain measurements from the scikit-image regionprops list of measurements library:\\n\\n\\nsize:\\n\\n\\narea\\n\\n\\nbbox_area\\n\\n\\nconvex_area\\n\\n\\nequivalent_diameter\\n\\n\\nintensity:\\n\\n\\nmax_intensity \\n\\n\\nmean_intensity\\n\\n\\nmin_intensity\\n\\n\\nstandard_deviation_intensity (extra_properties implementation using numpy)\\n\\n\\nperimeter:\\n\\n\\nperimeter\\n\\n\\nperimeter_crofton\\n\\n\\nshape\\n\\n\\nmajor_axis_length\\n\\n\\nminor_axis_length\\n\\n\\norientation\\n\\n\\nsolidity\\n\\n\\neccentricity\\n\\n\\nextent\\n\\n\\nferet_diameter_max\\n\\n\\nlocal_centroid\\n\\n\\nroundness as defined for 2D labels by ImageJ\\n\\n\\ncircularity as defined for 2D labels  by ImageJ\\n\\n\\naspect_ratio as defined for 2D labels by ImageJ\\n\\n\\nposition:\\n\\n\\ncentroid\\n\\n\\nbbox\\n\\n\\nweighted_centroid\\n\\n\\nmoments:\\n\\n\\nmoments\\n\\n\\nmoments_central\\n\\n\\nmoments_hu\\n\\n\\nmoments_normalized\\n\\n\\nThis napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.\\nSee also\\nThere are other napari plugins with similar functionality for extracting features:\\n\\n\\nmorphometrics\\n\\n\\nPartSeg\\n\\n\\nnapari-simpleitk-image-processing\\n\\n\\nnapari-cupy-image-processing\\n\\n\\nnapari-pyclesperanto-assistant\\n\\n\\nFurthermore, there are plugins for postprocessing extracted measurements\\n\\n\\nnapari-feature-classifier\\n\\n\\nnapari-clusters-plotter\\n\\n\\nnapari-accelerated-pixel-and-object-classification\\n\\n\\nInstallation\\nYou can install napari-skimage-regionprops via pip:\\npip install napari-skimage-regionprops\\n\\nOr if you plan to develop it:\\ngit clone https://github.com/haesleinhuepf/napari-skimage-regionprops\\n\\ncd napari-skimage-regionprops\\n\\npip install -e .\\n\\nIf there is an error message suggesting that git is not installed, run conda install git.\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-skimage-regionprops\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please create a thread on image.sc along with a detailed description and tag @haesleinhuepf.\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"napari-skimage-regionprops\",\"documentation\":\"\",\"first_released\":\"2021-06-07T19:49:01.148199Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-skimage-regionprops\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/haesleinhuepf/napari-skimage-regionprops\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2023-02-02T19:29:34.828429Z\",\"report_issues\":\"\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"scikit-image\",\"napari\",\"pandas\",\"napari-tools-menu (>=0.1.19)\",\"napari-workflows\",\"imageio (!=2.22.1)\"],\"summary\":\"A regionprops table widget plugin for napari\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.8.1\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"ddoncilapop@contractor.chanzuckerberg.com\",\"name\":\"Draga Doncila Pop\"}],\"citations\":{\"APA\":\"Doncila Pop D. (2021). napari Accelerator Grant Demo Plugin (version 2.0.4). DOI: 10.5281/zenodo.FAKE\\n\",\"BibTex\":\"@misc{YourReferenceHere,\\nauthor = {Doncila Pop, Draga},\\ndoi = {10.5281/zenodo.FAKE},\\nmonth = {12},\\ntitle = {napari Accelerator Grant Demo Plugin},\\nyear = {2021}\\n}\\n\",\"RIS\":\"TY  - GEN\\nAU  - Doncila Pop, Draga\\nDA  - 2021-12-15\\nDO  - 10.5281/zenodo.FAKE\\nPY  - 2021\\nTI  - napari Accelerator Grant Demo Plugin\\nER\\n\",\"citation\":\"\\ncff-version: 1.2.0\\nmessage: \\\"Thank you for citing our plugin!\\\"\\nauthors:\\n  - family-names: Doncila Pop\\n    given-names: Draga\\ntitle: \\\"napari Accelerator Grant Demo Plugin\\\"\\nversion: 2.0.4\\ndoi: 10.5281/zenodo.FAKE\\ndate-released: 2021-12-15\\n\"},\"code_repository\":\"https://github.com/DragaDoncila/workshop-demo\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"workshop-demo\"}],\"description\":\"## What is this?\\n\\nThis plugin was created to serve as a semi-meaningful example of a plugin using\\nthe new napari [npe2](https://pypi.org/project/npe2/) architecture.\\n\\nIt provides a reader, a writer and two dock widgets to support opening, processing\\nand writing out [cell tracking challenge](https://celltrackingchallenge.net/) data.\\n\\nWe've provided comments and example tests that can be used as a reference\\nwhen building your own plugin.\\n\\n## Using this plugin\\n\\n### Sample Data\\nYou can download sample data for this plugin from the tracking challenge website. Any 2D+T\\nsequence should work, but this plugin has been tested only with the \\n[Human hepatocarcinoma-derived cells expressing the fusion protein YFP-TIA-1](http://data.celltrackingchallenge.net/training-datasets/Fluo-C2DL-Huh7.zip) \\ndataset.\\n### Reading Data\\nThis plugin's reader is designed for tracking challenge segmentation gold standard ground truth\\ndata conforming to the file format described in the [data specification](https://public.celltrackingchallenge.net/documents/Naming%20and%20file%20content%20conventions.pdf).\\n\\nGround truth data is only provided for a subset of the frames of the entire sequence. This\\nreader will attempt to find the number of frames of the associated sequence in a sister\\ndirectory of the ground truth data directory and open a labels layer with the same number\\nof frames, thus ensuring the labelled data is correctly overlaid onto the original sequence.\\n\\nhttps://user-images.githubusercontent.com/17995243/146114062-36124c05-f44a-488e-8991-f39a702c917f.mov\\n\\n### Segmenting Data\\nOne of the dock widgets provided by this plugin is \\\"Segment by Threshold\\\". The widget\\nallows you to select a 2D+T image layer in the viewer (e.g. any of the sequences in the Human \\nhepatocarcinoma dataset above) and segment it using a selection of scikit-image thresholding functions.\\n\\nThe segmentation is then returned as a `Labels` layer into the viewer.\\n\\nhttps://user-images.githubusercontent.com/17995243/146114088-f6fb645e-8d78-4880-827b-2f0334dad859.mov\\n\\n### Highlighting Segmentation Differences\\nThe second dock widget provided by this plugin allows you to visually compare your segmentation\\nagainst the ground truth data by computing the difference between the two and adding this as a\\nlayer in the napari viewer.\\n\\nTo use this widget, open it from the Plugins menu and select the two layers you wish to compare.\\n\\nhttps://user-images.githubusercontent.com/17995243/146114112-c891723f-8640-4708-8014-c78731fb3396.mov\\n\\n### Writing to Zip\\nFinally, you can save your segmentation to a zip file whose internal directory structure\\nwill closely mimic that of the tracking challenge datasets, so that it may be opened \\nagain in the viewer.\\n\\nTo save your layer, choose File -> Save selected layer(s) with *one* labels layer selected,\\nthen select label zipper from the dropdown choices.\\n\\nhttps://user-images.githubusercontent.com/17995243/146114163-ee886990-979c-4756-97c5-aaf2c39dccde.mov\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"What is this?\\nThis plugin was created to serve as a semi-meaningful example of a plugin using\\nthe new napari npe2 architecture.\\nIt provides a reader, a writer and two dock widgets to support opening, processing\\nand writing out cell tracking challenge data.\\nWe've provided comments and example tests that can be used as a reference\\nwhen building your own plugin.\\nUsing this plugin\\nSample Data\\nYou can download sample data for this plugin from the tracking challenge website. Any 2D+T\\nsequence should work, but this plugin has been tested only with the \\nHuman hepatocarcinoma-derived cells expressing the fusion protein YFP-TIA-1 \\ndataset.\\nReading Data\\nThis plugin's reader is designed for tracking challenge segmentation gold standard ground truth\\ndata conforming to the file format described in the data specification.\\nGround truth data is only provided for a subset of the frames of the entire sequence. This\\nreader will attempt to find the number of frames of the associated sequence in a sister\\ndirectory of the ground truth data directory and open a labels layer with the same number\\nof frames, thus ensuring the labelled data is correctly overlaid onto the original sequence.\\nhttps://user-images.githubusercontent.com/17995243/146114062-36124c05-f44a-488e-8991-f39a702c917f.mov\\nSegmenting Data\\nOne of the dock widgets provided by this plugin is \\\"Segment by Threshold\\\". The widget\\nallows you to select a 2D+T image layer in the viewer (e.g. any of the sequences in the Human \\nhepatocarcinoma dataset above) and segment it using a selection of scikit-image thresholding functions.\\nThe segmentation is then returned as a Labels layer into the viewer.\\nhttps://user-images.githubusercontent.com/17995243/146114088-f6fb645e-8d78-4880-827b-2f0334dad859.mov\\nHighlighting Segmentation Differences\\nThe second dock widget provided by this plugin allows you to visually compare your segmentation\\nagainst the ground truth data by computing the difference between the two and adding this as a\\nlayer in the napari viewer.\\nTo use this widget, open it from the Plugins menu and select the two layers you wish to compare.\\nhttps://user-images.githubusercontent.com/17995243/146114112-c891723f-8640-4708-8014-c78731fb3396.mov\\nWriting to Zip\\nFinally, you can save your segmentation to a zip file whose internal directory structure\\nwill closely mimic that of the tracking challenge datasets, so that it may be opened \\nagain in the viewer.\\nTo save your layer, choose File -> Save selected layer(s) with one labels layer selected,\\nthen select label zipper from the dropdown choices.\\nhttps://user-images.githubusercontent.com/17995243/146114163-ee886990-979c-4756-97c5-aaf2c39dccde.mov\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"workshop demo\",\"documentation\":\"https://github.com/DragaDoncila/workshop-demo#README.md\",\"first_released\":\"2021-12-15T04:03:25.373260Z\",\"license\":\"BSD-3-Clause\",\"name\":\"workshop-demo\",\"npe2\":true,\"operating_system\":[\"Operating System :: MacOS :: MacOS X\",\"Operating System :: POSIX :: Linux\"],\"plugin_types\":[\"reader\",\"writer\",\"widget\"],\"project_site\":\"https://github.com/DragaDoncila/workshop-demo\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-03-10T23:52:12.666134Z\",\"report_issues\":\"https://github.com/DragaDoncila/workshop-demo/issues\",\"requirements\":[\"dask[array]\",\"imagecodecs\",\"napari\",\"napari-plugin-engine (>=0.1.4)\",\"npe2\",\"numpy\",\"scikit-image\",\"tifffile\"],\"summary\":\"A demo napari plugin incorporating reader, writer and dock widget contributions using the new npe2 plugin architecture.\",\"support\":\"https://github.com/DragaDoncila/workshop-demo/issues\",\"twitter\":\"\",\"version\":\"0.0.2\",\"visibility\":\"public\",\"writer_file_extensions\":[\".zip\"],\"writer_save_layers\":[\"labels\"]}",
  "{\"authors\":[{\"name\":\"Elaine Ho\"}],\"code_repository\":\"https://github.com/rosalindfranklininstitute/napari-quoll\",\"conda\":[],\"description\":\"<!-- This file is a placeholder for customizing description of your plugin \\non the napari hub if you wish. The readme file will be used by default if\\nyou wish not to do any customization for the napari hub listing.\\n\\nIf you need some help writing a good description, check out our \\n[guide](https://github.com/chanzuckerberg/napari-hub/wiki/Writing-the-Perfect-Description-for-your-Plugin)\\n-->\\n\\n# Description\\n\\nThis Napari plugin allows you to estimate the local resolution of images using the Fourier Ring Correlation (FRC). \\n\\n## How it works\\n\\nQuoll splits the input image into square tiles of a user specified size. This tile is then split into two sets of sub-image pairs, and the average FRC resolution of these sets is calculated. A calibration factor is applied to match this one-image FRC measurement to the standard two-image FRC resolution.\\n\\n## What was Quoll developed for?\\n\\nQuoll was developed for cryo-electron microscopy images. \\n\\n# Who is this for?\\n\\nAnyone who might want to measure spatial variations in image resolution. See [Quoll](https://github.com/rosalindfranklininstitute/quoll) if you would like to access the command-line interface option for batch-scripting, or if you would like to customise the tool for your needs.\\n\\n# Getting started\\n\\n## Installation\\n\\nIn a terminal, create a new conda environment with Python 3.7 and install the latest version of `napari-quoll`. \\n\\n```\\nconda create -n napari-quoll python=3.7\\nconda activate napari-quoll\\npip install git+https://github.com/rosalindfranklininstitute/napari-quoll.git\\n```\\n\\nLaunch Napari\\n```\\npython -m napari\\n```\\n\\n(If that does not work, run `pip install napari[all]` and try again)\\n\\n## Napari-quoll plugin\\n\\nNapari-quoll should be under the plugins menu at the top.\\n\\nThe menu should pop up on the right. \\n\\n1. Choose the filename for the image you would like to try this with (a test image is included [here](https://github.com/rosalindfranklininstitute/napari-quoll/tree/main/test_data)). \\n2. Specify the pixel size in physical units. The test image pixel size is 3 nm.\\n3. Specify the tile-size in pixel units (how fine the resolution map should be)\\n4. Specify if you would like to save the results as .csv and the filepath for this.\\n5. Click run! \\n\\nOnce complete, the resolution map should be shown in the main window, and summary statistics and a histogram of resolutions should pop up on the bottom right, though these windows might need resizing.\\n\\n# FAQ\\n\\n### 1. What tile size should I use?\\n\\nThe minimum tile size is 128 x 128 pixels, the smaller the tile the finer the resolution map, though if the tile is too small, there might not be any features to calculate resolution on.\\n\\n### 2. How was the one-image FRC calibrated to match the two-image FRC?\\n\\nThe original implementation by [Koho et al](https://github.com/sakoho81/miplib) obtained a calibration function based on a set of pairs of optical microscopy images taken at different pixel sizes. We have repeated this with cryo-electron microscopy images to ensure that our calibration is correct for EM images.\\n\\n# Getting help\\n\\nSubmit an [issue](https://github.com/rosalindfranklininstitute/napari-quoll/issues) :)\\n\\n# Citations/Links\\n\\n### 1. Original implementation of single-image FRC resolution measurement\\n\\nKoho, S. et al. Fourier ring correlation simplifies image restoration in fluorescence microscopy. [Nat. Commun. 10 3103 (2019).](https://www.nature.com/articles/s41467-019-11024-z)\\n\\n### 2. Quoll - adapted implementation for local resolution measurement of cryo-EM images\\nElaine Ho. (2022). rosalindfranklininstitute/quoll: v0.0.1 (v0.0.1). Zenodo. https://doi.org/10.5281/zenodo.6958768\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"\\nDescription\\nThis Napari plugin allows you to estimate the local resolution of images using the Fourier Ring Correlation (FRC). \\nHow it works\\nQuoll splits the input image into square tiles of a user specified size. This tile is then split into two sets of sub-image pairs, and the average FRC resolution of these sets is calculated. A calibration factor is applied to match this one-image FRC measurement to the standard two-image FRC resolution.\\nWhat was Quoll developed for?\\nQuoll was developed for cryo-electron microscopy images. \\nWho is this for?\\nAnyone who might want to measure spatial variations in image resolution. See Quoll if you would like to access the command-line interface option for batch-scripting, or if you would like to customise the tool for your needs.\\nGetting started\\nInstallation\\nIn a terminal, create a new conda environment with Python 3.7 and install the latest version of napari-quoll. \\nconda create -n napari-quoll python=3.7\\nconda activate napari-quoll\\npip install git+https://github.com/rosalindfranklininstitute/napari-quoll.git\\nLaunch Napari\\npython -m napari\\n(If that does not work, run pip install napari[all] and try again)\\nNapari-quoll plugin\\nNapari-quoll should be under the plugins menu at the top.\\nThe menu should pop up on the right. \\n\\nChoose the filename for the image you would like to try this with (a test image is included here). \\nSpecify the pixel size in physical units. The test image pixel size is 3 nm.\\nSpecify the tile-size in pixel units (how fine the resolution map should be)\\nSpecify if you would like to save the results as .csv and the filepath for this.\\nClick run! \\n\\nOnce complete, the resolution map should be shown in the main window, and summary statistics and a histogram of resolutions should pop up on the bottom right, though these windows might need resizing.\\nFAQ\\n1. What tile size should I use?\\nThe minimum tile size is 128 x 128 pixels, the smaller the tile the finer the resolution map, though if the tile is too small, there might not be any features to calculate resolution on.\\n2. How was the one-image FRC calibrated to match the two-image FRC?\\nThe original implementation by Koho et al obtained a calibration function based on a set of pairs of optical microscopy images taken at different pixel sizes. We have repeated this with cryo-electron microscopy images to ensure that our calibration is correct for EM images.\\nGetting help\\nSubmit an issue :)\\nCitations/Links\\n1. Original implementation of single-image FRC resolution measurement\\nKoho, S. et al. Fourier ring correlation simplifies image restoration in fluorescence microscopy. Nat. Commun. 10 3103 (2019).\\n2. Quoll - adapted implementation for local resolution measurement of cryo-EM images\\nElaine Ho. (2022). rosalindfranklininstitute/quoll: v0.0.1 (v0.0.1). Zenodo. https://doi.org/10.5281/zenodo.6958768\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari Quoll\",\"documentation\":\"https://github.com/rosalindfranklininstitute/napari-quoll#README.md\",\"first_released\":\"2022-08-26T15:11:31.357188Z\",\"license\":\"Apache-2.0\",\"name\":\"napari-quoll\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/rosalindfranklininstitute/napari-quoll\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-08-26T15:11:31.357188Z\",\"report_issues\":\"https://github.com/rosalindfranklininstitute/napari-quoll/issues\",\"requirements\":[\"numpy\",\"magicgui\",\"qtpy\",\"quoll\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\"],\"summary\":\"Resolution estimation for electron tomography\",\"support\":\"https://github.com/rosalindfranklininstitute/napari-quoll/issues\",\"twitter\":\"\",\"version\":\"0.0.1\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"meriadec.prigent@gmail.com\",\"name\":\"Sylvain Prigent\"}],\"code_repository\":\"https://github.com/sylvainprigent/napari-sairyscan\",\"conda\":[],\"description\":\"This plugin implements various methods to reconstruct high resolution images for the Airyscan \\nmicroscope raw data \\n\\n## Description\\n\\nAiryscan raw images are confocal images obtained from 32 sub-detectors. Several methods can be used\\nto reconstruct a high resolution image from these 32 sub-detectors images. This plugin implements\\nthe following methods:\\n- **Pseudo-confocal**: creates a pseudo confocal image by summing 7, 19 or 19 detectors\\n- **ISM**: creates a higher resolution image by summing the images from the 32 sub-detectors after \\nco-registering all the images to the central detector. A deconvolution algorithm can be applied in \\npost-processing to gain more resolution\\n- **IFED**: reconstructs a high resolution image by subtracting the outer ring detector to the central \\ndetector. This method can be interpreted as a 'virtual STED'\\n- **ISFED**: reconstructs a high resolution image by combining the co-registered detectors images and the\\nraw detectors images     \\n- **Join deconvolution**: reconstructs a high resolution image by jointly deblurring all 32 detectors \\nwith a variational approach.\\n\\n![Example image](https://raw.githubusercontent.com/sylvainprigent/napari-sairyscan/main/docs/images/screenshot.png)\\n\\n## Intended Audience & Supported Data\\n\\nSupported data are raw images from the Airyscan microscope. These images must be stacks of 32 \\nlayers corresponding to the 32 detectors. The Airyscan reader plugin can open .czi raw files. Data\\ncan also be stored in any format that napari can open. \\n\\n## Quickstart\\n\\n- Open the sample image from the menu *File > Open samples > napari-sairyscan > SAiryscan*\\n\\n![Open image](https://raw.githubusercontent.com/sylvainprigent/napari-sairyscan/main/docs/images/open_sample.png)\\n![Open image](https://raw.githubusercontent.com/sylvainprigent/napari-sairyscan/main/docs/images/samples.png)\\n\\n\\n- Open the SAiryscan plugin from the menu *Plugins > napari-sairyscan: Airyscan reconstruction*\\n\\n![Open image](https://raw.githubusercontent.com/sylvainprigent/napari-sairyscan/main/docs/images/open_plugin.png)\\n![Open image](https://raw.githubusercontent.com/sylvainprigent/napari-sairyscan/main/docs/images/join_deconv_plugin.png)\\n\\n- We select the `join deconvolution` method and run it with the default parameters. Default parameters\\nare optimized for the sample image:\\n\\n![Open image](https://raw.githubusercontent.com/sylvainprigent/napari-sairyscan/main/docs/images/join_deconv_result.png)\\n\\n\\n## Getting Help\\n\\nFor any bug report or feature request please [file an issue]\\n\\n## How to Cite\\n\\nIf you use this plugin please cite the [paper](https://ieeexplore.ieee.org/document/9054640):\\n    \\n    @INPROCEEDINGS{9054640,\\n    author={Prigent, Sylvain and Dutertre, Stephanie and Kervrann, Charles},\\n    booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, \\n    title={Empirical Sure-Guided Microscopy Super-Resolution Image Reconstruction from Confocal Multi-Array Detectors}, \\n    year={2020},\\n    volume={},\\n    number={},\\n    pages={1075-1079},\\n    doi={10.1109/ICASSP40776.2020.9054640}}\\n\\n\\n\\n[file an issue]: https://github.com/sylvainprigent/napari-sairyscan/issues\",\"description_content_type\":\"text/markdown\",\"description_text\":\"This plugin implements various methods to reconstruct high resolution images for the Airyscan \\nmicroscope raw data \\nDescription\\nAiryscan raw images are confocal images obtained from 32 sub-detectors. Several methods can be used\\nto reconstruct a high resolution image from these 32 sub-detectors images. This plugin implements\\nthe following methods:\\n- Pseudo-confocal: creates a pseudo confocal image by summing 7, 19 or 19 detectors\\n- ISM: creates a higher resolution image by summing the images from the 32 sub-detectors after \\nco-registering all the images to the central detector. A deconvolution algorithm can be applied in \\npost-processing to gain more resolution\\n- IFED: reconstructs a high resolution image by subtracting the outer ring detector to the central \\ndetector. This method can be interpreted as a 'virtual STED'\\n- ISFED: reconstructs a high resolution image by combining the co-registered detectors images and the\\nraw detectors images   \\n- Join deconvolution: reconstructs a high resolution image by jointly deblurring all 32 detectors \\nwith a variational approach.\\n\\nIntended Audience & Supported Data\\nSupported data are raw images from the Airyscan microscope. These images must be stacks of 32 \\nlayers corresponding to the 32 detectors. The Airyscan reader plugin can open .czi raw files. Data\\ncan also be stored in any format that napari can open. \\nQuickstart\\n\\nOpen the sample image from the menu File > Open samples > napari-sairyscan > SAiryscan\\n\\n\\n\\n\\nOpen the SAiryscan plugin from the menu Plugins > napari-sairyscan: Airyscan reconstruction\\n\\n\\n\\n\\nWe select the join deconvolution method and run it with the default parameters. Default parameters\\nare optimized for the sample image:\\n\\n\\nGetting Help\\nFor any bug report or feature request please file an issue\\nHow to Cite\\nIf you use this plugin please cite the paper:\\n@INPROCEEDINGS{9054640,\\nauthor={Prigent, Sylvain and Dutertre, Stephanie and Kervrann, Charles},\\nbooktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, \\ntitle={Empirical Sure-Guided Microscopy Super-Resolution Image Reconstruction from Confocal Multi-Array Detectors}, \\nyear={2020},\\nvolume={},\\nnumber={},\\npages={1075-1079},\\ndoi={10.1109/ICASSP40776.2020.9054640}}\\n\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari sairyscan\",\"documentation\":\"https://github.com/sylvainprigent/napari-sairyscan#README.md\",\"first_released\":\"2022-06-03T17:56:24.998564Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-sairyscan\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\",\"widget\",\"sample_data\"],\"project_site\":\"https://github.com/sylvainprigent/napari-sairyscan\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[\"*.czi\"],\"release_date\":\"2022-06-07T10:20:46.204193Z\",\"report_issues\":\"https://github.com/sylvainprigent/napari-sairyscan/issues\",\"requirements\":[\"numpy\",\"magicgui\",\"qtpy\",\"sairyscan (>=0.0.2)\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\"],\"summary\":\"Airyscan image reconstruction\",\"support\":\"https://github.com/sylvainprigent/napari-sairyscan/issues\",\"twitter\":\"\",\"version\":\"0.0.2\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"talley.lambert@gmail.com\",\"name\":\"Talley Lambert\"}],\"code_repository\":\"https://github.com/tlambert03/psfmodels\",\"conda\":[],\"description\":\"# psfmodels\\n\\n[![PyPI](https://img.shields.io/pypi/v/psfmodels.svg?color=green)](https://pypi.org/project/psfmodels)\\n[![Python\\nVersion](https://img.shields.io/pypi/pyversions/psfmodels.svg?color=green)](https://python.org)\\n[![CI](https://github.com/tlambert03/psfmodels/actions/workflows/ci.yml/badge.svg)](https://github.com/tlambert03/psfmodels/actions/workflows/ci.yml)\\n[![codecov](https://codecov.io/gh/tlambert03/psfmodels/branch/main/graph/badge.svg)](https://codecov.io/gh/tlambert03/psfmodels)\\n\\nPython bindings for scalar and vectorial models of the point spread function.\\n\\nOriginal C++ code and MATLAB MEX bindings Copyright &copy; 2006-2013, [Francois\\nAguet](http://www.francoisaguet.net/software.html), distributed under GPL-3.0\\nlicense. Python bindings by Talley Lambert\\n\\nThis package contains three models:\\n\\n1. The vectorial model is described in Auget et al 2009<sup>1</sup>. For more\\ninformation and implementation details, see Francois' Thesis<sup>2</sup>.\\n2. A scalar model, based on Gibson & Lanni<sup>3</sup>.\\n3. A gaussian approximation (both paraxial and non-paraxial), using paramters from Zhang et al (2007)<sup>4</sup>.\\n\\n<small>\\n\\n<sup>1</sup> [F. Aguet et al., (2009) Opt. Express 17(8), pp.\\n6829-6848](https://doi.org/10.1364/OE.17.006829)\\n\\n<sup>2</sup> [F. Aguet. (2009) Super-Resolution Fluorescence Microscopy Based on\\nPhysical Models. Swiss Federal Institute of Technology Lausanne, EPFL Thesis no.\\n4418](http://bigwww.epfl.ch/publications/aguet0903.html)\\n\\n<sup>3</sup> [F. Gibson and F. Lanni (1992) J. Opt. Soc. Am. A, vol. 9, no. 1, pp. 154-166](https://opg.optica.org/josaa/abstract.cfm?uri=josaa-9-1-154)\\n\\n<sup>4</sup> [Zhang et al (2007). Appl Opt\\n. 2007 Apr 1;46(10):1819-29.](https://doi.org/10.1364/AO.46.001819)\\n\\n</small>\\n\\n### see also:\\n\\nFor a different (faster) scalar-based Gibson–Lanni PSF model, see the\\n[MicroscPSF](https://github.com/MicroscPSF) project, based on [Li et al\\n(2017)](https://doi.org/10.1364/JOSAA.34.001029) which has been implemented in\\n[Python](https://github.com/MicroscPSF/MicroscPSF-Py),\\n[MATLAB](https://github.com/MicroscPSF/MicroscPSF-Matlab), and\\n[ImageJ/Java](https://github.com/MicroscPSF/MicroscPSF-ImageJ)\\n\\n## Install\\n\\n```sh\\npip install psfmodels\\n```\\n\\n### from source\\n\\n```sh\\ngit clone https://github.com/tlambert03/PSFmodels.git\\ncd PSFmodels\\npip install -e \\\".[dev]\\\"  # will compile c code via pybind11\\n```\\n\\n## Usage\\n\\nThere are two main functions in `psfmodels`: `vectorial_psf` and `scalar_psf`.\\nAdditionally, each version has a helper function called `vectorial_psf_centered`\\nand `scalar_psf_centered` respectively. The main difference is that the `_psf`\\nfunctions accept a vector of Z positions `zv` (relative to coverslip) at which\\nPSF is calculated. As such, the point source may or may not actually be in the\\ncenter of the rendered volume. The `_psf_centered` variants, by contrast, do\\n_not_ accecpt `zv`, but rather accept `nz` (the number of z planes) and `dz`\\n(the z step size in microns), and always generates an output volume in which the\\npoint source is positioned in the middle of the Z range, with planes equidistant\\nfrom each other. All functions accept an argument `pz`, specifying the position\\nof the point source relative to the coverslip. See additional keyword arguments\\nbelow\\n\\n_Note, all output dimensions (`nx` and `nz`) should be odd._\\n\\n```python\\nimport psfmodels as psfm\\nimport matplotlib.pyplot as plt\\nfrom matplotlib.colors import PowerNorm\\n\\n# generate centered psf with a point source at `pz` microns from coverslip\\n# shape will be (127, 127, 127)\\npsf = psfm.make_psf(127, 127, dxy=0.05, dz=0.05, pz=0)\\nfig, (ax1, ax2) = plt.subplots(1, 2)\\nax1.imshow(psf[nz//2], norm=PowerNorm(gamma=0.4))\\nax2.imshow(psf[:, nx//2], norm=PowerNorm(gamma=0.4))\\nplt.show()\\n```\\n\\n![Image of PSF](fig.png)\\n\\n```python\\n# instead of nz and dz, you can directly specify a vector of z positions\\nimport numpy as np\\n\\n# generate 31 evenly spaced Z positions from -3 to 3 microns\\npsf = psfm.make_psf(np.linspace(-3, 3, 31), nx=127)\\npsf.shape  # (31, 127, 127)\\n```\\n\\n**all** PSF functions accept the following parameters. Units should be provided\\nin microns unless otherwise stated. Python API may change slightly in the\\nfuture.  See function docstrings as well.\\n\\n```\\nnx (int):       XY size of output PSF in pixels, must be odd.\\ndxy (float):    pixel size in sample space (microns) [default: 0.05]\\npz (float):     depth of point source relative to coverslip (in microns) [default: 0]\\nti0 (float):    working distance of the objective (microns) [default: 150.0]\\nni0 (float):    immersion medium refractive index, design value [default: 1.515]\\nni (float):     immersion medium refractive index, experimental value [default: 1.515]\\ntg0 (float):    coverslip thickness, design value (microns) [default: 170.0]\\ntg (float):     coverslip thickness, experimental value (microns) [default: 170.0]\\nng0 (float):    coverslip refractive index, design value [default: 1.515]\\nng (float):     coverslip refractive index, experimental value [default: 1.515]\\nns (float):     sample refractive index [default: 1.47]\\nwvl (float):    emission wavelength (microns) [default: 0.6]\\nNA (float):     numerical aperture [default: 1.4]\\n```\\n\\n## Comparison with other models\\n\\nWhile these models are definitely slower than the one implemented in [Li et al\\n(2017)](https://doi.org/10.1364/JOSAA.34.001029) and\\n[MicroscPSF](https://github.com/MicroscPSF), there are some interesting\\ndifferences between the scalar and vectorial approximations, particularly with\\nhigher NA lenses, non-ideal sample refractive index, and increasing spherical\\naberration with depth from the coverslip.\\n\\nFor an interactive comparison, see the [examples.ipynb](notebooks/examples.ipynb) Jupyter\\nnotebook.\\n\\n## Lightsheet PSF utility function\\n\\nThe `psfmodels.tot_psf()` function provides a quick way to simulate the total\\nsystem PSF (excitation x detection) as might be observed on a light sheet\\nmicroscope (currently, only strictly orthogonal illumination and detection are\\nsupported).  See the [lightsheet.ipynb](notebooks/lightsheet.ipynb) Jupyter notebook for\\nexamples.\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"psfmodels\\n\\n\\n\\n\\nPython bindings for scalar and vectorial models of the point spread function.\\nOriginal C++ code and MATLAB MEX bindings Copyright © 2006-2013, Francois\\nAguet, distributed under GPL-3.0\\nlicense. Python bindings by Talley Lambert\\nThis package contains three models:\\n\\nThe vectorial model is described in Auget et al 20091. For more\\ninformation and implementation details, see Francois' Thesis2.\\nA scalar model, based on Gibson & Lanni3.\\nA gaussian approximation (both paraxial and non-paraxial), using paramters from Zhang et al (2007)4.\\n\\n\\n1 F. Aguet et al., (2009) Opt. Express 17(8), pp.\\n6829-6848\\n2 F. Aguet. (2009) Super-Resolution Fluorescence Microscopy Based on\\nPhysical Models. Swiss Federal Institute of Technology Lausanne, EPFL Thesis no.\\n4418\\n3 F. Gibson and F. Lanni (1992) J. Opt. Soc. Am. A, vol. 9, no. 1, pp. 154-166\\n4 Zhang et al (2007). Appl Opt\\n. 2007 Apr 1;46(10):1819-29.\\n\\nsee also:\\nFor a different (faster) scalar-based Gibson–Lanni PSF model, see the\\nMicroscPSF project, based on Li et al\\n(2017) which has been implemented in\\nPython,\\nMATLAB, and\\nImageJ/Java\\nInstall\\nsh\\npip install psfmodels\\nfrom source\\nsh\\ngit clone https://github.com/tlambert03/PSFmodels.git\\ncd PSFmodels\\npip install -e \\\".[dev]\\\"  # will compile c code via pybind11\\nUsage\\nThere are two main functions in psfmodels: vectorial_psf and scalar_psf.\\nAdditionally, each version has a helper function called vectorial_psf_centered\\nand scalar_psf_centered respectively. The main difference is that the _psf\\nfunctions accept a vector of Z positions zv (relative to coverslip) at which\\nPSF is calculated. As such, the point source may or may not actually be in the\\ncenter of the rendered volume. The _psf_centered variants, by contrast, do\\nnot accecpt zv, but rather accept nz (the number of z planes) and dz\\n(the z step size in microns), and always generates an output volume in which the\\npoint source is positioned in the middle of the Z range, with planes equidistant\\nfrom each other. All functions accept an argument pz, specifying the position\\nof the point source relative to the coverslip. See additional keyword arguments\\nbelow\\nNote, all output dimensions (nx and nz) should be odd.\\n```python\\nimport psfmodels as psfm\\nimport matplotlib.pyplot as plt\\nfrom matplotlib.colors import PowerNorm\\ngenerate centered psf with a point source at pz microns from coverslip\\nshape will be (127, 127, 127)\\npsf = psfm.make_psf(127, 127, dxy=0.05, dz=0.05, pz=0)\\nfig, (ax1, ax2) = plt.subplots(1, 2)\\nax1.imshow(psf[nz//2], norm=PowerNorm(gamma=0.4))\\nax2.imshow(psf[:, nx//2], norm=PowerNorm(gamma=0.4))\\nplt.show()\\n```\\n\\n```python\\ninstead of nz and dz, you can directly specify a vector of z positions\\nimport numpy as np\\ngenerate 31 evenly spaced Z positions from -3 to 3 microns\\npsf = psfm.make_psf(np.linspace(-3, 3, 31), nx=127)\\npsf.shape  # (31, 127, 127)\\n```\\nall PSF functions accept the following parameters. Units should be provided\\nin microns unless otherwise stated. Python API may change slightly in the\\nfuture.  See function docstrings as well.\\nnx (int):       XY size of output PSF in pixels, must be odd.\\ndxy (float):    pixel size in sample space (microns) [default: 0.05]\\npz (float):     depth of point source relative to coverslip (in microns) [default: 0]\\nti0 (float):    working distance of the objective (microns) [default: 150.0]\\nni0 (float):    immersion medium refractive index, design value [default: 1.515]\\nni (float):     immersion medium refractive index, experimental value [default: 1.515]\\ntg0 (float):    coverslip thickness, design value (microns) [default: 170.0]\\ntg (float):     coverslip thickness, experimental value (microns) [default: 170.0]\\nng0 (float):    coverslip refractive index, design value [default: 1.515]\\nng (float):     coverslip refractive index, experimental value [default: 1.515]\\nns (float):     sample refractive index [default: 1.47]\\nwvl (float):    emission wavelength (microns) [default: 0.6]\\nNA (float):     numerical aperture [default: 1.4]\\nComparison with other models\\nWhile these models are definitely slower than the one implemented in Li et al\\n(2017) and\\nMicroscPSF, there are some interesting\\ndifferences between the scalar and vectorial approximations, particularly with\\nhigher NA lenses, non-ideal sample refractive index, and increasing spherical\\naberration with depth from the coverslip.\\nFor an interactive comparison, see the examples.ipynb Jupyter\\nnotebook.\\nLightsheet PSF utility function\\nThe psfmodels.tot_psf() function provides a quick way to simulate the total\\nsystem PSF (excitation x detection) as might be observed on a light sheet\\nmicroscope (currently, only strictly orthogonal illumination and detection are\\nsupported).  See the lightsheet.ipynb Jupyter notebook for\\nexamples.\",\"development_status\":[\"Development Status :: 3 - Alpha\"],\"display_name\":\"psfmodels\",\"documentation\":\"\",\"first_released\":\"2019-09-15T18:38:17.604354Z\",\"license\":\"GPL-3.0\",\"name\":\"psfmodels\",\"npe2\":true,\"operating_system\":[],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/tlambert03/psfmodels\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-04-23T15:54:59.302073Z\",\"report_issues\":\"\",\"requirements\":[\"numpy\",\"scipy (>=0.14.0)\",\"typing-extensions\",\"black ; extra == 'dev'\",\"flake8 ; extra == 'dev'\",\"flake8-docstrings ; extra == 'dev'\",\"flake8-typing-imports ; extra == 'dev'\",\"ipython ; extra == 'dev'\",\"isort ; extra == 'dev'\",\"mypy ; extra == 'dev'\",\"pre-commit ; extra == 'dev'\",\"pydocstyle ; extra == 'dev'\",\"pytest ; extra == 'dev'\",\"pytest-cov ; extra == 'dev'\",\"tox ; extra == 'dev'\",\"tox-conda ; extra == 'dev'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"tox ; extra == 'testing'\",\"tox-conda ; extra == 'testing'\",\"magicgui ; (platform_system != \\\"Linux\\\") and extra == 'testing'\",\"pyside2 ; (platform_system != \\\"Linux\\\") and extra == 'testing'\",\"qtpy ; (platform_system != \\\"Linux\\\") and extra == 'testing'\"],\"summary\":\"Scalar and vectorial models of the microscope point spread function (PSF).\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.3.2\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Nicolas Pielawski\"}],\"code_repository\":null,\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-tissuumaps\"}],\"description\":\"# 🏝 napari-tissuumaps 🧫\\n\\n[![License MIT](https://img.shields.io/pypi/l/napari-tissuumaps.svg?color=green)](https://github.com/npielawski/napari-tissuumaps/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-tissuumaps.svg?color=green)](https://pypi.org/project/napari-tissuumaps)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-tissuumaps.svg?color=green)](https://python.org)\\n[![tests](https://github.com/npielawski/napari-tissuumaps/workflows/tests/badge.svg)](https://github.com/npielawski/napari-tissuumaps/actions)\\n[![codecov](https://codecov.io/gh/npielawski/napari-tissuumaps/branch/main/graph/badge.svg)](https://codecov.io/gh/npielawski/napari-tissuumaps)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-tissuumaps)](https://napari-hub.org/plugins/napari-tissuumaps)\\n\\nA plugin to export Napari projects to [TissUUmaps](https://tissuumaps.research.it.uu.se/).\\n\\n----------------------------------\\n\\nThis plugins adds a new writer to [Napari] to export projects to [TissUUmaps](https://github.com/TissUUmaps/TissUUmaps). Exported projects can then be open on the browser or on a standalone GUI with [TissUUmaps](https://github.com/TissUUmaps/TissUUmaps). More information and demonstrations are available on the [TissUUmaps webpage](https://tissuumaps.research.it.uu.se/).\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/stable/plugins/index.html\\n-->\\n\\n## 🚀 Features\\n\\n<p align=\\\"center\\\">\\n  <img src=\\\"images/screenshot.jpg\\\" alt=\\\"Demonstration of a project exported from Napari to TissUUmaps.\\\" width=\\\"500\\\" />\\n</p>\\n\\nThe plugin now supports:\\n\\n* Exporting images\\n* Exporting labels\\n* Exporting points\\n* Exporting shapes, including:\\n    * Polygons\\n    * Rectangles\\n    * Lines\\n    * Paths\\n    * Ellipses\\n\\nThe plugin exports the right color for the points, shapes and labels and also saves the visibility/opacity of each layers. The shapes are exported in the GeoJSON format, the points in CSV files, and images as TIFFs.\\n\\n## 📺 Installation\\n\\nYou can install `napari-tissuumaps` via [pip]:\\n\\n    pip install napari-tissuumaps\\n\\nYou can also install `napari-tissumaps` via [napari]:\\n\\nIn Napari, access the menubar, Plugins > Install/Uninstall Plugins.\\nSearch for napari-tissuumaps in the list and choose install, or type\\n`napari-tissuumaps` in the \\\"install by name/url, or drop file...\\\" text area and choose\\ninstall.\\n\\n## ⛏ Usage\\n\\nTo export a project for TissUUmaps, access the menubar, File > Save All Layers... and\\ntype in a filename. Choose the `.tmap` extension in the dropdown and click on the Save\\nbutton, It will create a folder containing all the necessary files for TissUUmaps.\\n\\n## 👩‍💻 Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## ⚖️ License\\n\\nDistributed under the terms of the [MIT] license,\\n\\\"napari-tissuumaps\\\" is free and open source software\\n\\n## 🚒 Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"🏝 napari-tissuumaps 🧫\\n\\n\\n\\n\\n\\n\\nA plugin to export Napari projects to TissUUmaps.\\n\\nThis plugins adds a new writer to Napari to export projects to TissUUmaps. Exported projects can then be open on the browser or on a standalone GUI with TissUUmaps. More information and demonstrations are available on the TissUUmaps webpage.\\n\\n🚀 Features\\n\\n\\n\\nThe plugin now supports:\\n\\nExporting images\\nExporting labels\\nExporting points\\nExporting shapes, including:\\nPolygons\\nRectangles\\nLines\\nPaths\\nEllipses\\n\\n\\n\\nThe plugin exports the right color for the points, shapes and labels and also saves the visibility/opacity of each layers. The shapes are exported in the GeoJSON format, the points in CSV files, and images as TIFFs.\\n📺 Installation\\nYou can install napari-tissuumaps via pip:\\npip install napari-tissuumaps\\n\\nYou can also install napari-tissumaps via napari:\\nIn Napari, access the menubar, Plugins > Install/Uninstall Plugins.\\nSearch for napari-tissuumaps in the list and choose install, or type\\nnapari-tissuumaps in the \\\"install by name/url, or drop file...\\\" text area and choose\\ninstall.\\n⛏ Usage\\nTo export a project for TissUUmaps, access the menubar, File > Save All Layers... and\\ntype in a filename. Choose the .tmap extension in the dropdown and click on the Save\\nbutton, It will create a folder containing all the necessary files for TissUUmaps.\\n👩‍💻 Contributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n⚖️ License\\nDistributed under the terms of the MIT license,\\n\\\"napari-tissuumaps\\\" is free and open source software\\n🚒 Issues\\nIf you encounter any problems, please [file an issue] along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"Napari TissUUmaps\",\"documentation\":\"\",\"first_released\":\"2021-09-02T12:25:02.194075Z\",\"license\":\"MIT\",\"name\":\"napari-tissuumaps\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"writer\"],\"project_site\":\"\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-08-05T09:38:47.546257Z\",\"report_issues\":\"\",\"requirements\":[\"numpy\",\"napari ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"tox ; extra == 'testing'\"],\"summary\":\"A plugin to export Napari projects to TissUUmaps.\",\"support\":\"\",\"twitter\":\"\",\"version\":\"1.1.2\",\"writer_file_extensions\":[\".tmap\"],\"writer_save_layers\":[\"labels\",\"points\",\"shapes\",\"image\"]}",
  "{\"authors\":[{\"name\":\"Robert Haase\"},{\"name\":\"Talley Lambert\"}],\"category\":{\"Supported data\":[\"2D\",\"3D\"],\"Workflow step\":[\"Image Segmentation\",\"Image correction\",\"Image reconstruction\",\"Image enhancement\",\"Object feature extraction\",\"Morphological operations\",\"Image feature detection\",\"Visualization\"]},\"category_hierarchy\":{\"Supported data\":[[\"2D\"],[\"3D\"]],\"Workflow step\":[[\"Image Segmentation\",\"Cell segmentation\"],[\"Image Segmentation\",\"Connected-component analysis\"],[\"Image correction\",\"Illumination correction\"],[\"Image correction\"],[\"Image reconstruction\",\"Image denoising\"],[\"Image enhancement\",\"Image denoising\"],[\"Image Segmentation\",\"Image thresholding\"],[\"Image Segmentation\"],[\"Object feature extraction\"],[\"Image enhancement\",\"Smoothing\"],[\"Morphological operations\"],[\"Image feature detection\"],[\"Visualization\",\"Image visualisation\",\"Image projection\"],[\"Image feature detection\",\"Spot detection\"],[\"Image Segmentation\",\"Semi-automatic segmentation\"],[\"Image Segmentation\",\"Overlap analysis\"],[\"Object feature extraction\",\"Overlap analysis\"],[\"Image feature detection\",\"Edge detection\"],[\"Morphological operations\",\"Top-hat transform\"],[\"Morphological operations\",\"Closing\"],[\"Morphological operations\",\"Dilation\"],[\"Morphological operations\",\"Opening\"],[\"Morphological operations\",\"Erosion\"],[\"Image enhancement\"]]},\"code_repository\":\"https://github.com/clEsperanto/napari_pyclesperanto_assistant\",\"description\":\"# napari-pyclesperanto-assistant\\r\\n[![Image.sc forum](https://img.shields.io/badge/dynamic/json.svg?label=forum&url=https%3A%2F%2Fforum.image.sc%2Ftag%2Fclesperanto.json&query=%24.topic_list.tags.0.topic_count&colorB=brightgreen&suffix=%20topics&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA4AAAAOCAYAAAAfSC3RAAABPklEQVR42m3SyyqFURTA8Y2BER0TDyExZ+aSPIKUlPIITFzKeQWXwhBlQrmFgUzMMFLKZeguBu5y+//17dP3nc5vuPdee6299gohUYYaDGOyyACq4JmQVoFujOMR77hNfOAGM+hBOQqB9TjHD36xhAa04RCuuXeKOvwHVWIKL9jCK2bRiV284QgL8MwEjAneeo9VNOEaBhzALGtoRy02cIcWhE34jj5YxgW+E5Z4iTPkMYpPLCNY3hdOYEfNbKYdmNngZ1jyEzw7h7AIb3fRTQ95OAZ6yQpGYHMMtOTgouktYwxuXsHgWLLl+4x++Kx1FJrjLTagA77bTPvYgw1rRqY56e+w7GNYsqX6JfPwi7aR+Y5SA+BXtKIRfkfJAYgj14tpOF6+I46c4/cAM3UhM3JxyKsxiOIhH0IO6SH/A1Kb1WBeUjbkAAAAAElFTkSuQmCC)](https://forum.image.sc/tag/clesperanto)\\r\\n[![website](https://img.shields.io/website?url=http%3A%2F%2Fclesperanto.net)](http://clesperanto.net)\\r\\n[![License](https://img.shields.io/pypi/l/napari-pyclesperanto-assistant.svg?color=green)](https://github.com/clesperanto/napari-pyclesperanto-assistant/raw/master/LICENSE)\\r\\n[![PyPI](https://img.shields.io/pypi/v/napari-pyclesperanto-assistant.svg?color=green)](https://pypi.org/project/napari-pyclesperanto-assistant)\\r\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-pyclesperanto-assistant.svg?color=green)](https://python.org)\\r\\n[![tests](https://github.com/clesperanto/napari_pyclesperanto_assistant/workflows/tests/badge.svg)](https://github.com/clesperanto/napari_pyclesperanto_assistant/actions)\\r\\n[![codecov](https://codecov.io/gh/clesperanto/napari_pyclesperanto_assistant/branch/master/graph/badge.svg)](https://codecov.io/gh/clesperanto/napari_pyclesperanto_assistant)\\r\\n[![Development Status](https://img.shields.io/pypi/status/napari_pyclesperanto_assistant.svg)](https://en.wikipedia.org/wiki/Software_release_life_cycle#Alpha)\\r\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-pyclesperanto-assistant)](https://napari-hub.org/plugins/napari-pyclesperanto-assistant)\\r\\n[![DOI](https://zenodo.org/badge/322312181.svg)](https://zenodo.org/badge/latestdoi/322312181)\\r\\n\\r\\nThe py-clEsperanto-assistant is a yet experimental [napari](https://github.com/napari/napari) plugin for building GPU-accelerated image processing workflows. \\r\\nIt is part of the [clEsperanto](http://clesperanto.net) project and thus, aims at removing programming language related barriers between image processing ecosystems in the life sciences. \\r\\nIt uses [pyclesperanto](https://github.com/clEsperanto/pyclesperanto_prototype) and with that [pyopencl](https://documen.tician.de/pyopencl/) as backend for processing images.\\r\\n\\r\\nThis napari plugin adds some menu entries to the Tools menu. You can recognize them with their suffix `(clEsperanto)` in brackets.\\r\\nFurthermore, it can be used from the [napari-assistant](https://www.napari-hub.org/plugins/napari-assistant) graphical user interface. \\r\\nTherefore, just click the menu `Tools > Utilities > Assistant (na)` or run `naparia` from the command line.\\r\\n\\r\\n![](https://github.com/clEsperanto/napari_pyclesperanto_assistant/raw/master/docs/images/virtual_4d_support1.gif)\\r\\n\\r\\n## Usage\\r\\n\\r\\n### Start up the assistant\\r\\nStart up napari, e.g. from the command line:\\r\\n```\\r\\nnapari\\r\\n```\\r\\n\\r\\nLoad example data, e.g. from the menu `File > Open Samples > clEsperanto > CalibZAPWfixed` and \\r\\nstart the assistant from the menu `Tools > Utilities > Assistant (na)`.\\r\\n\\r\\n![](https://github.com/clEsperanto/napari_pyclesperanto_assistant/raw/master/docs/images/screenshot1.png)\\r\\n\\r\\nIn case of two dimensional timelapse data, an initial conversion step might be necessary depending on your data source. \\r\\nClick the menu `Tools > Utilities > Convert to 2d timelapse`. In the dialog, select the dataset and click ok. \\r\\nYou can delete the original dataset afterwards:\\r\\n\\r\\n![](https://github.com/clEsperanto/napari_pyclesperanto_assistant/raw/master/docs/images/screenshot1a.png)\\r\\n\\r\\n### Set up a workflow\\r\\n\\r\\nChoose categories of operations in the top right panel, for example start with denoising using a Gaussian Blur with sigma 1 in x and y.\\r\\n\\r\\n![](https://github.com/clEsperanto/napari_pyclesperanto_assistant/raw/master/docs/images/screenshot2.png)\\r\\n\\r\\nContinue with background removal using the top-hat filter with radius 5 in x and y.\\r\\n\\r\\n![](https://github.com/clEsperanto/napari_pyclesperanto_assistant/raw/master/docs/images/screenshot2a.png)\\r\\n\\r\\nFor labeling the objects, use [Voronoi-Otsu-Labeling](https://nbviewer.jupyter.org/github/clEsperanto/pyclesperanto_prototype/blob/master/demo/segmentation/voronoi_otsu_labeling.ipynb) with both sigma parameters set to 2.\\r\\n\\r\\n![](https://github.com/clEsperanto/napari_pyclesperanto_assistant/raw/master/docs/images/screenshot2b.png)\\r\\n\\r\\nThe labeled objects can be extended using a Voronoi diagram to derive a estimations of cell boundaries.\\r\\n\\r\\n![](https://github.com/clEsperanto/napari_pyclesperanto_assistant/raw/master/docs/images/screenshot2c.png)\\r\\n\\r\\nYou can then configure napari to show the label boundaries on top of the original image:\\r\\n\\r\\n![](https://github.com/clEsperanto/napari_pyclesperanto_assistant/raw/master/docs/images/screenshot2d.png)\\r\\n\\r\\nWhen your workflow is set up, click the play button below your dataset:\\r\\n\\r\\n![](https://github.com/clEsperanto/napari_pyclesperanto_assistant/raw/master/docs/images/timelapse_2d.gif)\\r\\n\\r\\n### Neighbor statistics\\r\\n\\r\\nWhen working with 2D or 3D data you can analyze measurements in relationship with their neighbors. \\r\\nFor example, you can measure the area of blobs as shown in the example shown below using the menu \\r\\n`Tools > Measurements > Statistics of labeled pixels (clesperant)` and visualize it as `area` image by double-clicking on the table column (1).\\r\\nAdditionally, you can measure the maximum area of the 6 nearest neighbors using the menu `Tools > Measurments > Neighborhood statistics of measurements`.\\r\\nThe new column will then be called \\\"max_nn6_area...\\\" (2). When visualizing such parametric images next by each other, it is recommended to use\\r\\n[napari-brightness-contrast](https://www.napari-hub.org/plugins/napari-brightness-contrast) and visualize the same intensity range to see differences correctly.\\r\\n\\r\\n![](https://github.com/clEsperanto/napari_pyclesperanto_assistant/raw/master/docs/images/neighbor_statistics.png)\\r\\n\\r\\n### Code generation\\r\\nYou can also export your workflow as Python/Jython code or as notebook. See the [napari-assistant documentation](https://www.napari-hub.org/plugins/napari-assistant) for details.\\r\\n\\r\\n## Features\\r\\n[pyclesperanto](https://github.com/clEsperanto/pyclesperanto_prototype) offers various possibilities for processing images. It comes from developers who work in life sciences and thus, it may be focused towards processing two- and three-dimensional microscopy image data showing cells and tissues. A selection of pyclesperanto's functionality is available via the assistant user interface. Typical workflows which can be built with this assistant include\\r\\n* image filtering\\r\\n  * denoising / noise reduction (mean, median, Gaussian blur)\\r\\n  * background subtraction for uneven illumination or out-of-focus light (bottom-hat, top-hat, subtract Gaussian background)\\r\\n  * grey value morphology (local minimum, maximum. variance)\\r\\n  * gamma correction\\r\\n  * Laplace operator\\r\\n  * Sobel operator\\r\\n* combining images\\r\\n  * masking\\r\\n  * image math (adding, subtracting, multiplying, dividing images) \\r\\n  * absolute / squared difference\\r\\n* image transformations\\r\\n  * translation\\r\\n  * rotation\\r\\n  * scale\\r\\n  * reduce stack  \\r\\n  * sub-stacks\\r\\n* image projections\\r\\n  * minimum / mean / maximum / sum / standard deviation projections\\r\\n* image segmentation\\r\\n  * binarization (thresholding, local maxima detection)\\r\\n  * labeling\\r\\n  * regionalization\\r\\n  * instance segmentation\\r\\n  * semantic segmentation\\r\\n  * detect label edges\\r\\n  * label spots\\r\\n  * connected component labeling\\r\\n  * Voronoi-Otsu-labeling\\r\\n* post-processing of binary images\\r\\n  * dilation\\r\\n  * erosion\\r\\n  * binary opening\\r\\n  * binary closing \\r\\n  * binary and / or / xor\\r\\n* post-processing of label images\\r\\n  * dilation (expansion) of labels\\r\\n  * extend labels via Voronoi\\r\\n  * exclude labels on edges\\r\\n  * exclude labels within / out of size / value range\\r\\n  * merge touching labels\\r\\n* parametric maps\\r\\n  * proximal / touching neighbor count\\r\\n  * distance measurements to touching / proximal / n-nearest neighbors\\r\\n  * pixel count map\\r\\n  * mean / maximum / extension ratio map\\r\\n* label measurements / post processing of parametric maps\\r\\n  * minimum / mean / maximum / standard deviation intensity maps\\r\\n  * minimum / mean / maximum / standard deviation of touching / n-nearest / neighbors\\r\\n* neighbor meshes\\r\\n  * touching neighbors\\r\\n  * n-nearest neighbors\\r\\n  * proximal neighbors\\r\\n  * distance meshes\\r\\n* measurements based on label images\\r\\n  * bounding box 2D / 3D\\r\\n  * minimum / mean / maximum / sum / standard deviation intensity\\r\\n  * center of mass\\r\\n  * centroid\\r\\n  * mean / maximum distance to centroid (and extension ratio shape descriptor)\\r\\n  * mean / maximum distance to center of mass (and extension ratio shape descriptor)\\r\\n  * statistics of neighbors (See related [publication](https://www.frontiersin.org/articles/10.3389/fcomp.2021.774396/full))\\r\\n* code export\\r\\n  * python / Fiji-compatible jython\\r\\n  * python jupyter notebooks\\r\\n* pyclesperanto scripting\\r\\n  * cell segmentation\\r\\n  * cell counting\\r\\n  * cell differentiation\\r\\n  * tissue classification\\r\\n\\r\\n## Installation\\r\\n\\r\\nIt is recommended to install the assistant using conda. If you have never used conda before, it is recommended to read \\r\\n[this blog post](https://biapol.github.io/blog/johannes_mueller/anaconda_getting_started/) first. \\r\\n\\r\\n```shell\\r\\nconda create --name cle_39 python=3.9 napari-pyclesperanto-assistant\\r\\nconda activate cle_39\\r\\n```\\r\\n\\r\\nMac-users please also install this:\\r\\n\\r\\n    conda install -c conda-forge ocl_icd_wrapper_apple\\r\\n    \\r\\nLinux users please also install this:\\r\\n    \\r\\n    conda install -c conda-forge ocl-icd-system\\r\\n\\r\\nYou can then start the napari-assistant using this command:\\r\\n\\r\\n```\\r\\nnaparia\\r\\n```\\r\\n\\r\\n\\r\\n## Feedback and contributions welcome!\\r\\nclEsperanto is developed in the open because we believe in the open source community. See our [community guidelines](https://clij.github.io/clij2-docs/community_guidelines). Feel free to drop feedback as [github issue](https://github.com/clEsperanto/pyclesperanto_prototype/issues) or via [image.sc](https://image.sc)\\r\\n\\r\\n## Acknowledgements\\r\\nThis project was supported by the Deutsche Forschungsgemeinschaft under Germanyâ€™s Excellence Strategy â€“ EXC2068 - Cluster of Excellence \\\"Physics of Life\\\" of TU Dresden.\\r\\nThis project has been made possible in part by grant number [2021-240341 (Napari plugin accelerator grant)](https://chanzuckerberg.com/science/programs-resources/imaging/napari/improving-image-processing/) from the Chan Zuckerberg Initiative DAF, an advised fund of the Silicon Valley Community Foundation.\\r\\n\\r\\n[Imprint](https://clesperanto.github.io/imprint)\\r\\n\\r\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-pyclesperanto-assistant\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nThe py-clEsperanto-assistant is a yet experimental napari plugin for building GPU-accelerated image processing workflows. \\nIt is part of the clEsperanto project and thus, aims at removing programming language related barriers between image processing ecosystems in the life sciences. \\nIt uses pyclesperanto and with that pyopencl as backend for processing images.\\nThis napari plugin adds some menu entries to the Tools menu. You can recognize them with their suffix (clEsperanto) in brackets.\\nFurthermore, it can be used from the napari-assistant graphical user interface. \\nTherefore, just click the menu Tools > Utilities > Assistant (na) or run naparia from the command line.\\n\\nUsage\\nStart up the assistant\\nStart up napari, e.g. from the command line:\\nnapari\\nLoad example data, e.g. from the menu File > Open Samples > clEsperanto > CalibZAPWfixed and \\nstart the assistant from the menu Tools > Utilities > Assistant (na).\\n\\nIn case of two dimensional timelapse data, an initial conversion step might be necessary depending on your data source. \\nClick the menu Tools > Utilities > Convert to 2d timelapse. In the dialog, select the dataset and click ok. \\nYou can delete the original dataset afterwards:\\n\\nSet up a workflow\\nChoose categories of operations in the top right panel, for example start with denoising using a Gaussian Blur with sigma 1 in x and y.\\n\\nContinue with background removal using the top-hat filter with radius 5 in x and y.\\n\\nFor labeling the objects, use Voronoi-Otsu-Labeling with both sigma parameters set to 2.\\n\\nThe labeled objects can be extended using a Voronoi diagram to derive a estimations of cell boundaries.\\n\\nYou can then configure napari to show the label boundaries on top of the original image:\\n\\nWhen your workflow is set up, click the play button below your dataset:\\n\\nNeighbor statistics\\nWhen working with 2D or 3D data you can analyze measurements in relationship with their neighbors. \\nFor example, you can measure the area of blobs as shown in the example shown below using the menu \\nTools > Measurements > Statistics of labeled pixels (clesperant) and visualize it as area image by double-clicking on the table column (1).\\nAdditionally, you can measure the maximum area of the 6 nearest neighbors using the menu Tools > Measurments > Neighborhood statistics of measurements.\\nThe new column will then be called \\\"max_nn6_area...\\\" (2). When visualizing such parametric images next by each other, it is recommended to use\\nnapari-brightness-contrast and visualize the same intensity range to see differences correctly.\\n\\nCode generation\\nYou can also export your workflow as Python/Jython code or as notebook. See the napari-assistant documentation for details.\\nFeatures\\npyclesperanto offers various possibilities for processing images. It comes from developers who work in life sciences and thus, it may be focused towards processing two- and three-dimensional microscopy image data showing cells and tissues. A selection of pyclesperanto's functionality is available via the assistant user interface. Typical workflows which can be built with this assistant include\\n* image filtering\\n  * denoising / noise reduction (mean, median, Gaussian blur)\\n  * background subtraction for uneven illumination or out-of-focus light (bottom-hat, top-hat, subtract Gaussian background)\\n  * grey value morphology (local minimum, maximum. variance)\\n  * gamma correction\\n  * Laplace operator\\n  * Sobel operator\\n* combining images\\n  * masking\\n  * image math (adding, subtracting, multiplying, dividing images) \\n  * absolute / squared difference\\n* image transformations\\n  * translation\\n  * rotation\\n  * scale\\n  * reduce stack\\n  * sub-stacks\\n* image projections\\n  * minimum / mean / maximum / sum / standard deviation projections\\n* image segmentation\\n  * binarization (thresholding, local maxima detection)\\n  * labeling\\n  * regionalization\\n  * instance segmentation\\n  * semantic segmentation\\n  * detect label edges\\n  * label spots\\n  * connected component labeling\\n  * Voronoi-Otsu-labeling\\n* post-processing of binary images\\n  * dilation\\n  * erosion\\n  * binary opening\\n  * binary closing \\n  * binary and / or / xor\\n* post-processing of label images\\n  * dilation (expansion) of labels\\n  * extend labels via Voronoi\\n  * exclude labels on edges\\n  * exclude labels within / out of size / value range\\n  * merge touching labels\\n* parametric maps\\n  * proximal / touching neighbor count\\n  * distance measurements to touching / proximal / n-nearest neighbors\\n  * pixel count map\\n  * mean / maximum / extension ratio map\\n* label measurements / post processing of parametric maps\\n  * minimum / mean / maximum / standard deviation intensity maps\\n  * minimum / mean / maximum / standard deviation of touching / n-nearest / neighbors\\n* neighbor meshes\\n  * touching neighbors\\n  * n-nearest neighbors\\n  * proximal neighbors\\n  * distance meshes\\n* measurements based on label images\\n  * bounding box 2D / 3D\\n  * minimum / mean / maximum / sum / standard deviation intensity\\n  * center of mass\\n  * centroid\\n  * mean / maximum distance to centroid (and extension ratio shape descriptor)\\n  * mean / maximum distance to center of mass (and extension ratio shape descriptor)\\n  * statistics of neighbors (See related publication)\\n* code export\\n  * python / Fiji-compatible jython\\n  * python jupyter notebooks\\n* pyclesperanto scripting\\n  * cell segmentation\\n  * cell counting\\n  * cell differentiation\\n  * tissue classification\\nInstallation\\nIt is recommended to install the assistant using conda. If you have never used conda before, it is recommended to read \\nthis blog post first. \\nshell\\nconda create --name cle_39 python=3.9 napari-pyclesperanto-assistant\\nconda activate cle_39\\nMac-users please also install this:\\nconda install -c conda-forge ocl_icd_wrapper_apple\\n\\nLinux users please also install this:\\nconda install -c conda-forge ocl-icd-system\\n\\nYou can then start the napari-assistant using this command:\\nnaparia\\nFeedback and contributions welcome!\\nclEsperanto is developed in the open because we believe in the open source community. See our community guidelines. Feel free to drop feedback as github issue or via image.sc\\nAcknowledgements\\nThis project was supported by the Deutsche Forschungsgemeinschaft under Germanyâ€™s Excellence Strategy â€“ EXC2068 - Cluster of Excellence \\\"Physics of Life\\\" of TU Dresden.\\nThis project has been made possible in part by grant number 2021-240341 (Napari plugin accelerator grant) from the Chan Zuckerberg Initiative DAF, an advised fund of the Silicon Valley Community Foundation.\\nImprint\",\"development_status\":[\"Development Status :: 3 - Alpha\"],\"display_name\":\"napari-pyclesperanto-assistant\",\"documentation\":\"https://github.com/clEsperanto/napari_pyclesperanto_assistant/\",\"first_released\":\"2020-12-19T16:01:10.190324Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-pyclesperanto-assistant\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\",\"sample_data\"],\"project_site\":\"https://github.com/clesperanto/napari_pyclesperanto_assistant\",\"python_version\":\">=3.6\",\"reader_file_extensions\":[],\"release_date\":\"2023-01-15T16:54:50.042400Z\",\"report_issues\":\"https://github.com/clEsperanto/napari_pyclesperanto_assistant/issues\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"pyopencl\",\"toolz\",\"scikit-image\",\"napari (>=0.4.15)\",\"pyclesperanto-prototype (>=0.22.0)\",\"magicgui\",\"numpy (!=1.19.4)\",\"pyperclip\",\"loguru\",\"jupytext\",\"jupyter\",\"pandas\",\"napari-tools-menu (>=0.1.8)\",\"napari-time-slicer (>=0.4.0)\",\"napari-skimage-regionprops (>=0.2.0)\",\"napari-workflows (>=0.1.1)\",\"napari-assistant (>=0.2.0)\"],\"summary\":\"GPU-accelerated image processing in napari using OpenCL\",\"support\":\"https://forum.image.sc/tag/clij\",\"twitter\":\"\",\"version\":\"0.22.0\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"code@adamltyson.com\",\"name\":\"Adam Tyson\"}],\"category\":{\"Supported data\":[\"3D\"],\"Workflow step\":[\"Visualization\",\"Image classification\",\"Image registration\"]},\"category_hierarchy\":{\"Supported data\":[[\"3D\"]],\"Workflow step\":[[\"Visualization\",\"Image visualisation\"],[\"Visualization\",\"Image visualisation\",\"Overlay\"],[\"Visualization\",\"Image visualisation\",\"Slice rendering\"],[\"Image classification\"],[\"Image registration\"]]},\"code_repository\":\"https://github.com/brainglobe/brainglobe-napari-io\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"brainglobe-napari-io\"}],\"description\":\"# napari-brainglobe-io\\n\\n[![License](https://img.shields.io/pypi/l/brainglobe-napari-io.svg?color=green)](https://github.com/napari/brainglobe-napari-io/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/brainglobe-napari-io.svg?color=green)](https://pypi.org/project/brainglobe-napari-io)\\n[![Python Version](https://img.shields.io/pypi/pyversions/brainglobe-napari-io.svg?color=green)](https://python.org)\\n[![tests](https://github.com/brainglobe/brainglobe-napari-io/workflows/tests/badge.svg)](https://github.com/brainglobe/brainglobe-napari-io/actions)\\n[![codecov](https://codecov.io/gh/brainglobe/brainglobe-napari-io/branch/master/graph/badge.svg)](https://codecov.io/gh/brainglobe/brainglobe-napari-io)\\n\\nVisualise cellfinder and brainreg results with napari\\n\\n\\n----------------------------------\\n\\n\\n## Installation\\nThis package is likely already installed \\n(e.g. with cellfinder, brainreg or another napari plugin), but if you want to \\ninstall it again, either use the napari plugin install GUI or you can \\ninstall `brainglobe-napari-io` via [pip]:\\n\\n    pip install brainglobe-napari-io\\n\\n## Usage\\n* Open napari (however you normally do it, but typically just type `napari` into your terminal, or click on your desktop icon)\\n\\n### brainreg\\n#### Sample space\\nDrag your [brainreg](https://github.com/brainglobe/brainreg) output directory (the one with the log file) onto the napari window.\\n    \\nVarious images should then open, including:\\n* `Registered image` - the image used for registration, downsampled to atlas resolution\\n* `atlas_name` - e.g. `allen_mouse_25um` the atlas labels, warped to your sample brain\\n* `Boundaries` - the boundaries of the atlas regions\\n\\nIf you downsampled additional channels, these will also be loaded.\\n\\nMost of these images will not be visible by default. Click the little eye icon to toggle visibility.\\n\\n_N.B. If you use a high resolution atlas (such as `allen_mouse_10um`), then the files can take a little while to load._\\n\\n![sample_space](https://raw.githubusercontent.com/brainglobe/brainglobe-napari-io/master/resources/sample_space.gif)\\n\\n\\n#### Atlas space\\n`napari-brainreg` also comes with an additional plugin, for visualising your data \\nin atlas space. \\n\\nThis is typically only used in other software, but you can enable it yourself:\\n* Open napari\\n* Navigate to `Plugins` -> `Plugin Call Order`\\n* In the `Plugin Sorter` window, select `napari_get_reader` from the `select hook...` dropdown box\\n* Drag `brainreg_read_dir_standard_space` (the atlas space viewer plugin) above `brainreg_read_dir` (the normal plugin) to ensure that the atlas space plugin is used preferentially.\\n\\n\\n### cellfinder\\n#### Load cellfinder XML file\\n* Load your raw data (drag and drop the data directories into napari, one at a time)\\n* Drag and drop your cellfinder XML file (e.g. `cell_classification.xml`) into napari.\\n\\n#### Load cellfinder directory\\n* Load your raw data (drag and drop the data directories into napari, one at a time)\\n* Drag and drop your cellfinder output directory into napari.\\n\\nThe plugin will then load your detected cells (in yellow) and the rejected cell \\ncandidates (in blue). If you carried out registration, then these results will be \\noverlaid (similarly to the loading brainreg data, but transformed to the \\ncoordinate space of your raw data).\\n\\n![load_data](https://raw.githubusercontent.com/brainglobe/brainglobe-napari-io/master/resources/load_data.gif)\\n**Loading raw data**\\n\\n![load_data](https://raw.githubusercontent.com/brainglobe/brainglobe-napari-io/master/resources/load_results.gif)\\n**Loading cellfinder results**\\n\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [MIT] license,\\n\\\"brainglobe-napari-io\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/brainglobe/brainglobe-napari-io/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\\n---\\nThe BrainGlobe project is generously supported by the Sainsbury Wellcome Centre and the Institute of Neuroscience, Technical University of Munich, with funding from Wellcome, the Gatsby Charitable Foundation and the Munich Cluster for Systems Neurology - Synergy.\\n\\n<img src='https://brainglobe.info/images/logos_combined.png' width=\\\"550\\\">\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-brainglobe-io\\n\\n\\n\\n\\n\\nVisualise cellfinder and brainreg results with napari\\n\\nInstallation\\nThis package is likely already installed \\n(e.g. with cellfinder, brainreg or another napari plugin), but if you want to \\ninstall it again, either use the napari plugin install GUI or you can \\ninstall brainglobe-napari-io via pip:\\npip install brainglobe-napari-io\\n\\nUsage\\n\\nOpen napari (however you normally do it, but typically just type napari into your terminal, or click on your desktop icon)\\n\\nbrainreg\\nSample space\\nDrag your brainreg output directory (the one with the log file) onto the napari window.\\nVarious images should then open, including:\\n* Registered image - the image used for registration, downsampled to atlas resolution\\n* atlas_name - e.g. allen_mouse_25um the atlas labels, warped to your sample brain\\n* Boundaries - the boundaries of the atlas regions\\nIf you downsampled additional channels, these will also be loaded.\\nMost of these images will not be visible by default. Click the little eye icon to toggle visibility.\\nN.B. If you use a high resolution atlas (such as allen_mouse_10um), then the files can take a little while to load.\\n\\nAtlas space\\nnapari-brainreg also comes with an additional plugin, for visualising your data \\nin atlas space. \\nThis is typically only used in other software, but you can enable it yourself:\\n* Open napari\\n* Navigate to Plugins -> Plugin Call Order\\n* In the Plugin Sorter window, select napari_get_reader from the select hook... dropdown box\\n* Drag brainreg_read_dir_standard_space (the atlas space viewer plugin) above brainreg_read_dir (the normal plugin) to ensure that the atlas space plugin is used preferentially.\\ncellfinder\\nLoad cellfinder XML file\\n\\nLoad your raw data (drag and drop the data directories into napari, one at a time)\\nDrag and drop your cellfinder XML file (e.g. cell_classification.xml) into napari.\\n\\nLoad cellfinder directory\\n\\nLoad your raw data (drag and drop the data directories into napari, one at a time)\\nDrag and drop your cellfinder output directory into napari.\\n\\nThe plugin will then load your detected cells (in yellow) and the rejected cell \\ncandidates (in blue). If you carried out registration, then these results will be \\noverlaid (similarly to the loading brainreg data, but transformed to the \\ncoordinate space of your raw data).\\n\\nLoading raw data\\n\\nLoading cellfinder results\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the MIT license,\\n\\\"brainglobe-napari-io\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\\n\\nThe BrainGlobe project is generously supported by the Sainsbury Wellcome Centre and the Institute of Neuroscience, Technical University of Munich, with funding from Wellcome, the Gatsby Charitable Foundation and the Munich Cluster for Systems Neurology - Synergy.\\n\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"brainglobe-napari-io\",\"documentation\":\"https://docs.brainglobe.info\",\"first_released\":\"2021-03-12T12:52:23.068881Z\",\"license\":\"BSD-3-Clause\",\"name\":\"brainglobe-napari-io\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\",\"writer\"],\"project_site\":\"https://brainglobe.info\",\"python_version\":\">=3.6\",\"reader_file_extensions\":[\"*.tiff\",\"*.tif\",\"*.xml\"],\"release_date\":\"2022-03-18T11:54:20.055771Z\",\"report_issues\":\"https://github.com/brainglobe/brainglobe-napari-io/issues\",\"requirements\":[\"napari\",\"napari-plugin-engine (>=0.1.4)\",\"napari-ndtiffs\",\"tifffile (>=2020.8.13)\",\"imlib\",\"bg-space\",\"bg-atlasapi\",\"black ; extra == 'dev'\",\"pytest-cov ; extra == 'dev'\",\"pytest ; extra == 'dev'\",\"gitpython ; extra == 'dev'\",\"coverage (>=5.0.3) ; extra == 'dev'\",\"bump2version ; extra == 'dev'\",\"pre-commit ; extra == 'dev'\",\"flake8 ; extra == 'dev'\"],\"summary\":\"Read and write files from the BrainGlobe neuroanatomy suite\",\"support\":\"https://forum.image.sc/tag/brainglobe\",\"twitter\":\"https://twitter.com/brain_globe\",\"version\":\"0.1.5\",\"visibility\":\"public\",\"writer_file_extensions\":[\".xml\"],\"writer_save_layers\":[\"points\"]}",
  "{\"authors\":[{\"name\":\"Gregor Lichtner\"}],\"code_repository\":\"https://github.com/glichtner/napari-pystackreg\",\"description\":\"# napari-pystackreg\\n\\n[![License](https://img.shields.io/pypi/l/napari-pystackreg.svg?color=green)](https://github.com/glichtner/napari-pystackreg/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-pystackreg.svg?color=green)](https://pypi.org/project/napari-pystackreg)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-pystackreg.svg?color=green)](https://python.org)\\n[![tests](https://github.com/glichtner/napari-pystackreg/workflows/tests/badge.svg)](https://github.com/glichtner/napari-pystackreg/actions)\\n[![codecov](https://codecov.io/gh/glichtner/napari-pystackreg/branch/main/graph/badge.svg)](https://codecov.io/gh/glichtner/napari-pystackreg)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-pystackreg)](https://napari-hub.org/plugins/napari-pystackreg)\\n\\nRobust image registration for napari.\\n\\n## Summary\\nnapari-pystackreg offers the image registration capabilities of the python package\\n[pystackreg](https://github.com/glichtner/pystackreg) for napari.\\n\\n![](https://github.com/glichtner/napari-pystackreg/raw/main/docs/napari-pystackreg.gif)\\n\\n## Description\\n\\npyStackReg is used to align (register) one or more images to a common reference image, as is required usually in\\ntime-resolved fluorescence or wide-field microscopy.\\nIt is directly ported from the source code of the ImageJ plugin ``TurboReg`` and provides additionally the\\nfunctionality of the ImageJ plugin ``StackReg``, both of which were written by Philippe Thevenaz/EPFL\\n(available at http://bigwww.epfl.ch/thevenaz/turboreg/).\\n\\npyStackReg provides the following five types of distortion:\\n\\n- Translation\\n- Rigid body (translation + rotation)\\n- Scaled rotation (translation + rotation + scaling)\\n- Affine (translation + rotation + scaling + shearing)\\n- Bilinear (non-linear transformation; does not preserve straight lines)\\n\\npyStackReg supports the full functionality of StackReg plus some additional options, e.g., using different reference\\nimages and having access to the actual transformation matrices (please see the examples below). Note that pyStackReg\\nuses the high quality (i.e. high accuracy) mode of TurboReg that uses cubic spline interpolation for transformation.\\n\\nPlease note: The bilinear transformation cannot be propagated, as a combination of bilinear transformations does not\\ngenerally result in a bilinear transformation. Therefore, stack registration/transform functions won't work with\\nbilinear transformation when using \\\"previous\\\" image as reference image. You can either use another reference (\\n\\\"first\\\" or \\\"mean\\\" for first or mean image, respectively), or try to register/transform each image of the stack\\nseparately to its respective previous image (and use the already transformed previous image as reference for the\\nnext image).\\n\\n## Installation\\n\\nYou can install ``napari-pystackreg`` via [pip](https://pypi.org/project/pip/) from [PyPI](https://pypi.org/):\\n\\n    pip install napari-pystackreg\\n\\nYou can also install ``napari-pystackreg`` via [conda](https://docs.conda.io/en/latest/):\\n\\n    conda install -c conda-forge napari-pystackreg\\n\\nOr install it via napari's plugin installer.\\n\\n    Plugins > Install/Uninstall Plugins... > Filter for \\\"napari-pystackreg\\\" > Install\\n\\nTo install latest development version:\\n\\n    pip install git+https://github.com/glichtner/napari-pystackreg.git\\n\\n## Usage\\n\\n\\n### Open Plugin User Interface\\n\\nStart up napari, e.g. from the command line:\\n\\n    napari\\n\\nThen, load an image stack (e.g. via ``File > Open Image...``) that you want to register. You can also use the example\\nstack provided by the pluging (``File > Open Sample > napari-pystackreg: PC12 moving example``).\\nThen, select the ``napari-pystackreg`` plugin from the ``Plugins > napari-pystackreg: pystackreg`` menu.\\n\\n![](https://github.com/glichtner/napari-pystackreg/raw/main/docs/ui-initial.png)\\n\\n### User Interface Options\\nA variety of options are available to control the registration process:\\n\\n* `Image Stack`: The image layer that should be registered/transformed.\\n* `Transformation`: The type of transformation that should be applied.\\n  - `Translation`: translation\\n  - `Rigid body`: translation + rotation\\n  - `Scaled rotation`: translation + rotation + scaling\\n  - `Affine`: translation + rotation + scaling + shearing\\n  - `Bilinear`: non-linear transformation; does not preserve straight lines\\n* `Reference frame:` The reference image for registration.\\n  - `Previous frame`: Aligns each frame (image) to its previous frame in the stack\\n  - `Mean (all frames)`: Aligns each frame (image) to the average of all images in the stack\\n  - `Mean (first n frames)`: Aligns each frame (image) to the mean of the first n frames in the stack. n is a tuneable parameter.\\n* `Moving-average stack before register`: Apply a moving average to the stack before registration. This can be useful to\\n  reduce noise in the stack (if the signal-to-noise ratio is very low). The moving average is applied to the stack only\\n  for determining the transformation matrices, but not for the actual transforming of the stack.\\n* `Transformation matrix file`: Transformation matrices can be saved to or loaded from a file for permanent storage.\\n\\n### Reference frame\\nThe reference frame is the frame to which the other frames are aligned. The default option is to use the\\n`Previous frame`, which will register each frame to its respective previous frame in the stack. Alternatively, the\\nreference frame can be set to the mean of all frames in the stack (`Mean (all frames)`) or the mean of the first n\\nframes in the stack (`Mean (first n frames)`). The latter option can be useful if the first frames in the stack are more\\nstable than the later frames (e.g. if the first frames are taken before the sample is moved). When selecting the\\n`Mean (first n frames)` option, the number of frames to use for the mean can be set via the spinbox below the option.\\n\\n![](https://github.com/glichtner/napari-pystackreg/raw/main/docs/ui-reference-mean-n.png)\\n\\n### Moving average before registration\\nTo increase registration performance with low signal-to-noise ratio stacks, a moving average can be applied to the\\nstack before registration. The moving average is applied to the stack only for determining the\\ntransformation matrices, but not for the actual transforming of the stack. That means that the transformed stack will\\nstill contain the original frames (however registered), but not the averaged frames.\\n\\nWhen selecting the `Moving-average stack before register` option, the number of frames to use for the moving average can\\nbe set via the spinbox below the option.\\n\\n![](https://github.com/glichtner/napari-pystackreg/raw/main/docs/ui-moving-average.png)\\n\\n### Transformation matrix file\\nThe transformation matrices can be saved to or loaded from a file for permanent storage. This can be useful if you want\\nto apply the same transformation to another stack (e.g. a different channel of the same sample). The transformation\\nmatrices are saved as a numpy array in a binary file (``.npy``). The file can be loaded via the `Load` button and saved\\nvia the `Save` button.\\n\\n![](https://github.com/glichtner/napari-pystackreg/raw/main/docs/ui-register-tmat.png)\\n\\n### Register/Transform\\nTo perform the actual registration and transformation steps, click the `Register` and `Transform` buttons, respectively.\\n\\nThe `Register` button will register the stack to the reference by determining the appropriate transformation matrices,\\nwithout actually transforming the stack. The transformation matrices can be saved to a file via the `Save` button in the\\n`Transformation matrix file` section.\\n\\n![](https://github.com/glichtner/napari-pystackreg/raw/main/docs/ui-registered.png)\\n\\nThe `Transform` button (1) will transform the stack to the reference by applying the transformation matrices that are\\ncurrently loaded to the stack selected in `Image Stack`. For the button to become active, either the transformation\\nmatrices have to be loaded from a file via the `Load` button in the `Transformation matrix file` section, or the\\n`Register` button has to be clicked first to determine the transformation matrices.\\n\\nThe `Transform` button will also add a new image layer to the napari viewer (2) with the transformed stack. The name of the\\nnew layer will be the name of the original stack with the prefix `Registered`.\\n\\n![](https://github.com/glichtner/napari-pystackreg/raw/main/docs/ui-transformed.png)\\n\\nFinally, the `Register & Transform` button will perform both the registration and transformation steps in one go.\\n\\n----------------------------------\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [Apache Software License 2.0] license,\\n\\\"napari-pystackreg\\\" is free and open source software.\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n## Acknowledgments\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/glichtner/napari-pystackreg/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-pystackreg\\n\\n\\n\\n\\n\\n\\nRobust image registration for napari.\\nSummary\\nnapari-pystackreg offers the image registration capabilities of the python package\\npystackreg for napari.\\n\\nDescription\\npyStackReg is used to align (register) one or more images to a common reference image, as is required usually in\\ntime-resolved fluorescence or wide-field microscopy.\\nIt is directly ported from the source code of the ImageJ plugin TurboReg and provides additionally the\\nfunctionality of the ImageJ plugin StackReg, both of which were written by Philippe Thevenaz/EPFL\\n(available at http://bigwww.epfl.ch/thevenaz/turboreg/).\\npyStackReg provides the following five types of distortion:\\n\\nTranslation\\nRigid body (translation + rotation)\\nScaled rotation (translation + rotation + scaling)\\nAffine (translation + rotation + scaling + shearing)\\nBilinear (non-linear transformation; does not preserve straight lines)\\n\\npyStackReg supports the full functionality of StackReg plus some additional options, e.g., using different reference\\nimages and having access to the actual transformation matrices (please see the examples below). Note that pyStackReg\\nuses the high quality (i.e. high accuracy) mode of TurboReg that uses cubic spline interpolation for transformation.\\nPlease note: The bilinear transformation cannot be propagated, as a combination of bilinear transformations does not\\ngenerally result in a bilinear transformation. Therefore, stack registration/transform functions won't work with\\nbilinear transformation when using \\\"previous\\\" image as reference image. You can either use another reference (\\n\\\"first\\\" or \\\"mean\\\" for first or mean image, respectively), or try to register/transform each image of the stack\\nseparately to its respective previous image (and use the already transformed previous image as reference for the\\nnext image).\\nInstallation\\nYou can install napari-pystackreg via pip from PyPI:\\npip install napari-pystackreg\\n\\nYou can also install napari-pystackreg via conda:\\nconda install -c conda-forge napari-pystackreg\\n\\nOr install it via napari's plugin installer.\\nPlugins > Install/Uninstall Plugins... > Filter for \\\"napari-pystackreg\\\" > Install\\n\\nTo install latest development version:\\npip install git+https://github.com/glichtner/napari-pystackreg.git\\n\\nUsage\\nOpen Plugin User Interface\\nStart up napari, e.g. from the command line:\\nnapari\\n\\nThen, load an image stack (e.g. via File > Open Image...) that you want to register. You can also use the example\\nstack provided by the pluging (File > Open Sample > napari-pystackreg: PC12 moving example).\\nThen, select the napari-pystackreg plugin from the Plugins > napari-pystackreg: pystackreg menu.\\n\\nUser Interface Options\\nA variety of options are available to control the registration process:\\n\\nImage Stack: The image layer that should be registered/transformed.\\nTransformation: The type of transformation that should be applied.\\nTranslation: translation\\nRigid body: translation + rotation\\nScaled rotation: translation + rotation + scaling\\nAffine: translation + rotation + scaling + shearing\\nBilinear: non-linear transformation; does not preserve straight lines\\nReference frame: The reference image for registration.\\nPrevious frame: Aligns each frame (image) to its previous frame in the stack\\nMean (all frames): Aligns each frame (image) to the average of all images in the stack\\nMean (first n frames): Aligns each frame (image) to the mean of the first n frames in the stack. n is a tuneable parameter.\\nMoving-average stack before register: Apply a moving average to the stack before registration. This can be useful to\\n  reduce noise in the stack (if the signal-to-noise ratio is very low). The moving average is applied to the stack only\\n  for determining the transformation matrices, but not for the actual transforming of the stack.\\nTransformation matrix file: Transformation matrices can be saved to or loaded from a file for permanent storage.\\n\\nReference frame\\nThe reference frame is the frame to which the other frames are aligned. The default option is to use the\\nPrevious frame, which will register each frame to its respective previous frame in the stack. Alternatively, the\\nreference frame can be set to the mean of all frames in the stack (Mean (all frames)) or the mean of the first n\\nframes in the stack (Mean (first n frames)). The latter option can be useful if the first frames in the stack are more\\nstable than the later frames (e.g. if the first frames are taken before the sample is moved). When selecting the\\nMean (first n frames) option, the number of frames to use for the mean can be set via the spinbox below the option.\\n\\nMoving average before registration\\nTo increase registration performance with low signal-to-noise ratio stacks, a moving average can be applied to the\\nstack before registration. The moving average is applied to the stack only for determining the\\ntransformation matrices, but not for the actual transforming of the stack. That means that the transformed stack will\\nstill contain the original frames (however registered), but not the averaged frames.\\nWhen selecting the Moving-average stack before register option, the number of frames to use for the moving average can\\nbe set via the spinbox below the option.\\n\\nTransformation matrix file\\nThe transformation matrices can be saved to or loaded from a file for permanent storage. This can be useful if you want\\nto apply the same transformation to another stack (e.g. a different channel of the same sample). The transformation\\nmatrices are saved as a numpy array in a binary file (.npy). The file can be loaded via the Load button and saved\\nvia the Save button.\\n\\nRegister/Transform\\nTo perform the actual registration and transformation steps, click the Register and Transform buttons, respectively.\\nThe Register button will register the stack to the reference by determining the appropriate transformation matrices,\\nwithout actually transforming the stack. The transformation matrices can be saved to a file via the Save button in the\\nTransformation matrix file section.\\n\\nThe Transform button (1) will transform the stack to the reference by applying the transformation matrices that are\\ncurrently loaded to the stack selected in Image Stack. For the button to become active, either the transformation\\nmatrices have to be loaded from a file via the Load button in the Transformation matrix file section, or the\\nRegister button has to be clicked first to determine the transformation matrices.\\nThe Transform button will also add a new image layer to the napari viewer (2) with the transformed stack. The name of the\\nnew layer will be the name of the original stack with the prefix Registered.\\n\\nFinally, the Register & Transform button will perform both the registration and transformation steps in one go.\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the Apache Software License 2.0 license,\\n\\\"napari-pystackreg\\\" is free and open source software.\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\\nAcknowledgments\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"napari pystackreg\",\"documentation\":\"https://github.com/glichtner/napari-pystackreg#README.md\",\"first_released\":\"2022-07-07T10:39:13.117097Z\",\"license\":\"Apache-2.0\",\"name\":\"napari-pystackreg\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\",\"sample_data\"],\"project_site\":\"https://github.com/glichtner/napari-pystackreg\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2023-01-15T13:51:23.258961Z\",\"report_issues\":\"https://github.com/glichtner/napari-pystackreg/issues\",\"requirements\":[\"magicgui\",\"numpy\",\"pystackreg (>=0.2.6)\",\"qtpy\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"tox ; extra == 'testing'\"],\"summary\":\"Robust image registration for napari\",\"support\":\"https://github.com/glichtner/napari-pystackreg/issues\",\"twitter\":\"\",\"version\":\"0.1.4\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Varun kapoor\"}],\"code_repository\":\"https://github.com/Kapoorlabs-CAPED/vollseg-napari-trackmate\",\"description\":\"# vollseg-napari-trackmate\\n\\n[![License BSD-3](https://img.shields.io/pypi/l/vollseg-napari-trackmate.svg?color=green)](https://github.com/Kapoorlabs-CAPED/vollseg-napari-trackmate/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/vollseg-napari-trackmate.svg?color=green)](https://pypi.org/project/vollseg-napari-trackmate)\\n[![Python Version](https://img.shields.io/pypi/pyversions/vollseg-napari-trackmate.svg?color=green)](https://python.org)\\n[![tests](https://github.com/Kapoorlabs-CAPED/vollseg-napari-trackmate/workflows/tests/badge.svg)](https://github.com/Kapoorlabs-CAPED/vollseg-napari-trackmate/actions)\\n[![codecov](https://codecov.io/gh/Kapoorlabs-CAPED/vollseg-napari-trackmate/branch/main/graph/badge.svg)](https://codecov.io/gh/Kapoorlabs-CAPED/vollseg-napari-trackmate)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/vollseg-napari-trackmate)](https://napari-hub.org/plugins/vollseg-napari-trackmate)\\n\\nTrack analysis using TrackMate xml and csv generated tracks using NapaTrackMater as the base library\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/stable/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `vollseg-napari-trackmate` via [pip]:\\n\\n    pip install vollseg-napari-trackmate\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/Kapoorlabs-CAPED/vollseg-napari-trackmate.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"vollseg-napari-trackmate\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/Kapoorlabs-CAPED/vollseg-napari-trackmate/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"vollseg-napari-trackmate\\n\\n\\n\\n\\n\\n\\nTrack analysis using TrackMate xml and csv generated tracks using NapaTrackMater as the base library\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install vollseg-napari-trackmate via pip:\\npip install vollseg-napari-trackmate\\n\\nTo install latest development version :\\npip install git+https://github.com/Kapoorlabs-CAPED/vollseg-napari-trackmate.git\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"vollseg-napari-trackmate\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"vollseg-napari-trackmate\",\"documentation\":\"https://github.com/Kapoorlabs-CAPED/vollseg-napari-trackmate#README.md\",\"first_released\":\"2022-12-31T13:18:19.819345Z\",\"license\":\"BSD-3-Clause\",\"name\":\"vollseg-napari-trackmate\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"writer\",\"widget\",\"sample_data\"],\"project_site\":\"https://github.com/Kapoorlabs-CAPED/vollseg-napari-trackmate\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2023-02-06T18:09:07.399184Z\",\"report_issues\":\"https://github.com/Kapoorlabs-CAPED/vollseg-napari-trackmate/issues\",\"requirements\":[\"numpy\",\"magicgui\",\"qtpy\",\"caped-ai\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\"],\"summary\":\"Track analysis using TrackMate xml and csv generated tracks using NapaTrackMater as the base library\",\"support\":\"https://github.com/Kapoorlabs-CAPED/vollseg-napari-trackmate/issues\",\"twitter\":\"\",\"version\":\"1.3.6\",\"visibility\":\"public\",\"writer_file_extensions\":[\".npy\"],\"writer_save_layers\":[\"labels\",\"image\"]}",
  "{\"authors\":[{\"name\":\"Andrea Bassi\"},{\"name\":\"Giorgia Tortora\"}],\"category\":{\"Supported data\":[\"Time series\"],\"Workflow step\":[\"Image registration\"]},\"category_hierarchy\":{\"Supported data\":[[\"Time series\"]],\"Workflow step\":[[\"Image registration\"]]},\"code_repository\":\"https://github.com/GiorgiaTortora/napari-roi-registration\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-roi-registration\"}],\"description\":\"# napari-roi-registration\\n\\n[![License](https://img.shields.io/pypi/l/napari-roi-registration.svg?color=green)](https://github.com/GiorgiaTortora/napari-roi-registration/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-roi-registration.svg?color=green)](https://pypi.org/project/napari-roi-registration)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-roi-registration.svg?color=green)](https://python.org)\\n[![tests](https://github.com/GiorgiaTortora/napari-roi-registration/workflows/tests/badge.svg)](https://github.com/GiorgiaTortora/napari-roi-registration/actions)\\n[![codecov](https://codecov.io/gh/GiorgiaTortora/napari-roi-registration/branch/main/graph/badge.svg)](https://codecov.io/gh/GiorgiaTortora/napari-roi-registration)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-roi-registration)](https://napari-hub.org/plugins/napari-roi-registration)\\n\\nA Napari plugin for the registration of regions of interests (ROI) in a time lapse acquistion and processing of the intensity of the registered data.\\n\\nThe ROI are defined using a Labels layer. Registration of multiple ROIs is supported.  \\n\\nThe `Registration` widget uses the user-defined labels, constructs a rectangular ROI around each of them and registers the ROIs in each time frame.\\n\\nThe `Processing` widget measures the ROI displacements and extracts the average intensity of the ROI, calculated on the area of the labels.\\n\\nThe `Subtract background` widget subtracts a background on each frame, calculated as the mean intensity on a Labels layer.\\nTipically useful when ambient light affects the measurement.  \\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/plugins/stable/index.html\\n-->\\n\\n![raw](https://github.com/GiorgiaTortora/napari-roi-registration/raw/main/images/roi_registration.gif)\\n\\n## Installation\\n\\nYou can install `napari-roi-registration` via [pip]:\\n\\n    pip install napari-roi-registration\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/GiorgiaTortora/napari-roi-registration.git\\n\\n## Usage\\n\\n### Registration Widget\\n\\n1. Create a new Labels layer and draw one or more labels where you want to select a ROI (Region Of Interest). Each color in the same Labels layer represents a different label which will correspond to a different ROI.\\n\\n![raw](https://github.com/GiorgiaTortora/napari-roi-registration/raw/main/images/Picture1.png)\\n\\n2. Push the `Register ROIs` button: registration of the entire stack will be performed. When the registration is finished two new layers will appear in the viewer. One layer contains the centroids of the drawn labels while the other contains the bounding boxes enclosing the ROIs.\\nThe registration starts from the currently selected frame. If `register entire stack` is selected, the registration will create a new layer for each label, with the registered ROI stacks.\\n\\n![raw](https://github.com/GiorgiaTortora/napari-roi-registration/raw/main/images/Picture2.png)\\n\\n### Processing Widget\\n\\nPushing the `Process registered ROIs` button, the registered ROIs will be analyzed. The intensity of the registered ROIs (measured on the area of the selected label) and the displacement of the ROIs will be calculated.\\nIf `plot results` is selected the plot of displacement vs time index and mean intensity vs time index will appear in the Console.\\nChoosing the `save results` option, an excel file containing ROIs positions, displacements and intensities, will be saved. \\n\\n![raw](https://github.com/GiorgiaTortora/napari-roi-registration/raw/main/images/Picture3.png)\\n\\n### Background Widget\\n\\n1. Create a new Labels layer and draw a label on the area where you want to calculate the background. \\n\\n![raw](https://github.com/GiorgiaTortora/napari-roi-registration/raw/main/images/Picture4.png)\\n\\n2. Push the `Subtract background` button. A new image layer will appear in the viewer. This layer contains the image to which the background was subtracted.\\n\\n## Contributing \\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-roi-registration\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/GiorgiaTortora/napari-roi-registration/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-roi-registration\\n\\n\\n\\n\\n\\n\\nA Napari plugin for the registration of regions of interests (ROI) in a time lapse acquistion and processing of the intensity of the registered data.\\nThe ROI are defined using a Labels layer. Registration of multiple ROIs is supported.  \\nThe Registration widget uses the user-defined labels, constructs a rectangular ROI around each of them and registers the ROIs in each time frame.\\nThe Processing widget measures the ROI displacements and extracts the average intensity of the ROI, calculated on the area of the labels.\\nThe Subtract background widget subtracts a background on each frame, calculated as the mean intensity on a Labels layer.\\nTipically useful when ambient light affects the measurement.  \\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\n\\nInstallation\\nYou can install napari-roi-registration via pip:\\npip install napari-roi-registration\\n\\nTo install latest development version :\\npip install git+https://github.com/GiorgiaTortora/napari-roi-registration.git\\n\\nUsage\\nRegistration Widget\\n\\nCreate a new Labels layer and draw one or more labels where you want to select a ROI (Region Of Interest). Each color in the same Labels layer represents a different label which will correspond to a different ROI.\\n\\n\\n\\nPush the Register ROIs button: registration of the entire stack will be performed. When the registration is finished two new layers will appear in the viewer. One layer contains the centroids of the drawn labels while the other contains the bounding boxes enclosing the ROIs.\\nThe registration starts from the currently selected frame. If register entire stack is selected, the registration will create a new layer for each label, with the registered ROI stacks.\\n\\n\\nProcessing Widget\\nPushing the Process registered ROIs button, the registered ROIs will be analyzed. The intensity of the registered ROIs (measured on the area of the selected label) and the displacement of the ROIs will be calculated.\\nIf plot results is selected the plot of displacement vs time index and mean intensity vs time index will appear in the Console.\\nChoosing the save results option, an excel file containing ROIs positions, displacements and intensities, will be saved. \\n\\nBackground Widget\\n\\nCreate a new Labels layer and draw a label on the area where you want to calculate the background. \\n\\n\\n\\nPush the Subtract background button. A new image layer will appear in the viewer. This layer contains the image to which the background was subtracted.\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-roi-registration\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"Roi Registration\",\"documentation\":\"https://github.com/GiorgiaTortora/napari-roi-registration#README.md\",\"first_released\":\"2022-05-04T11:35:41.317049Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-roi-registration\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/GiorgiaTortora/napari-roi-registration\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-06-28T09:44:37.605195Z\",\"report_issues\":\"https://github.com/GiorgiaTortora/napari-roi-registration/issues\",\"requirements\":[\"numpy\",\"magicgui\",\"qtpy\",\"scikit-image\",\"opencv-python\",\"matplotlib\",\"openpyxl\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\",\"scikit-image ; extra == 'testing'\",\"opencv-python-headless ; extra == 'testing'\",\"matplotlib ; extra == 'testing'\",\"openpyxl ; extra == 'testing'\"],\"summary\":\"A plugin to perform roi registration.\",\"support\":\"https://github.com/GiorgiaTortora/napari-roi-registration/issues\",\"twitter\":\"\",\"version\":\"0.1.2\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"robert.haase@tu-dresden.de\",\"name\":\"Robert Haase\"}],\"code_repository\":\"https://github.com/haesleinhuepf/napari-tabu\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-tabu\"}],\"description\":\"# napari-tabu\\n\\n[![License](https://img.shields.io/pypi/l/napari-tabu.svg?color=green)](https://github.com/haesleinhuepf/napari-tabu/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-tabu.svg?color=green)](https://pypi.org/project/napari-tabu)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-tabu.svg?color=green)](https://python.org)\\n[![tests](https://github.com/haesleinhuepf/napari-tabu/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-tabu/actions)\\n[![codecov](https://codecov.io/gh/haesleinhuepf/napari-tabu/branch/master/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-tabu)\\n\\nA plugin for handling multiple napari windows\\n![](https://github.com/haesleinhuepf/napari-tabu/raw/main/docs/napari-tabu-screencast.gif)\\n\\n----------------------------------\\n\\n## Usage\\n\\nTo open a new window, first click the menu `Plugins > napari-tabu: open new window`\\n![](https://github.com/haesleinhuepf/napari-tabu/raw/main/docs/new_window_menu.png)\\n\\nAfterwards, select the layer which should be opened in the new window and click on `Run`:\\n![](https://github.com/haesleinhuepf/napari-tabu/raw/main/docs/new_window_dialog.png)\\n\\nWhen you're done with working with the new window, you can send back the result of your work using the `Send current layer back to main napari` butoon:\\n![](https://github.com/haesleinhuepf/napari-tabu/raw/main/docs/send_back.png)\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n## Installation\\n\\nYou can install `napari-tabu` via [pip]:\\n\\n    pip install napari-tabu\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-tabu\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please open a thread on [image.sc](https://image.sc) along with a detailed description and tag [@haesleinhuepf](https://github.com/haesleinhuepf).\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/haesleinhuepf/napari-tabu/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-tabu\\n\\n\\n\\n\\n\\nA plugin for handling multiple napari windows\\n\\n\\nUsage\\nTo open a new window, first click the menu Plugins > napari-tabu: open new window\\n\\nAfterwards, select the layer which should be opened in the new window and click on Run:\\n\\nWhen you're done with working with the new window, you can send back the result of your work using the Send current layer back to main napari butoon:\\n\\nThis napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.\\nInstallation\\nYou can install napari-tabu via pip:\\npip install napari-tabu\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-tabu\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please open a thread on image.sc along with a detailed description and tag @haesleinhuepf.\",\"development_status\":[\"Development Status :: 3 - Alpha\"],\"display_name\":\"napari-tabu\",\"documentation\":\"https://github.com/haesleinhuepf/napari-tabu#README.md\",\"first_released\":\"2021-10-14T14:47:39.487563Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-tabu\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/haesleinhuepf/napari-tabu\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-01-01T15:58:42.300925Z\",\"report_issues\":\"https://github.com/haesleinhuepf/napari-tabu/issues\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"napari\",\"napari-tools-menu\"],\"summary\":\"A plugin for handling multiple napari windows\",\"support\":\"https://github.com/haesleinhuepf/napari-tabu/issues\",\"twitter\":\"\",\"version\":\"0.1.5\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"chris.havlin@gmail.com\",\"name\":\"Chris Havlin\"}],\"code_repository\":\"https://github.com/data-exp-lab/yt-napari\",\"conda\":[],\"description\":\"\\n\\n<!-- This file is designed to provide you with a starting template for documenting\\nthe functionality of your plugin. Its content will be rendered on your plugin's\\nnapari hub page.\\n\\nThe sections below are given as a guide for the flow of information only, and\\nare in no way prescriptive. You should feel free to merge, remove, add and\\nrename sections at will to make this document work best for your plugin.\\n\\n# Description\\n\\nThis should be a detailed description of the context of your plugin and its\\nintended purpose.\\n\\nIf you have videos or screenshots of your plugin in action, you should include them\\nhere as well, to make them front and center for new users.\\n\\nYou should use absolute links to these assets, so that we can easily display them\\non the hub. The easiest way to include a video is to use a GIF, for example hosted\\non imgur. You can then reference this GIF as an image.\\n\\n![Example GIF hosted on Imgur](https://i.imgur.com/A5phCX4.gif)\\n\\nNote that GIFs larger than 5MB won't be rendered by GitHub - we will however,\\nrender them on the napari hub.\\n\\nThe other alternative, if you prefer to keep a video, is to use GitHub's video\\nembedding feature.\\n\\n1. Push your `DESCRIPTION.md` to GitHub on your repository (this can also be done\\nas part of a Pull Request)\\n2. Edit `.napari/DESCRIPTION.md` **on GitHub**.\\n3. Drag and drop your video into its desired location. It will be uploaded and\\nhosted on GitHub for you, but will not be placed in your repository.\\n4. We will take the resolved link to the video and render it on the hub.\\n\\nHere is an example of an mp4 video embedded this way.\\n\\nhttps://user-images.githubusercontent.com/17995243/120088305-6c093380-c132-11eb-822d-620e81eb5f0e.mp4\\n\\n# Intended Audience & Supported Data\\n\\nThis section should describe the target audience for this plugin (any knowledge,\\nskills and experience required), as well as a description of the types of data\\nsupported by this plugin.\\n\\nTry to make the data description as explicit as possible, so that users know the\\nformat your plugin expects. This applies both to reader plugins reading file formats\\nand to function/dock widget plugins accepting layers and/or layer data.\\nFor example, if you know your plugin only works with 3D integer data in \\\"tyx\\\" order,\\nmake sure to mention this.\\n\\nIf you know of researchers, groups or labs using your plugin, or if it has been cited\\nanywhere, feel free to also include this information here.\\n\\n# Quickstart\\n\\nThis section should go through step-by-step examples of how your plugin should be used.\\nWhere your plugin provides multiple dock widgets or functions, you should split these\\nout into separate subsections for easy browsing. Include screenshots and videos\\nwherever possible to elucidate your descriptions.\\n\\nIdeally, this section should start with minimal examples for those who just want a\\nquick overview of the plugin's functionality, but you should definitely link out to\\nmore complex and in-depth tutorials highlighting any intricacies of your plugin, and\\nmore detailed documentation if you have it.\\n\\n# Additional Install Steps (uncommon)\\nWe will be providing installation instructions on the hub, which will be sufficient\\nfor the majority of plugins. They will include instructions to pip install, and\\nto install via napari itself.\\n\\nMost plugins can be installed out-of-the-box by just specifying the package requirements\\nover in `setup.cfg`. However, if your plugin has any more complex dependencies, or\\nrequires any additional preparation before (or after) installation, you should add\\nthis information here.\\n\\n# Getting Help\\n\\nThis section should point users to your preferred support tools, whether this be raising\\nan issue on GitHub, asking a question on image.sc, or using some other method of contact.\\nIf you distinguish between usage support and bug/feature support, you should state that\\nhere.\\n\\n# How to Cite\\n\\nMany plugins may be used in the course of published (or publishable) research, as well as\\nduring conference talks and other public facing events. If you'd like to be cited in\\na particular format, or have a DOI you'd like used, you should provide that information here. -->\\n\\nyt-napari provides plugins to help load data from [yt](https://yt-project.org/) into napari. It includes a graphical data loader, json-based file loading and helper-functions for jupyter notebook interaction. See the [full documentation](https://yt-napari.readthedocs.io) for more details.\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"\\nyt-napari provides plugins to help load data from yt into napari. It includes a graphical data loader, json-based file loading and helper-functions for jupyter notebook interaction. See the full documentation for more details.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"yt-napari\",\"documentation\":\"https://github.com/data-exp-lab/yt-napari#README.md\",\"first_released\":\"2022-05-02T22:27:00.024562Z\",\"license\":\"BSD-3-Clause\",\"name\":\"yt-napari\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\",\"widget\"],\"project_site\":\"https://github.com/data-exp-lab/yt-napari\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[\"*.json\"],\"release_date\":\"2022-05-02T22:27:00.024562Z\",\"report_issues\":\"https://github.com/data-exp-lab/yt-napari/issues\",\"requirements\":[\"magicgui (>=0.4.0)\",\"napari (>=0.4.13)\",\"npe2\",\"numpy\",\"pydantic\",\"yt (>=4.0.1)\"],\"summary\":\"A napari plugin for loading data from yt\",\"support\":\"https://github.com/data-exp-lab/yt-napari/issues\",\"twitter\":\"\",\"version\":\"0.0.1\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Niklas Netter\"}],\"code_repository\":\"https://github.com/gatoniel/napari-timeseries-opener-plugin\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-timeseries-opener-plugin\"}],\"description\":\"# napari-timeseries-opener-plugin\\n\\n[![License](https://img.shields.io/pypi/l/napari-timeseries-opener-plugin.svg?color=green)](https://github.com/gatoniel/napari-timeseries-opener-plugin/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-timeseries-opener-plugin.svg?color=green)](https://pypi.org/project/napari-timeseries-opener-plugin)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-timeseries-opener-plugin.svg?color=green)](https://python.org)\\n[![tests](https://github.com/gatoniel/napari-timeseries-opener-plugin/workflows/tests/badge.svg)](https://github.com/gatoniel/napari-timeseries-opener-plugin/actions)\\n[![codecov](https://codecov.io/gh/gatoniel/napari-timeseries-opener-plugin/branch/main/graph/badge.svg)](https://codecov.io/gh/gatoniel/napari-timeseries-opener-plugin)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-timeseries-opener-plugin)](https://napari-hub.org/plugins/napari-timeseries-opener-plugin)\\n\\nSimple plugin that opens separate .tif files as a 3-dimensional layer.\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/plugins/stable/index.html\\n-->\\n\\n## Run\\n\\nIn powershell run when you do not have sufficient GPU support in your environment\\n```\\n$env:CUDA_VISIBLE_DEVICES=-1; napari\\n```\\n\\n## Installation\\n\\nYou can install `napari-timeseries-opener-plugin` via [pip]:\\n\\n    pip install napari-timeseries-opener-plugin\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/gatoniel/napari-timeseries-opener-plugin.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-timeseries-opener-plugin\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/gatoniel/napari-timeseries-opener-plugin/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-timeseries-opener-plugin\\n\\n\\n\\n\\n\\n\\nSimple plugin that opens separate .tif files as a 3-dimensional layer.\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nRun\\nIn powershell run when you do not have sufficient GPU support in your environment\\n$env:CUDA_VISIBLE_DEVICES=-1; napari\\nInstallation\\nYou can install napari-timeseries-opener-plugin via pip:\\npip install napari-timeseries-opener-plugin\\n\\nTo install latest development version :\\npip install git+https://github.com/gatoniel/napari-timeseries-opener-plugin.git\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-timeseries-opener-plugin\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari-timeseries-opener-plugin\",\"documentation\":\"https://github.com/gatoniel/napari-timeseries-opener-plugin#README.md\",\"first_released\":\"2022-02-28T09:50:37.838408Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-timeseries-opener-plugin\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/gatoniel/napari-timeseries-opener-plugin\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-10-04T13:28:17.361703Z\",\"report_issues\":\"https://github.com/gatoniel/napari-timeseries-opener-plugin/issues\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"qtpy\",\"magicgui\",\"tifffile\",\"stardist\",\"tensorflow\"],\"summary\":\"Simple plugin that opens separate .tif files as a 3-dimensional layer.\",\"support\":\"https://github.com/gatoniel/napari-timeseries-opener-plugin/issues\",\"twitter\":\"\",\"version\":\"0.1.8\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Luis Perdigao\"}],\"code_repository\":null,\"description\":\"# okapi-em\\r\\n\\r\\nhttps://github.com/rosalindfranklininstitute/okapi-em\\r\\n\\r\\n<!--\\r\\n[![License](https://img.shields.io/pypi/l/okapi-em.svg?color=green)](https://github.com/rosalindfranklininstitute/okapi-em/raw/main/LICENSE)\\r\\n[![PyPI](https://img.shields.io/pypi/v/okapi-em.svg?color=green)](https://pypi.org/project/okapi-em)\\r\\n[![Python Version](https://img.shields.io/pypi/pyversions/okapi-em.svg?color=green)](https://python.org)\\r\\n[![tests](https://github.com/perdigao1/okapi-em/workflows/tests/badge.svg)](https://github.com/rosalindfranklininstitute/okapi-em/actions)\\r\\n[![codecov](https://codecov.io/gh/perdigao1/okapi-em/branch/main/graph/badge.svg)](https://codecov.io/gh/rosalindfranklininstitute/okapi-em)\\r\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/okapi-em)](https://napari-hub.org/plugins/okapi-em)\\r\\n-->\\r\\n\\r\\nA napari plugin for processing serial-FIB-SEM data.\\r\\n\\r\\nPowered by [chafer] and [quoll].\\r\\n\\r\\nThis [napari] plugin contains the following tools:\\r\\n\\r\\n- slice alignment using constrained SIFT\\r\\n- two charge artifact suppression filters\\r\\n    - directional fourier bandapass filter\\r\\n    - line-by-line filter function optimiser and subtraction (requires charge artifact labels) - uses [chafer]\\r\\n- fourier ring correlation (FRC) resolution estimation - uses [quoll]\\r\\n\\r\\n----------------------------------\\r\\n\\r\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\r\\n\\r\\n<!--\\r\\nDon't miss the full getting started guide to set up your new package:\\r\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\r\\n\\r\\nand review the napari docs for plugin developers:\\r\\nhttps://napari.org/plugins/stable/index.html\\r\\n-->\\r\\n\\r\\n## Installation\\r\\n\\r\\nYou can install `okapi-em` via [pip]:\\r\\n\\r\\n`pip install okapi-em`\\r\\n\\r\\nFor development mode it can be installed, clone this package then navigate to the cloned `okapi-em` folder and run:\\r\\n\\r\\n`pip install -e .`\\r\\n\\r\\nThis should install in any machine, however ...\\r\\n\\r\\nCurrently the FRC calculation provided by the [quoll] package which is optional because\\r\\nof its stringent environmemt requirements from miplib package. These currently are:\\r\\n\\r\\n- python 3.7\\r\\n- linux OS\\r\\n\\r\\nThis issue will be addressed in future version.\\r\\n\\r\\n\\r\\nTo install okapi-em with quoll included:\\r\\n\\r\\n`pip install okapi-em[all]`\\r\\n\\r\\nNote that to launch napari in python 3.7 you will need to use the command:\\r\\n\\r\\n`python -m napari`\\r\\n\\r\\n\\r\\n\\r\\n## Contributing\\r\\n\\r\\nContributions are very welcome. Tests can be run with [tox], please ensure\\r\\nthe coverage at least stays the same before you submit a pull request.\\r\\n\\r\\n## License\\r\\n\\r\\nDistributed under the terms of the [Apache Software License 2.0] license,\\r\\n\\\"okapi-em\\\" is free and open source software\\r\\n\\r\\n## Issues\\r\\n\\r\\nIf you encounter any problems, please file an issue along with a detailed description.\\r\\n\\r\\n[quoll]: https://github.com/rosalindfranklininstitute/quoll\\r\\n[chafer]: https://github.com/rosalindfranklininstitute/chafer\\r\\n[napari]: https://github.com/napari/napari\\r\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\r\\n[@napari]: https://github.com/napari\\r\\n[MIT]: http://opensource.org/licenses/MIT\\r\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\r\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\r\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\r\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\r\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\r\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\r\\n\\r\\n\\r\\n[tox]: https://tox.readthedocs.io/en/latest/\\r\\n[pip]: https://pypi.org/project/pip/\\r\\n[PyPI]: https://pypi.org/\\r\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"okapi-em\\nhttps://github.com/rosalindfranklininstitute/okapi-em\\n\\nA napari plugin for processing serial-FIB-SEM data.\\nPowered by chafer and quoll.\\nThis napari plugin contains the following tools:\\n\\nslice alignment using constrained SIFT\\ntwo charge artifact suppression filters\\ndirectional fourier bandapass filter\\nline-by-line filter function optimiser and subtraction (requires charge artifact labels) - uses chafer\\n\\n\\nfourier ring correlation (FRC) resolution estimation - uses quoll\\n\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install okapi-em via pip:\\npip install okapi-em\\nFor development mode it can be installed, clone this package then navigate to the cloned okapi-em folder and run:\\npip install -e .\\nThis should install in any machine, however ...\\nCurrently the FRC calculation provided by the quoll package which is optional because\\nof its stringent environmemt requirements from miplib package. These currently are:\\n\\npython 3.7\\nlinux OS\\n\\nThis issue will be addressed in future version.\\nTo install okapi-em with quoll included:\\npip install okapi-em[all]\\nNote that to launch napari in python 3.7 you will need to use the command:\\npython -m napari\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the Apache Software License 2.0 license,\\n\\\"okapi-em\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari okapi-em\",\"documentation\":\"\",\"first_released\":\"2022-10-19T18:19:19.663408Z\",\"license\":\"Apache-2.0\",\"name\":\"okapi-em\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2023-01-10T22:12:02.851781Z\",\"report_issues\":\"\",\"requirements\":[\"numpy\",\"magicgui\",\"chafer\",\"napari[all]\",\"opencv-python\",\"quoll (>=0.0.4)\",\"pyqt5\",\"imageio-ffmpeg ; extra == 'all'\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\"],\"summary\":\"napari plugin to deal with charging artifacts in tomography electron microscopy data\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.0.8\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"talley.lambert@gmail.com\",\"name\":\"The Open Microscopy and napari teams\"}],\"code_repository\":\"https://github.com/tlambert03/napari-omero\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-omero\"}],\"description\":\"# napari-omero\\n\\n[![License](https://img.shields.io/github/license/tlambert03/napari-omero)](LICENSE)\\n[![Version](https://img.shields.io/pypi/v/napari-omero.svg)](https://pypi.python.org/pypi/napari-omero)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-omero.svg)](https://python.org)\\n[![CI](https://github.com/tlambert03/napari-omero/workflows/CI/badge.svg)](https://github.com/tlambert03/napari-omero/actions)\\n<!-- [![conda-forge](https://img.shields.io/conda/vn/conda-forge/napari-omero)](https://anaconda.org/conda-forge/napari-omero) -->\\n\\nThis package provides interoperability between the\\n[OMERO](https://www.openmicroscopy.org/omero/) image management platform, and\\n[napari](https://github.com/napari/napari): a fast, multi-dimensional image\\nviewer for python.\\n\\nIt provides a GUI interface for browsing an OMERO instance from within napari,\\nas well as command line interface extensions for both OMERO and napari CLIs.\\n\\n![demo](https://github.com/tlambert03/napari-omero/blob/master/demo.gif?raw=true)\\n\\n## Features\\n\\n- GUI interface to browse remote OMERO data, with thumbnail previews.\\n- Loads remote nD images from an OMERO server into napari\\n- Planes are loading on demand as sliders are moved (\\\"lazy loading\\\").\\n- session management (login memory)\\n- OMERO rendering settings (contrast limits, colormaps, active channels, current\\n  Z/T position) are applied in napari\\n\\n### as a napari dock widget\\n\\nTo launch napari with the OMERO browser added, [install](#installation) this\\npackage and run:\\n\\n```bash\\nnapari-omero\\n```\\n\\nThe OMERO browser widget can also be manually added to the napari viewer:\\n\\n```python\\nimport napari\\n\\nviewer = napari.Viewer()\\nviewer.window.add_plugin_dock_widget('napari-omero')\\n\\nnapari.run()\\n```\\n\\n### as a napari plugin\\n\\nThis package provides a napari reader plugin that accepts OMERO resources as\\n\\\"proxy strings\\\" (e.g. `omero://Image:<ID>`) or as [OMERO webclient\\nURLS](https://help.openmicroscopy.org/urls-to-data.html).\\n\\n```python\\nviewer = napari.Viewer()\\n\\n# omero object identifier string\\nviewer.open(\\\"omero://Image:1\\\")\\n\\n# or URLS: https://help.openmicroscopy.org/urls-to-data.html\\nviewer.open(\\\"http://yourdomain.example.org/omero/webclient/?show=image-314\\\")\\n```\\n\\nthese will also work on the napari command line interface, e.g.:\\n\\n```bash\\nnapari omero://Image:1\\n# or\\nnapari http://yourdomain.example.org/omero/webclient/?show=image-314\\n```\\n\\n### as an OMERO CLI plugin\\n\\nThis package also serves as a plugin to the OMERO CLI\\n\\n```bash\\nomero napari view Image:1\\n```\\n\\n- ROIs created in napari can be saved back to OMERO via a \\\"Save ROIs\\\" button.\\n- napari viewer console has BlitzGateway 'conn' and 'omero_image' in context.\\n\\n## installation\\n\\nRequires python 3.7 - 3.10.\\n\\n### from conda\\n\\nIt's easiest to install `omero-py` from conda, so the recommended procedure\\nis to install everything from conda, using the `conda-forge` channel\\n\\n```python\\nconda install -c conda-forge napari-omero\\n```\\n\\n### from pip\\n\\n`napari-omero` itself can be installed from pip, but you will still need\\n`omero-py`\\n\\n```sh\\nconda create -n omero -c conda-forge python=3.9 omero-py\\nconda activate omero\\npip install napari-omero[all]  # the [all] here is the same as `napari[all]`\\n```\\n\\n## issues\\n\\n| ❗  | This is alpha software & some things will be broken or sub-optimal!  |\\n| --- | -------------------------------------------------------------------- |\\n\\n- experimental & definitely still buggy!  [Bug\\n  reports](https://github.com/tlambert03/napari-omero/issues/new) are welcome!\\n- remote loading can be very slow still... though this is not strictly an issue\\n  of this plugin.  Datasets are wrapped as delayed dask stacks, and remote data\\n  fetching time can be significant.  Plans for [asynchronous\\n  rendering](https://napari.org/guides/stable/rendering.html) in\\n  napari and\\n  [tiled loading from OMERO](https://github.com/tlambert03/napari-omero/pull/1)\\n  may eventually improve the subjective performance... but remote data loading\\n  will likely always be a limitation here.\\n  To try asyncronous loading, start the program with `NAPARI_ASYNC=1 napari-omero`.\\n\\n## contributing\\n\\nContributions are welcome!  To get setup with a development environment:\\n\\n```bash\\n# clone this repo:\\ngit clone https://github.com/tlambert03/napari-omero.git\\n# change into the new directory\\ncd napari-omero\\n# create conda environment\\nconda env create -f environment.yml\\n# activate the new env\\nconda activate napari-omero\\n\\n# install in editable mode\\npip install -e .\\n```\\n\\nTo maintain good code quality, this repo uses\\n[flake8](https://gitlab.com/pycqa/flake8),\\n[mypy](https://github.com/python/mypy), and\\n[black](https://github.com/psf/black).  To enforce code quality when you commit\\ncode, you can install pre-commit\\n\\n```bash\\n# install pre-commit which will run code checks prior to commits\\npre-commit install\\n```\\n\\nThe original OMERO data loader and CLI extension was created by [Will\\nMoore](https://github.com/will-moore).\\n\\nThe napari reader plugin and GUI browser was created by [Talley\\nLambert](https://github.com/tlambert03/)\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-omero\\n\\n\\n\\n\\n\\nThis package provides interoperability between the\\nOMERO image management platform, and\\nnapari: a fast, multi-dimensional image\\nviewer for python.\\nIt provides a GUI interface for browsing an OMERO instance from within napari,\\nas well as command line interface extensions for both OMERO and napari CLIs.\\n\\nFeatures\\n\\nGUI interface to browse remote OMERO data, with thumbnail previews.\\nLoads remote nD images from an OMERO server into napari\\nPlanes are loading on demand as sliders are moved (\\\"lazy loading\\\").\\nsession management (login memory)\\nOMERO rendering settings (contrast limits, colormaps, active channels, current\\n  Z/T position) are applied in napari\\n\\nas a napari dock widget\\nTo launch napari with the OMERO browser added, install this\\npackage and run:\\nbash\\nnapari-omero\\nThe OMERO browser widget can also be manually added to the napari viewer:\\n```python\\nimport napari\\nviewer = napari.Viewer()\\nviewer.window.add_plugin_dock_widget('napari-omero')\\nnapari.run()\\n```\\nas a napari plugin\\nThis package provides a napari reader plugin that accepts OMERO resources as\\n\\\"proxy strings\\\" (e.g. omero://Image:<ID>) or as OMERO webclient\\nURLS.\\n```python\\nviewer = napari.Viewer()\\nomero object identifier string\\nviewer.open(\\\"omero://Image:1\\\")\\nor URLS: https://help.openmicroscopy.org/urls-to-data.html\\nviewer.open(\\\"http://yourdomain.example.org/omero/webclient/?show=image-314\\\")\\n```\\nthese will also work on the napari command line interface, e.g.:\\n```bash\\nnapari omero://Image:1\\nor\\nnapari http://yourdomain.example.org/omero/webclient/?show=image-314\\n```\\nas an OMERO CLI plugin\\nThis package also serves as a plugin to the OMERO CLI\\nbash\\nomero napari view Image:1\\n\\nROIs created in napari can be saved back to OMERO via a \\\"Save ROIs\\\" button.\\nnapari viewer console has BlitzGateway 'conn' and 'omero_image' in context.\\n\\ninstallation\\nRequires python 3.7 - 3.10.\\nfrom conda\\nIt's easiest to install omero-py from conda, so the recommended procedure\\nis to install everything from conda, using the conda-forge channel\\npython\\nconda install -c conda-forge napari-omero\\nfrom pip\\nnapari-omero itself can be installed from pip, but you will still need\\nomero-py\\nsh\\nconda create -n omero -c conda-forge python=3.9 omero-py\\nconda activate omero\\npip install napari-omero[all]  # the [all] here is the same as `napari[all]`\\nissues\\n| ❗  | This is alpha software & some things will be broken or sub-optimal!  |\\n| --- | -------------------------------------------------------------------- |\\n\\nexperimental & definitely still buggy!  Bug\\n  reports are welcome!\\nremote loading can be very slow still... though this is not strictly an issue\\n  of this plugin.  Datasets are wrapped as delayed dask stacks, and remote data\\n  fetching time can be significant.  Plans for asynchronous\\n  rendering in\\n  napari and\\n  tiled loading from OMERO\\n  may eventually improve the subjective performance... but remote data loading\\n  will likely always be a limitation here.\\n  To try asyncronous loading, start the program with NAPARI_ASYNC=1 napari-omero.\\n\\ncontributing\\nContributions are welcome!  To get setup with a development environment:\\n```bash\\nclone this repo:\\ngit clone https://github.com/tlambert03/napari-omero.git\\nchange into the new directory\\ncd napari-omero\\ncreate conda environment\\nconda env create -f environment.yml\\nactivate the new env\\nconda activate napari-omero\\ninstall in editable mode\\npip install -e .\\n```\\nTo maintain good code quality, this repo uses\\nflake8,\\nmypy, and\\nblack.  To enforce code quality when you commit\\ncode, you can install pre-commit\\n```bash\\ninstall pre-commit which will run code checks prior to commits\\npre-commit install\\n```\\nThe original OMERO data loader and CLI extension was created by Will\\nMoore.\\nThe napari reader plugin and GUI browser was created by Talley\\nLambert\",\"development_status\":[\"Development Status :: 3 - Alpha\"],\"display_name\":\"napari-omero\",\"documentation\":\"\",\"first_released\":\"2020-06-24T20:52:31.571671Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-omero\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\",\"widget\"],\"project_site\":\"https://github.com/tlambert03/napari-omero\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[\"omero://*\"],\"release_date\":\"2022-05-22T23:27:08.824649Z\",\"report_issues\":\"https://github.com/tlambert03/napari-omero/issues\",\"requirements\":[\"napari (>=0.4.13)\",\"omero-py\",\"omero-rois\",\"napari[all] ; extra == 'all'\",\"black ; extra == 'dev'\",\"flake8 ; extra == 'dev'\",\"mypy ; extra == 'dev'\",\"pre-commit ; extra == 'dev'\",\"pytest ; extra == 'tests'\",\"pytest-cov ; extra == 'tests'\",\"pytest-qt ; extra == 'tests'\",\"pytest-xvfb ; (sys_platform == \\\"linux\\\") and extra == 'tests'\",\"pywin32 ; (sys_platform == \\\"win32\\\") and extra == 'tests'\"],\"summary\":\"napari/OMERO interoperability\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.2.0\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Léo Guignard\"}],\"code_repository\":\"https://github.com/leoguignard/napari-turing\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-turing\"}],\"description\":\"# napari-turing\\n\\n[![License MIT](https://img.shields.io/pypi/l/napari-turing.svg?color=green)](https://github.com/leoguignard/napari-turing/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-turing.svg?color=green)](https://pypi.org/project/napari-turing)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-turing.svg?color=green)](https://python.org)\\n[![tests](https://github.com/leoguignard/napari-turing/workflows/tests/badge.svg)](https://github.com/leoguignard/napari-turing/actions)\\n[![codecov](https://codecov.io/gh/leoguignard/napari-turing/branch/main/graph/badge.svg)](https://codecov.io/gh/leoguignard/napari-turing)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-turing)](https://napari-hub.org/plugins/napari-turing)\\n\\nA plugin to run simple simulations of Turing patterns\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/plugins/index.html\\n-->\\n![example 1](img/turing_patterns.gif)\\n![example 2](img/turing_patterns2.gif)\\n\\n## Installation\\n\\nYou can install `napari-turing` via [pip] after downloading the content of\\n\\n    pip install napari-turing\\n\\n\\nTo install latest version :\\n\\n    pip install git+https://github.com/leoguignard/napari-turing.git\\n\\n## Troubleshooting\\n\\nIf the installation does not work just with the previous command, it might be useful to first install [napari] for example that way:\\n\\n    conda install napari\\n\\n## Creating a new model\\n\\nTo create your own model, you can use the template for models [here](src/napari_turing/Models/ModelTemplate.py).\\n\\nNote that a bit of knowledge in Python is probably necessary and it might not be completely trivial at first but you'll manage :)\\n\\nFirst you need to name the concentrations that you will use with the following [line](src/napari_turing/Models/ModelTemplate.py#L40):\\n```python\\n    _concentration_names = [\\\"A\\\", \\\"I\\\"]\\n```\\n\\nThen, you need to declare its variables. For example you can create a parameter named `mu_a` the following way ([here in the code](src/napari_turing/Models/ModelTemplate.py#L53-L60)):\\n```python\\n    mu_a = ModelParameter(\\n        name=\\\"mu_a\\\",  # Name of the parameter\\n        description=\\\"Activator diffusion coefficient (10^-4)\\\",  # Description of the parameter for napari\\n        value=2.8,  # Initial and default value\\n        min=1,  # Minimum value the parameter can take\\n        max=5,  # Maximum value the parameter can take\\n        exponent=1e-4,  # All values given to this instance of the class will but multiplied by this value\\n    )\\n```\\n\\nThen you need to list the parameters that are necessary to run the model (usually all the paramaters declared previously) and the parameters that you will allow the user to tune (for example, sometimes some of the parameters are co-dependent and there is no point in being able to tune both of them). That should be done the following way ([here in the code](src/napari_turing/Models/ModelTemplate.py#L86-89)):\\n```python\\n    # These are the parameters that are necessary to run the equations.\\n    _necessary_parameters = [tau, k, mu_a, mu_i]\\n    # These are the parameters that can be modified via napari\\n    _tunable_parameters = _necessary_parameters\\n```\\n\\nIf you want, you can specify what the method will return as a string, it will be displayed in the napari viewer ([here in the code](src/napari_turing/Models/ModelTemplate.py#L90-L98)):\\n```python\\n    # This function allows to display some information about the model\\n    # in napari\\n    def __str__(self) -> str:\\n        return (\\n            \\\"Equations (FitzHugh-Nagumo model):\\\\n\\\"\\n            \\\"  Concentration of Activator (a) and Inhibitor (i)\\\\n\\\"\\n            \\\"    - da/dt = mu_a * diffusion(a) + a - a^3 - i + k\\\\n\\\"\\n            \\\"    - tau * di/dt = mu_i * diffusion(i) + a - i\\\"\\n        )\\n```\\n\\nNow that the basics are declared, you will need to declare how to initialize your concentrations the following way ([here in the code](src/napari_turing/Models/ModelTemplate.py#L100-L116)):\\n```python\\n    # The following allows to reset the values of the concentrations.\\n    # The function takes the name of the concentration to initialize.\\n    # If no name is given or if it is None all the concentrations are\\n    # reinitialized.\\n    #\\n    # The reason why this function is useful is that some models \\n    # require specific initialisations for them to work correctly\\n    # In the following example the concentrations are reintinalized\\n    # to a random value between -1 and 1.\\n    # This is the default behavior, so if you don't need to change\\n    # it you don't have to implement the function.\\n    def init_concentrations(self, C: Optional[str] = None) -> None:\\n        if C is None:\\n            for ci in self.concentration_names():\\n                self[ci] = np.random.random((self.size, self.size)) * 2 - 1\\n        else:\\n            self[C] = np.random.random((self.size, self.size)) * 2 - 1\\n```\\nIn the previous example, the all concentrations are initialized the same way. If you need to have different initializations, you can do it the following way for example ([from the GrayScott model](src/napari_turing/Models/GrayScott.py#L68-L76)):\\n```python\\n    def init_concentrations(self, C: Optional[str] = None) -> None:\\n        if C == \\\"X\\\" or C is None:\\n            self[\\\"X\\\"] = np.ones((self.size, self.size))\\n        if C == \\\"Y\\\" or C is None:\\n            Y = np.zeros((self.size, self.size))\\n            nb_pos = 20\\n            pos = (np.random.random((2, nb_pos)) * self.size).astype(int)\\n            Y[pos[0], pos[1]] = 1\\n            self[\\\"Y\\\"] = Y\\n```\\nIn this model, there are two concentrations, `X` and `Y` which are initialized differenty. Note that they can be accessed using `self[\\\"X\\\"]` or `self.X`.\\n\\nFinally, you of course have to define the reaction equations and the diffusion equations. The way it is defined is with two functions, one for the reaction and one for the diffusion, that take as an input the name of the concentration to apply the function to and returns the new values. Then for each of your concentrations, their new values will be computed as followed:\\n```python\\nnew_concentration = current_concentration + dt*(reaction + diffusion)\\n```\\n\\nHere is an example for the reaction function ([here in the code](src/napari_turing/Models/ModelTemplate.py#L127-L136)):\\n```python \\n    # This function defines the equations of the reactions.\\n    # It takes as an input which concentration to compute\\n    # (in this example we have to define how to compute A and I)\\n    def _reaction(self, c: str) -> np.ndarray:\\n        if c == \\\"A\\\":\\n            # Below is the reaction part of the equation (1)\\n            return self.A - self.A**3 - self.I + self.k \\n        elif c == \\\"I\\\":\\n            # Below is the reaction part of the equation (2)\\n            return (self.A - self.I) / self.tau\\n```\\nOf course, if you have more concentrations, you will need to define more equations.\\n\\nHere is an example for the reaction function ([here in the code](src/napari_turing/Models/ModelTemplate.py#L138-L166)):\\n```python\\n    # This function defines the equations of the diffusion.\\n    # It takes as an input which concentration to compute\\n    # (in this example we have to define how to compute A and I)\\n    # Here we compute the diffusion as follow:\\n    # A cell gives an equal fraction mu of its concentration to its neighbors\\n    # A cell recieves an equal fraction mu of concentration from its neighbors\\n    # Neighbors = (left, right, above, below)\\n    # In the case of oriented diffusion the amount recieved and given to the neighbors\\n    # is imbalanced according to the position of the neighbor.\\n    def _diffusion(self, c: str) -> np.ndarray:\\n        if c == \\\"A\\\":\\n            arr = self.A # Define the array of concentrations to diffuse for the reageant A\\n            mu = self.mu_a # Define the diffusion coefficient for the reageant A\\n        elif c == \\\"I\\\":\\n            arr = self.I # Define the array of concentrations to diffuse for the reageant I\\n            mu = self.mu_i # Define the diffusion coefficient for the reageant I\\n        \\n        # Computes what is recieved from neighboring cells\\n        from_cell = convolve(arr, self.kernel.value, mode=\\\"constant\\\", cval=0)\\n        # Computes what is given to neighboring cells\\n        to_cell = self.nb_neighbs * arr\\n\\n        # Computes the diffusion\\n        out = mu * (from_cell - to_cell) / (self.dx * self.dy)\\n\\n        # In our case, the equation (2), for I specify that it has to be divided by tau\\n        if c == \\\"I\\\":\\n            out /= self.tau\\n        return out\\n```\\nThe diffusion function is usually a standard one so it might not be necessary to overly change it.\\n\\nYou can find other model examples:\\n- [Brusselator](src/napari_turing/Models/Brusselator.py)\\n- [GrayScott](src/napari_turing/Models/GrayScott.py)\\n- [GameOfLife](src/napari_turing/Models/GameOfLife.py)\\n\\nOnce all that is done, let say you've saved your new model in the folder [Models](src/napari_turing/Models) under the name `NewModel.py` and the model class created is name `NewModel`. Then you need to declare you model in the [`_model_list.py`](src/napari_turing/Models/_model_list.py) file. To do so you need to add the following lines in the file:\\n```python\\nfrom enum import Enum\\nfrom .FitzHughNagumo import FitzHughNagumo\\nfrom .Brusselator import Brusselator\\nfrom .GrayScott import GrayScott\\nfrom .GameOfLife import GameOfLife\\nfrom .NewModel import NewModel ## THAT LINE HERE\\n\\nclass AvailableModels(Enum):\\n    FitzHughNagumo = FitzHughNagumo\\n    Brusselator = Brusselator\\n    GrayScott = GrayScott\\n    GameOfLife = GameOfLife\\n    NewModel = NewModel ## AND THAT OTHER LINE HERE\\n```\\n\\n## Contributing\\n\\nContributions are very welcome.\\n\\n## License\\n\\nDistributed under the terms of the [MIT] license,\\n\\\"napari-turing\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/leoguignard/napari-turing/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-turing\\n\\n\\n\\n\\n\\n\\nA plugin to run simple simulations of Turing patterns\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\n\\n\\nInstallation\\nYou can install napari-turing via pip after downloading the content of\\npip install napari-turing\\n\\nTo install latest version :\\npip install git+https://github.com/leoguignard/napari-turing.git\\n\\nTroubleshooting\\nIf the installation does not work just with the previous command, it might be useful to first install napari for example that way:\\nconda install napari\\n\\nCreating a new model\\nTo create your own model, you can use the template for models here.\\nNote that a bit of knowledge in Python is probably necessary and it might not be completely trivial at first but you'll manage :)\\nFirst you need to name the concentrations that you will use with the following line:\\npython\\n    _concentration_names = [\\\"A\\\", \\\"I\\\"]\\nThen, you need to declare its variables. For example you can create a parameter named mu_a the following way (here in the code):\\npython\\n    mu_a = ModelParameter(\\n        name=\\\"mu_a\\\",  # Name of the parameter\\n        description=\\\"Activator diffusion coefficient (10^-4)\\\",  # Description of the parameter for napari\\n        value=2.8,  # Initial and default value\\n        min=1,  # Minimum value the parameter can take\\n        max=5,  # Maximum value the parameter can take\\n        exponent=1e-4,  # All values given to this instance of the class will but multiplied by this value\\n    )\\nThen you need to list the parameters that are necessary to run the model (usually all the paramaters declared previously) and the parameters that you will allow the user to tune (for example, sometimes some of the parameters are co-dependent and there is no point in being able to tune both of them). That should be done the following way (here in the code):\\npython\\n    # These are the parameters that are necessary to run the equations.\\n    _necessary_parameters = [tau, k, mu_a, mu_i]\\n    # These are the parameters that can be modified via napari\\n    _tunable_parameters = _necessary_parameters\\nIf you want, you can specify what the method will return as a string, it will be displayed in the napari viewer (here in the code):\\npython\\n    # This function allows to display some information about the model\\n    # in napari\\n    def __str__(self) -> str:\\n        return (\\n            \\\"Equations (FitzHugh-Nagumo model):\\\\n\\\"\\n            \\\"  Concentration of Activator (a) and Inhibitor (i)\\\\n\\\"\\n            \\\"    - da/dt = mu_a * diffusion(a) + a - a^3 - i + k\\\\n\\\"\\n            \\\"    - tau * di/dt = mu_i * diffusion(i) + a - i\\\"\\n        )\\nNow that the basics are declared, you will need to declare how to initialize your concentrations the following way (here in the code):\\npython\\n    # The following allows to reset the values of the concentrations.\\n    # The function takes the name of the concentration to initialize.\\n    # If no name is given or if it is None all the concentrations are\\n    # reinitialized.\\n    #\\n    # The reason why this function is useful is that some models \\n    # require specific initialisations for them to work correctly\\n    # In the following example the concentrations are reintinalized\\n    # to a random value between -1 and 1.\\n    # This is the default behavior, so if you don't need to change\\n    # it you don't have to implement the function.\\n    def init_concentrations(self, C: Optional[str] = None) -> None:\\n        if C is None:\\n            for ci in self.concentration_names():\\n                self[ci] = np.random.random((self.size, self.size)) * 2 - 1\\n        else:\\n            self[C] = np.random.random((self.size, self.size)) * 2 - 1\\nIn the previous example, the all concentrations are initialized the same way. If you need to have different initializations, you can do it the following way for example (from the GrayScott model):\\npython\\n    def init_concentrations(self, C: Optional[str] = None) -> None:\\n        if C == \\\"X\\\" or C is None:\\n            self[\\\"X\\\"] = np.ones((self.size, self.size))\\n        if C == \\\"Y\\\" or C is None:\\n            Y = np.zeros((self.size, self.size))\\n            nb_pos = 20\\n            pos = (np.random.random((2, nb_pos)) * self.size).astype(int)\\n            Y[pos[0], pos[1]] = 1\\n            self[\\\"Y\\\"] = Y\\nIn this model, there are two concentrations, X and Y which are initialized differenty. Note that they can be accessed using self[\\\"X\\\"] or self.X.\\nFinally, you of course have to define the reaction equations and the diffusion equations. The way it is defined is with two functions, one for the reaction and one for the diffusion, that take as an input the name of the concentration to apply the function to and returns the new values. Then for each of your concentrations, their new values will be computed as followed:\\npython\\nnew_concentration = current_concentration + dt*(reaction + diffusion)\\nHere is an example for the reaction function (here in the code):\\npython \\n    # This function defines the equations of the reactions.\\n    # It takes as an input which concentration to compute\\n    # (in this example we have to define how to compute A and I)\\n    def _reaction(self, c: str) -> np.ndarray:\\n        if c == \\\"A\\\":\\n            # Below is the reaction part of the equation (1)\\n            return self.A - self.A**3 - self.I + self.k \\n        elif c == \\\"I\\\":\\n            # Below is the reaction part of the equation (2)\\n            return (self.A - self.I) / self.tau\\nOf course, if you have more concentrations, you will need to define more equations.\\nHere is an example for the reaction function (here in the code):\\n```python\\n    # This function defines the equations of the diffusion.\\n    # It takes as an input which concentration to compute\\n    # (in this example we have to define how to compute A and I)\\n    # Here we compute the diffusion as follow:\\n    # A cell gives an equal fraction mu of its concentration to its neighbors\\n    # A cell recieves an equal fraction mu of concentration from its neighbors\\n    # Neighbors = (left, right, above, below)\\n    # In the case of oriented diffusion the amount recieved and given to the neighbors\\n    # is imbalanced according to the position of the neighbor.\\n    def _diffusion(self, c: str) -> np.ndarray:\\n        if c == \\\"A\\\":\\n            arr = self.A # Define the array of concentrations to diffuse for the reageant A\\n            mu = self.mu_a # Define the diffusion coefficient for the reageant A\\n        elif c == \\\"I\\\":\\n            arr = self.I # Define the array of concentrations to diffuse for the reageant I\\n            mu = self.mu_i # Define the diffusion coefficient for the reageant I\\n    # Computes what is recieved from neighboring cells\\n    from_cell = convolve(arr, self.kernel.value, mode=\\\"constant\\\", cval=0)\\n    # Computes what is given to neighboring cells\\n    to_cell = self.nb_neighbs * arr\\n\\n    # Computes the diffusion\\n    out = mu * (from_cell - to_cell) / (self.dx * self.dy)\\n\\n    # In our case, the equation (2), for I specify that it has to be divided by tau\\n    if c == \\\"I\\\":\\n        out /= self.tau\\n    return out\\n\\n```\\nThe diffusion function is usually a standard one so it might not be necessary to overly change it.\\nYou can find other model examples:\\n- Brusselator\\n- GrayScott\\n- GameOfLife\\nOnce all that is done, let say you've saved your new model in the folder Models under the name NewModel.py and the model class created is name NewModel. Then you need to declare you model in the _model_list.py file. To do so you need to add the following lines in the file:\\n```python\\nfrom enum import Enum\\nfrom .FitzHughNagumo import FitzHughNagumo\\nfrom .Brusselator import Brusselator\\nfrom .GrayScott import GrayScott\\nfrom .GameOfLife import GameOfLife\\nfrom .NewModel import NewModel ## THAT LINE HERE\\nclass AvailableModels(Enum):\\n    FitzHughNagumo = FitzHughNagumo\\n    Brusselator = Brusselator\\n    GrayScott = GrayScott\\n    GameOfLife = GameOfLife\\n    NewModel = NewModel ## AND THAT OTHER LINE HERE\\n```\\nContributing\\nContributions are very welcome.\\nLicense\\nDistributed under the terms of the MIT license,\\n\\\"napari-turing\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"Turing Patterns\",\"documentation\":\"https://github.com/leoguignard/napari-turing#README.md\",\"first_released\":\"2022-08-08T16:42:05.972803Z\",\"license\":\"MIT\",\"name\":\"napari-turing\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/leoguignard/napari-turing\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-10-27T14:43:28.246116Z\",\"report_issues\":\"https://github.com/leoguignard/napari-turing/issues\",\"requirements\":[\"numpy\",\"scipy\",\"scikit-image\",\"magicgui\",\"qtpy\",\"napari\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\"],\"summary\":\"A plugin to run simmple simulations of Turing patterns\",\"support\":\"https://github.com/leoguignard/napari-turing/issues\",\"twitter\":\"\",\"version\":\"0.3.2\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Tom Burke\"},{\"name\":\"Joran Deschamps\"}],\"code_repository\":\"https://github.com/juglab/napari-n2v\",\"description\":\"# napari-n2v\\n\\n[![License](https://img.shields.io/pypi/l/napari-n2v.svg?color=green)](https://github.com/juglab/napari-n2v/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-n2v.svg?color=green)](https://pypi.org/project/napari-n2v)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-n2v.svg?color=green)](https://python.org)\\n[![tests](https://github.com/juglab/napari-n2v/workflows/build/badge.svg)](https://github.com/juglab/napari-n2v/actions)\\n[![codecov](https://codecov.io/gh/juglab/napari-n2v/branch/main/graph/badge.svg)](https://codecov.io/gh/juglab/napari-n2v)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-n2v)](https://napari-hub.org/plugins/napari-n2v)\\n\\nA self-supervised denoising algorithm now usable by all in napari.\\n\\n<img src=\\\"https://raw.githubusercontent.com/juglab/napari-n2v/master/docs/images/noisy_denoised.png\\\" width=\\\"800\\\" />\\n----------------------------------\\n\\n## Installation\\n\\nCheck out the [documentation](https://juglab.github.io/napari-n2v/installation.html) for more detailed installation \\ninstructions. \\n\\n\\n## Quick demo\\n\\nYou can try out a demo by loading the `N2V Demo prediction` plugin and directly clicking on `Predict`. This model was trained using the [N2V2 example](https://juglab.github.io/napari-n2v/examples.html).\\n\\n\\n<img src=\\\"https://raw.githubusercontent.com/juglab/napari-n2v/master/docs/images/demo.gif\\\" width=\\\"800\\\" />\\n\\n\\n## Documentation\\n\\nDocumentation is available on the [project website](https://juglab.github.io/napari-n2v/).\\n\\n\\n## Contributing and feedback\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request. You can also \\nhelp us improve by [filing an issue] along with a detailed description or contact us\\nthrough the [image.sc](https://forum.image.sc/) forum (tag @jdeschamps).\\n\\n\\n## Citations\\n\\n### N2V\\n\\nAlexander Krull, Tim-Oliver Buchholz, and Florian Jug. \\\"[Noise2void-learning denoising from single noisy images.](https://ieeexplore.ieee.org/document/8954066)\\\" \\n*Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*. 2019.\\n\\n### structN2V\\n\\nColeman Broaddus, et al. \\\"[Removing structured noise with self-supervised blind-spot networks.](https://ieeexplore.ieee.org/document/9098336)\\\" *2020 IEEE 17th \\nInternational Symposium on Biomedical Imaging (ISBI)*. IEEE, 2020.\\n\\n### N2V2\\n\\nEva Hoeck, Tim-Oliver Buchholz, et al. \\\"[N2V2 - Fixing Noise2Void Checkerboard Artifacts with Modified Sampling Strategies and a Tweaked Network Architecture](https://arxiv.org/abs/2211.08512)\\\", arXiv (2022). \\n\\n## Acknowledgements\\n\\nThis plugin was developed thanks to the support of the Silicon Valley Community Foundation (SCVF) and the \\nChan-Zuckerberg Initiative (CZI) with the napari Plugin Accelerator grant _2021-240383_.\\n\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-n2v\\\" is a free and open source software.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[filing an issue]: https://github.com/juglab/napari-n2v/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-n2v\\n\\n\\n\\n\\n\\n\\nA self-supervised denoising algorithm now usable by all in napari.\\n\\nInstallation\\nCheck out the documentation for more detailed installation \\ninstructions. \\nQuick demo\\nYou can try out a demo by loading the N2V Demo prediction plugin and directly clicking on Predict. This model was trained using the N2V2 example.\\n\\nDocumentation\\nDocumentation is available on the project website.\\nContributing and feedback\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request. You can also \\nhelp us improve by filing an issue along with a detailed description or contact us\\nthrough the image.sc forum (tag @jdeschamps).\\nCitations\\nN2V\\nAlexander Krull, Tim-Oliver Buchholz, and Florian Jug. \\\"Noise2void-learning denoising from single noisy images.\\\" \\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019.\\nstructN2V\\nColeman Broaddus, et al. \\\"Removing structured noise with self-supervised blind-spot networks.\\\" 2020 IEEE 17th \\nInternational Symposium on Biomedical Imaging (ISBI). IEEE, 2020.\\nN2V2\\nEva Hoeck, Tim-Oliver Buchholz, et al. \\\"N2V2 - Fixing Noise2Void Checkerboard Artifacts with Modified Sampling Strategies and a Tweaked Network Architecture\\\", arXiv (2022). \\nAcknowledgements\\nThis plugin was developed thanks to the support of the Silicon Valley Community Foundation (SCVF) and the \\nChan-Zuckerberg Initiative (CZI) with the napari Plugin Accelerator grant 2021-240383.\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-n2v\\\" is a free and open source software.\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"napari n2v\",\"documentation\":\"https://juglab.github.io/napari-n2v/\",\"first_released\":\"2022-10-24T13:33:45.533918Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-n2v\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\",\"sample_data\"],\"project_site\":\"https://github.com/juglab/napari-n2v\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2023-01-30T11:59:02.017806Z\",\"report_issues\":\"https://github.com/juglab/napari-n2v/issues\",\"requirements\":[\"scikit-image\",\"bioimageio.core\",\"n2v (>=0.3.2)\",\"napari-time-slicer (>=0.4.9)\",\"napari (<=0.4.15)\",\"vispy (<=0.9.6)\",\"magicgui\",\"qtpy\",\"pyqtgraph\",\"imageio (!=2.11.0,!=2.22.1,>=2.5.0)\",\"protobuf (<3.20,>=3.9.2)\",\"tensorflow (>=2.7.0) ; platform_system != \\\"Darwin\\\" or platform_machine != \\\"arm64\\\"\",\"tensorflow-macos ; platform_system == \\\"Darwin\\\" and platform_machine == \\\"arm64\\\"\",\"tensorflow-metal ; platform_system == \\\"Darwin\\\" and platform_machine == \\\"arm64\\\"\",\"numpy (<1.24.0) ; python_version < \\\"3.9\\\"\",\"numpy ; python_version >= \\\"3.9\\\"\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\"],\"summary\":\"A self-supervised denoising algorithm now usable by all in napari.\",\"support\":\"https://github.com/juglab/napari-n2v/issues\",\"twitter\":\"\",\"version\":\"0.1.0\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"alisterburt@gmail.com\",\"name\":\"Alister Burt\"}],\"code_repository\":\"https://github.com/alisterburt/napari-subboxer\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-subboxer\"}],\"description\":\"# napari-subboxer\\n\\n[![License](https://img.shields.io/pypi/l/napari-subboxer.svg?color=green)](https://github.com/alisterburt/napari-subboxer/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-subboxer.svg?color=green)](https://pypi.org/project/napari-subboxer)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-subboxer.svg?color=green)](https://python.org)\\n[![tests](https://github.com/alisterburt/napari-subboxer/workflows/tests/badge.svg)](https://github.com/alisterburt/napari-subboxer/actions)\\n[![codecov](https://codecov.io/gh/alisterburt/napari-subboxer/branch/master/graph/badge.svg)](https://codecov.io/gh/alisterburt/napari-subboxer)\\n\\nA napari plugin for visualising and interacting with electron cryotomograms.\\n\\n\\n## Installation\\n\\nYou can install `napari-subboxer` via [pip]:\\n\\n    pip install napari-subboxer\\n\\n## Usage\\n\\nThis plugin provides a user interface for opening electron cryotomograms in \\nnapari as both volumes and slices through volumes.\\n\\n![demo](https://user-images.githubusercontent.com/7307488/138575305-b05c4735-9c03-4629-bfb0-9612ea8f26fd.gif)\\n\\nThe plugin can be opened from the `plugins` menu in napari, or with \\n`napari-subboxer` at the command line.\\n\\n![plugins-menu](https://user-images.githubusercontent.com/7307488/138575015-00ea78d9-02c1-44bc-9034-0c0a7fa8d973.png)\\n\\n```yaml\\nUsage: napari-subboxer [TOMOGRAM_FILE]\\n\\n  An interactive tool for defining and applying relative transforms\\n  on sets of particles in napari.\\n\\nArguments:\\n  [TOMOGRAM_FILE]\\n\\nOptions:\\n  --help                          Show this message and exit.\\n\\n```\\n\\n## Contributing\\n\\nContributions are very welcome. \\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-subboxer\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/alisterburt/napari-subboxer/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-subboxer\\n\\n\\n\\n\\n\\nA napari plugin for visualising and interacting with electron cryotomograms.\\nInstallation\\nYou can install napari-subboxer via pip:\\npip install napari-subboxer\\n\\nUsage\\nThis plugin provides a user interface for opening electron cryotomograms in \\nnapari as both volumes and slices through volumes.\\n\\nThe plugin can be opened from the plugins menu in napari, or with \\nnapari-subboxer at the command line.\\n\\n```yaml\\nUsage: napari-subboxer [TOMOGRAM_FILE]\\nAn interactive tool for defining and applying relative transforms\\n  on sets of particles in napari.\\nArguments:\\n  [TOMOGRAM_FILE]\\nOptions:\\n  --help                          Show this message and exit.\\n```\\nContributing\\nContributions are very welcome. \\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-subboxer\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari-subboxer\",\"documentation\":\"https://github.com/alisterburt/napari-subboxer#README.md\",\"first_released\":\"2021-11-22T10:40:55.622000Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-subboxer\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/alisterburt/napari-subboxer\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2021-11-22T10:40:55.622000Z\",\"report_issues\":\"https://github.com/alisterburt/napari-subboxer/issues\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"napari[pyqt5] (==0.4.12)\",\"mrcfile\",\"typer\",\"eulerangles\",\"starfile\",\"einops\",\"pydantic\"],\"summary\":\"A napari plugin for interacting with electron cryotomograms\",\"support\":\"https://github.com/alisterburt/napari-subboxer/issues\",\"twitter\":\"\",\"version\":\"0.0.1\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Robert Haase\"},{\"name\":\"Tim Morello\"},{\"name\":\"Marcelo Leomil Zoccoler\"}],\"code_repository\":\"https://github.com/biapol/napari-crop\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-crop\"}],\"description\":\"# napari-crop\\n\\n[![License](https://img.shields.io/pypi/l/napari-crop.svg?color=green)](https://github.com/haesleinhuepf/napari-crop/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-crop.svg?color=green)](https://pypi.org/project/napari-crop)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-crop.svg?color=green)](https://python.org)\\n[![tests](https://github.com/haesleinhuepf/napari-crop/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-crop/actions)\\n[![codecov](https://codecov.io/gh/haesleinhuepf/napari-crop/branch/master/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-crop)\\n\\nCrop regions in napari manually\\n\\n![](https://github.com/haesleinhuepf/napari-crop/raw/main/images/screencast.gif)\\n\\nCrop in any dimension\\n\\n![](https://github.com/haesleinhuepf/napari-crop/blob/main/images/side_crop.gif)\\n\\n## Usage\\nCreate a new shapes layer to annotate the region you would like to crop:\\n\\n![](https://github.com/haesleinhuepf/napari-crop/raw/main/images/shapes.png)\\n\\nUse the rectangle tool to annotate a region. Start the `crop` tool from the `Tools > Utilities > Crop region` menu. \\nClick the `Run` button to crop the region.\\n\\n![](https://github.com/haesleinhuepf/napari-crop/raw/main/images/draw_rectangle.png)\\n\\nYou can also use the `Select shapes` tool to move the rectangle to a new place and crop another region by clicking on `Run`.\\n\\n![](https://github.com/haesleinhuepf/napari-crop/raw/main/images/move_rectangle.png)\\n\\nHint: You can also use the [napari-tabu](https://www.napari-hub.org/plugins/napari-tabu) plugin to send all your cropped images to a new napari window.\\n\\n![](https://github.com/haesleinhuepf/napari-crop/raw/main/images/new_window.gif)\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n## Installation\\n\\nYou can install `napari-crop` via [pip]:\\n\\n    pip install napari-crop\\n\\n## Contributing\\n\\nContributions are very welcome. \\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-crop\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please create a thread on [image.sc] along with a detailed description and tag [@haesleinhuepf].\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/haesleinhuepf/napari-crop/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n[image.sc]: https://image.sc\\n[@haesleinhuepf]: https://twitter.com/haesleinhuepf\\n\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-crop\\n\\n\\n\\n\\n\\nCrop regions in napari manually\\n\\nCrop in any dimension\\n\\nUsage\\nCreate a new shapes layer to annotate the region you would like to crop:\\n\\nUse the rectangle tool to annotate a region. Start the crop tool from the Tools > Utilities > Crop region menu. \\nClick the Run button to crop the region.\\n\\nYou can also use the Select shapes tool to move the rectangle to a new place and crop another region by clicking on Run.\\n\\nHint: You can also use the napari-tabu plugin to send all your cropped images to a new napari window.\\n\\n\\nThis napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.\\nInstallation\\nYou can install napari-crop via pip:\\npip install napari-crop\\n\\nContributing\\nContributions are very welcome. \\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-crop\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please create a thread on image.sc along with a detailed description and tag @haesleinhuepf.\",\"development_status\":[\"Development Status :: 3 - Alpha\"],\"display_name\":\"napari-crop\",\"documentation\":\"https://github.com/biapol/napari-crop#README.md\",\"first_released\":\"2021-10-21T18:34:09.956294Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-crop\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/biapol/napari-crop\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-01-05T18:25:40.159587Z\",\"report_issues\":\"https://github.com/biapol/napari-crop/issues\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"scikit-image\",\"napari-tools-menu\"],\"summary\":\"Crop regions in napari manually\",\"support\":\"https://github.com/biapol/napari-crop/issues\",\"twitter\":\"\",\"version\":\"0.1.5\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"sofroniewn@gmail.com\",\"name\":\"Nicholas Sofroniew\"}],\"code_repository\":\"https://github.com/napari/napari-svg\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-svg\"}],\"description\":\"# napari-svg\\n\\n[![License](https://img.shields.io/pypi/l/napari-svg.svg?color=green)](https://github.com/napari/napari-svg/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-svg.svg?color=green)](https://pypi.org/project/napari-svg)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-svg.svg?color=green)](https://python.org)\\n[![tests](https://github.com/napari/napari-svg/workflows/tests/badge.svg)](https://github.com/napari/napari-svg/actions)\\n[![codecov](https://codecov.io/gh/napari/napari-svg/branch/master/graph/badge.svg)](https://codecov.io/gh/napari/napari-svg)\\n\\nA plugin for reading and writing svg files with napari\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-svg` via [pip]:\\n\\n    pip install napari-svg\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-svg\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/napari/napari-svg/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-svg\\n\\n\\n\\n\\n\\nA plugin for reading and writing svg files with napari\\n\\nThis napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-svg via pip:\\npip install napari-svg\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-svg\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"napari SVG\",\"documentation\":\"\",\"first_released\":\"2020-04-13T03:37:20.169990Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-svg\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"writer\"],\"project_site\":\"https://github.com/napari/napari-svg\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-01-13T14:20:45.453540Z\",\"report_issues\":\"\",\"requirements\":[\"imageio (>=2.5.0)\",\"numpy (>=1.16.0)\",\"napari-plugin-engine (>=0.1.4)\",\"vispy (>=0.6.4)\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"napari (>=0.4) ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\"],\"summary\":\"A plugin for reading and writing svg files with napari\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.1.6\",\"visibility\":\"public\",\"writer_file_extensions\":[\".svg\"],\"writer_save_layers\":[\"points\",\"labels\",\"vectors\",\"image\",\"shapes\"]}",
  "{\"authors\":[{\"name\":\"Tom Burke\"}],\"code_repository\":\"https://github.com/juglab/napari-patchcreator\",\"conda\":[],\"description\":\"# napari-patchcreator\\n\\n[![License BSD-3](https://img.shields.io/pypi/l/napari-patchcreator.svg?color=green)](https://github.com/juglab/napari-patchcreator/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-patchcreator.svg?color=green)](https://pypi.org/project/napari-patchcreator)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-patchcreator.svg?color=green)](https://python.org)\\n[![tests](https://github.com/juglab/napari-patchcreator/workflows/tests/badge.svg)](https://github.com/juglab/napari-patchcreator/actions)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-patchcreator)](https://napari-hub.org/plugins/napari-patchcreator)\\n\\nA simple plugin to create quadratic patches from images through selection and clicking with the left mouse button.\\nThe patches can then be exported to a folder of your own choosing.\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/stable/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-patchcreator` via [pip]:\\n\\n    pip install napari-patchcreator\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/juglab/napari-patchcreator.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-patchcreator\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/juglab/napari-patchcreator/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-patchcreator\\n\\n\\n\\n\\n\\nA simple plugin to create quadratic patches from images through selection and clicking with the left mouse button.\\nThe patches can then be exported to a folder of your own choosing.\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-patchcreator via pip:\\npip install napari-patchcreator\\n\\nTo install latest development version :\\npip install git+https://github.com/juglab/napari-patchcreator.git\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-patchcreator\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 5 - Production/Stable\"],\"display_name\":\"napari patch creator\",\"documentation\":\"https://github.com/juglab/napari-patchcreator#README.md\",\"first_released\":\"2022-08-23T12:21:19.519400Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-patchcreator\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/juglab/napari-patchcreator\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-08-24T10:40:52.431008Z\",\"report_issues\":\"https://github.com/juglab/napari-patchcreator/issues\",\"requirements\":[\"numpy\",\"napari\",\"napari-plugin-engine (>=0.1.4)\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\"],\"summary\":\"A simple plugin to use with napari\",\"support\":\"https://github.com/juglab/napari-patchcreator/issues\",\"twitter\":\"\",\"version\":\"0.1.4\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"delgrosso.nick@gmail.com\",\"name\":\"Nicholas A. Del Grosso\"}],\"code_repository\":\"https://github.com/nickdelgrosso/napari-video-cvdask\",\"conda\":[],\"description\":\"\\n# Description\\n\\nThis is a simple video reader plugin that creates Dask Arrays using OpenCV' for file reading.  \\nAnything OpenCV's VideoCapture() function can read should work here.\\n\\n# Supported Data\\n\\nSupports mp4, mov, and avi extensions.s\\n\\n# Quickstart\\n\\n```python\\nnapari.view_image('myvid.avi')\\n```\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"Description\\nThis is a simple video reader plugin that creates Dask Arrays using OpenCV' for file reading.\\nAnything OpenCV's VideoCapture() function can read should work here.\\nSupported Data\\nSupports mp4, mov, and avi extensions.s\\nQuickstart\\npython\\nnapari.view_image('myvid.avi')\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari VideoCVDask\",\"documentation\":\"https://github.com/nickdelgrosso/napari-video-cvdask#README.md\",\"first_released\":\"2022-02-24T01:38:30.436652Z\",\"license\":\"MIT\",\"name\":\"napari-video-cvdask\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\"],\"project_site\":\"https://github.com/nickdelgrosso/napari-video-cvdask\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[\"*.avi\",\"*.mp4\",\"*.mov\"],\"release_date\":\"2022-02-25T19:53:29.796712Z\",\"report_issues\":\"https://github.com/nickdelgrosso/napari-video-cvdask/issues\",\"requirements\":[\"dask-image\",\"av\"],\"summary\":\"A Video File Reader that uses OpenCV2 and Dask Arrays\",\"support\":\"https://github.com/nickdelgrosso/napari-video-cvdask/issues\",\"twitter\":\"\",\"version\":\"0.2.1\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Alan R. Lowe\"}],\"category\":{\"Workflow step\":[\"Object tracking\"]},\"category_hierarchy\":{\"Workflow step\":[[\"Object tracking\",\"Isolated object tracking\",\"Cell tracking\"],[\"Object tracking\",\"Cell lineage extraction\"],[\"Object tracking\"]]},\"code_repository\":\"https://github.com/quantumjot/arboretum\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-arboretum\"}],\"description\":\" <!--[![Downloads](https://pepy.tech/badge/napari-arboretum)](https://pepy.tech/project/napari-arboretum)-->\\n[![License](https://img.shields.io/pypi/l/napari-arboretum.svg?color=green)](https://github.com/lowe-lab-ucl/napari-arboretum/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-arboretum.svg?color=green)](https://pypi.org/project/napari-arboretum)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-arboretum.svg?color=green)](https://python.org)\\n[![tests](https://github.com/lowe-lab-ucl/arboretum/workflows/tests/badge.svg)](https://github.com/quantumjot/arboretum/actions)\\n[![codecov](https://codecov.io/gh/lowe-lab-ucl/arboretum/branch/master/graph/badge.svg?token=2M2HhM60op)](https://codecov.io/gh/lowe-lab-ucl/arboretum)\\n\\n# Arboretum\\n\\n\\n![](https://raw.githubusercontent.com/lowe-lab-ucl/arboretum/master/examples/arboretum.gif)\\n*Automated cell tracking and lineage tree reconstruction*.\\n\\n### Overview\\n\\nA dockable widget for [Napari](https://github.com/napari) for visualizing cell lineage trees.\\n\\nFeatures:\\n+ Lineage tree plot widget\\n+ Integration with [btrack](https://github.com/quantumjot/BayesianTracker)\\n\\n---\\n\\n### Usage\\n\\nOnce installed, Arboretum will be visible in the `Plugins > Add Dock Widget > napari-arboretum` menu in napari.  To visualize a lineage tree, (double) click on one of the tracks in a napari `Tracks` layer.\\n\\n\\n\\n### Examples\\n\\nYou can use the example script to display some sample tracking data in napari and load the arboretum tree viewer:\\n\\n```sh\\npython ./examples/show_sample_data.py\\n```\\n\\nAlternatively, you can use *btrack* to generate tracks from your image data. See the example notebook here:\\nhttps://github.com/quantumjot/BayesianTracker/blob/master/examples\\n\\n---\\n\\n### History\\n\\nThis project has changed considerably. The `Tracks` layer, originally developed for this plugin, is now an official layer type in napari. Read the napari documentation here:\\n https://napari.org/api/napari.layers.Tracks.html\\n\\n\\nTo view the legacy version of this plugin, visit the legacy branch:\\nhttps://github.com/quantumjot/arboretum/tree/v1-legacy\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"\\n\\n\\n\\n\\n\\nArboretum\\n\\nAutomated cell tracking and lineage tree reconstruction.\\nOverview\\nA dockable widget for Napari for visualizing cell lineage trees.\\nFeatures:\\n+ Lineage tree plot widget\\n+ Integration with btrack\\n\\nUsage\\nOnce installed, Arboretum will be visible in the Plugins > Add Dock Widget > napari-arboretum menu in napari.  To visualize a lineage tree, (double) click on one of the tracks in a napari Tracks layer.\\nExamples\\nYou can use the example script to display some sample tracking data in napari and load the arboretum tree viewer:\\nsh\\npython ./examples/show_sample_data.py\\nAlternatively, you can use btrack to generate tracks from your image data. See the example notebook here:\\nhttps://github.com/quantumjot/BayesianTracker/blob/master/examples\\n\\nHistory\\nThis project has changed considerably. The Tracks layer, originally developed for this plugin, is now an official layer type in napari. Read the napari documentation here:\\n https://napari.org/api/napari.layers.Tracks.html\\nTo view the legacy version of this plugin, visit the legacy branch:\\nhttps://github.com/quantumjot/arboretum/tree/v1-legacy\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"napari-arboretum\",\"documentation\":\"\",\"first_released\":\"2021-05-11T11:19:31.102493Z\",\"license\":\"MIT\",\"name\":\"napari-arboretum\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/quantumjot/arboretum\",\"python_version\":\"\",\"reader_file_extensions\":[],\"release_date\":\"2022-09-13T20:19:14.261861Z\",\"report_issues\":\"\",\"requirements\":[\"matplotlib\",\"napari (>=0.4.0)\",\"napari-matplotlib (>=0.2.1)\",\"numpy (>=1.17.3)\",\"pandas\",\"pooch (>=1)\",\"vispy\"],\"summary\":\"Track graph and lineage tree visualization with napari\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.1.1\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Robert Haase\"},{\"name\":\"Johannes Müller\"}],\"category\":{\"Workflow step\":[\"Clustering\",\"Visualization\",\"Image annotation\",\"Image Segmentation\"]},\"category_hierarchy\":{\"Workflow step\":[[\"Clustering\",\"Distribution-based clustering\"],[\"Visualization\",\"Image visualisation\",\"Surface rendering\"],[\"Image annotation\",\"Sparse image annotation\",\"Landmark assignment\"],[\"Image Segmentation\",\"Cell segmentation\"]]},\"code_repository\":\"https://github.com/haesleinhuepf/napari-process-points-and-surfaces\",\"conda\":[],\"description\":\"# napari-process-points-and-surfaces (nppas)\\n\\n[![License](https://img.shields.io/pypi/l/napari-process-points-and-surfaces.svg?color=green)](https://github.com/haesleinhuepf/napari-process-points-and-surfaces/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-process-points-and-surfaces.svg?color=green)](https://pypi.org/project/napari-process-points-and-surfaces)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-process-points-and-surfaces.svg?color=green)](https://python.org)\\n[![tests](https://github.com/haesleinhuepf/napari-process-points-and-surfaces/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-process-points-and-surfaces/actions)\\n[![codecov](https://codecov.io/gh/haesleinhuepf/napari-process-points-and-surfaces/branch/master/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-process-points-and-surfaces)\\n[![Development Status](https://img.shields.io/pypi/status/napari-process-points-and-surfaces.svg)](https://en.wikipedia.org/wiki/Software_release_life_cycle#Alpha)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-process-points-and-surfaces)](https://napari-hub.org/plugins/napari-process-points-and-surfaces)\\n\\nProcess and analyze surfaces using [open3d](http://www.open3d.org/) and [vedo](https://vedo.embl.es/) in [napari].\\n\\n## Usage\\n\\nYou find a couple of surface generation, smoothing and analysis functions in the menu `Tools > Surfaces` and `Tools > Points`. For detailed explanation of the underlying algorithms, please refer to the [open3d](http://www.open3d.org/docs/release/) documentation.\\nSome code snippets and the knot example data have been taken from the open3d project which is \\n[MIT licensed](https://github.com/haesleinhuepf/napari-process-points-and-surfaces/blob/main/licenses_third_party/open3d_LICENSE) \\nand from the [vedo documentation](https://vedo.embl.es/autodocs/index.html) \\nwhich is [MIT licensed](https://github.com/haesleinhuepf/napari-process-points-and-surfaces/blob/main/licenses_third_party/vedo_LICENSE).\\nThe Standford Bunny example dataset has been taken from the [The Stanford 3D Scanning Repository](http://graphics.stanford.edu/data/3Dscanrep/).\\n\\nFor processing meshes in Python scripts, see the [demo notebook](https://github.com/haesleinhuepf/napari-process-points-and-surfaces/blob/main/docs/demo.ipynb). There you also learn how this screenshot is made:\\n\\n![img.png](https://github.com/haesleinhuepf/napari-process-points-and-surfaces/raw/main/docs/screenshot.png)\\n\\nFor performing quantitative measurements of meshes in Python scripts, see the [demo notebook](https://github.com/haesleinhuepf/napari-process-points-and-surfaces/blob/main/docs/quality_measurements.ipynb). \\nThere you also learn how this screenshot is made:\\n\\n![img.png](https://github.com/haesleinhuepf/napari-process-points-and-surfaces/raw/main/docs/screenshot2.png)\\n\\n### Surface measurements and annotations\\n\\nUsing the menu `Tools > Measurement > Surface quality table (vedo, nppas)` you can derived quantiative measurements of\\nthe vertices in a given surface layer. \\n\\n![img_1.png](https://github.com/haesleinhuepf/napari-process-points-and-surfaces/raw/main/docs/surface_measurements.png)\\n\\nTo differentiate regions when analyzing those measurements it is recommended to use the menu `Tools > Surfaces > Annotate surface manually (nppas)`\\nafter measurements have been made. This tool allows you to draw annotation label values on the surface. \\nIt is recommended to do activate a colorful colormap such as `hsv` before starting to draw annotations. \\nFurthermore, set the maximum of the contrast limit range to the number of regions you want to annotate + 1.\\nAnnotations can be drawn as freehand lines and circles.\\n\\n![img.png](https://github.com/haesleinhuepf/napari-process-points-and-surfaces/raw/main/docs/surface_annotation.png)\\n\\nAfter measurements and annotations were done, you can save the annotation in the same measurement table using the menu\\n`Tools > Measurement > Surface quality/annotation to table (nppas)`\\n\\n![img.png](https://github.com/haesleinhuepf/napari-process-points-and-surfaces/raw/main/docs/surface_annotation_in_table.png)\\n\\n### Measurement visualization\\n\\nTo visualize measurements on the surface, just double-click on the table column headers. This also allows to visualize \\nmeasurements and annotations side-by-side.\\n\\n![img.png](https://github.com/haesleinhuepf/napari-process-points-and-surfaces/raw/main/docs/measurement_visualization.gif)\\n\\n## Installation\\n\\nYou can install `napari-process-points-and-surfaces` via [pip]:\\n\\n```\\npip install napari-process-points-and-surfaces\\n```\\n\\n## See also\\n\\nThere are other napari plugins with similar / overlapping functionality\\n* [pymeshlab](https://www.napari-hub.org/plugins/napari-pymeshlab)\\n* [morphometrics](https://www.napari-hub.org/plugins/morphometrics)  \\n* [napari-pyclesperanto-assistant](https://www.napari-hub.org/plugins/napari-pyclesperanto-assistant)\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-process-points-and-surfaces\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please create a thread on [image.sc] along with a detailed description and tag [@haesleinhuepf].\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/haesleinhuepf/napari-process-points-and-surfaces/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n[image.sc]: https://image.sc\\n[@haesleinhuepf]: https://twitter.com/haesleinhuepf\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-process-points-and-surfaces (nppas)\\n\\n\\n\\n\\n\\n\\n\\nProcess and analyze surfaces using open3d and vedo in napari.\\nUsage\\nYou find a couple of surface generation, smoothing and analysis functions in the menu Tools > Surfaces and Tools > Points. For detailed explanation of the underlying algorithms, please refer to the open3d documentation.\\nSome code snippets and the knot example data have been taken from the open3d project which is \\nMIT licensed \\nand from the vedo documentation \\nwhich is MIT licensed.\\nThe Standford Bunny example dataset has been taken from the The Stanford 3D Scanning Repository.\\nFor processing meshes in Python scripts, see the demo notebook. There you also learn how this screenshot is made:\\n\\nFor performing quantitative measurements of meshes in Python scripts, see the demo notebook. \\nThere you also learn how this screenshot is made:\\n\\nSurface measurements and annotations\\nUsing the menu Tools > Measurement > Surface quality table (vedo, nppas) you can derived quantiative measurements of\\nthe vertices in a given surface layer. \\n\\nTo differentiate regions when analyzing those measurements it is recommended to use the menu Tools > Surfaces > Annotate surface manually (nppas)\\nafter measurements have been made. This tool allows you to draw annotation label values on the surface. \\nIt is recommended to do activate a colorful colormap such as hsv before starting to draw annotations. \\nFurthermore, set the maximum of the contrast limit range to the number of regions you want to annotate + 1.\\nAnnotations can be drawn as freehand lines and circles.\\n\\nAfter measurements and annotations were done, you can save the annotation in the same measurement table using the menu\\nTools > Measurement > Surface quality/annotation to table (nppas)\\n\\nMeasurement visualization\\nTo visualize measurements on the surface, just double-click on the table column headers. This also allows to visualize \\nmeasurements and annotations side-by-side.\\n\\nInstallation\\nYou can install napari-process-points-and-surfaces via pip:\\npip install napari-process-points-and-surfaces\\nSee also\\nThere are other napari plugins with similar / overlapping functionality\\n* pymeshlab\\n* morphometrics\\n* napari-pyclesperanto-assistant\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-process-points-and-surfaces\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please create a thread on image.sc along with a detailed description and tag @haesleinhuepf.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari-process-points-and-surfaces\",\"documentation\":\"https://github.com/haesleinhuepf/napari-process-points-and-surfaces#README.md\",\"first_released\":\"2022-02-05T13:31:53.600823Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-process-points-and-surfaces\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/haesleinhuepf/napari-process-points-and-surfaces\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-10-15T21:45:49.977082Z\",\"report_issues\":\"https://github.com/haesleinhuepf/napari-process-points-and-surfaces/issues\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"napari\",\"napari-tools-menu (>=0.1.14)\",\"napari-time-slicer (>=0.4.5)\",\"napari-workflows (>=0.2.3)\",\"open3d\",\"ipyvtklink\",\"vedo\",\"napari-skimage-regionprops (>=0.5.5)\",\"pandas\",\"imageio (!=2.22.1)\"],\"summary\":\"Process and analyze surfaces using open3d and vedo in napari\",\"support\":\"https://github.com/haesleinhuepf/napari-process-points-and-surfaces/issues\",\"twitter\":\"\",\"version\":\"0.3.3\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Guillaume Witz\"}],\"code_repository\":\"https://github.com/guiwitz/napari-steinpose\",\"description\":\"# napari-steinpose\\n\\n[![License BSD-3](https://img.shields.io/pypi/l/napari-steinpose.svg?color=green)](https://github.com/guiwitz/napari-steinpose/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-steinpose.svg?color=green)](https://pypi.org/project/napari-steinpose)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-steinpose.svg?color=green)](https://python.org)\\n[![tests](https://github.com/guiwitz/napari-steinpose/workflows/tests/badge.svg)](https://github.com/guiwitz/napari-steinpose/actions)\\n[![codecov](https://codecov.io/gh/guiwitz/napari-steinpose/branch/main/graph/badge.svg)](https://codecov.io/gh/guiwitz/napari-steinpose)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-steinpose)](https://napari-hub.org/plugins/napari-steinpose)\\n\\nThis napari plugin allows to segment and extract information from Imaging Mass Cytometry data by combining the [cellpose](http://www.cellpose.org/) and [steinbock](https://bodenmillergroup.github.io/steinbock/v0.14.2/) tools.\\n\\n## Installation\\n\\nIn order to use this plugin, whe highly recommend to create a specific environment and to install the required software in it. You can create a conda environment using:\\n\\n    conda create -n steinpose python=3.8.5 napari -c conda-forge\\n\\nThen activate it and install the plugin:\\n    \\n    conda activate steinpose\\n    pip install napari-steinpose\\n\\n### Potential issue with PyTorch\\n\\nCellpose and therefore the plugin and napari can crash without warning in some cases with ```torch==1.12.0```. This can be fixed by reverting to an earlier version using:\\n    \\n    pip install torch==1.11.0\\n\\n### GPU\\n\\nIn order to use a GPU:\\n\\n1. Uninstall the PyTorch version that gets installed by default with Cellpose:\\n\\n        pip uninstall torch\\n\\n2. Make sure your have up-to-date drivers for your NVIDIA card installed.\\n\\n3. Re-install a GPU version of PyTorch via conda using a command that you can find [here](https://pytorch.org/get-started/locally/) (this takes care of the cuda toolkit, cudnn etc. so **no need to install manually anything more than the driver**). The command will look like this:\\n\\n        conda install pytorch torchvision cudatoolkit=11.3 -c pytorch\\n\\n### Plugin Updates\\n\\nTo update the plugin, you only need to activate the existing environment and install the new version:\\n\\n    conda activate steinpose\\n    pip install git+https://github.com/guiwitz/napari-steinpose.git -U\\n\\n## Usage\\n\\nHere is a short summary on how to proceed to use the plugin. For more detailed information, please visit [this page](https://guiwitz.github.io/napari-steinpose).\\n\\n### Load data\\n\\nUsing the \\\"Select data folder\\\" button, select a folder containing your .mcd files. The contents of the folder will appear in the List of images box. When you select one of the files it is loaded in the viewer. Using the ROI spinpox, you can change the roi (or acquisition) to be visualized.\\n### Segmentation\\n\\n1. In the channels tab, choose the combination of channels to use to define images to segment. You can choose what type of projection (mean, min etc.) is used to combine channels. You can either select channels defining both cells and nuclei or just a single channel. **Note that if you want to just segment nuclei, you need to select them as \\\"cell channel\\\".**\\n\\n2. To save the output, select a folder using the \\\"Select output folder\\\" button.\\n\\n3. In the segmentation tab, pick a cellpose model to use. If you use one of the built-in models, you can specify the average diameter of objects to detect.\\n\\n4. In the Options tab, you can set a few more options:\\n   - cellpose options: you can adjust the flow threshold and cell probabilities. If cells are missing try to use higher values of flow threshold (close to 1) and lower values for the cell probabilities (around -6)\\n   - segmentation options: you can decide to remove segmentation touching the image border, and you can also decide to expand the segmented objects by a fixed number of pixels. If a segmentation is displayed in the viewer, adjusting this parameter will live-adjust the mask.\\n\\n5. You can first test the segmentation using the \\\"Run on current image\\\" button. Once segmentation is done, the corresponding mask is displayed. You can then run the segmentation over all ROIs of **all .mcd files** present in the folder by using the \\\"Run on folder\\\" button.\\n\\n### Post-processing\\n\\nIn the Segmentation tab, if you tick the box \\\"Run steinbock post-processing\\\", information will directly be extracted from images and masks at the end of segmentation. Processing is done via steinbock and generates files compatible with further downstream processing.\\n\\nIn the Export tab, you can select what type of information to export: object intensities, geometric properties and object neighbourhood. Note that if you have performed a segmentation without post-processing, you can still run post-processing using the \\\"Run steinbock postproc\\\" button.\\n\\n### Saving settings\\n\\nTo avoid having to re-type the same settings repeatedly, you can export a give configuration using the \\\"Export config\\\" button in the Options tab. This generates a human readable .yml file with:\\n- segmentation options\\n- channels selected for projections\\n\\nThe file is saved in the output folder. You can just copy the file in a new empty output folder to use it for an other analysis run. Once you select that folder containing a configuration file, you can import it with the \\\"Import config\\\" button. **Note that you need to have an image opened so that channels can be selected properly.**\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-steinpose\\\" is free and open source software\\n\\n## Authors\\n\\nThe author of this plugin is Guillaume Witz, Data Science Lab and Microscopy Imaging Center, University of Bern. This plugin is the result of a collaboration with the Imaging Mass Cytometry and Mass Cytometry Platform, University of Bern.\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/guiwitz/napari-steinpose/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-steinpose\\n\\n\\n\\n\\n\\n\\nThis napari plugin allows to segment and extract information from Imaging Mass Cytometry data by combining the cellpose and steinbock tools.\\nInstallation\\nIn order to use this plugin, whe highly recommend to create a specific environment and to install the required software in it. You can create a conda environment using:\\nconda create -n steinpose python=3.8.5 napari -c conda-forge\\n\\nThen activate it and install the plugin:\\nconda activate steinpose\\npip install napari-steinpose\\n\\nPotential issue with PyTorch\\nCellpose and therefore the plugin and napari can crash without warning in some cases with torch==1.12.0. This can be fixed by reverting to an earlier version using:\\npip install torch==1.11.0\\n\\nGPU\\nIn order to use a GPU:\\n\\n\\nUninstall the PyTorch version that gets installed by default with Cellpose:\\npip uninstall torch\\n\\n\\n\\nMake sure your have up-to-date drivers for your NVIDIA card installed.\\n\\n\\nRe-install a GPU version of PyTorch via conda using a command that you can find here (this takes care of the cuda toolkit, cudnn etc. so no need to install manually anything more than the driver). The command will look like this:\\nconda install pytorch torchvision cudatoolkit=11.3 -c pytorch\\n\\n\\n\\nPlugin Updates\\nTo update the plugin, you only need to activate the existing environment and install the new version:\\nconda activate steinpose\\npip install git+https://github.com/guiwitz/napari-steinpose.git -U\\n\\nUsage\\nHere is a short summary on how to proceed to use the plugin. For more detailed information, please visit this page.\\nLoad data\\nUsing the \\\"Select data folder\\\" button, select a folder containing your .mcd files. The contents of the folder will appear in the List of images box. When you select one of the files it is loaded in the viewer. Using the ROI spinpox, you can change the roi (or acquisition) to be visualized.\\nSegmentation\\n\\n\\nIn the channels tab, choose the combination of channels to use to define images to segment. You can choose what type of projection (mean, min etc.) is used to combine channels. You can either select channels defining both cells and nuclei or just a single channel. Note that if you want to just segment nuclei, you need to select them as \\\"cell channel\\\".\\n\\n\\nTo save the output, select a folder using the \\\"Select output folder\\\" button.\\n\\n\\nIn the segmentation tab, pick a cellpose model to use. If you use one of the built-in models, you can specify the average diameter of objects to detect.\\n\\n\\nIn the Options tab, you can set a few more options:\\n\\ncellpose options: you can adjust the flow threshold and cell probabilities. If cells are missing try to use higher values of flow threshold (close to 1) and lower values for the cell probabilities (around -6)\\n\\nsegmentation options: you can decide to remove segmentation touching the image border, and you can also decide to expand the segmented objects by a fixed number of pixels. If a segmentation is displayed in the viewer, adjusting this parameter will live-adjust the mask.\\n\\n\\nYou can first test the segmentation using the \\\"Run on current image\\\" button. Once segmentation is done, the corresponding mask is displayed. You can then run the segmentation over all ROIs of all .mcd files present in the folder by using the \\\"Run on folder\\\" button.\\n\\n\\nPost-processing\\nIn the Segmentation tab, if you tick the box \\\"Run steinbock post-processing\\\", information will directly be extracted from images and masks at the end of segmentation. Processing is done via steinbock and generates files compatible with further downstream processing.\\nIn the Export tab, you can select what type of information to export: object intensities, geometric properties and object neighbourhood. Note that if you have performed a segmentation without post-processing, you can still run post-processing using the \\\"Run steinbock postproc\\\" button.\\nSaving settings\\nTo avoid having to re-type the same settings repeatedly, you can export a give configuration using the \\\"Export config\\\" button in the Options tab. This generates a human readable .yml file with:\\n- segmentation options\\n- channels selected for projections\\nThe file is saved in the output folder. You can just copy the file in a new empty output folder to use it for an other analysis run. Once you select that folder containing a configuration file, you can import it with the \\\"Import config\\\" button. Note that you need to have an image opened so that channels can be selected properly.\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-steinpose\\\" is free and open source software\\nAuthors\\nThe author of this plugin is Guillaume Witz, Data Science Lab and Microscopy Imaging Center, University of Bern. This plugin is the result of a collaboration with the Imaging Mass Cytometry and Mass Cytometry Platform, University of Bern.\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari Steinpose\",\"documentation\":\"https://github.com/guiwitz/napari-steinpose#README.md\",\"first_released\":\"2022-11-17T13:54:34.626169Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-steinpose\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\",\"widget\"],\"project_site\":\"https://github.com/guiwitz/napari-steinpose\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[\"*.mcd\"],\"release_date\":\"2022-11-17T13:54:34.626169Z\",\"report_issues\":\"https://github.com/guiwitz/napari-steinpose/issues\",\"requirements\":[\"torch (==1.11.0)\",\"cellpose\",\"numpy\",\"magicgui\",\"qtpy\",\"matplotlib\",\"readimc\",\"steinbock\",\"pandas\",\"aicsimageio\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\",\"pytest-order ; extra == 'testing'\"],\"summary\":\"A plugin to process Imaging Mass Cytometry data with cellpose and steinbock\",\"support\":\"https://github.com/guiwitz/napari-steinpose/issues\",\"twitter\":\"\",\"version\":\"0.1.0\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Clément Cazorla\"}],\"code_repository\":\"https://bitbucket.org/koopa31/napari_svetlana/src/main/\",\"description\":\"# napari_svetlana\\n\\n[![License](https://img.shields.io/pypi/l/napari_svetlana.svg?color=green)](https://bitbucket.org/koopa31/napari_svetlana/src/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari_svetlana.svg?color=green)](https://pypi.org/project/napari_svetlana)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari_svetlana.svg?color=green)](https://python.org)\\n[![tests](https://bitbucket.org/koopa31/napari_svetlana/workflows/tests/badge.svg)](https://bitbucket.org/koopa31/napari_svetlana/actions)\\n[![codecov](https://codecov.io/gh/koopa31/napari_svetlana/branch/main/graph/badge.svg)](https://codecov.io/gh/koopa31/napari_svetlana)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-svetlana)](https://napari-hub.org/plugins/napari-svetlana)\\n[![Documentation](https://readthedocs.org/projects/svetlana-documentation/badge/?version=latest)](https://svetlana-documentation.readthedocs.io/en/latest/)\\n\\nThe aim of this plugin is to classify the output of a segmentation algorithm.\\nThe inputs are :\\n<ul>\\n  <li>A folder of raw images</li>\\n  <li>Their segmentation masks where each ROI has its own label.</li>\\n</ul>\\n\\nSvetlana can process 2D, 3D and multichannel image. If you want to use it to work on cell images, we strongly\\nrecommend the use of [Cellpose](https://www.cellpose.org) for the segmentation part, as it provides excellent quality results and a standard output format\\naccepted by Svetlana (labels masks). \\n\\nIf you use this plugin please cite the [paper](https://hal.inria.fr/hal-03927879): \\n\\nCazorla, C., Weiss, P., & Morin, R. (2023). Svetlana: a Supervised Segmentation Classifier for Napari.\\n\\n```bibtex\\n@unpublished{weiss:hal-03927879,\\n  TITLE = {{Svetlana: a Supervised Segmentation Classifier for Napari}},\\n  AUTHOR = {Weiss, Pierre and Cazorla, Cl{\\\\'e}ment and Morin, Renaud},\\n  URL = {https://hal.inria.fr/hal-03927879},\\n  NOTE = {working paper or preprint},\\n  YEAR = {2023},\\n  MONTH = Jan,\\n  PDF = {https://hal.inria.fr/hal-03927879/file/main_nature.pdf},\\n  HAL_ID = {hal-03927879},\\n  HAL_VERSION = {v1},\\n}\\n\\n```\\n\\n\\n![](https://bitbucket.org/koopa31/napari_svetlana/raw/bca8788111b38d97bd172c7caac87cc488ace699/images/Videogif.gif)\\n\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/plugins/stable/index.html\\n-->\\n\\n## Installation\\n\\nFirst install Napari in a Python 3.9 Conda environment following these instructions:\\n\\n```bash\\nconda create -n svetlana_env python=3.9\\nconda activate svetlana_env\\nconda install pip\\npython -m pip install \\\"napari[all]\\\" --upgrade\\n```\\n\\nThen, you can install `napari_svetlana` via [pip](https://pypi.org/project/napari-svetlana/), or directly from the Napari plugin manager (see Napari documentation):\\n```bash\\npip install napari_svetlana\\n```\\nWARNING:\\n\\nIf you have a Cuda compatible GPU on your computer, some computations may be accelerated\\nusing [Cupy](https://pypi.org/project/cupy/). Unfortunately, Cupy needs Cudatoolkit to be installed. This library can only be installed via \\nConda while the plugin is a pip plugin, so it must be installed manually for the moment:\\n```bash\\nconda install cudatoolkit=10.2 \\n```\\nAlso note that the library ([Cucim](https://pypi.org/project/cucim/)) that we use to improve these performances, computing morphological operations on GPU\\nis unfortunately only available for Linux systems. Hence, if you are a Windows user, this installation is not necessary.\\n\\n## Tutorial\\n\\nMany advanced features are available in Svetlana, such as data augmentation or contextual information reduction, to optimize the performance of your classifier. Thus, we strongly encourage you to\\ncheck our [Youtube tutorial](https://www.youtube.com/watch?v=u_FKuHta-RE) and\\nour [documentation](https://svetlana-documentation.readthedocs.io/en/latest/).\\nA folder in the repository called \\\"[Demo images](https://bitbucket.org/koopa31/napari_svetlana/src/main/Demo%20images/)\\\",\\ncontains two demo images, identical to the ones of the Youtube tutorial. Feel free to use them to test all the features\\nthat Sevtlana offers.\\n\\n## Contributing\\n\\nContributions are very welcome.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari_svetlana\\\" is free and open source software\\n\\n## Acknowledgements\\n\\nThe method was developed by [Clément Cazorla](https://koopa31.github.io/), [Renaud Morin](https://www.linkedin.com/in/renaud-morin-6a42665b/?originalSubdomain=fr) and [Pierre Weiss](https://www.math.univ-toulouse.fr/~weiss/). And the plugin was written by\\nClément Cazorla. The project is co-funded by [Imactiv-3D](https://www.imactiv-3d.com/) and [CNRS](https://www.cnrs.fr/fr).\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue](https://bitbucket.org/koopa31/napari_svetlana/issues?status=new&status=open) along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari_svetlana\\n\\n\\n\\n\\n\\n\\n\\nThe aim of this plugin is to classify the output of a segmentation algorithm.\\nThe inputs are :\\n\\nA folder of raw images\\nTheir segmentation masks where each ROI has its own label.\\n\\nSvetlana can process 2D, 3D and multichannel image. If you want to use it to work on cell images, we strongly\\nrecommend the use of Cellpose for the segmentation part, as it provides excellent quality results and a standard output format\\naccepted by Svetlana (labels masks). \\nIf you use this plugin please cite the paper: \\nCazorla, C., Weiss, P., & Morin, R. (2023). Svetlana: a Supervised Segmentation Classifier for Napari.\\n```bibtex\\n@unpublished{weiss:hal-03927879,\\n  TITLE = {{Svetlana: a Supervised Segmentation Classifier for Napari}},\\n  AUTHOR = {Weiss, Pierre and Cazorla, Cl{\\\\'e}ment and Morin, Renaud},\\n  URL = {https://hal.inria.fr/hal-03927879},\\n  NOTE = {working paper or preprint},\\n  YEAR = {2023},\\n  MONTH = Jan,\\n  PDF = {https://hal.inria.fr/hal-03927879/file/main_nature.pdf},\\n  HAL_ID = {hal-03927879},\\n  HAL_VERSION = {v1},\\n}\\n```\\n\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nFirst install Napari in a Python 3.9 Conda environment following these instructions:\\nbash\\nconda create -n svetlana_env python=3.9\\nconda activate svetlana_env\\nconda install pip\\npython -m pip install \\\"napari[all]\\\" --upgrade\\nThen, you can install napari_svetlana via pip, or directly from the Napari plugin manager (see Napari documentation):\\nbash\\npip install napari_svetlana\\nWARNING:\\nIf you have a Cuda compatible GPU on your computer, some computations may be accelerated\\nusing Cupy. Unfortunately, Cupy needs Cudatoolkit to be installed. This library can only be installed via \\nConda while the plugin is a pip plugin, so it must be installed manually for the moment:\\nbash\\nconda install cudatoolkit=10.2\\nAlso note that the library (Cucim) that we use to improve these performances, computing morphological operations on GPU\\nis unfortunately only available for Linux systems. Hence, if you are a Windows user, this installation is not necessary.\\nTutorial\\nMany advanced features are available in Svetlana, such as data augmentation or contextual information reduction, to optimize the performance of your classifier. Thus, we strongly encourage you to\\ncheck our Youtube tutorial and\\nour documentation.\\nA folder in the repository called \\\"Demo images\\\",\\ncontains two demo images, identical to the ones of the Youtube tutorial. Feel free to use them to test all the features\\nthat Sevtlana offers.\\nContributing\\nContributions are very welcome.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari_svetlana\\\" is free and open source software\\nAcknowledgements\\nThe method was developed by Clément Cazorla, Renaud Morin and Pierre Weiss. And the plugin was written by\\nClément Cazorla. The project is co-funded by Imactiv-3D and CNRS.\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari-svetlana\",\"documentation\":\"https://svetlana-documentation.readthedocs.io/en/latest/\",\"first_released\":\"2022-11-22T10:45:31.312794Z\",\"license\":\"GPL-3.0-only\",\"name\":\"napari-svetlana\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\",\"writer\",\"widget\"],\"project_site\":\"\",\"python_version\":\">=3.9\",\"reader_file_extensions\":[\"*\"],\"release_date\":\"2023-01-26T15:15:28.717138Z\",\"report_issues\":\"https://bitbucket.org/koopa31/napari_svetlana/issues?status=new&status=open\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"albumentations (==1.0.3)\",\"joblib (==1.1.0)\",\"light-the-torch\",\"matplotlib\",\"opencv-python (==4.5.5.62)\",\"PyQt5\",\"cupy-cuda102 (==10.6.0)\",\"xlsxwriter\",\"pandas\",\"cucim (==22.6.0) ; platform_system == \\\"Linux\\\"\"],\"summary\":\"A classification plugin for the ROIs of a segmentation mask.\",\"support\":\"https://bitbucket.org/koopa31/napari_svetlana/issues?status=new&status=open\",\"twitter\":\"\",\"version\":\"0.2.5\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[\"image\"]}",
  "{\"authors\":[{\"name\":\"Dr. Andrew Annex\"}],\"code_repository\":\"https://github.com/AndrewAnnex/napari-pdr-reader\",\"conda\":[],\"description\":\"# napari-pdr-reader\\n\\n[![License BSD-3](https://img.shields.io/pypi/l/napari-pdr-reader.svg?color=green)](https://github.com/AndrewAnnex/napari-pdr-reader/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-pdr-reader.svg?color=green)](https://pypi.org/project/napari-pdr-reader)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-pdr-reader.svg?color=green)](https://python.org)\\n[![tests](https://github.com/AndrewAnnex/napari-pdr-reader/workflows/tests/badge.svg)](https://github.com/AndrewAnnex/napari-pdr-reader/actions)\\n[![codecov](https://codecov.io/gh/AndrewAnnex/napari-pdr-reader/branch/main/graph/badge.svg)](https://codecov.io/gh/AndrewAnnex/napari-pdr-reader)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-pdr-reader)](https://napari-hub.org/plugins/napari-pdr-reader)\\n\\nA reader plugin for Napari for PDS data powered by the PDR library\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-pdr-reader` via [pip]:\\n\\n    pip install napari-pdr-reader\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/AndrewAnnex/napari-pdr-reader.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-pdr-reader\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/AndrewAnnex/napari-pdr-reader/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-pdr-reader\\n\\n\\n\\n\\n\\n\\nA reader plugin for Napari for PDS data powered by the PDR library\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-pdr-reader via pip:\\npip install napari-pdr-reader\\n\\nTo install latest development version :\\npip install git+https://github.com/AndrewAnnex/napari-pdr-reader.git\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-pdr-reader\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"PDS reader plugin for Napari\",\"documentation\":\"https://github.com/AndrewAnnex/napari-pdr-reader#README.md\",\"first_released\":\"2022-07-14T15:49:45.814517Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-pdr-reader\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\",\"sample_data\"],\"project_site\":\"https://github.com/AndrewAnnex/napari-pdr-reader\",\"python_version\":\">=3.9\",\"reader_file_extensions\":[\"*.LBL\",\"*.img\",\"*.IMG\",\"*.fits\",\"*.FITS\",\"*.lbl\"],\"release_date\":\"2022-07-14T15:49:45.814517Z\",\"report_issues\":\"https://github.com/AndrewAnnex/napari-pdr-reader/issues\",\"requirements\":[\"astropy\",\"dustgoggles\",\"napari\",\"numpy\",\"pandas\",\"pdr\",\"pds4-tools\",\"pillow\",\"pvl\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"tox ; extra == 'testing'\"],\"summary\":\"A reader plugin for Napari for PDS data powered by the PDR library\",\"support\":\"https://github.com/AndrewAnnex/napari-pdr-reader/issues\",\"twitter\":\"\",\"version\":\"0.0.1\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Niklas Netter\"}],\"code_repository\":\"https://github.com/gatoniel/napari-nd2-folder-viewer\",\"description\":\"# napari-nd2-folder-viewer\\n\\n[![License BSD-3](https://img.shields.io/pypi/l/napari-nd2-folder-viewer.svg?color=green)](https://github.com/gatoniel/napari-nd2-folder-viewer/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-nd2-folder-viewer.svg?color=green)](https://pypi.org/project/napari-nd2-folder-viewer)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-nd2-folder-viewer.svg?color=green)](https://python.org)\\n[![tests](https://github.com/gatoniel/napari-nd2-folder-viewer/workflows/tests/badge.svg)](https://github.com/gatoniel/napari-nd2-folder-viewer/actions)\\n[![codecov](https://codecov.io/gh/gatoniel/napari-nd2-folder-viewer/branch/main/graph/badge.svg)](https://codecov.io/gh/gatoniel/napari-nd2-folder-viewer)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-nd2-folder-viewer)](https://napari-hub.org/plugins/napari-nd2-folder-viewer)\\n\\nLook through separate nd2 files in one viewer.\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/stable/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-nd2-folder-viewer` via [pip]:\\n\\n    pip install napari-nd2-folder-viewer\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/gatoniel/napari-nd2-folder-viewer.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-nd2-folder-viewer\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/gatoniel/napari-nd2-folder-viewer/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-nd2-folder-viewer\\n\\n\\n\\n\\n\\n\\nLook through separate nd2 files in one viewer.\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-nd2-folder-viewer via pip:\\npip install napari-nd2-folder-viewer\\n\\nTo install latest development version :\\npip install git+https://github.com/gatoniel/napari-nd2-folder-viewer.git\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-nd2-folder-viewer\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari nd2 folder viewer\",\"documentation\":\"https://github.com/gatoniel/napari-nd2-folder-viewer#README.md\",\"first_released\":\"2022-08-02T11:44:47.716377Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-nd2-folder-viewer\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/gatoniel/napari-nd2-folder-viewer\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2023-02-01T19:08:05.205625Z\",\"report_issues\":\"https://github.com/gatoniel/napari-nd2-folder-viewer/issues\",\"requirements\":[\"numpy\",\"magicgui\",\"qtpy\",\"pyyaml\",\"marshmallow\",\"desert\",\"nd2 (>=0.4.3)\",\"dask\",\"pandas\",\"openpyxl\",\"julian\",\"napari-animation\",\"scikit-learn\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\"],\"summary\":\"Look through separate nd2 files in one viewer.\",\"support\":\"https://github.com/gatoniel/napari-nd2-folder-viewer/issues\",\"twitter\":\"\",\"version\":\"0.0.13\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"dkrentzel@pm.me\",\"name\":\"Daniel Krentzel\"}],\"code_repository\":\"https://github.com/krentzd/napari-pdf-reader\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-pdf-reader\"}],\"description\":\"# PDF reader for napari\\nReads PDF files into napari\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"PDF reader for napari\\nReads PDF files into napari\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari-pdf-reader\",\"documentation\":\"https://github.com/krentzd/napari-pdf-reader#README.md\",\"first_released\":\"2021-11-05T16:24:49.069732Z\",\"license\":\"MIT\",\"name\":\"napari-pdf-reader\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\"],\"project_site\":\"https://github.com/krentzd/napari-pdf-reader\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[\"*\"],\"release_date\":\"2021-11-05T16:24:49.069732Z\",\"report_issues\":\"https://github.com/krentzd/napari-pdf-reader/issues\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"pillow\",\"pdf2image\"],\"summary\":\"Reader for PDF files\",\"support\":\"https://github.com/krentzd/napari-pdf-reader/issues\",\"twitter\":\"\",\"version\":\"0.0.1a3\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"sofroniewn@gmail.com\",\"name\":\"Nicholas Sofroniew\"}],\"category\":{\"Workflow step\":[\"Synthetic image generation\",\"Visualization\"]},\"category_hierarchy\":{\"Workflow step\":[[\"Synthetic image generation\"],[\"Visualization\"]]},\"code_repository\":\"https://github.com/sofroniewn/waver\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"waver\"}],\"description\":\"# waver\\n\\n[![License](https://img.shields.io/pypi/l/waver.svg?color=green)](https://github.com/sofroniewn/waver/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/waver.svg?color=green)](https://pypi.org/project/waver)\\n[![Python Version](https://img.shields.io/pypi/pyversions/waver.svg?color=green)](https://python.org)\\n[![tests](https://github.com/sofroniewn/waver/workflows/tests/badge.svg)](https://github.com/sofroniewn/waver/actions)\\n[![codecov](https://codecov.io/gh/sofroniewn/waver/branch/main/graph/badge.svg?token=QBP7K6YUT7)](https://codecov.io/gh/sofroniewn/waver)\\n\\nRun simulations of the [wave equation](https://en.wikipedia.org/wiki/Wave_equation) in nD on grids of variable speed in Python. This library owes a lot of its design and approach to the [fdtd](https://github.com/flaport/fdtd) library, a Python 3D electromagnetic FDTD simulator.\\n\\nThis package allows for a fair amount of customization over your wave simulation. You can\\n - specify the size and spacing of the grid\\n - specify the time step for the simulation, which will be checked to ensure stability of the simulation\\n - specify the duration of the simulation\\n - setting a variable speed array (one value per grid point) to allow for \\\"objects\\\" in your environment\\n - set the source of the wave, which can be a point, line, or any (n-1)D subarray\\n - record the wave with a detector, which can be the full grid, the full boundary, or a particular boundary\\n - use convenience methods to run many simulations with different sources on the same grid and detector combination\\n\\nYou can use [napari](https://napari.org/), a multi-dimensional image viewer for Python, to allow for easy visualization of the detected wave. Some functionality is also available as a napari plugin to allow for running simulations from a graphical user interface.\\n\\nResults can look like\\n\\nhttps://user-images.githubusercontent.com/6531703/128283012-a784ec06-4df9-4ddf-bf4f-e21b927fe4a3.mov\\n\\n----------------------------------\\n\\n## Installation\\n\\nYou can install `waver` via [pip]:\\n\\n    pip install waver\\n\\n## Usage\\n\\n### Convenience Methods\\n\\nThe most convenient way to use waver is to use one of two convenience methods that will create and run a simulation\\nfor you and return the results.\\n\\nThe first method `run_single_source` allows you to run a single simulation with a single source on one grid and \\nrecord the results using a detector. For example\\n\\n```python\\nfrom waver.simulation import run_single_source\\n\\nsingle_sim_params = {\\n    'size': (12.8e-3, 12.8e-3),\\n    'spacing': 100e-6,\\n    'duration': 80e-6,\\n    'min_speed': 343,\\n    'max_speed': 686,\\n    'speed': 686,\\n    'time_step': 50e-9,\\n    'temporal_downsample': 2,\\n    'location': (6.4e-3, 6.4e-3),\\n    'period': 5e-6,\\n    'ncycles':1,\\n}\\n\\ndetected_wave, speed_grid = run_single_source(**single_sim_params)\\n```\\n\\nThe second method `run_multiple_sources` allows you to run multiple simulations with multiple sources on the same\\ngrid and with the same detector and return the results. For example\\n\\n```python\\nfrom waver.simulation import run_multiple_sources\\n\\nmulti_sim_params = {\\n    'size': (12.8e-3, 12.8e-3),\\n    'spacing': 100e-6,\\n    'duration': 80e-6,\\n    'min_speed': 343,\\n    'max_speed': 686,\\n    'speed': 686,\\n    'time_step': 50e-9,\\n    'temporal_downsample': 2,\\n    'sources': [{\\n        'location': (6.4e-3, 6.4e-3),\\n        'period': 5e-6,\\n        'ncycles':1,\\n    }]\\n}\\n\\ndetected_wave, speed_grid = run_multiple_sources(**multi_sim_params)\\n```\\n\\nThe main difference between these two methods is that `run_multiple_sources` takes a `sources` parameter which takes a list \\nof dictionaries with keys corresponding to source related keyword arguments found in `run_single_source`.\\n\\n### Visualization\\n\\nIf you want to quickly visualize the results of `run_multiple_sources`, you can use the `run_and_visualize` command which will \\nrun the simulation and then launch napari with the results, as seen in [examples/2D/point_source.py](./examples/2D/point_source.py)\\n\\n```python\\nfrom waver.datasets import run_and_visualize\\n\\nrun_and_visualize(**multi_sim_params)\\n```\\n\\n### Datasets\\n\\nIf you want to run simulations with on many different speed grids you can use the `generate_simulation_dataset` method as a convenience. The results will be saved to a [zarr](https://zarr.readthedocs.io/en/stable/) file of your chosing. You can then use the `load_simulation_dataset` to load the dataset.\\n\\n```python\\nfrom waver.datasets import generate_simulation_dataset\\n\\n# Define root path for simulation\\npath = './simulation_dataset.zarr'\\nruns = 5\\n\\n# Define a simulation, 12.8mm, 100um spacing\\ndataset_sim_params = {\\n    'size': (12.8e-3, 12.8e-3),\\n    'spacing': 100e-6,\\n    'duration': 80e-6,\\n    'min_speed': 343,\\n    'max_speed': 686,\\n    'speed': 'mixed_random_ifft',\\n    'time_step': 50e-9,\\n    'sources': [{\\n        'location': (None, 0),\\n        'period': 5e-6,\\n        'ncycles':1,\\n    }],\\n    'temporal_downsample': 2,\\n    'boundary': 1,\\n    'edge': 1,\\n}\\n\\n# Run and save simulation\\ngenerate_simulation_dataset(path, runs, **dataset_sim_params)\\n```\\n\\nThe `generate_simulation_dataset` allows the `speed` to be a string that will specify a particular method of randomly generating speed values for the simulation grid.\\n\\n### The Simulation Object\\n\\nIf you'd like to understand in a little bit more detail how a simulation is defined then you might want to use the unerlying simulation object `Simulation` and manually set key objects like the `Source` and `Detector`. A full example of this is as follows\\n\\n```python\\n# Create a simulation\\nsim = Simulation(size=size, spacing=spacing, max_speed=max_speed, time_step=time_step)\\n\\n# Set speed array\\nsim.set_speed(speed=speed, min_speed=min_speed, max_speed=max_speed)\\n\\n# Add source\\nsim.add_source(location=location, period=period, ncycles=ncycles, phase=phase)\\n\\n# Add detector grid\\nsim.add_detector(spatial_downsample=spatial_downsample,\\n                    boundary=boundary, edge=edge)\\n\\n# Run simulation\\nsim.run(duration=duration, temporal_downsample=temporal_downsample, progress=progress, leave=leave)\\n\\n# Print simulation wave and speed data\\nprint('wave: ', sim.detected_wave)\\nprint('speed: ', sim.grid_speed)\\n```\\n\\nNote these steps are done inside the `run_single_source` method for you as a convenience.\\n\\n## Known Limitations\\n\\nA [perfectly matched layer](https://en.wikipedia.org/wiki/Perfectly_matched_layer) boundary has recently been added, but might not perform well under all conditions. Additional contributions would be welcome here.\\n\\nRight now the simulations are quite slow. I'd like to add a [JAX](https://github.com/google/jax) backend, but \\nhavn't done so yet. Contributions would be welcome.\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"waver\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/sofroniewn/waver/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"waver\\n\\n\\n\\n\\n\\nRun simulations of the wave equation in nD on grids of variable speed in Python. This library owes a lot of its design and approach to the fdtd library, a Python 3D electromagnetic FDTD simulator.\\nThis package allows for a fair amount of customization over your wave simulation. You can\\n - specify the size and spacing of the grid\\n - specify the time step for the simulation, which will be checked to ensure stability of the simulation\\n - specify the duration of the simulation\\n - setting a variable speed array (one value per grid point) to allow for \\\"objects\\\" in your environment\\n - set the source of the wave, which can be a point, line, or any (n-1)D subarray\\n - record the wave with a detector, which can be the full grid, the full boundary, or a particular boundary\\n - use convenience methods to run many simulations with different sources on the same grid and detector combination\\nYou can use napari, a multi-dimensional image viewer for Python, to allow for easy visualization of the detected wave. Some functionality is also available as a napari plugin to allow for running simulations from a graphical user interface.\\nResults can look like\\nhttps://user-images.githubusercontent.com/6531703/128283012-a784ec06-4df9-4ddf-bf4f-e21b927fe4a3.mov\\n\\nInstallation\\nYou can install waver via pip:\\npip install waver\\n\\nUsage\\nConvenience Methods\\nThe most convenient way to use waver is to use one of two convenience methods that will create and run a simulation\\nfor you and return the results.\\nThe first method run_single_source allows you to run a single simulation with a single source on one grid and \\nrecord the results using a detector. For example\\n```python\\nfrom waver.simulation import run_single_source\\nsingle_sim_params = {\\n    'size': (12.8e-3, 12.8e-3),\\n    'spacing': 100e-6,\\n    'duration': 80e-6,\\n    'min_speed': 343,\\n    'max_speed': 686,\\n    'speed': 686,\\n    'time_step': 50e-9,\\n    'temporal_downsample': 2,\\n    'location': (6.4e-3, 6.4e-3),\\n    'period': 5e-6,\\n    'ncycles':1,\\n}\\ndetected_wave, speed_grid = run_single_source(**single_sim_params)\\n```\\nThe second method run_multiple_sources allows you to run multiple simulations with multiple sources on the same\\ngrid and with the same detector and return the results. For example\\n```python\\nfrom waver.simulation import run_multiple_sources\\nmulti_sim_params = {\\n    'size': (12.8e-3, 12.8e-3),\\n    'spacing': 100e-6,\\n    'duration': 80e-6,\\n    'min_speed': 343,\\n    'max_speed': 686,\\n    'speed': 686,\\n    'time_step': 50e-9,\\n    'temporal_downsample': 2,\\n    'sources': [{\\n        'location': (6.4e-3, 6.4e-3),\\n        'period': 5e-6,\\n        'ncycles':1,\\n    }]\\n}\\ndetected_wave, speed_grid = run_multiple_sources(**multi_sim_params)\\n```\\nThe main difference between these two methods is that run_multiple_sources takes a sources parameter which takes a list \\nof dictionaries with keys corresponding to source related keyword arguments found in run_single_source.\\nVisualization\\nIf you want to quickly visualize the results of run_multiple_sources, you can use the run_and_visualize command which will \\nrun the simulation and then launch napari with the results, as seen in examples/2D/point_source.py\\n```python\\nfrom waver.datasets import run_and_visualize\\nrun_and_visualize(**multi_sim_params)\\n```\\nDatasets\\nIf you want to run simulations with on many different speed grids you can use the generate_simulation_dataset method as a convenience. The results will be saved to a zarr file of your chosing. You can then use the load_simulation_dataset to load the dataset.\\n```python\\nfrom waver.datasets import generate_simulation_dataset\\nDefine root path for simulation\\npath = './simulation_dataset.zarr'\\nruns = 5\\nDefine a simulation, 12.8mm, 100um spacing\\ndataset_sim_params = {\\n    'size': (12.8e-3, 12.8e-3),\\n    'spacing': 100e-6,\\n    'duration': 80e-6,\\n    'min_speed': 343,\\n    'max_speed': 686,\\n    'speed': 'mixed_random_ifft',\\n    'time_step': 50e-9,\\n    'sources': [{\\n        'location': (None, 0),\\n        'period': 5e-6,\\n        'ncycles':1,\\n    }],\\n    'temporal_downsample': 2,\\n    'boundary': 1,\\n    'edge': 1,\\n}\\nRun and save simulation\\ngenerate_simulation_dataset(path, runs, **dataset_sim_params)\\n```\\nThe generate_simulation_dataset allows the speed to be a string that will specify a particular method of randomly generating speed values for the simulation grid.\\nThe Simulation Object\\nIf you'd like to understand in a little bit more detail how a simulation is defined then you might want to use the unerlying simulation object Simulation and manually set key objects like the Source and Detector. A full example of this is as follows\\n```python\\nCreate a simulation\\nsim = Simulation(size=size, spacing=spacing, max_speed=max_speed, time_step=time_step)\\nSet speed array\\nsim.set_speed(speed=speed, min_speed=min_speed, max_speed=max_speed)\\nAdd source\\nsim.add_source(location=location, period=period, ncycles=ncycles, phase=phase)\\nAdd detector grid\\nsim.add_detector(spatial_downsample=spatial_downsample,\\n                    boundary=boundary, edge=edge)\\nRun simulation\\nsim.run(duration=duration, temporal_downsample=temporal_downsample, progress=progress, leave=leave)\\nPrint simulation wave and speed data\\nprint('wave: ', sim.detected_wave)\\nprint('speed: ', sim.grid_speed)\\n```\\nNote these steps are done inside the run_single_source method for you as a convenience.\\nKnown Limitations\\nA perfectly matched layer boundary has recently been added, but might not perform well under all conditions. Additional contributions would be welcome here.\\nRight now the simulations are quite slow. I'd like to add a JAX backend, but \\nhavn't done so yet. Contributions would be welcome.\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"waver\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"waver\",\"documentation\":\"\",\"first_released\":\"2021-05-15T19:23:11.731841Z\",\"license\":\"BSD-3-Clause\",\"name\":\"waver\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\",\"widget\"],\"project_site\":\"https://github.com/sofroniewn/waver\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[\"*\"],\"release_date\":\"2021-08-15T23:04:51.974505Z\",\"report_issues\":\"\",\"requirements\":[\"magicgui (>=0.2.10)\",\"napari (>=0.4.10)\",\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"zarr\"],\"summary\":\"Wave simulations\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.0.4\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"trevor.j.manz@gmail.com\",\"name\":\"Trevor Manz\"}],\"code_repository\":\"https://github.com/manzt/napari-lazy-openslide\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-lazy-openslide\"}],\"description\":\"# napari-lazy-openslide\\n\\n[![License](https://img.shields.io/pypi/l/napari-lazy-openslide.svg?color=green)](https://github.com/manzt/napari-lazy-openslide/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-lazy-openslide.svg?color=green)](https://pypi.org/project/napari-lazy-openslide)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-lazy-openslide.svg?color=green)](https://python.org)\\n[![tests](https://github.com/manzt/napari-lazy-openslide/workflows/tests/badge.svg)](https://github.com/manzt/napari-lazy-openslide/actions)\\n\\nAn experimental plugin to lazily load multiscale whole-slide tiff images with openslide and dask.\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n## Installation\\n\\n**Step 1.)** Make sure you have OpenSlide installed. Download instructions [here](https://openslide.org/download/).\\n\\n> NOTE: Installation on macOS is easiest via Homebrew: `brew install openslide`. Up-to-date and multiplatform \\n> binaries for `openslide` are also avaiable via `conda`: `conda install -c sdvillal openslide-python`\\n\\n**Step 2.)** Install `napari-lazy-openslide` via `pip`:\\n\\n    pip install napari-lazy-openslide\\n\\n## Usage\\n\\n### Napari plugin\\n\\n```bash\\n$ napari tumor_004.tif\\n```\\nBy installing this package via `pip`, the plugin should be recognized by `napari`. The plugin\\nattempts to read image formats recognized by `openslide` that are multiscale \\n(`openslide.OpenSlide.level_count > 1`). \\n\\nIt should be noted that `napari-lazy-openslide` is experimental and has primarily \\nbeen tested with `CAMELYON16` and `CAMELYON17` datasets, which can be \\ndownloaded [here](https://camelyon17.grand-challenge.org/Data/).\\n\\n![Interactive deep zoom of whole-slide image](tumor_004.gif)\\n\\n\\n### Using `OpenSlideStore` with Zarr and Dask\\n\\nThe `OpenSlideStore` class wraps an `openslide.OpenSlide` object as a valid Zarr store. \\nThe underlying `openslide` image pyramid is translated to the Zarr multiscales extension,\\nwhere each level of the pyramid is a separate 3D `zarr.Array` with shape `(y, x, 4)`.\\n\\n```python\\nimport dask.array as da\\nimport zarr\\n\\nfrom napari_lazy_openslide import OpenSlideStore\\n\\nstore = OpenSlideStore('tumor_004.tif')\\ngrp = zarr.open(store, mode=\\\"r\\\")\\n\\n# The OpenSlideStore implements the multiscales extension\\n# https://forum.image.sc/t/multiscale-arrays-v0-1/37930\\ndatasets = grp.attrs[\\\"multiscales\\\"][0][\\\"datasets\\\"]\\n\\npyramid = [grp.get(d[\\\"path\\\"]) for d in datasets]\\nprint(pyramid)\\n# [\\n#   <zarr.core.Array '/0' (23705, 29879, 4) uint8 read-only>,\\n#   <zarr.core.Array '/1' (5926, 7469, 4) uint8 read-only>,\\n#   <zarr.core.Array '/2' (2963, 3734, 4) uint8 read-only>,\\n# ]\\n\\npyramid = [da.from_zarr(store, component=d[\\\"path\\\"]) for d in datasets]\\nprint(pyramid)\\n# [\\n#   dask.array<from-zarr, shape=(23705, 29879, 4), dtype=uint8, chunksize=(512, 512, 4), chunktype=numpy.ndarray>,\\n#   dask.array<from-zarr, shape=(5926, 7469, 4), dtype=uint8, chunksize=(512, 512, 4), chunktype=numpy.ndarray>,\\n#   dask.array<from-zarr, shape=(2963, 3734, 4), dtype=uint8, chunksize=(512, 512, 4), chunktype=numpy.ndarray>,\\n# ]\\n\\n# Now you can use numpy-like indexing with openslide, reading data into memory lazily!\\nlow_res = pyramid[-1][:]\\nregion = pyramid[0][y_start:y_end, x_start:x_end]\\n```\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with `tox`, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/manzt/napari-lazy-openslide/issues\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-lazy-openslide\\n\\n\\n\\n\\nAn experimental plugin to lazily load multiscale whole-slide tiff images with openslide and dask.\\n\\nThis napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.\\nInstallation\\nStep 1.) Make sure you have OpenSlide installed. Download instructions here.\\n\\nNOTE: Installation on macOS is easiest via Homebrew: brew install openslide. Up-to-date and multiplatform \\nbinaries for openslide are also avaiable via conda: conda install -c sdvillal openslide-python\\n\\nStep 2.) Install napari-lazy-openslide via pip:\\npip install napari-lazy-openslide\\n\\nUsage\\nNapari plugin\\nbash\\n$ napari tumor_004.tif\\nBy installing this package via pip, the plugin should be recognized by napari. The plugin\\nattempts to read image formats recognized by openslide that are multiscale \\n(openslide.OpenSlide.level_count > 1). \\nIt should be noted that napari-lazy-openslide is experimental and has primarily \\nbeen tested with CAMELYON16 and CAMELYON17 datasets, which can be \\ndownloaded here.\\n\\nUsing OpenSlideStore with Zarr and Dask\\nThe OpenSlideStore class wraps an openslide.OpenSlide object as a valid Zarr store. \\nThe underlying openslide image pyramid is translated to the Zarr multiscales extension,\\nwhere each level of the pyramid is a separate 3D zarr.Array with shape (y, x, 4).\\n```python\\nimport dask.array as da\\nimport zarr\\nfrom napari_lazy_openslide import OpenSlideStore\\nstore = OpenSlideStore('tumor_004.tif')\\ngrp = zarr.open(store, mode=\\\"r\\\")\\nThe OpenSlideStore implements the multiscales extension\\nhttps://forum.image.sc/t/multiscale-arrays-v0-1/37930\\ndatasets = grp.attrs[\\\"multiscales\\\"][0][\\\"datasets\\\"]\\npyramid = [grp.get(d[\\\"path\\\"]) for d in datasets]\\nprint(pyramid)\\n[\\n,\\n,\\n,\\n]\\npyramid = [da.from_zarr(store, component=d[\\\"path\\\"]) for d in datasets]\\nprint(pyramid)\\n[\\ndask.array,\\ndask.array,\\ndask.array,\\n]\\nNow you can use numpy-like indexing with openslide, reading data into memory lazily!\\nlow_res = pyramid[-1][:]\\nregion = pyramid[0][y_start:y_end, x_start:x_end]\\n```\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"napari-lazy-openslide\",\"documentation\":\"\",\"first_released\":\"2020-07-14T17:50:55.269908Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-lazy-openslide\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\"],\"project_site\":\"https://github.com/manzt/napari-lazy-openslide\",\"python_version\":\">=3.6\",\"reader_file_extensions\":[\"*\"],\"release_date\":\"2022-05-19T13:32:58.189644Z\",\"report_issues\":\"\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"zarr (>=2.11.0)\",\"numpy\",\"dask[array]\",\"openslide-python\"],\"summary\":\"A plugin to lazily load multiscale whole-slide images with openslide and dask\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.3.0\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Robert Haase\"}],\"category\":{\"Image modality\":[\"Fluorescence microscopy\"],\"Workflow step\":[\"Image Segmentation\",\"Image enhancement\",\"Image reconstruction\"]},\"category_hierarchy\":{\"Image modality\":[[\"Fluorescence microscopy\"]],\"Workflow step\":[[\"Image Segmentation\",\"Cell segmentation\"],[\"Image enhancement\",\"Smoothing\"],[\"Image Segmentation\",\"Image thresholding\"],[\"Image Segmentation\",\"Region growing\",\"Watershed segmentation\"],[\"Image reconstruction\",\"Image denoising\"],[\"Image enhancement\",\"Image denoising\"]]},\"code_repository\":\"https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-segment-blobs-and-things-with-membranes\"}],\"description\":\"# napari-segment-blobs-and-things-with-membranes (nsbatwm)\\n\\n[![License](https://img.shields.io/pypi/l/napari-segment-blobs-and-things-with-membranes.svg?color=green)](https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-segment-blobs-and-things-with-membranes.svg?color=green)](https://pypi.org/project/napari-segment-blobs-and-things-with-membranes)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-segment-blobs-and-things-with-membranes.svg?color=green)](https://python.org)\\n[![tests](https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes/actions)\\n[![codecov](https://codecov.io/gh/haesleinhuepf/napari-segment-blobs-and-things-with-membranes/branch/master/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-segment-blobs-and-things-with-membranes)\\n[![Development Status](https://img.shields.io/pypi/status/napari-segment-blobs-and-things-with-membranes.svg)](https://en.wikipedia.org/wiki/Software_release_life_cycle#Alpha)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-segment-blobs-and-things-with-membranes)](https://napari-hub.org/plugins/napari-segment-blobs-and-things-with-membranes)\\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.7027634.svg)](https://doi.org/10.5281/zenodo.7027634)\\n\\nThis napari-plugin is based on scikit-image and allows segmenting nuclei and cells based on fluorescence microscopy images with high intensity in nuclei and/or membranes.\\n\\n## Usage\\n\\nThis plugin populates image processing operations to the `Tools` menu in napari.\\nYou can recognize them with their suffix `(nsbatwm)` in brackets.\\nFurthermore, it can be used from the [napari-assistant](https://www.napari-hub.org/plugins/napari-assistant) graphical user interface. \\nTherefore, just click the menu `Tools > Utilities > Assistant (na)` or run `naparia` from the command line.\\n\\n![img.png](https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes/raw/main/docs/tools_menu_screenshot.png)\\n\\nYou can also call these functions as shown in [the demo notebook](https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes/blob/main/docs/demo.ipynb).\\n\\n### Voronoi-Otsu-Labeling\\n\\nThis algorithm uses [Otsu's thresholding method](https://ieeexplore.ieee.org/document/4310076) in combination with \\n[Gaussian blur](https://scikit-image.org/docs/dev/api/skimage.filters.html#skimage.filters.gaussian) and a \\n[Voronoi-Tesselation](https://en.wikipedia.org/wiki/Voronoi_diagram) \\napproach to label bright objects such as nuclei in an intensity image. The alogrithm has two sigma parameters which allow\\nyou to fine-tune where objects should be cut (`spot_sigma`) and how smooth outlines should be (`outline_sigma`).\\nThis implementation aims to be similar to [Voronoi-Otsu-Labeling in clesperanto](https://github.com/clEsperanto/pyclesperanto_prototype/blob/master/demo/segmentation/voronoi_otsu_labeling.ipynb).\\n\\n![img.png](https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes/raw/main/docs/voronoi_otsu_labeling.png)\\n\\n### Seeded Watershed\\n\\nStarting from an image showing high-intensity membranes and a seed-image where objects have been labeled (e.g. using Voronoi-Otsu-Labeling),\\nobjects are labeled that are constrained by the membranes.\\n\\n![img.png](https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes/raw/main/docs/seeded_watershed.png)\\n\\n### Seeded Watershed with mask\\n\\nIf there is additionally a mask image available, one can use the `Seeded Watershed with mask`, to constraint the flooding \\non a membrane image (1), starting from nuclei (2), limited by a mask image (3) to produce a cell segmentation within the mask (4).\\n\\n![img.png](https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes/raw/main/docs/seeded_watershed_with_mask.png)\\n\\n### Seeded Watershed using local minima as starting points\\n\\nSimilar to the Seeded Watershed and Voronoi-Otsu-Labeling explained above, you can use this tool to segment an image\\nshowing membranes without an additional image showing nuclei. The two sigma parameters allow to fine tune how close \\nobjects can be and how precise their boundaries are detected.\\n\\n![img.png](https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes/raw/main/docs/local_minima_seeded_watershed.png)\\n\\n### Gaussian blur\\n\\nApplies a [Gaussian blur](https://scikit-image.org/docs/dev/api/skimage.filters.html#skimage.filters.gaussian) to an\\nimage. This might be useful for denoising, e.g. before applying the Threshold-Otsu method.\\n\\n![img.png](https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes/raw/main/docs/gaussian_blur.png)\\n\\n### Subtract background\\n\\nSubtracts background using [scikit-image's rolling-ball algorithm](https://scikit-image.org/docs/stable/auto_examples/segmentation/plot_rolling_ball.html). \\nThis might be useful, for example to make intensity of membranes more similar in different regions of an image.\\n\\n![img.png](https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes/raw/main/docs/subtract_background.png)\\n\\n### Threshold Otsu\\n\\nBinarizes an image using [scikit-image's threshold Otsu algorithm](https://scikit-image.org/docs/dev/auto_examples/segmentation/plot_thresholding.html), also known as \\n[Otsu's method](https://ieeexplore.ieee.org/document/4310076).\\n\\n![img.png](https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes/raw/main/docs/threshold_otsu.png)\\n\\n### Split touching objects (formerly known as binary watershed).\\n\\nIn case objects stick together after thresholding, this tool might help.\\nIt aims to deliver similar results as [ImageJ's watershed implementation](https://imagej.nih.gov/ij/docs/menus/process.html#watershed).\\n\\n![img.png](https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes/raw/main/docs/binary_watershed.png)\\n\\n### Connected component labeling\\n\\nTakes a binary image and produces a label image with all separated objects labeled differently. Under the hood, it uses\\n[scikit-image's label function](https://scikit-image.org/docs/dev/auto_examples/segmentation/plot_label.html).\\n\\n![img.png](https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes/raw/main/docs/connected_component_labeling.png)\\n\\n### Manual split and merge labels\\n\\nSplit and merge labels in napari manually via the `Tools > Utilities menu`:\\n\\n![](https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes/raw/main/docs/split_and_merge_demo.gif)\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n## Installation\\n\\nYou can install `napari-segment-blobs-and-things-with-membranes` using `conda` and `pip`.\\nIf you have never used `conda` before, please go through [this tutorial](https://biapol.github.io/blog/johannes_mueller/anaconda_getting_started/) first.\\n\\n    conda install -c conda-forge napari\\n    pip install napari-segment-blobs-and-things-with-membranes\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-segment-blobs-and-things-with-membranes\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please create a thread on [image.sc] along with a detailed description and tag [@haesleinhuepf].\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n[image.sc]: https://image.sc\\n[@haesleinhuepf]: https://twitter.com/haesleinhuepf\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-segment-blobs-and-things-with-membranes (nsbatwm)\\n\\n\\n\\n\\n\\n\\n\\n\\nThis napari-plugin is based on scikit-image and allows segmenting nuclei and cells based on fluorescence microscopy images with high intensity in nuclei and/or membranes.\\nUsage\\nThis plugin populates image processing operations to the Tools menu in napari.\\nYou can recognize them with their suffix (nsbatwm) in brackets.\\nFurthermore, it can be used from the napari-assistant graphical user interface. \\nTherefore, just click the menu Tools > Utilities > Assistant (na) or run naparia from the command line.\\n\\nYou can also call these functions as shown in the demo notebook.\\nVoronoi-Otsu-Labeling\\nThis algorithm uses Otsu's thresholding method in combination with \\nGaussian blur and a \\nVoronoi-Tesselation \\napproach to label bright objects such as nuclei in an intensity image. The alogrithm has two sigma parameters which allow\\nyou to fine-tune where objects should be cut (spot_sigma) and how smooth outlines should be (outline_sigma).\\nThis implementation aims to be similar to Voronoi-Otsu-Labeling in clesperanto.\\n\\nSeeded Watershed\\nStarting from an image showing high-intensity membranes and a seed-image where objects have been labeled (e.g. using Voronoi-Otsu-Labeling),\\nobjects are labeled that are constrained by the membranes.\\n\\nSeeded Watershed with mask\\nIf there is additionally a mask image available, one can use the Seeded Watershed with mask, to constraint the flooding \\non a membrane image (1), starting from nuclei (2), limited by a mask image (3) to produce a cell segmentation within the mask (4).\\n\\nSeeded Watershed using local minima as starting points\\nSimilar to the Seeded Watershed and Voronoi-Otsu-Labeling explained above, you can use this tool to segment an image\\nshowing membranes without an additional image showing nuclei. The two sigma parameters allow to fine tune how close \\nobjects can be and how precise their boundaries are detected.\\n\\nGaussian blur\\nApplies a Gaussian blur to an\\nimage. This might be useful for denoising, e.g. before applying the Threshold-Otsu method.\\n\\nSubtract background\\nSubtracts background using scikit-image's rolling-ball algorithm. \\nThis might be useful, for example to make intensity of membranes more similar in different regions of an image.\\n\\nThreshold Otsu\\nBinarizes an image using scikit-image's threshold Otsu algorithm, also known as \\nOtsu's method.\\n\\nSplit touching objects (formerly known as binary watershed).\\nIn case objects stick together after thresholding, this tool might help.\\nIt aims to deliver similar results as ImageJ's watershed implementation.\\n\\nConnected component labeling\\nTakes a binary image and produces a label image with all separated objects labeled differently. Under the hood, it uses\\nscikit-image's label function.\\n\\nManual split and merge labels\\nSplit and merge labels in napari manually via the Tools > Utilities menu:\\n\\n\\nThis napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.\\nInstallation\\nYou can install napari-segment-blobs-and-things-with-membranes using conda and pip.\\nIf you have never used conda before, please go through this tutorial first.\\nconda install -c conda-forge napari\\npip install napari-segment-blobs-and-things-with-membranes\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-segment-blobs-and-things-with-membranes\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please create a thread on image.sc along with a detailed description and tag @haesleinhuepf.\",\"development_status\":[\"Development Status :: 3 - Alpha\"],\"display_name\":\"napari-segment-blobs-and-things-with-membranes\",\"documentation\":\"https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes#README.md\",\"first_released\":\"2021-09-25T14:21:45.864946Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-segment-blobs-and-things-with-membranes\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-09-08T12:29:31.639886Z\",\"report_issues\":\"https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes/issues\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"scikit-image\",\"scipy\",\"napari-tools-menu (>=0.1.17)\",\"napari-time-slicer (>=0.4.8)\",\"napari-assistant\",\"stackview (>=0.3.2)\"],\"summary\":\"A plugin based on scikit-image for segmenting nuclei and cells based on fluorescent microscopy images with high intensity in nuclei and/or membranes\",\"support\":\"https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes/issues\",\"twitter\":\"\",\"version\":\"0.3.3\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Dr. Thorsten Beier\"}],\"code_repository\":\"https://github.com/DerThorsten/napari-splinedist\",\"description\":\"# napari-splinedist\\n\\n[![License MIT](https://img.shields.io/pypi/l/napari-splinedist.svg?color=green)](https://github.com/DerThorsten/napari-splinedist/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-splinedist.svg?color=green)](https://pypi.org/project/napari-splinedist)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-splinedist.svg?color=green)](https://python.org)\\n[![tests](https://github.com/DerThorsten/napari-splinedist/workflows/tests/badge.svg)](https://github.com/DerThorsten/napari-splinedist/actions)\\n[![codecov](https://codecov.io/gh/DerThorsten/napari-splinedist/branch/main/graph/badge.svg)](https://codecov.io/gh/DerThorsten/napari-splinedist)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-splinedist)](https://napari-hub.org/plugins/napari-splinedist)\\n\\nA napari SplineDist plugin\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/stable/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-splinedist` via [pip]:\\n\\n    pip install napari-splinedist\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/DerThorsten/napari-splinedist.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [MIT] license,\\n\\\"napari-splinedist\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/DerThorsten/napari-splinedist/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-splinedist\\n\\n\\n\\n\\n\\n\\nA napari SplineDist plugin\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-splinedist via pip:\\npip install napari-splinedist\\n\\nTo install latest development version :\\npip install git+https://github.com/DerThorsten/napari-splinedist.git\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the MIT license,\\n\\\"napari-splinedist\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"SplineDist\",\"documentation\":\"https://github.com/DerThorsten/napari-splinedist#README.md\",\"first_released\":\"2022-10-18T14:03:01.283427Z\",\"license\":\"MIT\",\"name\":\"napari-splinedist\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\",\"sample_data\"],\"project_site\":\"https://github.com/DerThorsten/napari-splinedist\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-10-31T07:43:11.399242Z\",\"report_issues\":\"https://github.com/DerThorsten/napari-splinedist/issues\",\"requirements\":[\"pydantic\",\"numpy\",\"magicgui\",\"qtpy\",\"stardist (>=0.8.3)\",\"splinedist (>=0.1.2)\",\"napari-splineit (>=0.3.0)\",\"requests\",\"tensorflow\",\"opencv-python-headless\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\"],\"summary\":\"A napari SplineDist plugin\",\"support\":\"https://github.com/DerThorsten/napari-splinedist/issues\",\"twitter\":\"\",\"version\":\"0.3.1\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"emma@emmazhou.com\",\"name\":\"Emma Zhou\"}],\"code_repository\":\"https://github.com/emmazhou/napari-dvid\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-dvid\"}],\"description\":\"# napari-dvid\\n\\n[![License](https://img.shields.io/pypi/l/napari-dvid.svg?color=green)](https://github.com/emmazhou/napari-dvid/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-dvid.svg?color=green)](https://pypi.org/project/napari-dvid)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-dvid.svg?color=green)](https://python.org)\\n[![tests](https://github.com/emmazhou/napari-dvid/workflows/tests/badge.svg)](https://github.com/emmazhou/napari-dvid/actions)\\n[![codecov](https://codecov.io/gh/emmazhou/napari-dvid/branch/master/graph/badge.svg)](https://codecov.io/gh/emmazhou/napari-dvid)\\n\\nDVID loader for napari, from a url\\n\\n---\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-dvid` via [pip]:\\n\\n    pip install napari-dvid\\n\\n## Examples\\n\\nOnce installed, run `napari --with napari-dvid` to get the plugin sidebar:\\n\\n![Screenshot](screenshot.png)\\n\\nPaste in a URL to a DVID volume and hit \\\"Load\\\" to load the volume! As an example, try:\\n\\n`https://emdata.janelia.org/api/node/ab6e610d4/grayscale/raw/0_1_2/256_256_256/7500_7000_4400`\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [MIT] license,\\n\\\"napari-dvid\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[mit]: http://opensource.org/licenses/MIT\\n[bsd-3]: http://opensource.org/licenses/BSD-3-Clause\\n[gnu gpl v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[gnu lgpl v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[apache software license 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[mozilla public license 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/emmazhou/napari-dvid/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[pypi]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-dvid\\n\\n\\n\\n\\n\\nDVID loader for napari, from a url\\n\\nThis napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-dvid via pip:\\npip install napari-dvid\\n\\nExamples\\nOnce installed, run napari --with napari-dvid to get the plugin sidebar:\\n\\nPaste in a URL to a DVID volume and hit \\\"Load\\\" to load the volume! As an example, try:\\nhttps://emdata.janelia.org/api/node/ab6e610d4/grayscale/raw/0_1_2/256_256_256/7500_7000_4400\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the MIT license,\\n\\\"napari-dvid\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari-dvid\",\"documentation\":\"https://github.com/emmazhou/napari-dvid#README.md\",\"first_released\":\"2021-06-09T21:18:41.905660Z\",\"license\":\"MIT\",\"name\":\"napari-dvid\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\",\"widget\"],\"project_site\":\"https://github.com/emmazhou/napari-dvid\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[\"*\"],\"release_date\":\"2021-06-09T21:45:26.613386Z\",\"report_issues\":\"https://github.com/emmazhou/napari-dvid/issues\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"requests\"],\"summary\":\"DVID loader for napari, from a url\",\"support\":\"https://github.com/emmazhou/napari-dvid/issues\",\"twitter\":\"\",\"version\":\"0.2.0\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Robert Haase\"}],\"category\":{\"Supported data\":[\"2D\",\"3D\"],\"Workflow step\":[\"Image Segmentation\",\"Image correction\",\"Image reconstruction\",\"Image enhancement\",\"Object feature extraction\",\"Morphological operations\",\"Image feature detection\"]},\"category_hierarchy\":{\"Supported data\":[[\"2D\"],[\"3D\"]],\"Workflow step\":[[\"Image Segmentation\",\"Cell segmentation\"],[\"Image Segmentation\",\"Connected-component analysis\"],[\"Image correction\",\"Illumination correction\"],[\"Image correction\"],[\"Image reconstruction\",\"Image denoising\"],[\"Image enhancement\",\"Image denoising\"],[\"Image Segmentation\",\"Image thresholding\"],[\"Image Segmentation\"],[\"Object feature extraction\"],[\"Image enhancement\",\"Smoothing\"],[\"Morphological operations\"],[\"Image feature detection\"],[\"Image Segmentation\",\"Semi-automatic segmentation\"],[\"Image feature detection\",\"Edge detection\"],[\"Morphological operations\",\"Top-hat transform\"],[\"Morphological operations\",\"Closing\"],[\"Morphological operations\",\"Dilation\"],[\"Morphological operations\",\"Opening\"],[\"Morphological operations\",\"Erosion\"],[\"Image enhancement\"]]},\"code_repository\":\"https://github.com/haesleinhuepf/napari-simpleitk-image-processing\",\"description\":\"# napari-simpleitk-image-processing (n-SimpleITK)\\r\\n\\r\\n[![License](https://img.shields.io/pypi/l/napari-simpleitk-image-processing.svg?color=green)](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/raw/main/LICENSE)\\r\\n[![PyPI](https://img.shields.io/pypi/v/napari-simpleitk-image-processing.svg?color=green)](https://pypi.org/project/napari-simpleitk-image-processing)\\r\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-simpleitk-image-processing.svg?color=green)](https://python.org)\\r\\n[![tests](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/actions)\\r\\n[![codecov](https://codecov.io/gh/haesleinhuepf/napari-simpleitk-image-processing/branch/main/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-simpleitk-image-processing)\\r\\n[![Development Status](https://img.shields.io/pypi/status/napari-simpleitk-image-processing.svg)](https://en.wikipedia.org/wiki/Software_release_life_cycle#Alpha)\\r\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-simpleitk-image-processing)](https://napari-hub.org/plugins/napari-simpleitk-image-processing)\\r\\n[![DOI](https://zenodo.org/badge/432729955.svg)](https://zenodo.org/badge/latestdoi/432729955)\\r\\n\\r\\nProcess images using [SimpleITK](https://simpleitk.org/) in [napari]\\r\\n\\r\\n## Usage\\r\\n\\r\\nFilters, segmentation algorithms and measurements provided by this napari plugin can be found in the `Tools` menu. \\r\\nYou can recognize them with their suffix `(n-SimpleITK)` in brackets.\\r\\nFurthermore, it can be used from the [napari-assistant](https://www.napari-hub.org/plugins/napari-assistant) graphical user interface. \\r\\nTherefore, just click the menu `Tools > Utilities > Assistant (na)` or run `naparia` from the command line.\\r\\n\\r\\n![img.png](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/raw/main/docs/screenshot_with_assistant.png)\\r\\n\\r\\nAll filters implemented in this napari plugin are also demonstrated in [this notebook](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/blob/main/docs/demo.ipynb).\\r\\n\\r\\n### Gaussian blur\\r\\n\\r\\nApplies a [Gaussian blur](https://en.wikipedia.org/wiki/Gaussian_blur)\\r\\nto an image. This might be useful for denoising, e.g. before applying the Threshold-Otsu method.\\r\\n\\r\\n![img.png](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/raw/main/docs/gaussian_blur.png)\\r\\n\\r\\n### Median filter\\r\\n\\r\\nApplies a [median filter](https://en.wikipedia.org/wiki/Median_filter) to an image. \\r\\nCompared to the Gaussian blur this method preserves edges in the image better. \\r\\nIt also performs slower.\\r\\n\\r\\n![img.png](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/raw/main/docs/median_filter.png)\\r\\n\\r\\n### Bilateral filter\\r\\n\\r\\nThe [bilateral filter](https://en.wikipedia.org/wiki/Bilateral_filter) allows denoising an image\\r\\nwhile preserving edges.\\r\\n\\r\\n![img.png](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/raw/main/docs/bilateral.png)\\r\\n\\r\\n### Threshold Otsu\\r\\n\\r\\nBinarizes an image using [Otsu's method](https://ieeexplore.ieee.org/document/4310076).\\r\\n\\r\\n![img.png](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/raw/main/docs/threshold_otsu.png)\\r\\n\\r\\n### Connected Component Labeling\\r\\n\\r\\nTakes a binary image and labels all objects with individual numbers to produce a label image.\\r\\n\\r\\n![img.png](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/raw/main/docs/connected_component_labeling.png)\\r\\n\\r\\n### Measurements\\r\\n\\r\\nThis function allows determining intensity and shape statistics from labeled images. I can be found in the `Tools > Measurement tables` menu.\\r\\n\\r\\n![img.png](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/raw/main/docs/measurements.png)\\r\\n\\r\\n### Signed Maurer distance map\\r\\n\\r\\nA distance map (more precise: [Signed Maurer Distance Map](https://itk.org/ITKExamples/src/Filtering/DistanceMap/MaurerDistanceMapOfBinary/Documentation.html)) can be useful for visualizing distances within binary images between black/white borders. \\r\\nPositive values in this image correspond to a white (value=1) pixel's distance to the next black pixel.\\r\\nBlack pixel's (value=0) distance to the next white pixel are represented in this map with negative values.\\r\\n\\r\\n![img.png](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/raw/main/docs/signed_maured_distance_map.png)\\r\\n\\r\\n### Binary fill holes\\r\\n\\r\\nFills holes in a binary image.\\r\\n\\r\\n![img.png](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/raw/main/docs/binary_fill_holes.png)\\r\\n\\r\\n### Touching objects labeling\\r\\n\\r\\nStarting from a binary image, touching objects can be splits into multiple regions, similar to the [Watershed segmentation in ImageJ](https://imagej.net/plugins/classic-watershed).\\r\\n\\r\\n![img.png](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/raw/main/docs/Touching_object_labeling.png)\\r\\n\\r\\n### Morphological Watershed\\r\\n\\r\\nThe [morhological watershed](http://insightsoftwareconsortium.github.io/SimpleITK-Notebooks/Python_html/32_Watersheds_Segmentation.html)\\r\\nallows to segment images showing membranes. Before segmentation, a filter such as the Gaussian blur or a median filter\\r\\nshould be used to eliminate noise. It also makes sense to increase the thickness of membranes using a maximum filter. \\r\\nSee [this notebook](https://github.com/clEsperanto/pyclesperanto_prototype/blob/master/demo/segmentation/segmentation_2d_membranes.ipynb) for details.\\r\\n\\r\\n![img.png](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/raw/main/docs/morphological_watershed.png)\\r\\n\\r\\n### Watershed-Otsu-Labeling\\r\\n\\r\\nThis algorithm uses [Otsu's thresholding method](https://ieeexplore.ieee.org/document/4310076) in combination with \\r\\n[Gaussian blur](https://en.wikipedia.org/wiki/Gaussian_blur) and the \\r\\n[Watershed-algorithm](https://en.wikipedia.org/wiki/Watershed_(image_processing)) \\r\\napproach to label bright objects such as nuclei in an intensity image. The alogrithm has two sigma parameters and a \\r\\nlevel parameter which allow you to fine-tune where objects should be cut (`spot_sigma`) and how smooth outlines \\r\\nshould be (`outline_sigma`). The `watershed_level` parameter determines how deep an intensity valley between two maxima \\r\\nhas to be to differentiate the two maxima. \\r\\nThis implementation is similar to [Voronoi-Otsu-Labeling in clesperanto](https://github.com/clEsperanto/pyclesperanto_prototype/blob/master/demo/segmentation/voronoi_otsu_labeling.ipynb).\\r\\n\\r\\n\\r\\n![img.png](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/raw/main/docs/watershed_otsu_labeling.png)\\r\\n\\r\\n### Richardson-Lucy Deconvolution\\r\\n\\r\\n[Richardson-Lucy deconvolution](https://en.wikipedia.org/wiki/Richardson%E2%80%93Lucy_deconvolution)\\r\\nallows to restore image quality if the point-spread-function of the optical system used \\r\\nfor acquisition is known or can be approximated.\\r\\n\\r\\n![img.png](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/raw/main/docs/Richardson-Lucy-Deconvolution.png)\\r\\n\\r\\n\\r\\n## Installation\\r\\n\\r\\nYou can install `napari-simpleitk-image-processing` via using `conda` and `pip`.\\r\\nIf you have never used `conda` before, please go through [this tutorial](https://biapol.github.io/blog/johannes_mueller/anaconda_getting_started/) first.\\r\\n\\r\\n    conda install -c conda-forge napari\\r\\n    pip install napari-simpleitk-image-processing\\r\\n\\r\\n## See also\\r\\n\\r\\nThere are other napari plugins with similar functionality for processing images and extracting features:\\r\\n* [morphometrics](https://www.napari-hub.org/plugins/morphometrics)\\r\\n* [PartSeg](https://www.napari-hub.org/plugins/PartSeg)\\r\\n* [napari-skimage-regionprops](https://www.napari-hub.org/plugins/napari-skimage-regionprops)\\r\\n* [napari-cupy-image-processing](https://www.napari-hub.org/plugins/napari-cupy-image-processing)\\r\\n* [napari-pyclesperanto-assistant](https://www.napari-hub.org/plugins/napari-pyclesperanto-assistant)\\r\\n* [napari-allencell-segmenter](https://napari-hub.org/plugins/napari-allencell-segmenter)\\r\\n* [RedLionfish](https://www.napari-hub.org/plugins/RedLionfish)\\r\\n* [bbii-decon](https://www.napari-hub.org/plugins/bbii-decon)  \\r\\n* [napari-segment-blobs-and-things-with-membranes](https://www.napari-hub.org/plugins/napari-segment-blobs-and-things-with-membranes)\\r\\n\\r\\nFurthermore, there are plugins for postprocessing extracted measurements\\r\\n* [napari-feature-classifier](https://www.napari-hub.org/plugins/napari-feature-classifier)\\r\\n* [napari-clusters-plotter](https://www.napari-hub.org/plugins/napari-clusters-plotter)\\r\\n\\r\\n## Contributing\\r\\n\\r\\nContributions are very welcome. There are many useful algorithms available in \\r\\n[SimpleITK](https://simpleitk.org/). If you want another one available here in this napari\\r\\nplugin, don't hesitate to send a [pull-request](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/pulls).\\r\\nThis repository just holds wrappers for SimpleITK-functions, see [this file](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/raw/main/src/napari_simpleitk_image_processing/_simpleitk_image_processing.py#L51) for how those wrappers\\r\\ncan be written.\\r\\n\\r\\n## License\\r\\n\\r\\nDistributed under the terms of the [BSD-3] license,\\r\\n\\\"napari-simpleitk-image-processing\\\" is free and open source software\\r\\n\\r\\n## Citation\\r\\n\\r\\nFor implementing this napari plugin, the \\r\\n[SimpleITK python notebooks](https://insightsoftwareconsortium.github.io/SimpleITK-Notebooks/) were very helpful. \\r\\nThus, if you find the plugin useful, consider citing the SimpleITK notebooks:\\r\\n\\r\\nZ. Yaniv, B. C. Lowekamp, H. J. Johnson, R. Beare, \\r\\n\\\"SimpleITK Image-Analysis Notebooks: a Collaborative Environment for Education and Reproducible Research\\\", \\\\\\r\\nJ Digit Imaging., 31(3): 290-303, 2018, [https://doi.org/10.1007/s10278-017-0037-8](https://doi.org/10.1007/s10278-017-0037-8).\\r\\n\\r\\n## Issues\\r\\n\\r\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\r\\n\\r\\n[napari]: https://github.com/napari/napari\\r\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\r\\n[@napari]: https://github.com/napari\\r\\n[MIT]: http://opensource.org/licenses/MIT\\r\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\r\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\r\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\r\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\r\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\r\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\r\\n\\r\\n[file an issue]: https://github.com/haesleinhuepf/napari-simpleitk-image-processing/issues\\r\\n\\r\\n[napari]: https://github.com/napari/napari\\r\\n[tox]: https://tox.readthedocs.io/en/latest/\\r\\n[pip]: https://pypi.org/project/pip/\\r\\n[PyPI]: https://pypi.org/\\r\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-simpleitk-image-processing (n-SimpleITK)\\n\\n\\n\\n\\n\\n\\n\\n\\nProcess images using SimpleITK in napari\\nUsage\\nFilters, segmentation algorithms and measurements provided by this napari plugin can be found in the Tools menu. \\nYou can recognize them with their suffix (n-SimpleITK) in brackets.\\nFurthermore, it can be used from the napari-assistant graphical user interface. \\nTherefore, just click the menu Tools > Utilities > Assistant (na) or run naparia from the command line.\\n\\nAll filters implemented in this napari plugin are also demonstrated in this notebook.\\nGaussian blur\\nApplies a Gaussian blur\\nto an image. This might be useful for denoising, e.g. before applying the Threshold-Otsu method.\\n\\nMedian filter\\nApplies a median filter to an image. \\nCompared to the Gaussian blur this method preserves edges in the image better. \\nIt also performs slower.\\n\\nBilateral filter\\nThe bilateral filter allows denoising an image\\nwhile preserving edges.\\n\\nThreshold Otsu\\nBinarizes an image using Otsu's method.\\n\\nConnected Component Labeling\\nTakes a binary image and labels all objects with individual numbers to produce a label image.\\n\\nMeasurements\\nThis function allows determining intensity and shape statistics from labeled images. I can be found in the Tools > Measurement tables menu.\\n\\nSigned Maurer distance map\\nA distance map (more precise: Signed Maurer Distance Map) can be useful for visualizing distances within binary images between black/white borders. \\nPositive values in this image correspond to a white (value=1) pixel's distance to the next black pixel.\\nBlack pixel's (value=0) distance to the next white pixel are represented in this map with negative values.\\n\\nBinary fill holes\\nFills holes in a binary image.\\n\\nTouching objects labeling\\nStarting from a binary image, touching objects can be splits into multiple regions, similar to the Watershed segmentation in ImageJ.\\n\\nMorphological Watershed\\nThe morhological watershed\\nallows to segment images showing membranes. Before segmentation, a filter such as the Gaussian blur or a median filter\\nshould be used to eliminate noise. It also makes sense to increase the thickness of membranes using a maximum filter. \\nSee this notebook for details.\\n\\nWatershed-Otsu-Labeling\\nThis algorithm uses Otsu's thresholding method in combination with \\nGaussian blur and the \\nWatershed-algorithm \\napproach to label bright objects such as nuclei in an intensity image. The alogrithm has two sigma parameters and a \\nlevel parameter which allow you to fine-tune where objects should be cut (spot_sigma) and how smooth outlines \\nshould be (outline_sigma). The watershed_level parameter determines how deep an intensity valley between two maxima \\nhas to be to differentiate the two maxima. \\nThis implementation is similar to Voronoi-Otsu-Labeling in clesperanto.\\n\\nRichardson-Lucy Deconvolution\\nRichardson-Lucy deconvolution\\nallows to restore image quality if the point-spread-function of the optical system used \\nfor acquisition is known or can be approximated.\\n\\nInstallation\\nYou can install napari-simpleitk-image-processing via using conda and pip.\\nIf you have never used conda before, please go through this tutorial first.\\nconda install -c conda-forge napari\\npip install napari-simpleitk-image-processing\\n\\nSee also\\nThere are other napari plugins with similar functionality for processing images and extracting features:\\n* morphometrics\\n* PartSeg\\n* napari-skimage-regionprops\\n* napari-cupy-image-processing\\n* napari-pyclesperanto-assistant\\n* napari-allencell-segmenter\\n* RedLionfish\\n* bbii-decon\\n* napari-segment-blobs-and-things-with-membranes\\nFurthermore, there are plugins for postprocessing extracted measurements\\n* napari-feature-classifier\\n* napari-clusters-plotter\\nContributing\\nContributions are very welcome. There are many useful algorithms available in \\nSimpleITK. If you want another one available here in this napari\\nplugin, don't hesitate to send a pull-request.\\nThis repository just holds wrappers for SimpleITK-functions, see this file for how those wrappers\\ncan be written.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-simpleitk-image-processing\\\" is free and open source software\\nCitation\\nFor implementing this napari plugin, the \\nSimpleITK python notebooks were very helpful. \\nThus, if you find the plugin useful, consider citing the SimpleITK notebooks:\\nZ. Yaniv, B. C. Lowekamp, H. J. Johnson, R. Beare, \\n\\\"SimpleITK Image-Analysis Notebooks: a Collaborative Environment for Education and Reproducible Research\\\", \\\\\\nJ Digit Imaging., 31(3): 290-303, 2018, https://doi.org/10.1007/s10278-017-0037-8.\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 3 - Alpha\"],\"display_name\":\"napari-simpleitk-image-processing\",\"documentation\":\"https://github.com/haesleinhuepf/napari-simpleitk-image-processing#README.md\",\"first_released\":\"2021-11-28T15:30:28.098377Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-simpleitk-image-processing\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/haesleinhuepf/napari-simpleitk-image-processing\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2023-01-15T16:52:24.503482Z\",\"report_issues\":\"https://github.com/haesleinhuepf/napari-simpleitk-image-processing/issues\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"simpleitk\",\"napari-tools-menu (>=0.1.17)\",\"napari-time-slicer\",\"napari-skimage-regionprops (>=0.5.1)\",\"napari-assistant (>=0.3.10)\",\"pandas\",\"stackview (>=0.3.2)\"],\"summary\":\"Process and analyze images using SimpleITK in napari\",\"support\":\"https://github.com/haesleinhuepf/napari-simpleitk-image-processing/issues\",\"twitter\":\"\",\"version\":\"0.4.5\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"lennart.kowitz@isas.de\",\"name\":\"Lennart Kowitz\"}],\"code_repository\":\"https://github.com/MMV-Lab/vessel-express-napari\",\"conda\":[],\"description\":\"# vessel-express-napari\\n\\n[![License](https://img.shields.io/pypi/l/vessel-express-napari.svg?color=green)](https://github.com/MMV-Lab/vessel-express-napari/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/vessel-express-napari.svg?color=green)](https://pypi.org/project/vessel-express)\\n[![Python Version](https://img.shields.io/pypi/pyversions/vessel-express-napari.svg?color=green)](https://python.org)\\n[![tests](https://github.com/MMV-Lab/vessel-express-napari/workflows/tests/badge.svg)](https://github.com/MMV-Lab/vessel-express-napari/actions)\\n[![codecov](https://codecov.io/gh/MMV-Lab/vessel-express-napari/branch/main/graph/badge.svg)](https://codecov.io/gh/MMV-Lab/vessel-express-napari)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/vessel-express-napari)](https://napari-hub.org/plugins/vessel-express)\\n\\nA simple plugin for 3D vessel segmentation of LSFM images\\n\\nThis [napari] plugin can be used to optimize the segmentation parameters for the [main VesselExpress software platform](https://github.com/RUB-Bioinf/VesselExpress).\\n\\n----------------------------------\\n\\n\\n## Installation\\n\\nThe easiest way to install the plugin is to open napari, go to Plugins, then Install/Uninstall plugins. You will be able to find the plugin by name \\\"vessel-express-napari\\\". \\n\\nOr, you can install `vessel-express-napari` via [pip]:\\n\\n    pip install vessel-express-napari\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/MMV-Lab/vessel-express-napari.git\\n\\n\\n## Documentation\\n\\nWe provide a [quick start guide] to explain the important pieces of this plugin. Suggestions and feature quests are very welcomed. \\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"vessel-express-napari\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/MMV-Lab/vessel-express-napari/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n[quick start guide]: https://github.com/MMV-Lab/vessel-express-napari/quick_start.md\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"vessel-express-napari\\n\\n\\n\\n\\n\\n\\nA simple plugin for 3D vessel segmentation of LSFM images\\nThis napari plugin can be used to optimize the segmentation parameters for the main VesselExpress software platform.\\n\\nInstallation\\nThe easiest way to install the plugin is to open napari, go to Plugins, then Install/Uninstall plugins. You will be able to find the plugin by name \\\"vessel-express-napari\\\". \\nOr, you can install vessel-express-napari via pip:\\npip install vessel-express-napari\\n\\nTo install latest development version :\\npip install git+https://github.com/MMV-Lab/vessel-express-napari.git\\n\\nDocumentation\\nWe provide a quick start guide to explain the important pieces of this plugin. Suggestions and feature quests are very welcomed. \\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"vessel-express-napari\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"vessel-express-napari\",\"documentation\":\"https://github.com/MMV-Lab/vessel-express-napari#README.md\",\"first_released\":\"2022-05-17T17:42:39.105873Z\",\"license\":\"BSD-3-Clause\",\"name\":\"vessel-express-napari\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\",\"widget\"],\"project_site\":\"https://github.com/MMV-Lab/vessel-express-napari\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[\"*\"],\"release_date\":\"2022-05-17T17:42:39.105873Z\",\"report_issues\":\"https://github.com/MMV-Lab/vessel-express-napari/issues\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"itk\",\"scikit-image\",\"aicssegmentation\"],\"summary\":\"A simple plugin for 3D vessel segmentation\",\"support\":\"https://github.com/MMV-Lab/vessel-express-napari/issues\",\"twitter\":\"\",\"version\":\"0.0.8\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"conda\":[],\"display_name\":\"Napari DeepFinder\",\"npe2\":true,\"plugin_types\":[\"reader\",\"writer\",\"widget\"],\"reader_file_extensions\":[\"*.xls\",\"*.xml\",\"*.ods\",\"*.xlsx\",\"*.rec\",\"*.tif\",\"*.map\",\"*.h5\",\"*.TIF\",\"*.mrc\"],\"writer_file_extensions\":[\".xml\",\".mrc\"],\"writer_save_layers\":[\"points\",\"labels\",\"image\"]}",
  "{\"authors\":[{\"name\":\"Allen Institute for Cell Science\"}],\"category\":{\"Image modality\":[\"Fluorescence microscopy\"],\"Supported data\":[\"3D\"],\"Workflow step\":[\"Image Segmentation\",\"Visualization\"]},\"category_hierarchy\":{\"Image modality\":[[\"Fluorescence microscopy\"]],\"Supported data\":[[\"3D\"]],\"Workflow step\":[[\"Image Segmentation\",\"Cell segmentation\"],[\"Image Segmentation\",\"Model-based segmentation\"],[\"Visualization\",\"Image visualisation\"]]},\"code_repository\":\"https://github.com/AllenCell/napari-allencell-segmenter\",\"description\":\"# napari-allencell-segmenter\\n\\n[![License](https://img.shields.io/pypi/l/napari-allencell-segmenter.svg?color=green)](https://github.com/AllenCell/napari-allencell-segmenter/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-allencell-segmenter.svg?color=green)](https://pypi.org/project/napari-allencell-segmenter)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-allencell-segmenter.svg?color=green)](https://python.org)\\n[![tests](https://github.com/AllenCell/napari-allencell-segmenter/workflows/tests/badge.svg)](https://github.com/AllenCell/napari-allencell-segmenter/actions)\\n[![codecov](https://codecov.io/gh/AllenCell/napari-allencell-segmenter/branch/main/graph/badge.svg)](https://codecov.io/gh/AllenCell/napari-allencell-segmenter)\\n\\n\\nA plugin that enables 3D image segmentation provided by Allen Institute for Cell Science\\n\\nThe Allen Cell & Structure Segmenter plugin for napari provides an intuitive graphical user interface to access the powerful segmentation capabilities of an open source 3D segmentation software package developed and maintained by the Allen Institute for Cell Science (classic workflows only with v1.0). ​[The Allen Cell & Structure Segmenter](https://allencell.org/segmenter) is a Python-based open source toolkit developed at the Allen Institute for Cell Science for 3D segmentation of intracellular structures in fluorescence microscope images. This toolkit brings together classic image segmentation and iterative deep learning workflows first to generate initial high-quality 3D intracellular structure segmentations and then to easily curate these results to generate the ground truths for building robust and accurate deep learning models. The toolkit takes advantage of the high replicate 3D live cell image data collected at the Allen Institute for Cell Science of over 30 endogenous fluorescently tagged human induced pluripotent stem cell (hiPSC) lines. Each cell line represents a different intracellular structure with one or more distinct localization patterns within undifferentiated hiPS cells and hiPSC-derived cardiomyocytes.\\n\\nMore details about Segmenter can be found at https://allencell.org/segmenter\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Installation\\n\\n### Option 1 (recommended):\\n\\nAfter you installed the lastest version of napari, you can go to \\\"Plugins\\\" --> \\\"Install/Uninstall Package(s)\\\". Then, you will be able to see all available napari plugins and you can find us by name `napari-allencell-segmenter`. Just click the \\\"install\\\" button to install the Segmenter plugin.\\n\\n### Option 2:\\n\\nYou can also install `napari-allencell-segmenter` via [pip]:\\n\\n    pip install napari-allencell-segmenter\\n\\n## Quick Start\\n\\nIn the current version, there are two parts in the plugin: **workflow editor** and **batch processing**. The **workflow editor** allows users adjusting parameters in all the existing workflows in the lookup table, so that the workflow can be optimized on users' data. The adjusted workflow can be saved and then applied to a large batch of files using the **batch processing** part of the plugin. \\n\\n1. Open a file in napari (the plugin is able to support multi-dimensional data in .tiff, .tif. ome.tif, .ome.tiff, .czi)\\n2. Start the plugin (open napari, go to \\\"Plugins\\\" --> \\\"napari-allencell-segmenter\\\" --> \\\"workflow editor\\\")\\n3. Select the image and channel to work on\\n4. Select a workflow based on the example image and target segmentation based on user's data. Ideally, it is recommend to start with the example with very similar morphology as user's data.\\n5. Click \\\"Run All\\\" to execute the whole workflow on the sample data.\\n6. Adjust the parameters of steps, based on the intermediate results. For instruction on the details on each function and the effect of each parameter, click the tooltip button. A complete list of all functions can be found [here](https://github.com/AllenCell/aics-segmentation/blob/main/aicssegmentation/structure_wrapper_config/function_params.md)\\n7. Click \\\"Run All\\\" again after adjusting the parameters and repeat step 6 and 7 until the result is satisfactory.\\n8. Save the workflow\\n9. Close the plugin and open the **batch processing** part by (go to \\\"Plugins\\\" --> \\\"napari-allencell-segmenter\\\" --> \\\"batch processing\\\")\\n10. Load the customized workflow (or an off-the-shelf workflow) json file\\n11. Load the folder with all the images to process\\n12. Click \\\"Run\\\"\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-allencell-segmenter\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/AllenCell/napari-allencell-segmenter/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-allencell-segmenter\\n\\n\\n\\n\\n\\nA plugin that enables 3D image segmentation provided by Allen Institute for Cell Science\\nThe Allen Cell & Structure Segmenter plugin for napari provides an intuitive graphical user interface to access the powerful segmentation capabilities of an open source 3D segmentation software package developed and maintained by the Allen Institute for Cell Science (classic workflows only with v1.0). ​The Allen Cell & Structure Segmenter is a Python-based open source toolkit developed at the Allen Institute for Cell Science for 3D segmentation of intracellular structures in fluorescence microscope images. This toolkit brings together classic image segmentation and iterative deep learning workflows first to generate initial high-quality 3D intracellular structure segmentations and then to easily curate these results to generate the ground truths for building robust and accurate deep learning models. The toolkit takes advantage of the high replicate 3D live cell image data collected at the Allen Institute for Cell Science of over 30 endogenous fluorescently tagged human induced pluripotent stem cell (hiPSC) lines. Each cell line represents a different intracellular structure with one or more distinct localization patterns within undifferentiated hiPS cells and hiPSC-derived cardiomyocytes.\\nMore details about Segmenter can be found at https://allencell.org/segmenter\\n\\nThis napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nOption 1 (recommended):\\nAfter you installed the lastest version of napari, you can go to \\\"Plugins\\\" --> \\\"Install/Uninstall Package(s)\\\". Then, you will be able to see all available napari plugins and you can find us by name napari-allencell-segmenter. Just click the \\\"install\\\" button to install the Segmenter plugin.\\nOption 2:\\nYou can also install napari-allencell-segmenter via pip:\\npip install napari-allencell-segmenter\\n\\nQuick Start\\nIn the current version, there are two parts in the plugin: workflow editor and batch processing. The workflow editor allows users adjusting parameters in all the existing workflows in the lookup table, so that the workflow can be optimized on users' data. The adjusted workflow can be saved and then applied to a large batch of files using the batch processing part of the plugin. \\n\\nOpen a file in napari (the plugin is able to support multi-dimensional data in .tiff, .tif. ome.tif, .ome.tiff, .czi)\\nStart the plugin (open napari, go to \\\"Plugins\\\" --> \\\"napari-allencell-segmenter\\\" --> \\\"workflow editor\\\")\\nSelect the image and channel to work on\\nSelect a workflow based on the example image and target segmentation based on user's data. Ideally, it is recommend to start with the example with very similar morphology as user's data.\\nClick \\\"Run All\\\" to execute the whole workflow on the sample data.\\nAdjust the parameters of steps, based on the intermediate results. For instruction on the details on each function and the effect of each parameter, click the tooltip button. A complete list of all functions can be found here\\nClick \\\"Run All\\\" again after adjusting the parameters and repeat step 6 and 7 until the result is satisfactory.\\nSave the workflow\\nClose the plugin and open the batch processing part by (go to \\\"Plugins\\\" --> \\\"napari-allencell-segmenter\\\" --> \\\"batch processing\\\")\\nLoad the customized workflow (or an off-the-shelf workflow) json file\\nLoad the folder with all the images to process\\nClick \\\"Run\\\"\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-allencell-segmenter\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 5 - Production/Stable\"],\"display_name\":\"napari-allencell-segmenter\",\"documentation\":\"https://github.com/AllenCell/napari-allencell-segmenter#README.md\",\"first_released\":\"2021-06-24T22:44:23.592978Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-allencell-segmenter\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/AllenCell/napari-allencell-segmenter\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2023-02-03T18:20:25.786947Z\",\"report_issues\":\"https://github.com/AllenCell/napari-allencell-segmenter/issues\",\"requirements\":[\"napari (>=0.4.9)\",\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"aicssegmentation (>=0.5.0)\",\"magicgui (>=0.2.9)\",\"aicsimageio (~=4.0.5)\",\"opencv-python-headless (>=4.5.1)\",\"importlib-metadata (==4.11.4)\",\"napari (>=0.4.9) ; extra == 'all'\",\"napari-plugin-engine (>=0.1.4) ; extra == 'all'\",\"numpy ; extra == 'all'\",\"aicssegmentation (>=0.5.0) ; extra == 'all'\",\"magicgui (>=0.2.9) ; extra == 'all'\",\"aicsimageio (~=4.0.5) ; extra == 'all'\",\"opencv-python-headless (>=4.5.1) ; extra == 'all'\",\"importlib-metadata (==4.11.4) ; extra == 'all'\",\"black (>=19.10b0) ; extra == 'all'\",\"codecov (>=2.0.22) ; extra == 'all'\",\"docutils (<0.16,>=0.10) ; extra == 'all'\",\"flake8 (>=3.7.7) ; extra == 'all'\",\"psutil (>=5.7.0) ; extra == 'all'\",\"pytest (>=4.3.0) ; extra == 'all'\",\"pytest-cov (==2.6.1) ; extra == 'all'\",\"pytest-raises (>=0.10) ; extra == 'all'\",\"pytest-qt (>=3.3.0) ; extra == 'all'\",\"quilt3 (>=3.1.12) ; extra == 'all'\",\"pytest-runner ; extra == 'all'\",\"bumpversion (>=0.5.3) ; extra == 'all'\",\"coverage (>=5.0a4) ; extra == 'all'\",\"gitchangelog (>=3.0.4) ; extra == 'all'\",\"ipython (>=7.5.0) ; extra == 'all'\",\"m2r (>=0.2.1) ; extra == 'all'\",\"pytest-runner (>=4.4) ; extra == 'all'\",\"Sphinx (<3,>=2.0.0b1) ; extra == 'all'\",\"sphinx-rtd-theme (>=0.1.2) ; extra == 'all'\",\"tox (==3.25.0) ; extra == 'all'\",\"twine (>=1.13.0) ; extra == 'all'\",\"wheel (>=0.33.1) ; extra == 'all'\",\"black (>=19.10b0) ; extra == 'dev'\",\"bumpversion (>=0.5.3) ; extra == 'dev'\",\"coverage (>=5.0a4) ; extra == 'dev'\",\"docutils (<0.16,>=0.10) ; extra == 'dev'\",\"flake8 (>=3.7.7) ; extra == 'dev'\",\"gitchangelog (>=3.0.4) ; extra == 'dev'\",\"ipython (>=7.5.0) ; extra == 'dev'\",\"m2r (>=0.2.1) ; extra == 'dev'\",\"pytest (>=4.3.0) ; extra == 'dev'\",\"pytest-cov (==2.6.1) ; extra == 'dev'\",\"pytest-raises (>=0.10) ; extra == 'dev'\",\"pytest-runner (>=4.4) ; extra == 'dev'\",\"pytest-qt (>=3.3.0) ; extra == 'dev'\",\"quilt3 (>=3.1.12) ; extra == 'dev'\",\"Sphinx (<3,>=2.0.0b1) ; extra == 'dev'\",\"sphinx-rtd-theme (>=0.1.2) ; extra == 'dev'\",\"tox (==3.25.0) ; extra == 'dev'\",\"twine (>=1.13.0) ; extra == 'dev'\",\"wheel (>=0.33.1) ; extra == 'dev'\",\"pytest-runner ; extra == 'setup'\",\"black (>=19.10b0) ; extra == 'test'\",\"codecov (>=2.0.22) ; extra == 'test'\",\"docutils (<0.16,>=0.10) ; extra == 'test'\",\"flake8 (>=3.7.7) ; extra == 'test'\",\"psutil (>=5.7.0) ; extra == 'test'\",\"pytest (>=4.3.0) ; extra == 'test'\",\"pytest-cov (==2.6.1) ; extra == 'test'\",\"pytest-raises (>=0.10) ; extra == 'test'\",\"pytest-qt (>=3.3.0) ; extra == 'test'\",\"quilt3 (>=3.1.12) ; extra == 'test'\"],\"summary\":\"A plugin that enables 3D image segmentation provided by Allen Institute for Cell Science\",\"support\":\"https://github.com/AllenCell/napari-allencell-segmenter/issues\",\"twitter\":\"\",\"version\":\"2.1.9\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Rocco D'Antuono\",\"orcid\":\"0000-0003-0180-6500\"},{\"name\":\"Giuseppina Pisignano\",\"orcid\":\"0000-0001-7476-3447\"}],\"category\":{},\"category_hierarchy\":{},\"code_repository\":\"https://github.com/RoccoDAnt/napari-zelda\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-zelda\"}],\"description\":\"# napari-zelda\\n\\n[![License](https://img.shields.io/pypi/l/napari-zelda.svg?color=green)](https://github.com/RoccoDAnt/napari-zelda/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-zelda.svg?color=green)](https://pypi.org/project/napari-zelda)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-zelda.svg?color=green)](https://python.org)\\n[![tests](https://github.com/RoccoDAnt/napari-zelda/workflows/tests/badge.svg)](https://github.com/RoccoDAnt/napari-zelda/actions)\\n[![codecov](https://codecov.io/gh/RoccoDAnt/napari-zelda/branch/master/graph/badge.svg)](https://codecov.io/gh/RoccoDAnt/napari-zelda)\\n\\n## ZELDA: a 3D Image Segmentation and Parent-Child relation plugin for microscopy image analysis in napari\\n##### Authors: Rocco D'Antuono, Giuseppina Pisignano\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## What you can do with ZELDA plugin for napari\\n1. Segment objects such as cells and organelles in 2D/3D.\\n\\n2. Segment two populations in 2D/3D (e.g. cells and organelles, nuclei and nuclear spots, tissue structures and cells) establishing the \\\"Parent-Child\\\" relation: count how many mitochondria are contained in each cell, how many spots localize in every nucleus, how many cells are within a tissue compartment.\\n\\n  Example: cell cytoplasms (parent objects) and mitochondria (child objects)\\n  ![](https://raw.githubusercontent.com/RoccoDAnt/napari-zelda/main/docs/2D-AF488.png) <br> **Actin** | ![](https://raw.githubusercontent.com/RoccoDAnt/napari-zelda/main/docs/2D-MT.png) <br> **Mitochondria**| ![](https://raw.githubusercontent.com/RoccoDAnt/napari-zelda/main/docs/2D-AF488_MT.png) <br> **Merge**\\n  ------ | ------| -----\\n  ![](https://raw.githubusercontent.com/RoccoDAnt/napari-zelda/main/docs/2D-AF488_parents.png) <br> **Parent cell cytoplasms** | ![](https://raw.githubusercontent.com/RoccoDAnt/napari-zelda/main/docs/2D-MT_children.png) <br> **Children mitochondria**| ![](https://raw.githubusercontent.com/RoccoDAnt/napari-zelda/main/docs/2D-MT_childrenbyParent.png) <br> **Children labelled by Parents**\\n\\n\\n3. Plot results within napari interface.\\n\\n    ![](https://raw.githubusercontent.com/RoccoDAnt/napari-zelda/main/docs/Plot_hist_Area.png) <br> **Histogram** | ![](https://raw.githubusercontent.com/RoccoDAnt/napari-zelda/main/docs/Plot_scatter_Area-EqDiam.png) <br> **Scatterplot**|\\n    ------ | ------|\\n\\n4. Customize an image analysis workflow in graphical mode (no scripting knowledge required).\\n\\n    | ![](https://raw.githubusercontent.com/RoccoDAnt/napari-zelda/main/docs/CustomProtocol.png) <br> **Custom image analysis workflow** |\\n    ------ |\\n\\n5. Import and Export Protocols (image analysis workflows) in graphical mode (share with the community!).\\n\\n    | ![](https://raw.githubusercontent.com/RoccoDAnt/napari-zelda/main/docs/Clipboard_Import_and_Export_Protocols.png) <br> **Import and Export of ZELDA Protocols** |\\n    ------ |\\n\\n## Installation\\n\\n**Option A.** You can install `napari-zelda` via [pip]. For the best experience, create a conda environment and use napari!=0.4.11, using the following instructions:\\n\\n    conda create -y -n napari-env python==3.8  \\n    conda activate napari-env  \\n    pip install \\\"napari[all]\\\"  \\n    pip install napari-zelda  \\n\\n\\n**Option B.** Alternatively, clone the repository and install locally via [pip]:\\n\\n    pip install -e .\\n\\n**Option C.** Another option is to use the napari interface to install it (make sure napari!=0.4.11):\\n1. Plugins / Install/Uninstall Package(s)\\n![](https://raw.githubusercontent.com/RoccoDAnt/napari-zelda/main/docs/Clipboard_ZELDA_Plugin_install_in_napari.png)\\n\\n2. Choose ZELDA\\n![](https://raw.githubusercontent.com/RoccoDAnt/napari-zelda/main/docs/Clipboard_ZELDA_Plugin_install_ZELDA_in_napari_Arrow.png)\\n\\n3. ZELDA is installed\\n![](https://raw.githubusercontent.com/RoccoDAnt/napari-zelda/main/docs/Clipboard_ZELDA_Plugin_installed_ZELDA_in_napari_Arrow.png)\\n\\n4. Launch ZELDA\\n![](https://raw.githubusercontent.com/RoccoDAnt/napari-zelda/main/docs/Clipboard_ZELDA_Launch_ZELDA.png)\\n\\n\\n## Contributing\\n\\nContributions are welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-zelda\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-zelda\\n\\n\\n\\n\\n\\nZELDA: a 3D Image Segmentation and Parent-Child relation plugin for microscopy image analysis in napari\\nAuthors: Rocco D'Antuono, Giuseppina Pisignano\\n\\nThis napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.\\n\\nWhat you can do with ZELDA plugin for napari\\n\\n\\nSegment objects such as cells and organelles in 2D/3D.\\n\\n\\nSegment two populations in 2D/3D (e.g. cells and organelles, nuclei and nuclear spots, tissue structures and cells) establishing the \\\"Parent-Child\\\" relation: count how many mitochondria are contained in each cell, how many spots localize in every nucleus, how many cells are within a tissue compartment.\\n\\n\\nExample: cell cytoplasms (parent objects) and mitochondria (child objects)\\n    Actin |   Mitochondria|   Merge\\n  ------ | ------| -----\\n    Parent cell cytoplasms |   Children mitochondria|   Children labelled by Parents\\n\\n\\nPlot results within napari interface.\\n  Histogram |   Scatterplot|\\n------ | ------|\\n\\n\\nCustomize an image analysis workflow in graphical mode (no scripting knowledge required).\\n|   Custom image analysis workflow |\\n------ |\\n\\n\\nImport and Export Protocols (image analysis workflows) in graphical mode (share with the community!).\\n|   Import and Export of ZELDA Protocols |\\n------ |\\n\\n\\nInstallation\\nOption A. You can install napari-zelda via pip. For the best experience, create a conda environment and use napari!=0.4.11, using the following instructions:\\nconda create -y -n napari-env python==3.8  \\nconda activate napari-env  \\npip install \\\"napari[all]\\\"  \\npip install napari-zelda\\n\\nOption B. Alternatively, clone the repository and install locally via pip:\\npip install -e .\\n\\nOption C. Another option is to use the napari interface to install it (make sure napari!=0.4.11):\\n1. Plugins / Install/Uninstall Package(s)\\n\\n\\n\\nChoose ZELDA\\n\\n\\n\\nZELDA is installed\\n\\n\\n\\nLaunch ZELDA\\n\\n\\n\\nContributing\\nContributions are welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-zelda\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please [file an issue] along with a detailed description.\",\"development_status\":[\"Development Status :: 3 - Alpha\"],\"display_name\":\"napari-zelda\",\"documentation\":\"\",\"first_released\":\"2021-10-17T18:55:43.014476Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-zelda\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/RoccoDAnt/napari-zelda\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-01-26T15:44:51.608705Z\",\"report_issues\":\"\",\"requirements\":null,\"summary\":\"ZELDA: a 3D Image Segmentation and Parent-Child relation plugin for microscopy image analysis in napari\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.1.10\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"William Patton\"}],\"code_repository\":\"https://github.com/pattonw/napari-pssr\",\"description\":\"# napari-pssr\\n\\n[![License](https://img.shields.io/pypi/l/napari-pssr.svg?color=green)](https://github.com/pattonw/napari-pssr/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-pssr.svg?color=green)](https://pypi.org/project/napari-pssr)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-pssr.svg?color=green)](https://python.org)\\n[![tests](https://github.com/pattonw/napari-pssr/workflows/tests/badge.svg)](https://github.com/pattonw/napari-pssr/actions)\\n[![codecov](https://codecov.io/gh/pattonw/napari-pssr/branch/main/graph/badge.svg)](https://codecov.io/gh/pattonw/napari-pssr)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-pssr)](https://napari-hub.org/plugins/napari-pssr)\\n\\nA plugin for training and applying pssr\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/plugins/stable/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-pssr` via [pip]:\\n\\n    pip install napari-pssr\\n\\nSome libraries need to be updated to the most recent version to get all features.\\nThese will be updated once they are released on pypi\\n    \\n    pip install git+https://github.com/bioimage-io/core-bioimage-io-python\\\",\\n    pip install git+https://github.com/funkey/gunpowder.git@patch-1.2.3\\\",\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/pattonw/napari-pssr.git\\n\\n## Model download\\n\\nA sample model can be downloaded from `https://github.com/pattonw/model-specs/tree/main/pssr`. This model comes with some restrictive dependencies. To use follow these steps.\\n1) install this plugin following the directions provided above\\n2) install bioimageio.core via `pip install bioimageio.core` or `conda install -c conda-forge bioimageio.core`\\n3) `pip install fastai==1.0.55 tifffile libtiff czifile scikit-image`\\n4) `pip uninstall torch torchvision` (may need multiple runs)\\n5) `conda install pytorch==1.1.0 torchvision==0.3.0 cudatoolkit=10.0 -c pytorch`\\n6) `pip install pillow==6.1.0`\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [MIT] license,\\n\\\"napari-pssr\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/pattonw/napari-pssr/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-pssr\\n\\n\\n\\n\\n\\n\\nA plugin for training and applying pssr\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-pssr via pip:\\npip install napari-pssr\\n\\nSome libraries need to be updated to the most recent version to get all features.\\nThese will be updated once they are released on pypi\\npip install git+https://github.com/bioimage-io/core-bioimage-io-python\\\",\\npip install git+https://github.com/funkey/gunpowder.git@patch-1.2.3\\\",\\n\\nTo install latest development version :\\npip install git+https://github.com/pattonw/napari-pssr.git\\n\\nModel download\\nA sample model can be downloaded from https://github.com/pattonw/model-specs/tree/main/pssr. This model comes with some restrictive dependencies. To use follow these steps.\\n1) install this plugin following the directions provided above\\n2) install bioimageio.core via pip install bioimageio.core or conda install -c conda-forge bioimageio.core\\n3) pip install fastai==1.0.55 tifffile libtiff czifile scikit-image\\n4) pip uninstall torch torchvision (may need multiple runs)\\n5) conda install pytorch==1.1.0 torchvision==0.3.0 cudatoolkit=10.0 -c pytorch\\n6) pip install pillow==6.1.0\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the MIT license,\\n\\\"napari-pssr\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari PSSR\",\"documentation\":\"https://github.com/pattonw/napari-pssr#README.md\",\"first_released\":\"2022-11-18T01:03:14.478148Z\",\"license\":\"MIT\",\"name\":\"napari-pssr\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\",\"sample_data\"],\"project_site\":\"https://github.com/pattonw/napari-pssr\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-11-18T01:03:14.478148Z\",\"report_issues\":\"https://github.com/pattonw/napari-pssr/issues\",\"requirements\":[\"numpy\",\"zarr\",\"magicgui\",\"bioimageio.core\",\"gunpowder\",\"matplotlib\",\"torch\",\"napari\"],\"summary\":\"A plugin for training and applying pssr\",\"support\":\"https://github.com/pattonw/napari-pssr/issues\",\"twitter\":\"\",\"version\":\"0.1\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"hthieu@illinois.edu\",\"name\":\"Hieu Hoang\"}],\"code_repository\":null,\"conda\":[],\"description\":\"# napari-pram\\n\\n[![License](https://img.shields.io/pypi/l/napari-pram.svg?color=green)](https://github.com/hthieu166/napari-pram/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-pram.svg?color=green)](https://pypi.org/project/napari-pram)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-pram.svg?color=green)](https://python.org)\\n[![tests](https://github.com/hthieu166/napari-pram/workflows/tests/badge.svg)](https://github.com/hthieu166/napari-pram/actions)\\n[![codecov](https://codecov.io/gh/hthieu166/napari-pram/branch/main/graph/badge.svg)](https://codecov.io/gh/hthieu166/napari-pram)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-pram)](https://napari-hub.org/plugins/napari-pram)\\n\\nPlugin for PRAM data annotation and processing.\\n\\n![PRAM Demo](https://raw.githubusercontent.com/hthieu166/napari-pram/main/docs/figs/demo.jpg)\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/plugins/stable/index.html\\n-->\\n\\n## Usage\\n\\n### Open `napari-pram` toolbox:\\n\\nOn the toolbar, select ``[Plugins] > napari-pram: Open PRAM's toolbox``\\n\\n### Load PRAM image and annotations:\\n\\nPress <kbd>Command/Control</kbd> + <kbd>O</kbd>: \\n- Select `*.json` files for annotations (from either [VGG Annotator](https://www.robots.ox.ac.uk/~vgg/software/via/) or `napari-pram`)\\n- Select `*.png` files for PRAM image\\n\\n### Annotate\\n- Press <kbd>Annotate</kbd>\\n- Click the plus-in-circle icon on the top-left panel and start editing\\n\\n### Run PRAM particles detector\\n- Select a proper threshold between 1 (ultra sensitive) - 10 (less sensitive)\\n- Press <kbd>Run Detector</kbd>\\n\\n### Evaluate\\n- Press <kbd>Evaluate</kbd>\\n- Hide/Unhide true positive/ false postive/false negative layers\\n\\n### Load new image\\n- Press <kbd>Clear All</kbd> to remove all layers\\n\\n### Export to JSON\\n- Press <kbd>Save to File</kbd> to export all annotations, predictions from the algorithm to a JSON file\\n## Installation\\nFollowing this [tutorial](https://napari.org/tutorials/fundamentals/quick_start.html) to install `napari`. \\n\\nAlternatively, you can follow my instructions as follows:\\n\\nYou will need a python environment. I recommend [Conda](https://docs.conda.io/en/latest/miniconda.html). Create a new environment, for example:\\n    \\n    conda create --name napari-env python=3.7 pip \\n\\nActivate the new environment:\\n\\n    conda activate napari-env \\n\\nInstall [napari](https://napari.org/tutorials/fundamentals/installation) via [pip]:\\n\\n    pip install napari[all]\\n\\nThen you can finally install our plugin `napari-pram` via [pip]:\\n\\n    pip install napari-pram\\n\\nAlternatively, the plugin can be installed using napari-GUI\\n\\n``[Plugins] > Install/Uninstall Plugins`` and search for `napari-pram`\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [MIT] license,\\n\\\"napari-pram\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-pram\\n\\n\\n\\n\\n\\n\\nPlugin for PRAM data annotation and processing.\\n\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nUsage\\nOpen napari-pram toolbox:\\nOn the toolbar, select [Plugins] > napari-pram: Open PRAM's toolbox\\nLoad PRAM image and annotations:\\nPress Command/Control + O: \\n- Select *.json files for annotations (from either VGG Annotator or napari-pram)\\n- Select *.png files for PRAM image\\nAnnotate\\n\\nPress Annotate\\nClick the plus-in-circle icon on the top-left panel and start editing\\n\\nRun PRAM particles detector\\n\\nSelect a proper threshold between 1 (ultra sensitive) - 10 (less sensitive)\\nPress Run Detector\\n\\nEvaluate\\n\\nPress Evaluate\\nHide/Unhide true positive/ false postive/false negative layers\\n\\nLoad new image\\n\\nPress Clear All to remove all layers\\n\\nExport to JSON\\n\\nPress Save to File to export all annotations, predictions from the algorithm to a JSON file\\n\\nInstallation\\nFollowing this tutorial to install napari. \\nAlternatively, you can follow my instructions as follows:\\nYou will need a python environment. I recommend Conda. Create a new environment, for example:\\nconda create --name napari-env python=3.7 pip\\n\\nActivate the new environment:\\nconda activate napari-env\\n\\nInstall napari via pip:\\npip install napari[all]\\n\\nThen you can finally install our plugin napari-pram via pip:\\npip install napari-pram\\n\\nAlternatively, the plugin can be installed using napari-GUI\\n[Plugins] > Install/Uninstall Plugins and search for napari-pram\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the MIT license,\\n\\\"napari-pram\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please [file an issue] along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari PRAM\",\"documentation\":\"\",\"first_released\":\"2022-04-29T22:32:47.607040Z\",\"license\":\"MIT\",\"name\":\"napari-pram\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\",\"writer\",\"widget\",\"sample_data\"],\"project_site\":\"\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[\"*.json\",\"*.png\"],\"release_date\":\"2022-05-03T04:27:54.896702Z\",\"report_issues\":\"\",\"requirements\":[\"numpy\",\"magicgui\",\"qtpy\",\"opencv-python\",\"scipy\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\"],\"summary\":\"plugin for PRAM data annotation and processing\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.1.3\",\"writer_file_extensions\":[\".npy\"],\"writer_save_layers\":[\"image\",\"labels\"]}",
  "{\"authors\":[{\"name\":\"Seongbin Lim\"}],\"code_repository\":null,\"conda\":[],\"description\":\"# napari-proofread-brainbow\\n\\n[![License](https://img.shields.io/pypi/l/napari-proofread-brainbow.svg?color=green)](https://github.com/sbinnee/napari-proofread-brainbow/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-proofread-brainbow.svg?color=green)](https://pypi.org/project/napari-proofread-brainbow)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-proofread-brainbow.svg?color=green)](https://python.org)\\n[![tests](https://github.com/sbinnee/napari-proofread-brainbow/workflows/tests/badge.svg)](https://github.com/sbinnee/napari-proofread-brainbow/actions)\\n[![codecov](https://codecov.io/gh/sbinnee/napari-proofread-brainbow/branch/main/graph/badge.svg)](https://codecov.io/gh/sbinnee/napari-proofread-brainbow)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-proofread-brainbow)](https://napari-hub.org/plugins/napari-proofread-brainbow)\\n\\nproofreading with napari\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/plugins/stable/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-proofread-brainbow` via [pip]:\\n\\n    pip install napari-proofread-brainbow\\n\\n\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [MIT] license,\\n\\\"napari-proofread-brainbow\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-proofread-brainbow\\n\\n\\n\\n\\n\\n\\nproofreading with napari\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-proofread-brainbow via pip:\\npip install napari-proofread-brainbow\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the MIT license,\\n\\\"napari-proofread-brainbow\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please [file an issue] along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari proofread brainbow\",\"documentation\":\"\",\"first_released\":\"2022-08-30T09:45:51.714619Z\",\"license\":\"MIT\",\"name\":\"napari-proofread-brainbow\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-09-21T08:33:03.270553Z\",\"report_issues\":\"\",\"requirements\":null,\"summary\":\"proofreading Brainbow images with napari\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.3.0\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Hiroki Kawai\",\"orcid\":\"0000-0002-7129-2384\"}],\"code_repository\":\"https://github.com/hiroalchem/napari-labelimg4classification\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-labelimg4classification\"}],\"description\":\"# napari-labelimg4classification\\n\\n[![License](https://img.shields.io/pypi/l/napari-labelimg4classification.svg?color=green)](https://github.com/hiroalchem/napari-labelimg4classification/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-labelimg4classification.svg?color=green)](https://pypi.org/project/napari-labelimg4classification)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-labelimg4classification.svg?color=green)](https://python.org)\\n[![tests](https://github.com/hiroalchem/napari-labelimg4classification/workflows/tests/badge.svg)](https://github.com/hiroalchem/napari-labelimg4classification/actions)\\n[![codecov](https://codecov.io/gh/hiroalchem/napari-labelimg4classification/branch/main/graph/badge.svg)](https://codecov.io/gh/hiroalchem/napari-labelimg4classification)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-labelimg4classification)](https://napari-hub.org/plugins/napari-labelimg4classification)\\n\\nA simple image-level annotation tool supporting multi-channel images for napari.\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n## Usage\\nStart the labeling tool from the menu `Utilities > label tool for classification`.   \\nFirst, click on the Choose directory button to open the folder selection window, and select the folder that contains the\\n images you want to label and annotate.   \\nIt will automatically list and display the images of tif, png, jpg, and bmp formats.\\nIf you want to view the channels of a multi-channel image separately, check the split channels checkbox.\\n![](https://github.com/hiroalchem/napari-labelimg4classification/raw/main/docs/open.gif)\\n\\nInitially, all channels will be opened in grayscale, but the pseudo-color and contrast adjustments you specified will be\\n carried over when you open the next image.   \\nThanks to napari, you can freely merge channels and turn them on and off.   \\nLabel classes can be added, and can be removed by typing the same name as an already added class.\\n![](https://github.com/hiroalchem/napari-labelimg4classification/raw/main/docs/color_and_label.gif)\\n\\n\\nIt will automatically save the labels.csv file with the image path and label, and the class.txt file with the class of the label.\\n![](https://github.com/hiroalchem/napari-labelimg4classification/raw/main/docs/class_and_labels.png)\\n\\nIf labels.csv and class.txt are already in the folder, they will be loaded and reflected automatically.\\n![](https://github.com/hiroalchem/napari-labelimg4classification/raw/main/docs/reopen.gif)\\n\\n## Installation\\n\\nYou can install `napari-labelimg4classification` via [pip]:\\n\\n    pip install napari-labelimg4classification\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/hiroalchem/napari-labelimg4classification.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [MIT] license,\\n\\\"napari-labelimg4classification\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/hiroalchem/napari-labelimg4classification/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-labelimg4classification\\n\\n\\n\\n\\n\\n\\nA simple image-level annotation tool supporting multi-channel images for napari.\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\nUsage\\nStart the labeling tool from the menu Utilities > label tool for classification. \\nFirst, click on the Choose directory button to open the folder selection window, and select the folder that contains the\\n images you want to label and annotate. \\nIt will automatically list and display the images of tif, png, jpg, and bmp formats.\\nIf you want to view the channels of a multi-channel image separately, check the split channels checkbox.\\n\\nInitially, all channels will be opened in grayscale, but the pseudo-color and contrast adjustments you specified will be\\n carried over when you open the next image. \\nThanks to napari, you can freely merge channels and turn them on and off. \\nLabel classes can be added, and can be removed by typing the same name as an already added class.\\n\\nIt will automatically save the labels.csv file with the image path and label, and the class.txt file with the class of the label.\\n\\nIf labels.csv and class.txt are already in the folder, they will be loaded and reflected automatically.\\n\\nInstallation\\nYou can install napari-labelimg4classification via pip:\\npip install napari-labelimg4classification\\n\\nTo install latest development version :\\npip install git+https://github.com/hiroalchem/napari-labelimg4classification.git\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the MIT license,\\n\\\"napari-labelimg4classification\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari-labelimg4classification\",\"documentation\":\"https://github.com/hiroalchem/napari-labelimg4classification#README.md\",\"first_released\":\"2021-12-02T02:23:27.798505Z\",\"license\":\"MIT\",\"name\":\"napari-labelimg4classification\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/hiroalchem/napari-labelimg4classification\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2021-12-03T01:44:21.284216Z\",\"report_issues\":\"https://github.com/hiroalchem/napari-labelimg4classification/issues\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"napari\",\"numpy\",\"napari-tools-menu\",\"pandas\"],\"summary\":\"A simple image-level annotation tool supporting multi-channel images.\",\"support\":\"https://github.com/hiroalchem/napari-labelimg4classification/issues\",\"twitter\":\"\",\"version\":\"0.1.1\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Talley Lambert\"}],\"code_repository\":\"https://github.com/tlambert03/pycudadecon\",\"description\":\"# pyCUDAdecon\\n\\nThis package provides a python wrapper and convenience functions for\\n[cudaDecon](https://github.com/scopetools/cudaDecon), which is a CUDA/C++\\nimplementation of an accelerated Richardson Lucy Deconvolution\\nalgorithm<sup>1</sup>.\\n\\n* CUDA accelerated deconvolution with a handful of artifact-reducing features.\\n* radially averaged OTF generation with interpolation for voxel size\\n  independence between PSF and data volumes\\n* 3D deskew, rotation, general affine transformations\\n* CUDA-based camera-correction for [sCMOS artifact correction](https://llspy.readthedocs.io/en/latest/camera.html)\\n\\n\\n### Install\\n\\nThe conda package includes the required pre-compiled libraries for Windows and Linux. See GPU driver requirements [below](#gpu-requirements)\\n\\n```sh\\nconda install -c conda-forge pycudadecon\\n```\\n\\n*macOS is not supported*\\n\\n### 📖   &nbsp; [Documentation](http://www.talleylambert.com/pycudadecon)\\n\\n\\n### GPU requirements\\n\\nThis software requires a CUDA-compatible NVIDIA GPU. The underlying cudadecon\\nlibraries have been compiled against different versions of the CUDA toolkit.\\nThe required CUDA libraries are bundled in the conda distributions so you don't\\nneed to install the CUDA toolkit separately.  If desired, you can pick which\\nversion of CUDA you'd like based on your needs, but please note that different\\nversions of the CUDA toolkit have different GPU driver requirements:\\n\\nTo specify a specific cudatoolkit version, install as follows (for instance, to\\nuse `cudatoolkit=10.2`)\\n\\n```sh\\nconda install -c conda-forge pycudadecon cudatoolkit=10.2\\n```\\n\\n| CUDA | Linux driver | Win driver |\\n| ---- | ------------ | ---------- |\\n| 10.2 | ≥ 440.33     | ≥ 441.22   |\\n| 11.0 | ≥ 450.36.06  | ≥ 451.22   |\\n| 11.1 | ≥ 455.23     | ≥ 456.38   |\\n| 11.2 | ≥ 460.27.03  | ≥ 460.82   |\\n\\n\\nIf you run into trouble, feel free to [open an\\nissue](https://github.com/tlambert03/pycudadecon/issues) and describe your\\nsetup.\\n\\n\\n## Usage\\n\\n\\nThe [`pycudadecon.decon()`](https://pycudadecon.readthedocs.io/en/latest/deconvolution.html#pycudadecon.decon) function is designed be able to handle most basic applications:\\n\\n```python\\nfrom pycudadecon import decon\\n\\n# pass filenames of an image and a PSF\\nresult = decon('/path/to/3D_image.tif', '/path/to/3D_psf.tif')\\n\\n# decon also accepts numpy arrays\\nresult = decon(img_array, psf_array)\\n\\n# the image source can also be a sequence of arrays or paths\\nresult = decon([img_array, '/path/to/3D_image.tif'], psf_array)\\n\\n# see docstrings for additional parameter options\\n```\\n\\nFor finer-tuned control, you may wish to make an OTF file from your PSF using [`pycudadecon.make_otf()`](https://pycudadecon.readthedocs.io/en/latest/otf.html?highlight=make_otf#pycudadecon.make_otf), and then use the [`pycudadecon.RLContext`](https://pycudadecon.readthedocs.io/en/latest/deconvolution.html?highlight=RLContext#pycudadecon.RLContext) context manager to setup the GPU for use with the [`pycudadecon.rl_decon()`](https://pycudadecon.readthedocs.io/en/latest/deconvolution.html?highlight=RLContext#pycudadecon.rl_decon) function.  (Note all images processed in the same context must have the same input shape).\\n\\n```python\\nfrom pycudadecon import RLContext, rl_decon\\nfrom glob import glob\\nimport tifffile\\n\\nimage_folder = '/path/to/some_images/'\\nimlist = glob(image_folder + '*488*.tif')\\notf_path = '/path/to/pregenerated_otf.tif'\\n\\nwith tifffile.TiffFile(imlist[0]) as tf:\\n    imshape = tf.series[0].shape\\n\\nwith RLContext(imshape, otf_path, dz) as ctx:\\n    for impath in imlist:\\n        image = tifffile.imread(impath)\\n        result = rl_decon(image, ctx.out_shape)\\n        # do something with result...\\n```\\n\\nIf you have a 3D PSF volume, the [`pycudadecon.TemporaryOTF`](https://pycudadecon.readthedocs.io/en/latest/otf.html?highlight=temporaryotf#pycudadecon.TemporaryOTF) context manager facilitates temporary OTF generation...\\n\\n```python\\n # continuing with the variables from the previous example...\\n psf_path = \\\"/path/to/psf_3D.tif\\\"\\n with TemporaryOTF(psf) as otf:\\n     with RLContext(imshape, otf.path, dz) as ctx:\\n         for impath in imlist:\\n             image = tifffile.imread(impath)\\n             result = rl_decon(image, ctx.out_shape)\\n             # do something with result...\\n```\\n\\n... and that bit of code is essentially what the [`pycudadecon.decon()`](https://pycudadecon.readthedocs.io/en/latest/deconvolution.html#pycudadecon.decon) function is doing, with a little bit of additional conveniences added in.\\n\\n*Each of these functions has many options and accepts multiple keyword arguments. See the [documentation](https://pycudadecon.readthedocs.io/en/latest/index.html) for further information on the respective functions.*\\n\\nFor examples and information on affine transforms, volume rotations, and deskewing (typical of light sheet volumes acquired with stage-scanning), see the [documentation on Affine Transformations](https://pycudadecon.readthedocs.io/en/latest/affine.html)\\n___\\n\\n<sup>1</sup> D.S.C. Biggs and M. Andrews, Acceleration of iterative image restoration algorithms, Applied Optics, Vol. 36, No. 8, 1997. https://doi.org/10.1364/AO.36.001766\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"pyCUDAdecon\\nThis package provides a python wrapper and convenience functions for\\ncudaDecon, which is a CUDA/C++\\nimplementation of an accelerated Richardson Lucy Deconvolution\\nalgorithm1.\\n\\nCUDA accelerated deconvolution with a handful of artifact-reducing features.\\nradially averaged OTF generation with interpolation for voxel size\\n  independence between PSF and data volumes\\n3D deskew, rotation, general affine transformations\\nCUDA-based camera-correction for sCMOS artifact correction\\n\\nInstall\\nThe conda package includes the required pre-compiled libraries for Windows and Linux. See GPU driver requirements below\\nsh\\nconda install -c conda-forge pycudadecon\\nmacOS is not supported\\n📖     Documentation\\nGPU requirements\\nThis software requires a CUDA-compatible NVIDIA GPU. The underlying cudadecon\\nlibraries have been compiled against different versions of the CUDA toolkit.\\nThe required CUDA libraries are bundled in the conda distributions so you don't\\nneed to install the CUDA toolkit separately.  If desired, you can pick which\\nversion of CUDA you'd like based on your needs, but please note that different\\nversions of the CUDA toolkit have different GPU driver requirements:\\nTo specify a specific cudatoolkit version, install as follows (for instance, to\\nuse cudatoolkit=10.2)\\nsh\\nconda install -c conda-forge pycudadecon cudatoolkit=10.2\\n| CUDA | Linux driver | Win driver |\\n| ---- | ------------ | ---------- |\\n| 10.2 | ≥ 440.33     | ≥ 441.22   |\\n| 11.0 | ≥ 450.36.06  | ≥ 451.22   |\\n| 11.1 | ≥ 455.23     | ≥ 456.38   |\\n| 11.2 | ≥ 460.27.03  | ≥ 460.82   |\\nIf you run into trouble, feel free to open an\\nissue and describe your\\nsetup.\\nUsage\\nThe pycudadecon.decon() function is designed be able to handle most basic applications:\\n```python\\nfrom pycudadecon import decon\\npass filenames of an image and a PSF\\nresult = decon('/path/to/3D_image.tif', '/path/to/3D_psf.tif')\\ndecon also accepts numpy arrays\\nresult = decon(img_array, psf_array)\\nthe image source can also be a sequence of arrays or paths\\nresult = decon([img_array, '/path/to/3D_image.tif'], psf_array)\\nsee docstrings for additional parameter options\\n```\\nFor finer-tuned control, you may wish to make an OTF file from your PSF using pycudadecon.make_otf(), and then use the pycudadecon.RLContext context manager to setup the GPU for use with the pycudadecon.rl_decon() function.  (Note all images processed in the same context must have the same input shape).\\n```python\\nfrom pycudadecon import RLContext, rl_decon\\nfrom glob import glob\\nimport tifffile\\nimage_folder = '/path/to/some_images/'\\nimlist = glob(image_folder + '488.tif')\\notf_path = '/path/to/pregenerated_otf.tif'\\nwith tifffile.TiffFile(imlist[0]) as tf:\\n    imshape = tf.series[0].shape\\nwith RLContext(imshape, otf_path, dz) as ctx:\\n    for impath in imlist:\\n        image = tifffile.imread(impath)\\n        result = rl_decon(image, ctx.out_shape)\\n        # do something with result...\\n```\\nIf you have a 3D PSF volume, the pycudadecon.TemporaryOTF context manager facilitates temporary OTF generation...\\npython\\n # continuing with the variables from the previous example...\\n psf_path = \\\"/path/to/psf_3D.tif\\\"\\n with TemporaryOTF(psf) as otf:\\n     with RLContext(imshape, otf.path, dz) as ctx:\\n         for impath in imlist:\\n             image = tifffile.imread(impath)\\n             result = rl_decon(image, ctx.out_shape)\\n             # do something with result...\\n... and that bit of code is essentially what the pycudadecon.decon() function is doing, with a little bit of additional conveniences added in.\\nEach of these functions has many options and accepts multiple keyword arguments. See the documentation for further information on the respective functions.\\nFor examples and information on affine transforms, volume rotations, and deskewing (typical of light sheet volumes acquired with stage-scanning), see the documentation on Affine Transformations\\n\\n1 D.S.C. Biggs and M. Andrews, Acceleration of iterative image restoration algorithms, Applied Optics, Vol. 36, No. 8, 1997. https://doi.org/10.1364/AO.36.001766\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"pyCUDAdecon\",\"documentation\":\"https://pycudadecon.readthedocs.io/en/latest/\",\"first_released\":\"2022-08-10T22:10:53.720438Z\",\"license\":\"MIT\",\"name\":\"pycudadecon\",\"npe2\":true,\"operating_system\":[],\"plugin_types\":[\"widget\"],\"project_site\":\"\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-11-07T14:48:45.317086Z\",\"report_issues\":\"\",\"requirements\":[\"numpy\",\"tifffile\",\"typing-extensions\",\"importlib-metadata ; python_version < \\\"3.8\\\"\",\"black ; extra == 'dev'\",\"cruft ; extra == 'dev'\",\"flake8-bugbear ; extra == 'dev'\",\"flake8-docstrings ; extra == 'dev'\",\"flake8-pyprojecttoml ; extra == 'dev'\",\"flake8-typing-imports ; extra == 'dev'\",\"flake8 ; extra == 'dev'\",\"ipython ; extra == 'dev'\",\"isort ; extra == 'dev'\",\"mypy ; extra == 'dev'\",\"pdbpp ; extra == 'dev'\",\"pre-commit ; extra == 'dev'\",\"pydocstyle ; extra == 'dev'\",\"pytest-cov ; extra == 'dev'\",\"pytest ; extra == 'dev'\",\"rich ; extra == 'dev'\",\"pytest (>=6.0) ; extra == 'test'\",\"pytest-cov ; extra == 'test'\"],\"summary\":\"Python wrapper for CUDA-accelerated 3D deconvolution\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.4.0\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Richard De Mets\"}],\"code_repository\":\"https://github.com/rdemets/napari-yolov5\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-yolov5\"}],\"description\":\"# napari-yolov5\\r\\n\\r\\n[![License](https://img.shields.io/pypi/l/napari-yolov5.svg?color=green)](https://github.com/rdemets/napari-yolov5/raw/main/LICENSE)\\r\\n[![PyPI](https://img.shields.io/pypi/v/napari-yolov5.svg?color=green)](https://pypi.org/project/napari-yolov5)\\r\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-yolov5.svg?color=green)](https://python.org)\\r\\n[![tests](https://github.com/rdemets/napari-yolov5/workflows/tests/badge.svg)](https://github.com/rdemets/napari-yolov5/actions)\\r\\n[![codecov](https://codecov.io/gh/rdemets/napari-yolov5/branch/main/graph/badge.svg)](https://codecov.io/gh/rdemets/napari-yolov5)\\r\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-yolov5)](https://napari-hub.org/plugins/napari-yolov5)\\r\\n\\r\\nPlugin adapted from Ultralytics to bring YOLOv5 into Napari. \\r\\n\\r\\nTraining and detection can be done using the GUI. Training dataset must be prepared prior to using this plugin. Further development will allow users to use Napari to prepare the dataset. Follow instructions stated on [Ultralytics Github](https://github.com/ultralytics/yolov5) to prepare the dataset.\\r\\n\\r\\nThe plugin includes 3 pre-trained networks that are able to identify mitosis stages or apoptosis on soSPIM images. More details can be found on the [pre-print](https://www.biorxiv.org/content/10.1101/2021.03.26.437121v1.full).\\r\\n\\r\\n----------------------------------\\r\\n\\r\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\r\\n\\r\\n<!--\\r\\nDon't miss the full getting started guide to set up your new package:\\r\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\r\\n\\r\\nand review the napari docs for plugin developers:\\r\\nhttps://napari.org/plugins/stable/index.html\\r\\n-->\\r\\n\\r\\n## Installation\\r\\n\\r\\nFirst install conda and create an environment for the plugin\\r\\n```\\r\\nconda create --prefix env-napari-yolov5 python=3.9\\r\\nconda activate env-napari-yolov5\\r\\n```\\r\\nYou can install `napari-yolov5` and `napari` via [pip]:\\r\\n\\r\\n    pip install napari-yolov5\\r\\n    pip install napari[all]\\r\\n\\r\\nFor GPU support :\\r\\n```\\r\\npip uninstall torch\\r\\npip install torchvision==0.10.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html\\r\\n```\\r\\n\\r\\n## Usage\\r\\n\\r\\nFirst select if you would like to train a new network or detect objects.\\r\\n\\r\\n![alt text](https://github.com/rdemets/napari-yolov5/blob/main/src/napari_yolov5/resources/Readme/1.jpg?raw=true)\\r\\n\\r\\n\\r\\n***For `Training` :***\\r\\n\\r\\nData preparation should be done following [Ultralytics'](https://github.com/ultralytics/yolov5) instructions.\\r\\n\\r\\nSelect the size of the network, the number of epochs, the number of images per batch to load on the GPU, the size of the images (must be a stride of 32), and the name of the network.\\r\\n\\r\\n![alt text](https://github.com/rdemets/napari-yolov5/blob/main/src/napari_yolov5/resources/Readme/2.jpg?raw=true)\\r\\n\\r\\nAn example of the YAML config file is provided in `src/napari_yolov5/resources` folder.\\r\\n\\r\\n![alt text](https://github.com/rdemets/napari-yolov5/blob/main/src/napari_yolov5/resources/Readme/3.jpg?raw=true)\\r\\n\\r\\n\\r\\nProgress can be seen on the Terminal or on the right-hand side of the viewer.\\r\\n\\r\\n![alt text](https://github.com/rdemets/napari-yolov5/blob/main/src/napari_yolov5/resources/Readme/4.jpg?raw=true)\\r\\n\\r\\n\\r\\n***For `Detection` :***\\r\\n\\r\\nIt is possible to perform the detection on a single layer chosen in the list, all the layers opened, or by giving a folder path. For folder detection, all the images will be loaded as a single stack.\\r\\n\\r\\n![alt text](https://github.com/rdemets/napari-yolov5/blob/main/src/napari_yolov5/resources/Readme/5.jpg?raw=true)\\r\\n\\r\\nNucleus size of the prediction layer has te be filled to resize the image to the training dataset. Nucleus size of the training dataset will be asked in case of a custom network.\\r\\n\\r\\nConfidence threshold defines the minimum value for a detected object to be considered positive. \\r\\niou nms threshold (intersection-over-union non-max-suppression) defines the overlapping area of two boxes as a single object. Only the box with the maximum confidence is kept.\\r\\nProgress can be seen on the Terminal.\\r\\n\\r\\n![alt text](https://github.com/rdemets/napari-yolov5/blob/main/src/napari_yolov5/resources/Readme/6.jpg?raw=true)\\r\\n\\r\\nFew options allow for modification on how the boxes are being displayed (default : box + class + confidence score ; box + class ; box only) and if the box coordinates and the image overlay will be exported.\\r\\nPost-processing option will perform a simple 3D assignment based on 3D connected component analysis. A median filter (1x1x3 XYZ) is applied prior to the assignment. \\r\\nThe centroid of each object is then saved into a new point layer as a 3D point with a random color for each class. \\r\\n\\r\\n![alt text](https://github.com/rdemets/napari-yolov5/blob/main/src/napari_yolov5/resources/Readme/7.jpg?raw=true)\\r\\n\\r\\nThe localisation of each centroid is saved and the path is shown in the Terminal at the end of the detection. It is also possible now to define the export folder.\\r\\n\\r\\n![alt text](https://github.com/rdemets/napari-yolov5/blob/main/src/napari_yolov5/resources/Readme/8.jpg?raw=true)\\r\\n\\r\\n\\r\\n## Contributing\\r\\n\\r\\nContributions are very welcome. Tests can be run with [tox], please ensure\\r\\nthe coverage at least stays the same before you submit a pull request.\\r\\n\\r\\n## License\\r\\n\\r\\nDistributed under the terms of the [GNU GPL v3.0] license,\\r\\n\\\"napari-yolov5\\\" is free and open source software\\r\\n\\r\\n## Issues\\r\\n\\r\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\r\\n\\r\\n[napari]: https://github.com/napari/napari\\r\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\r\\n[@napari]: https://github.com/napari\\r\\n[MIT]: http://opensource.org/licenses/MIT\\r\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\r\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\r\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\r\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\r\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\r\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\r\\n\\r\\n[napari]: https://github.com/napari/napari\\r\\n[tox]: https://tox.readthedocs.io/en/latest/\\r\\n[pip]: https://pypi.org/project/pip/\\r\\n[PyPI]: https://pypi.org/\\r\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-yolov5\\n\\n\\n\\n\\n\\n\\nPlugin adapted from Ultralytics to bring YOLOv5 into Napari. \\nTraining and detection can be done using the GUI. Training dataset must be prepared prior to using this plugin. Further development will allow users to use Napari to prepare the dataset. Follow instructions stated on Ultralytics Github to prepare the dataset.\\nThe plugin includes 3 pre-trained networks that are able to identify mitosis stages or apoptosis on soSPIM images. More details can be found on the pre-print.\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nFirst install conda and create an environment for the plugin\\nconda create --prefix env-napari-yolov5 python=3.9\\nconda activate env-napari-yolov5\\nYou can install napari-yolov5 and napari via pip:\\npip install napari-yolov5\\npip install napari[all]\\n\\nFor GPU support :\\npip uninstall torch\\npip install torchvision==0.10.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html\\nUsage\\nFirst select if you would like to train a new network or detect objects.\\n\\nFor Training :\\nData preparation should be done following Ultralytics' instructions.\\nSelect the size of the network, the number of epochs, the number of images per batch to load on the GPU, the size of the images (must be a stride of 32), and the name of the network.\\n\\nAn example of the YAML config file is provided in src/napari_yolov5/resources folder.\\n\\nProgress can be seen on the Terminal or on the right-hand side of the viewer.\\n\\nFor Detection :\\nIt is possible to perform the detection on a single layer chosen in the list, all the layers opened, or by giving a folder path. For folder detection, all the images will be loaded as a single stack.\\n\\nNucleus size of the prediction layer has te be filled to resize the image to the training dataset. Nucleus size of the training dataset will be asked in case of a custom network.\\nConfidence threshold defines the minimum value for a detected object to be considered positive. \\niou nms threshold (intersection-over-union non-max-suppression) defines the overlapping area of two boxes as a single object. Only the box with the maximum confidence is kept.\\nProgress can be seen on the Terminal.\\n\\nFew options allow for modification on how the boxes are being displayed (default : box + class + confidence score ; box + class ; box only) and if the box coordinates and the image overlay will be exported.\\nPost-processing option will perform a simple 3D assignment based on 3D connected component analysis. A median filter (1x1x3 XYZ) is applied prior to the assignment. \\nThe centroid of each object is then saved into a new point layer as a 3D point with a random color for each class. \\n\\nThe localisation of each centroid is saved and the path is shown in the Terminal at the end of the detection. It is also possible now to define the export folder.\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the GNU GPL v3.0 license,\\n\\\"napari-yolov5\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please [file an issue] along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari-yolov5\",\"documentation\":\"\",\"first_released\":\"2021-12-29T06:54:55.003736Z\",\"license\":\"GPL-3.0-only\",\"name\":\"napari-yolov5\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/rdemets/napari-yolov5\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-10-26T11:56:51.398591Z\",\"report_issues\":\"\",\"requirements\":null,\"summary\":\"Plugin adapted from Ultralytics to bring YOLOv5 into Napari\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.2.14\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Zach Marin\"},{\"name\":\"Robert Haase\"}],\"code_repository\":\"https://github.com/zacsimile/napari-pymeshlab\",\"conda\":[],\"description\":\"# napari-pymeshlab\\n\\n[![License](https://img.shields.io/pypi/l/napari-pymeshlab.svg?color=green)](https://github.com/zacsimile/napari-pymeshlab/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-pymeshlab.svg?color=green)](https://pypi.org/project/napari-pymeshlab)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-pymeshlab.svg?color=green)](https://python.org)\\n[![tests](https://github.com/zacsimile/napari-pymeshlab/workflows/tests/badge.svg)](https://github.com/zacsimile/napari-pymeshlab/actions)\\n[![codecov](https://codecov.io/gh/zacsimile/napari-pymeshlab/branch/main/graph/badge.svg)](https://codecov.io/gh/zacsimile/napari-pymeshlab)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-pymeshlab)](https://napari-hub.org/plugins/napari-pymeshlab)\\n\\nInterfaces between `napari` and the `pymeshlab` library to allow import, export, construction and processing of surfaces. \\n\\nThis is a WIP and feature requests are welcome. Please check [PyMeshLab](https://pymeshlab.readthedocs.io/en/latest/)\\nfor possible features.\\n\\n![img.png](docs/screenshot.png)\\n\\n## Feature list\\n\\n- Read/write .3ds, .apts, .asc, .bre, .ctm, .dae, .e57, .es, .fbx, .glb, .gltf, .obj, .off, .pdb, .ply,\\n                  .ptx, .qobj, .stl, .vmi, .wrl, .x3d, .x3dv\\n- [Screened Poisson Surface Reconstruction](https://www.cs.jhu.edu/~misha/MyPapers/ToG13.pdf)\\n- [Convex hull of a surface](https://pymeshlab.readthedocs.io/en/0.1.9/tutorials/apply_filter.html)\\n- [Laplacian smoothing of surfaces](https://pymeshlab.readthedocs.io/en/0.1.9/filter_list.html#laplacian_smooth)\\n- [Smoothing surfaces using Taubin's method](https://pymeshlab.readthedocs.io/en/0.1.9/filter_list.html#taubin_smooth)\\n- [Surface simplification using clustering decimation](https://pymeshlab.readthedocs.io/en/0.1.9/filter_list.html#simplification_clustering_decimation)\\n- [colorize_curvature_apss](https://pymeshlab.readthedocs.io/en/0.1.9/filter_list.html#colorize_curvature_apss)\\n\\nSome functions are shown in the [demo notebook](docs/demo.ipynb).\\n\\n----------------------------------\\n\\n<!--\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/plugins/stable/index.html\\n-->\\n\\n## Installation \\n\\nYou can install `napari-pymeshlab` via [pip]:\\n\\n    pip install napari-pymeshlab\\n\\n\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [GNU GPL v3.0] license,\\n\\\"napari-pymeshlab\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue](https://github.com/zacsimile/napari-pymeshlab/issues) along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-pymeshlab\\n\\n\\n\\n\\n\\n\\nInterfaces between napari and the pymeshlab library to allow import, export, construction and processing of surfaces. \\nThis is a WIP and feature requests are welcome. Please check PyMeshLab\\nfor possible features.\\n\\nFeature list\\n\\nRead/write .3ds, .apts, .asc, .bre, .ctm, .dae, .e57, .es, .fbx, .glb, .gltf, .obj, .off, .pdb, .ply,\\n                  .ptx, .qobj, .stl, .vmi, .wrl, .x3d, .x3dv\\nScreened Poisson Surface Reconstruction\\nConvex hull of a surface\\nLaplacian smoothing of surfaces\\nSmoothing surfaces using Taubin's method\\nSurface simplification using clustering decimation\\ncolorize_curvature_apss\\n\\nSome functions are shown in the demo notebook.\\n\\n\\nInstallation\\nYou can install napari-pymeshlab via pip:\\npip install napari-pymeshlab\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the GNU GPL v3.0 license,\\n\\\"napari-pymeshlab\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari pymeshlab\",\"documentation\":\"https://github.com/zacsimile/napari-pymeshlab#README.md\",\"first_released\":\"2022-01-14T04:47:54.142489Z\",\"license\":\"GPL-3.0\",\"name\":\"napari-pymeshlab\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\",\"writer\",\"widget\",\"sample_data\"],\"project_site\":\"https://github.com/zacsimile/napari-pymeshlab\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[\"*.e57\",\"*.3ds\",\"*.ply\",\"*.vmi\",\"*.wrl\",\"*.stl\",\"*.apts\",\".x3dv\",\"*.dae\",\"*.gltf\",\"*.x3d\",\"*.fbx\",\"*.qobj\",\"*.off\",\"*.pdb\",\"*.ctm\",\"*.es\",\"*.bre\",\"*.ptx\",\"*.asc\",\"*.obj\",\"*.glb\"],\"release_date\":\"2022-03-29T00:49:46.964913Z\",\"report_issues\":\"https://github.com/zacsimile/napari-pymeshlab/issues\",\"requirements\":[\"npe2\",\"numpy\",\"pymeshlab\"],\"summary\":\"Interfaces between napari and pymeshlab library to allow import, export and construction of surfaces.\",\"support\":\"https://github.com/zacsimile/napari-pymeshlab/issues\",\"twitter\":\"\",\"version\":\"0.0.5\",\"visibility\":\"public\",\"writer_file_extensions\":[\".asc\",\".glb\",\".vmi\",\".wrl\",\".stl\",\".qobj\",\".dae\",\".x3d\",\".gltf\",\".fbx\",\".e57\",\".x3dv\",\".off\",\".ptx\",\".ply\",\".ctm\",\".pdb\",\".3ds\",\".bre\",\".apts\",\".obj\",\".es\"],\"writer_save_layers\":[\"surface\"]}",
  "{\"authors\":[{\"email\":\"sylvain.prigent@inria.fr\",\"name\":\"Sylvain Prigent\"}],\"category\":{\"Supported data\":[\"Time series\"],\"Workflow step\":[\"Object tracking\"]},\"category_hierarchy\":{\"Supported data\":[[\"Time series\"]],\"Workflow step\":[[\"Object tracking\"]]},\"code_repository\":\"https://github.com/sylvainprigent/napari-tracks-reader\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-tracks-reader\"}],\"description\":\"# Description\\n\\nThis plugin allows to open particle tracking results from multiple formats into the Napari \\nTracks Layer.\\n\\n# Supported formats\\n\\nThe formats currently supported by this plugin are:\\n\\n## CSV\\n\\nThe most basic format to store particle tracking tracks is a CSV file containing the tracks table.\\nIn this format each line is a particle and each column a property of the particle. The table\\nheaders must be `TrackID`, `t`, `x`, `y`, `z`. Note that the header order does not matter:\\n\\n| TrackID       | t | x | y | z |\\n| :------------ | :----------: | :----------: | :----------: | -----------: |\\n| 0 | 16   | 41.5828343348868  | 47.505930020081664| 0 |\\n| 0 | 17   | 41.48425270538317 | 51.6023835597057 | 0 |\\n\\nThe raw CSV file is a classical comma-separated values format: \\n\\n```csv\\nTrackID,t,x,y,z\\n0,16, 41.5828343348868, 47.505930020081664, 0\\n0,17, 41.48425270538317, 51.6023835597057, 0\\n...\\n```\\n\\n[!NOTE]\\nThis CSV format does **not** support split and merge events\\n\\n## TrackMate\\n\\nThe TrackMate format is the XML model file générated by the [TrackMate](https://imagej.net/plugins/trackmate/) Fiji \\nplugin. \\nThe XML file from TrackMate should not be manually modified and and contains a `Model` element:\\n\\n```xml\\n<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n<TrackMate version=\\\"6.0.2\\\">\\n...\\n<Model spatialunits=\\\"pixel\\\" timeunits=\\\"sec\\\">\\n    <FeatureDeclarations>\\n      <SpotFeatures>\\n        <Feature feature=\\\"QUALITY\\\" name=\\\"Quality\\\" shortname=\\\"Quality\\\" dimension=\\\"QUALITY\\\" isint=\\\"false\\\" />\\n        <Feature feature=\\\"POSITION_X\\\" name=\\\"X\\\" shortname=\\\"X\\\" dimension=\\\"POSITION\\\" isint=\\\"false\\\" />\\n...\\n```\\n\\nAll the particles features from the TrackMate model file are loaded in the napari tracks properties. \\n\\n[!NOTE]\\nThis format supports split and merge events\\n\\n## Icy\\n\\nThe Icy format is a XML file generated by the [Icy](http://icy.bioimageanalysis.org/plugin/spot-tracking/) software.\\nThe XML file from ICY should not be manually modified and starts with the `root` element:\\n\\n```xml\\n<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n<root>\\n  <trackfile version=\\\"1\\\" />\\n  <trackgroup>\\n    <track id=\\\"265713726\\\">\\n      <detection t=\\\"16\\\" x=\\\"41.5828343348868\\\" y=\\\"47.505930020081664\\\" z=\\\"0\\\" classname=\\\"plugins.nchenouard.particleTracking.sequenceGenerator.ProfileSpotTrack\\\" type=\\\"1\\\" />\\n      <detection t=\\\"17\\\" x=\\\"41.48425270538317\\\" y=\\\"51.6023835597057\\\" z=\\\"0\\\" classname=\\\"plugins.nchenouard.particleTracking.sequenceGenerator.ProfileSpotTrack\\\" type=\\\"1\\\" />\\n      ...\\n```\\n\\n[!TIP]\\nThis format supports split and merge events\\n\\n## ISBI\\n\\nThe ISBI format is a XML format used for the ISBI tracking challenge. This format must contain \\na `root` element and a list of particles in a `TrackContestISBI2012` element:\\n\\n```xml\\n<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n<root>\\n  <TrackContestISBI2012 snr=\\\"?\\\" density=\\\"?\\\" scenario=\\\"FakeTracks.tif\\\" generationDateTime=\\\"Wed May 12 14:07:09 CEST 2021\\\">\\n    <particle>\\n      <detection t=\\\"0\\\" x=\\\"64.00558680057657\\\" y=\\\"3.9587411612103076\\\" z=\\\"0.0\\\" />\\n      <detection t=\\\"1\\\" x=\\\"63.98171495578894\\\" y=\\\"6.04150382894106\\\" z=\\\"0.0\\\" />\\n      <detection t=\\\"2\\\" x=\\\"63.95406806092088\\\" y=\\\"10.06085170348766\\\" z=\\\"0.0\\\" />\\n``` \\n\\n[!NOTE]\\nThis format does **not** support split and merge events\\n\\n\\n# Quickstart\\n\\nYou can open local tracks using `napari` at the terminal and the path to your file:\\n\\n```\\n$ napari /path/to/your/tracks.xml\\n```\\n\\nOR in python:\\n\\n```python\\nimport napari\\n\\nviewer = napari.Viewer()\\nviewer.open('/path/to/your/tracks.xml')\\nnapari.run()\\n```\\n\\n# Getting Help\\n\\nIf you discover a bug with the plugin, or would like to request a new feature, please\\nraise an issue on our repository at https://github.com/sylvainprigent/napari-tracks-reader.\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"Description\\nThis plugin allows to open particle tracking results from multiple formats into the Napari \\nTracks Layer.\\nSupported formats\\nThe formats currently supported by this plugin are:\\nCSV\\nThe most basic format to store particle tracking tracks is a CSV file containing the tracks table.\\nIn this format each line is a particle and each column a property of the particle. The table\\nheaders must be TrackID, t, x, y, z. Note that the header order does not matter:\\n| TrackID       | t | x | y | z |\\n| :------------ | :----------: | :----------: | :----------: | -----------: |\\n| 0 | 16   | 41.5828343348868  | 47.505930020081664| 0 |\\n| 0 | 17   | 41.48425270538317 | 51.6023835597057 | 0 |\\nThe raw CSV file is a classical comma-separated values format: \\ncsv\\nTrackID,t,x,y,z\\n0,16, 41.5828343348868, 47.505930020081664, 0\\n0,17, 41.48425270538317, 51.6023835597057, 0\\n...\\n[!NOTE]\\nThis CSV format does not support split and merge events\\nTrackMate\\nThe TrackMate format is the XML model file générated by the TrackMate Fiji \\nplugin. \\nThe XML file from TrackMate should not be manually modified and and contains a Model element:\\n```xml\\n\\n\\n...\\n\\n\\n\\n\\n\\n...\\n```\\nAll the particles features from the TrackMate model file are loaded in the napari tracks properties. \\n[!NOTE]\\nThis format supports split and merge events\\nIcy\\nThe Icy format is a XML file generated by the Icy software.\\nThe XML file from ICY should not be manually modified and starts with the root element:\\n```xml\\n\\n\\n\\n\\n\\n\\n\\n      ...\\n```\\n[!TIP]\\nThis format supports split and merge events\\nISBI\\nThe ISBI format is a XML format used for the ISBI tracking challenge. This format must contain \\na root element and a list of particles in a TrackContestISBI2012 element:\\n```xml\\n\\n\\n\\n\\n\\n\\n\\n``` \\n[!NOTE]\\nThis format does not support split and merge events\\nQuickstart\\nYou can open local tracks using napari at the terminal and the path to your file:\\n$ napari /path/to/your/tracks.xml\\nOR in python:\\n```python\\nimport napari\\nviewer = napari.Viewer()\\nviewer.open('/path/to/your/tracks.xml')\\nnapari.run()\\n```\\nGetting Help\\nIf you discover a bug with the plugin, or would like to request a new feature, please\\nraise an issue on our repository at https://github.com/sylvainprigent/napari-tracks-reader.\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"napari-tracks-reader\",\"documentation\":\"https://sylvainprigent.github.io/napari-tracks-reader/description.html\",\"first_released\":\"2021-05-11T18:46:21.984641Z\",\"license\":\"GPL-3.0\",\"name\":\"napari-tracks-reader\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\"],\"project_site\":\"https://sylvainprigent.github.io/napari-tracks-reader/\",\"python_version\":\">=3.6\",\"reader_file_extensions\":[\"*\"],\"release_date\":\"2021-05-26T17:18:32.900741Z\",\"report_issues\":\"\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"pandas (>=1.2.4)\"],\"summary\":\"Read tracks from txt (xml, csv) files to napari\",\"support\":\"https://github.com/sylvainprigent/napari-tracks-reader/issues\",\"twitter\":\"https://twitter.com/SylvainMPrigent\",\"version\":\"0.1.3\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"giovanni palla\"}],\"code_repository\":\"https://github.com/scverse/napari-spatialdata\",\"conda\":[],\"description\":\"# napari-spatialdata\\n\\n[![License](https://img.shields.io/pypi/l/napari-spatialdata.svg?color=green)](https://github.com/scverse/napari-spatialdata/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-spatialdata.svg?color=green)](https://pypi.org/project/napari-spatialdata)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-spatialdata.svg?color=green)](https://python.org)\\n[![tests](https://github.com/scverse/napari-spatialdata/workflows/tests/badge.svg)](https://github.com/scverse/napari-spatialdata/actions)\\n[![codecov](https://codecov.io/gh/scverse/napari-spatialdata/branch/main/graph/badge.svg?token=ASqlOKnOj7)](https://codecov.io/gh/scverse/napari-spatialdata)\\n[![pre-commit.ci status](https://results.pre-commit.ci/badge/github/scverse/napari-spatialdata/main.svg)](https://results.pre-commit.ci/latest/github/scverse/napari-spatialdata/main)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-spatialdata)](https://napari-hub.org/plugins/napari-spatialdata)\\n\\nInteractive visualization of spatial omics data with napari\\n\\n---\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/plugins/stable/index.html\\n-->\\n\\n# UNDER DEVELOPMENT\\n\\nClone repo and `pip install -e .`.\\nRun examples e.g. `python shape_regions.py` or via notebook for the shape export function.\\n\\nMore documentation on usage will be added later on.\\n\\nExpect breaking changes.\\n\\n## Installation\\n\\nYou can install `napari-spatialdata` via [pip]:\\n\\n    pip install napari-spatialdata\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-spatialdata\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[mit]: http://opensource.org/licenses/MIT\\n[bsd-3]: http://opensource.org/licenses/BSD-3-Clause\\n[gnu gpl v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[gnu lgpl v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[apache software license 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[mozilla public license 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[pypi]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-spatialdata\\n\\n\\n\\n\\n\\n\\n\\nInteractive visualization of spatial omics data with napari\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nUNDER DEVELOPMENT\\nClone repo and pip install -e ..\\nRun examples e.g. python shape_regions.py or via notebook for the shape export function.\\nMore documentation on usage will be added later on.\\nExpect breaking changes.\\nInstallation\\nYou can install napari-spatialdata via pip:\\npip install napari-spatialdata\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-spatialdata\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please [file an issue] along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari spatialdata\",\"documentation\":\"https://github.com/scverse/napari-spatialdata#README.md\",\"first_released\":\"2022-07-06T14:57:06.453889Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-spatialdata\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/scverse/napari-spatialdata.git\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-08-15T13:18:56.856934Z\",\"report_issues\":\"https://github.com/scverse/napari-spatialdata/issues\",\"requirements\":[\"numpy\",\"magicgui\",\"qtpy\",\"numba\",\"anndata\",\"scikit-image\",\"squidpy\",\"napari[all]\",\"loguru\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pre-commit (>=2.9.0) ; extra == 'testing'\",\"towncrier (>=21.3.0) ; extra == 'testing'\"],\"summary\":\"Interactive visualization of spatial omics data with napari\",\"support\":\"https://github.com/scverse/napari-spatialdata/issues\",\"twitter\":\"\",\"version\":\"0.1.0\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Nathan Heath Patterson\"}],\"category\":{\"Image modality\":[\"Fluorescence microscopy\"],\"Supported data\":[\"2D\",\"Multi-channel\"],\"Workflow step\":[\"Image registration\"]},\"category_hierarchy\":{\"Image modality\":[[\"Fluorescence microscopy\"]],\"Supported data\":[[\"2D\"],[\"Multi-channel\"]],\"Workflow step\":[[\"Image registration\"],[\"Image registration\",\"Affine registration\"],[\"Image registration\",\"Affine registration\",\"Rigid registration\"],[\"Image registration\",\"Deformable registration\"],[\"Image registration\",\"Intensity-based registration\"]]},\"code_repository\":\"https://github.com/nhpatterson/napari-wsireg\",\"conda\":[],\"description\":\"# napari-wsireg\\n\\n[//]: # ([![License]&#40;https://img.shields.io/pypi/l/napari-wsireg.svg?color=green&#41;]&#40;https://github.com/nhpatterson/napari-wsireg/raw/main/LICENSE&#41;)\\n\\n[//]: # ([![PyPI]&#40;https://img.shields.io/pypi/v/napari-wsireg.svg?color=green&#41;]&#40;https://pypi.org/project/napari-wsireg&#41;)\\n\\n[//]: # ([![Python Version]&#40;https://img.shields.io/pypi/pyversions/napari-wsireg.svg?color=green&#41;]&#40;https://python.org&#41;)\\n\\n[//]: # ([![tests]&#40;https://github.com/nhpatterson/napari-wsireg/workflows/tests/badge.svg&#41;]&#40;https://github.com/nhpatterson/napari-wsireg/actions&#41;)\\n\\n[//]: # ([![napari hub]&#40;https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-wsireg&#41;]&#40;https://napari-hub.org/plugins/napari-wsireg&#41;)\\n\\n\\nPlugin to perform whole slide image registration based on wsireg.\\n\\nPlease see [wsireg](https://github.com/nhpatterson/wsireg) for more info image formats, features and how registration works.\\n\\n\\n## Usage\\n\\nAdd images from napari layers or from file and set up \\\"registration paths\\\" between them. OME-TIFF is best supported format.\\n\\n## Installation\\n\\nYou can install `napari-wsireg` via [pip]:\\n\\n    pip install napari-wsireg\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/nhpatterson/napari-wsireg.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-wsireg\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/nhpatterson/napari-wsireg/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-wsireg\\nPlugin to perform whole slide image registration based on wsireg.\\nPlease see wsireg for more info image formats, features and how registration works.\\nUsage\\nAdd images from napari layers or from file and set up \\\"registration paths\\\" between them. OME-TIFF is best supported format.\\nInstallation\\nYou can install napari-wsireg via pip:\\npip install napari-wsireg\\n\\nTo install latest development version :\\npip install git+https://github.com/nhpatterson/napari-wsireg.git\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-wsireg\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"napari-wsireg\",\"documentation\":\"https://github.com/nhpatterson/napari-wsireg#README.md\",\"first_released\":\"2022-04-27T18:49:51.526126Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-wsireg\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/nhpatterson/napari-wsireg\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-08-16T17:32:20.887373Z\",\"report_issues\":\"https://github.com/nhpatterson/napari-wsireg/issues\",\"requirements\":[\"wsireg (>=0.3.6)\",\"SimpleITK\",\"czifile\",\"dask\",\"imagecodecs\",\"napari\",\"numpy\",\"ome-types\",\"pint\",\"qtpy\",\"tifffile\",\"zarr (>=2.10.3)\",\"napari-geojson\",\"networkx\",\"matplotlib\"],\"summary\":\"plugin to perform whole slide image registration with wsireg\",\"support\":\"https://github.com/nhpatterson/napari-wsireg/issues\",\"twitter\":\"\",\"version\":\"0.1.2\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Uwe Schmidt\"},{\"name\":\"Martin Weigert\"}],\"category\":{\"Image modality\":[\"Medical imaging\"],\"Supported data\":[\"2D\",\"3D\",\"Time series\",\"Multi-channel\"],\"Workflow step\":[\"Image Segmentation\"]},\"category_hierarchy\":{\"Image modality\":[[\"Medical imaging\"]],\"Supported data\":[[\"2D\"],[\"3D\"],[\"Time series\"],[\"Multi-channel\"]],\"Workflow step\":[[\"Image Segmentation\",\"Cell segmentation\"],[\"Image Segmentation\",\"Model-based segmentation\"]]},\"code_repository\":\"https://github.com/stardist/stardist-napari\",\"description\":\"# StarDist Napari Plugin\\n\\n[![PyPI version](https://img.shields.io/pypi/v/stardist-napari.svg)](https://pypi.org/project/stardist-napari)\\n[![Anaconda-Server Badge](https://anaconda.org/conda-forge/stardist-napari/badges/version.svg)](https://anaconda.org/conda-forge/stardist-napari)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/stardist-napari)](https://napari-hub.org/plugins/stardist-napari)\\n[![Image.sc forum](https://img.shields.io/badge/dynamic/json.svg?label=forum&url=https%3A%2F%2Fforum.image.sc%2Ftags%2Fstardist.json&query=%24.topic_list.tags.0.topic_count&colorB=brightgreen&suffix=%20topics&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA4AAAAOCAYAAAAfSC3RAAABPklEQVR42m3SyyqFURTA8Y2BER0TDyExZ+aSPIKUlPIITFzKeQWXwhBlQrmFgUzMMFLKZeguBu5y+//17dP3nc5vuPdee6299gohUYYaDGOyyACq4JmQVoFujOMR77hNfOAGM+hBOQqB9TjHD36xhAa04RCuuXeKOvwHVWIKL9jCK2bRiV284QgL8MwEjAneeo9VNOEaBhzALGtoRy02cIcWhE34jj5YxgW+E5Z4iTPkMYpPLCNY3hdOYEfNbKYdmNngZ1jyEzw7h7AIb3fRTQ95OAZ6yQpGYHMMtOTgouktYwxuXsHgWLLl+4x++Kx1FJrjLTagA77bTPvYgw1rRqY56e+w7GNYsqX6JfPwi7aR+Y5SA+BXtKIRfkfJAYgj14tpOF6+I46c4/cAM3UhM3JxyKsxiOIhH0IO6SH/A1Kb1WBeUjbkAAAAAElFTkSuQmCC)](https://forum.image.sc/tags/stardist)\\n\\nThis project provides the [napari](https://napari.org/) plugin for [StarDist](https://github.com/stardist/stardist), a deep learning based 2D and 3D object detection method with star-convex shapes. StarDist has originally been developed (see [papers](https://github.com/stardist/stardist#stardist---object-detection-with-star-convex-shapes)) for the segmentation of densely packed cell nuclei in challenging images with low signal-to-noise ratios. The plugin allows to apply pretrained and custom trained models from within napari.\\n\\nIf you use this plugin for your research, please [cite us](https://github.com/stardist/stardist#how-to-cite).\\n\\n![Screenshot](https://github.com/stardist/stardist-napari/raw/main/images/stardist_napari_screenshot_small.png)\\n\\n\\n## Installation\\n\\nInstall the plugin with `pip install stardist-napari` or from within napari via `Plugins > Install/Uninstall Plugins…`. If you want GPU-accelerated prediction, please read the more detailed [installation instructions](https://github.com/stardist/stardist#installation) for StarDist.\\n\\n- You can activate the plugin in napari via `Plugins > stardist-napari: StarDist`.\\n- Example images for testing are provided via `File > Open Sample > stardist-napari`.\\n\\n\\n## Documentation\\n\\nThe two main buttons at the bottom of the plugin are (see right side of screenshot above):\\n\\n**Restore Defaults**: Restore default values for [inputs](#inputs) (exceptions: *Input Image*, *Image Axes*, *Custom Model*).\\n\\n**Run**: Start the prediction with the selected inputs and create the [outputs](#outputs) when done.\\n\\nAll plugin activity is shown in the napari *activity dock*, which can be shown/hidden by clicking on the word `activity` next to the little arrow at the bottom right of the napari window.\\n\\n### Inputs\\n\\nThe plugin does perform input validation, i.e. it will disable the `Run` button if it detects a problem with the selected inputs. Problematic input fields are highlighted with a \\\"lightcoral\\\" background color ![](https://via.placeholder.com/15/f08080/f08080.png), and their [*tooltips*](https://en.wikipedia.org/wiki/Tooltip) typically explain what the problem is. Some error messages are shown at the bottom in napari's status bar, such as for incompatibilities between multiple input fields. Input fields with warnings (also explained via tooltips) are highlighted with an orange background color ![](https://via.placeholder.com/15/ffa500/ffa500.png).\\n\\n**Input Image**: Select a napari layer of type `Image` as the input.  \\n*Tooltip:* Shows the shape of the image.\\n\\n**Image Axes**: String that describes the semantic image axes and their order, e.g. `YX` for a 2D image. This parameter is automatically chosen (i.e. guessed) when a new input image is selected and should work in most cases. Permissible axis values are: `X` (width/columns), `Y` (height/rows), `Z` (depth/planes), `C` (channels), `T` (frames/time).  \\n*Tooltip:* Shows the mapping of semantic axes to the shape of the selected input image.\\n\\n**Predict on field of view (only for 2D models in 2D view)**: If enabled, the StarDist prediction is only applied to the current field of view of the napari viewer. As the name of this checkbox indicates, this only works for 2D StarDist models and when the napari viewer is in 2D viewing mode. The checkbox is not even shown if those conditions are not met.\\n\\n#### *Neural Network Prediction*\\n\\n**Model Type**: Choice whether to use registered pre-trained models (`2D`, `3D`) or provide a path to a model folder (`Custom 2D/3D`). Based on this choice, either the input for *Pre-trained Model* or *Custom Model* is shown below.  \\n(Further information regarding pre-trained models: [how to register your own model](https://nbviewer.org/github/CSBDeep/CSBDeep/blob/master/examples/other/technical.ipynb#Registry-for-pretrained-models), [model registration in StarDist](https://github.com/stardist/stardist/blob/f73cdc44f718d36844b38c1f1662dbb66d157182/stardist/models/__init__.py#L17-L29).)\\n\\n**Pre-trained Model**: Select a registered pre-trained model from a list. The first time a model is selected, it is downloaded and cached locally.\\n\\n**Custom Model**: Provide a path to a StarDist model folder, containing at least `config.json` and a compatible neural network weights file (with suffix `.h5` or `.hdf5`). If present, `thresholds.json` is also loaded and its values can be used via the button *Set optimized postprocessing thresholds (for selected model)*.\\n\\n**Model Axes**: A read-only text field that shows the semantic axes that the currently selected model expects as input. Additionally, we show the number of expected input channels, e.g. `YXC[2]` to indicate that the model expects a 2D input image with 2 channels. Seeing the model axes is helpful to understand whether the axes of the input image are compatible or not.\\n\\n**Normalize Image**: A checkbox to indicate whether to perform [percentile-based input image normalization](https://forum.image.sc/t/normalization-in-stardist/41696/2) or not. This should be checked if the input image wasn't [manually normalized](https://forum.image.sc/t/stardist-extension/37696/7) such that most pixel values are in the range 0 to 1. If unchecked, inputs *Percentile low* and *Percentile high* are hidden.\\n\\n**Percentile low**: Percentile value of input pixel distribution that is mapped to 0 (~min value). If there aren't any outlier pixels in your image, you may use percentile `0` to do a standard [min-max image normalization](https://www.codecademy.com/article/normalization).\\n\\n**Percentile high**: Percentile value of input pixel distribution that is mapped to 1 (~max value). If there aren't any outlier pixels in your image, you may use percentile `100` to do a standard [min-max image normalization](https://www.codecademy.com/article/normalization).\\n\\n**Input image scaling**: Number or list of numbers (one per input axis) to scale the input image before prediction and rescale the output accordingly. For example, a value of `0.5` indicates that all spatial axes are downscaled to half their size before prediction, and that the outputs are scaled to double their size. This is useful to adapt to different object sizes in the input image.  \\n*Tooltip:* Shows the mapping of scale values to the semantic axes of the selected input image.\\n\\n#### *NMS Postprocessing*\\n\\n**Probability/Score Threshold**: Determine the number of object candidates to enter non-maximum suppression. Higher values lead to fewer segmented objects, but will likely avoid false positives. The selected model may have an associated threshold value, which can be loaded via the *Set optimized postprocessing thresholds (for selected model)* button.\\n\\n**Overlap Threshold**: Determine when two objects are considered the same during non-maximum suppression. Higher values allow segmented objects to overlap substantially. The selected model may have an associated threshold value, which can be loaded via the *Set optimized postprocessing thresholds (for selected model)* button.\\n\\n**Output Type**: Choose format of [outputs](#outputs) (see below for details). Selecting `Label Image` will create the outputs *StarDist labels* and *StarDist class labels* (for multi-class models only) as napari `Labels` layers. Selecting `Polygons / Polyhedra` will instead return the output *StarDist polygons* as a napari `Shapes` layer for a 2D model, or *StarDist polyhedra* as a napari `Surface` layer for a 3D model. Selecting `Both` will return both types of outputs.\\n\\n#### *Advanced Options*\\n\\n**Number of Tiles**: String `None` (to disable tiling) or list of integer numbers (one per axis of input image) to determine how the input image is tiled before the CNN prediction is computed on each tile individually. This is needed to avoid (GPU) memory issues that can occur for large input images. Note that the NMS postprocessing is still run only once with candidates from the predictions of all image tiles.  \\n*Tooltip:* Shows the mapping of tile values to the semantic axes of the selected input image.\\n\\n**Normalization Axes**: String of semantic axes which are jointly normalized (if they are present in the input image). For example, the default value `ZYX` indicates that all spatial axes are always normalized together; if an image has multiple channels, the pixels will be normalized separately per channel (e.g. this is what typically makes sense for fluorescence microscopy where channels are independent). On the other hand, the channels in RGB color images typically need to be normalized jointly, hence using `ZYXC` makes sense in this case. Note: if an image is explicitly opened with `rgb=True` in napari, the channels are automatically normalized together.  \\n*Tooltip:* Shows a brief explanation.\\n\\n**Time-lapse Labels**: If the input is a time-lapse/movie, each frame is first independently processed by StarDist. If `Separate per frame (no processing)` is chosen, the object ids in the label images of each frame are not modified, i.e. they are consecutive integers that always start at 1. Selecting `Unique through time` will cause object ids to be unique over time, i.e. the smallest object id in a given frame is larger than the largest object id of the previous frame. Finally, choosing `Match to previous frame (via overlap)` will perform a simple form of [greedy](https://en.wikipedia.org/wiki/Greedy_algorithm) matching/tracking, where object ids are propagated from one frame to the next based on object overlap.\\n\\n**Show CNN Output**: Create additional [outputs](#outputs) (see below for details) *StarDist probability* and *StarDist distances* that show the direct results of the CNN prediction which are the inputs to the NMS postprocessing. Additionally, *StarDist class probabilities* is created for multi-class models.\\n\\n**Set optimized postprocessing thresholds (for selected model)**: Button to set *Probability/Score Threshold* and *Overlap Threshold* to the values provided by the selected model. Nothing is changed if the model does not provide threshold values.\\n\\n### Outputs\\n\\n**StarDist polygons**: The detected/segmented 2D objects as polygons (napari `Shapes` layer).\\n\\n**StarDist polyhedra**: The detected/segmented 3D objects as surfaces (napari `Surface` layer).\\n\\n**StarDist labels**: The detected/segmented 2D/3D objects as a *label image* (napari `Labels` layer). In an integer-valued label image, the value of a given pixel denotes the id of the object that it belongs to. For example, all pixels with value 5 belong to the object with id 5. All background pixels (that don't belong to any object) have value 0.\\n\\n**StarDist class labels** ([multi-class models](https://nbviewer.org/github/stardist/stardist/blob/master/examples/other2D/multiclass.ipynb) only): The classes of detected/segmented 2D/3D objects as a *semantic segmentation labeling* (napari `Labels` layer). The integer value of a given pixel denotes the class id of the object that it belongs to. For example, all pixels with value 3 belong to the object class 3. Note that all pixels that belong to a specific object instance (as returned by *StarDist labels*) do have the same object class here. All background pixels (that don't belong to an object class) have value 0.\\n\\n**StarDist probability**: The object probabilities predicted by the neural network as a single-channel image (napari `Image` layer).\\n\\n**StarDist distances**: The radial distances predicted by the neural network as a multi-channel image (napari `Image` layer).\\n\\n**StarDist class probabilities** ([multi-class models](https://nbviewer.org/github/stardist/stardist/blob/master/examples/other2D/multiclass.ipynb) only): The object class probabilities predicted by the neural network as a multi-channel image (napari `Image` layer).\\n\\n\\n## Troubleshooting & Support\\n\\n- The [image.sc forum](https://forum.image.sc/tag/stardist) is the best place to start getting help and support. Make sure to use the tag `stardist`, since we are monitoring all questions with this tag.\\n- For general questions about StarDist, it's worth taking a look at the [frequently asked questions (FAQ)]( https://stardist.net/docs/faq.html).\\n- If you have technical questions or found a bug, feel free to [open an issue](https://github.com/stardist/stardist-napari/issues).\\n\\n\\n## Other resources\\n\\nA demonstration of an earlier version of the plugin is shown in [this video](https://www.youtube.com/watch?v=Km1_TnUQ4FM&list=PLilvrWT8aLuZCaOkjucLjvDu2YRtCS-JT&index=5).\\n\\nMany of the parameters are identical to those of our [StarDist ImageJ/Fiji plugin](https://github.com/stardist/stardist-imagej), which are documented [here](https://imagej.net/plugins/stardist#usage).\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"StarDist Napari Plugin\\n\\n\\n\\n\\nThis project provides the napari plugin for StarDist, a deep learning based 2D and 3D object detection method with star-convex shapes. StarDist has originally been developed (see papers) for the segmentation of densely packed cell nuclei in challenging images with low signal-to-noise ratios. The plugin allows to apply pretrained and custom trained models from within napari.\\nIf you use this plugin for your research, please cite us.\\n\\nInstallation\\nInstall the plugin with pip install stardist-napari or from within napari via Plugins > Install/Uninstall Plugins…. If you want GPU-accelerated prediction, please read the more detailed installation instructions for StarDist.\\n\\nYou can activate the plugin in napari via Plugins > stardist-napari: StarDist.\\nExample images for testing are provided via File > Open Sample > stardist-napari.\\n\\nDocumentation\\nThe two main buttons at the bottom of the plugin are (see right side of screenshot above):\\nRestore Defaults: Restore default values for inputs (exceptions: Input Image, Image Axes, Custom Model).\\nRun: Start the prediction with the selected inputs and create the outputs when done.\\nAll plugin activity is shown in the napari activity dock, which can be shown/hidden by clicking on the word activity next to the little arrow at the bottom right of the napari window.\\nInputs\\nThe plugin does perform input validation, i.e. it will disable the Run button if it detects a problem with the selected inputs. Problematic input fields are highlighted with a \\\"lightcoral\\\" background color , and their tooltips typically explain what the problem is. Some error messages are shown at the bottom in napari's status bar, such as for incompatibilities between multiple input fields. Input fields with warnings (also explained via tooltips) are highlighted with an orange background color .\\nInput Image: Select a napari layer of type Image as the input.\\nTooltip: Shows the shape of the image.\\nImage Axes: String that describes the semantic image axes and their order, e.g. YX for a 2D image. This parameter is automatically chosen (i.e. guessed) when a new input image is selected and should work in most cases. Permissible axis values are: X (width/columns), Y (height/rows), Z (depth/planes), C (channels), T (frames/time).\\nTooltip: Shows the mapping of semantic axes to the shape of the selected input image.\\nPredict on field of view (only for 2D models in 2D view): If enabled, the StarDist prediction is only applied to the current field of view of the napari viewer. As the name of this checkbox indicates, this only works for 2D StarDist models and when the napari viewer is in 2D viewing mode. The checkbox is not even shown if those conditions are not met.\\nNeural Network Prediction\\nModel Type: Choice whether to use registered pre-trained models (2D, 3D) or provide a path to a model folder (Custom 2D/3D). Based on this choice, either the input for Pre-trained Model or Custom Model is shown below.\\n(Further information regarding pre-trained models: how to register your own model, model registration in StarDist.)\\nPre-trained Model: Select a registered pre-trained model from a list. The first time a model is selected, it is downloaded and cached locally.\\nCustom Model: Provide a path to a StarDist model folder, containing at least config.json and a compatible neural network weights file (with suffix .h5 or .hdf5). If present, thresholds.json is also loaded and its values can be used via the button Set optimized postprocessing thresholds (for selected model).\\nModel Axes: A read-only text field that shows the semantic axes that the currently selected model expects as input. Additionally, we show the number of expected input channels, e.g. YXC[2] to indicate that the model expects a 2D input image with 2 channels. Seeing the model axes is helpful to understand whether the axes of the input image are compatible or not.\\nNormalize Image: A checkbox to indicate whether to perform percentile-based input image normalization or not. This should be checked if the input image wasn't manually normalized such that most pixel values are in the range 0 to 1. If unchecked, inputs Percentile low and Percentile high are hidden.\\nPercentile low: Percentile value of input pixel distribution that is mapped to 0 (~min value). If there aren't any outlier pixels in your image, you may use percentile 0 to do a standard min-max image normalization.\\nPercentile high: Percentile value of input pixel distribution that is mapped to 1 (~max value). If there aren't any outlier pixels in your image, you may use percentile 100 to do a standard min-max image normalization.\\nInput image scaling: Number or list of numbers (one per input axis) to scale the input image before prediction and rescale the output accordingly. For example, a value of 0.5 indicates that all spatial axes are downscaled to half their size before prediction, and that the outputs are scaled to double their size. This is useful to adapt to different object sizes in the input image.\\nTooltip: Shows the mapping of scale values to the semantic axes of the selected input image.\\nNMS Postprocessing\\nProbability/Score Threshold: Determine the number of object candidates to enter non-maximum suppression. Higher values lead to fewer segmented objects, but will likely avoid false positives. The selected model may have an associated threshold value, which can be loaded via the Set optimized postprocessing thresholds (for selected model) button.\\nOverlap Threshold: Determine when two objects are considered the same during non-maximum suppression. Higher values allow segmented objects to overlap substantially. The selected model may have an associated threshold value, which can be loaded via the Set optimized postprocessing thresholds (for selected model) button.\\nOutput Type: Choose format of outputs (see below for details). Selecting Label Image will create the outputs StarDist labels and StarDist class labels (for multi-class models only) as napari Labels layers. Selecting Polygons / Polyhedra will instead return the output StarDist polygons as a napari Shapes layer for a 2D model, or StarDist polyhedra as a napari Surface layer for a 3D model. Selecting Both will return both types of outputs.\\nAdvanced Options\\nNumber of Tiles: String None (to disable tiling) or list of integer numbers (one per axis of input image) to determine how the input image is tiled before the CNN prediction is computed on each tile individually. This is needed to avoid (GPU) memory issues that can occur for large input images. Note that the NMS postprocessing is still run only once with candidates from the predictions of all image tiles.\\nTooltip: Shows the mapping of tile values to the semantic axes of the selected input image.\\nNormalization Axes: String of semantic axes which are jointly normalized (if they are present in the input image). For example, the default value ZYX indicates that all spatial axes are always normalized together; if an image has multiple channels, the pixels will be normalized separately per channel (e.g. this is what typically makes sense for fluorescence microscopy where channels are independent). On the other hand, the channels in RGB color images typically need to be normalized jointly, hence using ZYXC makes sense in this case. Note: if an image is explicitly opened with rgb=True in napari, the channels are automatically normalized together.\\nTooltip: Shows a brief explanation.\\nTime-lapse Labels: If the input is a time-lapse/movie, each frame is first independently processed by StarDist. If Separate per frame (no processing) is chosen, the object ids in the label images of each frame are not modified, i.e. they are consecutive integers that always start at 1. Selecting Unique through time will cause object ids to be unique over time, i.e. the smallest object id in a given frame is larger than the largest object id of the previous frame. Finally, choosing Match to previous frame (via overlap) will perform a simple form of greedy matching/tracking, where object ids are propagated from one frame to the next based on object overlap.\\nShow CNN Output: Create additional outputs (see below for details) StarDist probability and StarDist distances that show the direct results of the CNN prediction which are the inputs to the NMS postprocessing. Additionally, StarDist class probabilities is created for multi-class models.\\nSet optimized postprocessing thresholds (for selected model): Button to set Probability/Score Threshold and Overlap Threshold to the values provided by the selected model. Nothing is changed if the model does not provide threshold values.\\nOutputs\\nStarDist polygons: The detected/segmented 2D objects as polygons (napari Shapes layer).\\nStarDist polyhedra: The detected/segmented 3D objects as surfaces (napari Surface layer).\\nStarDist labels: The detected/segmented 2D/3D objects as a label image (napari Labels layer). In an integer-valued label image, the value of a given pixel denotes the id of the object that it belongs to. For example, all pixels with value 5 belong to the object with id 5. All background pixels (that don't belong to any object) have value 0.\\nStarDist class labels (multi-class models only): The classes of detected/segmented 2D/3D objects as a semantic segmentation labeling (napari Labels layer). The integer value of a given pixel denotes the class id of the object that it belongs to. For example, all pixels with value 3 belong to the object class 3. Note that all pixels that belong to a specific object instance (as returned by StarDist labels) do have the same object class here. All background pixels (that don't belong to an object class) have value 0.\\nStarDist probability: The object probabilities predicted by the neural network as a single-channel image (napari Image layer).\\nStarDist distances: The radial distances predicted by the neural network as a multi-channel image (napari Image layer).\\nStarDist class probabilities (multi-class models only): The object class probabilities predicted by the neural network as a multi-channel image (napari Image layer).\\nTroubleshooting & Support\\n\\nThe image.sc forum is the best place to start getting help and support. Make sure to use the tag stardist, since we are monitoring all questions with this tag.\\nFor general questions about StarDist, it's worth taking a look at the frequently asked questions (FAQ).\\nIf you have technical questions or found a bug, feel free to open an issue.\\n\\nOther resources\\nA demonstration of an earlier version of the plugin is shown in this video.\\nMany of the parameters are identical to those of our StarDist ImageJ/Fiji plugin, which are documented here.\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"stardist-napari\",\"documentation\":\"https://github.com/stardist/stardist-napari\",\"first_released\":\"2021-06-01T13:33:52.440542Z\",\"license\":\"BSD-3-Clause\",\"name\":\"stardist-napari\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\",\"sample_data\"],\"project_site\":\"https://github.com/stardist/stardist\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-12-06T10:44:01.951663Z\",\"report_issues\":\"https://github.com/stardist/stardist-napari/issues\",\"requirements\":[\"stardist (>=0.8.3)\",\"napari (>=0.4.13)\",\"magicgui (>=0.4.0)\",\"tensorflow ; platform_system != \\\"Darwin\\\" or platform_machine != \\\"arm64\\\"\",\"tensorflow-macos ; platform_system == \\\"Darwin\\\" and platform_machine == \\\"arm64\\\"\",\"tensorflow-metal ; platform_system == \\\"Darwin\\\" and platform_machine == \\\"arm64\\\"\",\"pytest ; extra == 'test'\",\"pytest-qt ; extra == 'test'\",\"napari[pyqt] (>=0.4.13) ; extra == 'test'\"],\"summary\":\"Object Detection with Star-convex Shapes\",\"support\":\"https://forum.image.sc/tag/stardist\",\"twitter\":\"https://twitter.com/martweig\",\"version\":\"2022.12.6\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"robert.haase@tu-dresden.de\",\"name\":\"Robert Haase\"}],\"code_repository\":\"https://github.com/haesleinhuepf/napari-time-slicer\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-time-slicer\"}],\"description\":\"# napari-time-slicer\\n\\n[![License](https://img.shields.io/pypi/l/napari-time-slicer.svg?color=green)](https://github.com/haesleinhuepf/napari-time-slicer/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-time-slicer.svg?color=green)](https://pypi.org/project/napari-time-slicer)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-time-slicer.svg?color=green)](https://python.org)\\n[![tests](https://github.com/haesleinhuepf/napari-time-slicer/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-time-slicer/actions)\\n[![codecov](https://codecov.io/gh/haesleinhuepf/napari-time-slicer/branch/main/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-time-slicer)\\n[![Development Status](https://img.shields.io/pypi/status/napari-time-slicer.svg)](https://en.wikipedia.org/wiki/Software_release_life_cycle#Alpha)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-time-slicer)](https://napari-hub.org/plugins/napari-time-slicer)\\n\\nA meta plugin for processing timelapse data timepoint by timepoint. It \\nenables a list of napari plugins to process 2D+t or 3D+t data step by step when the user goes \\nthrough the timelapse. Currently, these plugins are using `napari-time-slicer`:\\n* [napari-segment-blobs-and-things-with-membranes](https://www.napari-hub.org/plugins/napari-segment-blobs-and-things-with-membranes)\\n* [napari-cupy-image-processing](https://www.napari-hub.org/plugins/napari-cupy-image-processing)\\n* [napari-pyclesperanto-assistant](https://www.napari-hub.org/plugins/napari-pyclesperanto-assistant)\\n* [napari-accelerated-pixel-and-object-classification](https://www.napari-hub.org/plugins/napari-accelerated-pixel-and-object-classification)\\n* [napari-simpleitk-image-processing](https://www.napari-hub.org/plugins/napari-simpleitk-image-processing)\\n* [napari-stress](https://www.napari-hub.org/plugins/napari-stress)\\n* [napari-process-points-and-surfaces](https://www.napari-hub.org/plugins/napari-process-points-and-surfaces)\\n\\n`napari-time-slicer` enables inter-plugin communication, e.g. allowing to combine the plugins listed above in \\none image processing workflow for segmenting a timelapse dataset:\\n\\n![](https://github.com/haesleinhuepf/napari-time-slicer/raw/main/images/screencast1.gif)\\n\\nThe workflow can then also be exported as a script. The 'Generate Code' button can be found in the [Workflow Inspector](https://www.napari-hub.org/plugins/napari-workflow-inspector)\\n\\n\\nIf you want to convert a 3D dataset into a 2D + time dataset, use the \\nmenu `Tools > Utilities > Convert 3D stack to 2D timelapse (time-slicer)`. It will turn the 3D dataset to a 4D datset\\nwhere the Z-dimension (index 1) has only 1 element, which will in napari be displayed with a time-slider. Note: It is \\nrecommended to remove the original 3D dataset after this conversion.\\n\\n## Working with large on-the-fly processed datasets\\n\\nUsing the [napari-assistant](https://www.napari-hub.org/plugins/napari-assistant) complex image processing workflows on timelapse datasets can be setup. \\nIn combination with the time-slicer it is possible to process time-lapse data that is larger than available computer memory.\\nIn case the workflow only consists of images and label-images and out-of-memory issues arise, consider storing intermediate results on disk following this procedure: \\nAfter setting up the workflow and testing it on a couple of selected frames, store the entire processed timelapse dataset to disk \\nusing the menu `Tools > Utilities > Convert to file-backed timelapse data (time-slicer)`. It will open this dialog, where you can select \\n![img.png](https://github.com/haesleinhuepf/napari-time-slicer/raw/main/images/convert_to_file_backed_timelapse.png)\\n\\nIt is recommended to enter a folder location in the text field. \\nIf not provided, a temporary folder will be created, typically in the User's temp folder in the home directory. \\nThe user is responsible for emptying this folder from time to time.\\nThe data stored in this folder can also be loaded into napari using its `File > Open Folder...` menu.\\n\\nExecuting this operation can take time as every timepoint of the timelapse is computed. \\nAfterwards, there will be another layer available in napari, which is typically faster to navigate through. \\nConsider removing the layer(s) that were only needed to determine the new file-backed layer.\\n\\n![img.png](https://github.com/haesleinhuepf/napari-time-slicer/raw/main/images/new_file_backed_layer.png)\\n\\n## Usage for plugin developers\\n\\nPlugins which implement the `napari_experimental_provide_function` hook can make use of the `@time_slicer`. At the moment,\\nonly functions which take `napari.types.ImageData`, `napari.types.LabelsData` and basic python types such as `int` \\nand `float` are supported. If you annotate such a function with `@time_slicer` it will internally convert any 4D dataset\\nto a 3D dataset according to the timepoint currently selected in napari. Furthermore, when the napari user changes the\\ncurrent timepoint or the input data of the function changes, a re-computation is invoked. Thus, it is recommended to \\nonly use the `time_slicer` for functions which can provide [almost] real-time performance. Another constraint is that \\nthese annotated functions have to have a `viewer` parameter. This is necessary to read the current timepoint from the \\nviewer when invoking the re-computions.\\n\\nExample\\n```python\\nimport napari\\nfrom napari_time_slicer import time_slicer\\n\\n@time_slicer\\ndef threshold_otsu(image:napari.types.ImageData, viewer: napari.Viewer = None) -> napari.types.LabelsData:\\n    # ...\\n```\\n\\nYou can see a full implementations of this concept in the napari plugins listed above.\\n\\nIf you want to combine slicing in time and processing z-stack images slice-by-slice, you can use the `@slice_by_slice` annotation.\\nMake sure, to insert it after `@time_slicer` as shown below and implemented in [napari-pillow-image-processing](https://github.com/haesleinhuepf/napari-pillow-image-processing/blob/4d846b226739843124953f16059241d917cde8e1/src/napari_pillow_image_processing/__init__.py#L151)\\n\\n```python\\nfrom napari_time_slicer import slice_by_slice\\n\\n@time_slicer\\n@slice_by_slice\\ndef blur_2d(image:napari.types.ImageData, sigma:float = 1, viewer: napari.Viewer = None) -> napari.types.LabelsData:\\n    # ...\\n```\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n## Installation\\n\\nYou can install `napari-time-slicer` via [pip]:\\n\\n    pip install napari-time-slicer\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/haesleinhuepf/napari-time-slicer.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-time-slicer\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/haesleinhuepf/napari-time-slicer/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-time-slicer\\n\\n\\n\\n\\n\\n\\n\\nA meta plugin for processing timelapse data timepoint by timepoint. It \\nenables a list of napari plugins to process 2D+t or 3D+t data step by step when the user goes \\nthrough the timelapse. Currently, these plugins are using napari-time-slicer:\\n* napari-segment-blobs-and-things-with-membranes\\n* napari-cupy-image-processing\\n* napari-pyclesperanto-assistant\\n* napari-accelerated-pixel-and-object-classification\\n* napari-simpleitk-image-processing\\n* napari-stress\\n* napari-process-points-and-surfaces\\nnapari-time-slicer enables inter-plugin communication, e.g. allowing to combine the plugins listed above in \\none image processing workflow for segmenting a timelapse dataset:\\n\\nThe workflow can then also be exported as a script. The 'Generate Code' button can be found in the Workflow Inspector\\nIf you want to convert a 3D dataset into a 2D + time dataset, use the \\nmenu Tools > Utilities > Convert 3D stack to 2D timelapse (time-slicer). It will turn the 3D dataset to a 4D datset\\nwhere the Z-dimension (index 1) has only 1 element, which will in napari be displayed with a time-slider. Note: It is \\nrecommended to remove the original 3D dataset after this conversion.\\nWorking with large on-the-fly processed datasets\\nUsing the napari-assistant complex image processing workflows on timelapse datasets can be setup. \\nIn combination with the time-slicer it is possible to process time-lapse data that is larger than available computer memory.\\nIn case the workflow only consists of images and label-images and out-of-memory issues arise, consider storing intermediate results on disk following this procedure: \\nAfter setting up the workflow and testing it on a couple of selected frames, store the entire processed timelapse dataset to disk \\nusing the menu Tools > Utilities > Convert to file-backed timelapse data (time-slicer). It will open this dialog, where you can select \\n\\nIt is recommended to enter a folder location in the text field. \\nIf not provided, a temporary folder will be created, typically in the User's temp folder in the home directory. \\nThe user is responsible for emptying this folder from time to time.\\nThe data stored in this folder can also be loaded into napari using its File > Open Folder... menu.\\nExecuting this operation can take time as every timepoint of the timelapse is computed. \\nAfterwards, there will be another layer available in napari, which is typically faster to navigate through. \\nConsider removing the layer(s) that were only needed to determine the new file-backed layer.\\n\\nUsage for plugin developers\\nPlugins which implement the napari_experimental_provide_function hook can make use of the @time_slicer. At the moment,\\nonly functions which take napari.types.ImageData, napari.types.LabelsData and basic python types such as int \\nand float are supported. If you annotate such a function with @time_slicer it will internally convert any 4D dataset\\nto a 3D dataset according to the timepoint currently selected in napari. Furthermore, when the napari user changes the\\ncurrent timepoint or the input data of the function changes, a re-computation is invoked. Thus, it is recommended to \\nonly use the time_slicer for functions which can provide [almost] real-time performance. Another constraint is that \\nthese annotated functions have to have a viewer parameter. This is necessary to read the current timepoint from the \\nviewer when invoking the re-computions.\\nExample\\n```python\\nimport napari\\nfrom napari_time_slicer import time_slicer\\n@time_slicer\\ndef threshold_otsu(image:napari.types.ImageData, viewer: napari.Viewer = None) -> napari.types.LabelsData:\\n    # ...\\n```\\nYou can see a full implementations of this concept in the napari plugins listed above.\\nIf you want to combine slicing in time and processing z-stack images slice-by-slice, you can use the @slice_by_slice annotation.\\nMake sure, to insert it after @time_slicer as shown below and implemented in napari-pillow-image-processing\\n```python\\nfrom napari_time_slicer import slice_by_slice\\n@time_slicer\\n@slice_by_slice\\ndef blur_2d(image:napari.types.ImageData, sigma:float = 1, viewer: napari.Viewer = None) -> napari.types.LabelsData:\\n    # ...\\n```\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\nInstallation\\nYou can install napari-time-slicer via pip:\\npip install napari-time-slicer\\n\\nTo install latest development version :\\npip install git+https://github.com/haesleinhuepf/napari-time-slicer.git\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-time-slicer\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 3 - Alpha\"],\"display_name\":\"napari-time-slicer\",\"documentation\":\"https://github.com/haesleinhuepf/napari-time-slicer#README.md\",\"first_released\":\"2021-11-12T21:02:28.253302Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-time-slicer\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/haesleinhuepf/napari-time-slicer\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-07-10T14:24:14.887100Z\",\"report_issues\":\"https://github.com/haesleinhuepf/napari-time-slicer/issues\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"toolz\",\"napari-tools-menu\",\"napari-workflows\"],\"summary\":\"A meta plugin for processing timelapse data in napari timepoint by timepoint\",\"support\":\"https://github.com/haesleinhuepf/napari-time-slicer/issues\",\"twitter\":\"\",\"version\":\"0.4.9\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Robert Haase\"}],\"code_repository\":\"https://github.com/haesleinhuepf/napari-script-editor\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-script-editor\"}],\"description\":\"# napari-script-editor\\n\\n[![License](https://img.shields.io/pypi/l/napari-script-editor.svg?color=green)](https://github.com/haesleinhuepf/napari-script-editor/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-script-editor.svg?color=green)](https://pypi.org/project/napari-script-editor)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-script-editor.svg?color=green)](https://python.org)\\n[![tests](https://github.com/haesleinhuepf/napari-script-editor/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-script-editor/actions)\\n[![codecov](https://codecov.io/gh/haesleinhuepf/napari-script-editor/branch/main/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-script-editor)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-script-editor)](https://napari-hub.org/plugins/napari-script-editor)\\n\\nA python script editor for napari based on [haesleinhuepf's fork of PyQode](https://github.com/haesleinhuepf/pyqode.core).\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n![](https://github.com/haesleinhuepf/napari-script-editor/raw/main/docs/screenshot2.png)\\n\\n## Usage\\n\\nStart the script editor from the menu `Tools > Scripts > Script Editor`. Use the auto-completion while typing, \\ncheck out the [napari tutorials](https://napari.org/tutorials/) and the\\n[example scripts](https://github.com/haesleinhuepf/napari-script-editor/tree/main/example_scripts). \\nUse the `Run` button to execute a script.\\n\\n![](https://github.com/haesleinhuepf/napari-script-editor/raw/main/docs/type_and_run_screencast.gif)\\n\\nIf you save the script to the folder \\\".napari-scripts\\\" in your home directory, you will find the script in the \\n`Tools > Scripts` menu in napari. You can then also start it from there.\\n\\n![](https://github.com/haesleinhuepf/napari-script-editor/raw/main/docs/run_from_menu_screencast.gif)\\n\\nNote: If you have scripts, that might be useful to others, please send them as \\n[pull-request](https://github.com/haesleinhuepf/napari-script-editor/pulls) to the examples in \\nrepository or share them in any other way that suits you.\\n\\n## Installation\\n* Get a python environment, e.g. via [mini-conda](https://docs.conda.io/en/latest/miniconda.html). \\n  If you never used python/conda environments before, please follow the instructions \\n  [here](https://mpicbg-scicomp.github.io/ipf_howtoguides/guides/Python_Conda_Environments) first.\\n* Install [napari](https://github.com/napari/napari) using conda. \\n\\n```\\nconda install -c conda-forge napari\\n```\\n\\nAfterwards, install `napari-script-editor` using pip:\\n\\n```\\npip install napari-script-editor\\n```\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-script-editor\\\" is free and open source software\\n\\n## Known issues\\n\\n* Sometimes, the script editor thinks, the file has been changed on disk and asks to reload it.\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/haesleinhuepf/napari-script-editor/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-script-editor\\n\\n\\n\\n\\n\\n\\nA python script editor for napari based on haesleinhuepf's fork of PyQode.\\n\\nThis napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.\\n\\nUsage\\nStart the script editor from the menu Tools > Scripts > Script Editor. Use the auto-completion while typing, \\ncheck out the napari tutorials and the\\nexample scripts. \\nUse the Run button to execute a script.\\n\\nIf you save the script to the folder \\\".napari-scripts\\\" in your home directory, you will find the script in the \\nTools > Scripts menu in napari. You can then also start it from there.\\n\\nNote: If you have scripts, that might be useful to others, please send them as \\npull-request to the examples in \\nrepository or share them in any other way that suits you.\\nInstallation\\n\\nGet a python environment, e.g. via mini-conda. \\n  If you never used python/conda environments before, please follow the instructions \\n  here first.\\nInstall napari using conda. \\n\\nconda install -c conda-forge napari\\nAfterwards, install napari-script-editor using pip:\\npip install napari-script-editor\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-script-editor\\\" is free and open source software\\nKnown issues\\n\\nSometimes, the script editor thinks, the file has been changed on disk and asks to reload it.\\n\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 3 - Alpha\"],\"display_name\":\"napari-script-editor\",\"documentation\":\"https://github.com/haesleinhuepf/napari-script-editor#README.md\",\"first_released\":\"2021-11-06T10:19:52.572418Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-script-editor\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/haesleinhuepf/napari-script-editor\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-10-06T07:04:12.157901Z\",\"report_issues\":\"https://github.com/haesleinhuepf/napari-script-editor/issues\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"numpy\",\"napari\",\"haesleinhuepf-pyqode.core (>=2.15.5)\",\"haesleinhuepf-pyqode.python (>=2.15.2)\",\"napari-tools-menu\",\"jedi (>=0.18.0)\",\"pyflakes (<=2.4.0)\"],\"summary\":\"A python script editor for napari\",\"support\":\"https://github.com/haesleinhuepf/napari-script-editor/issues\",\"twitter\":\"\",\"version\":\"0.2.9\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Varun Kapoor\"}],\"code_repository\":\"https://github.com/Kapoorlabs-CAPED/vollseg-napari-mtrack\",\"description\":\"# vollseg-napari-mtrack\\n\\n[![License BSD-3](https://img.shields.io/pypi/l/vollseg-napari-mtrack.svg?color=green)](https://github.com/Kapoorlabs-CAPED/vollseg-napari-mtrack/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/vollseg-napari-mtrack.svg?color=green)](https://pypi.org/project/vollseg-napari-mtrack)\\n[![Python Version](https://img.shields.io/pypi/pyversions/vollseg-napari-mtrack.svg?color=green)](https://python.org)\\n[![tests](https://github.com/Kapoorlabs-CAPED/vollseg-napari-mtrack/workflows/tests/badge.svg)](https://github.com/Kapoorlabs-CAPED/vollseg-napari-mtrack/actions)\\n[![codecov](https://codecov.io/gh/Kapoorlabs-CAPED/vollseg-napari-mtrack/branch/main/graph/badge.svg)](https://codecov.io/gh/Kapoorlabs-CAPED/vollseg-napari-mtrack)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/vollseg-napari-mtrack)](https://napari-hub.org/plugins/vollseg-napari-mtrack)\\n\\nSegment kymographs of microtubules, actin filaments and perform Ransac based fits to compute dynamic instability parameters for individual kymographs and also in batch\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/stable/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `vollseg-napari-mtrack` via [pip]:\\n\\n    pip install vollseg-napari-mtrack\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/Kapoorlabs-CAPED/vollseg-napari-mtrack.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"vollseg-napari-mtrack\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/Kapoorlabs-CAPED/vollseg-napari-mtrack/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"vollseg-napari-mtrack\\n\\n\\n\\n\\n\\n\\nSegment kymographs of microtubules, actin filaments and perform Ransac based fits to compute dynamic instability parameters for individual kymographs and also in batch\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install vollseg-napari-mtrack via pip:\\npip install vollseg-napari-mtrack\\n\\nTo install latest development version :\\npip install git+https://github.com/Kapoorlabs-CAPED/vollseg-napari-mtrack.git\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"vollseg-napari-mtrack\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"VollSeg Napari MTrack Plugin\",\"documentation\":\"https://github.com/Kapoorlabs-CAPED/vollseg-napari-mtrack#README.md\",\"first_released\":\"2022-12-20T18:07:40.732817Z\",\"license\":\"BSD-3-Clause\",\"name\":\"vollseg-napari-mtrack\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\",\"writer\",\"widget\",\"sample_data\"],\"project_site\":\"https://github.com/Kapoorlabs-CAPED/vollseg-napari-mtrack\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2023-01-27T15:53:27.807886Z\",\"report_issues\":\"https://github.com/Kapoorlabs-CAPED/vollseg-napari-mtrack/issues\",\"requirements\":[\"numpy\",\"magicgui\",\"qtpy\",\"caped-ai\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\"],\"summary\":\"Segment kymographs of microtubules, actin filaments and perform Ransac based fits to compute dynamic instability parameters for individual kymographs and also in batch\",\"support\":\"https://github.com/Kapoorlabs-CAPED/vollseg-napari-mtrack/issues\",\"twitter\":\"\",\"version\":\"1.3.3\",\"visibility\":\"public\",\"writer_file_extensions\":[\".tif\"],\"writer_save_layers\":[\"labels\",\"image\"]}",
  "{\"authors\":[{\"email\":\"liuhanjin-sc@g.ecc.u-tokyo.ac.jp\",\"name\":\"Hanjin Liu\"}],\"code_repository\":\"https://github.com/hanjinliu/napari-multitask\",\"conda\":[{\"channel\":\"conda-forge\",\"package\":\"napari-multitask\"}],\"description\":\"# napari-multitask\\n\\nMultitasking on napari.\\n\\n![](https://github.com/hanjinliu/napari-multitask/raw/main/Figs/output.gif)\\n\\nLayers and opened dock widgets are stored in the task panels below. Switch your tasks at any time.\\n\\n# Installation\\n\\n```\\npip install napari-multitask\\n```\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-multitask\\nMultitasking on napari.\\n\\nLayers and opened dock widgets are stored in the task panels below. Switch your tasks at any time.\\nInstallation\\npip install napari-multitask\",\"development_status\":[],\"display_name\":\"napari-multitask\",\"documentation\":\"\",\"first_released\":\"2021-12-06T08:07:30.574504Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-multitask\",\"npe2\":false,\"operating_system\":[],\"plugin_types\":[\"widget\"],\"project_site\":\"\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2021-12-13T06:43:24.186430Z\",\"report_issues\":\"\",\"requirements\":[\"magic-class (>=0.5.11)\"],\"summary\":\"Multitasking in napari\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.0.2\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Abigail McGovern\"},{\"name\":\"Juan Nunez-Iglesias\"}],\"category\":{\"Workflow step\":[\"Image Segmentation\"]},\"category_hierarchy\":{\"Workflow step\":[[\"Image Segmentation\"]]},\"code_repository\":\"https://github.com/jni/zarpaint\",\"conda\":[],\"description\":\"# zarpaint\\n\\n[![License](https://img.shields.io/pypi/l/zarpaint.svg?color=green)](https://github.com/jni/zarpaint/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/zarpaint.svg?color=green)](https://pypi.org/project/zarpaint)\\n[![Python Version](https://img.shields.io/pypi/pyversions/zarpaint.svg?color=green)](https://python.org)\\n[![tests](https://github.com/jni/zarpaint/workflows/tests/badge.svg)](https://github.com/jni/zarpaint/actions)\\n[![codecov](https://codecov.io/gh/jni/zarpaint/branch/main/graph/badge.svg)](https://codecov.io/gh/jni/zarpaint)\\n\\nPaint segmentations directly to on-disk/remote zarr arrays\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `zarpaint` via [pip]:\\n\\n    pip install zarpaint\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"zarpaint\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/jni/zarpaint/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\",\"description_content_type\":\"text/markdown\",\"description_text\":\"zarpaint\\n\\n\\n\\n\\n\\nPaint segmentations directly to on-disk/remote zarr arrays\\n\\nThis napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install zarpaint via pip:\\npip install zarpaint\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"zarpaint\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 4 - Beta\"],\"display_name\":\"zarpaint\",\"documentation\":\"\",\"first_released\":\"2021-06-17T00:46:55.252216Z\",\"license\":\"BSD-3-Clause\",\"name\":\"zarpaint\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\",\"widget\"],\"project_site\":\"https://github.com/jni/zarpaint\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[\"*\"],\"release_date\":\"2021-08-10T05:40:33.725904Z\",\"report_issues\":\"\",\"requirements\":null,\"summary\":\"Paint segmentations directly to on-disk/remote zarr arrays\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.2.0\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"andrea1.bassi@polimi.it\",\"name\":\"Andrea Bassi\"}],\"category\":{\"Image modality\":[\"Fluorescence microscopy\"]},\"category_hierarchy\":{\"Image modality\":[[\"Fluorescence microscopy\",\"Fluorescence\"]]},\"code_repository\":\"https://github.com/andreabassi78/napari-psf-simulator\",\"conda\":[],\"description\":\"# napari-psf-simulator\\n\\n[![License](https://img.shields.io/pypi/l/napari-psf-simulator.svg?color=green)](https://github.com/andreabassi78/napari-psf-simulator/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-psf-simulator.svg?color=green)](https://pypi.org/project/napari-psf-simulator)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-psf-simulator.svg?color=green)](https://python.org)\\n[![tests](https://github.com/andreabassi78/napari-psf-simulator/workflows/tests/badge.svg)](https://github.com/andreabassi78/napari-psf-simulator/actions)\\n[![codecov](https://codecov.io/gh/andreabassi78/napari-psf-simulator/branch/main/graph/badge.svg)](https://codecov.io/gh/andreabassi78/napari-psf-simulator)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-psf-simulator)](https://napari-hub.org/plugins/napari-psf-simulator)\\n\\nA plugin for the simulation of the 3D Point Spread Function of an optical systen, particularly a microscope objective.\\n \\nCalculates the PSF as the squared Fourier Transform of the pupil and uses the angular spectrum to propagate the PSF in 3D.  \\nThe following aberrations are included:\\n- phase aberration described by a Zernike polynomials with n-m coefficients\\n- aberration induced by a slab, with a refractive index different from the one at the object (only scalar approximation is used, polarization not considered yet).  \\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/plugins/stable/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-psf-simulator` via [pip]:\\n\\n    pip install napari-psf-simulator\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/andreabassi78/napari-psf-simulator.git\\n\\n\\n## Usage\\n\\n1) Lauch the plugin and select the parameters of the microscope: `NA` (numerical aperture), `wavelenght`, `n` (refractive index at the object),\\n   `Nxy` (number of pixels), `Nz` (number of slices), `dxy` (pixel size, transverse sampling), `dz` (voxel depth, axial sampling)\\n\\n2) Select an aberration type (if needed) and press `Calculate PSF` to run the simulator. This will create a new image layer with the 3D PSF.\\n \\n   The option `Show Airy disk` creates 3 ellipses in a Shapes layer, showing the boundaries of the diffraction limited blob.\\n\\n![raw](https://github.com/andreabassi78/napari-psf-simulator/raw/main/images/figure.png)\\n**Napari viewer with the psf-simulator widget showing the in-focus plane of an aberrated PSF**\\n\\n![raw](https://github.com/andreabassi78/napari-psf-simulator/raw/main/images/animation.gif)\\n**Slicing through a PSF aberrated with Zernike polynomials of order N=3, M=1 (coma)**\\n\\n3) Click on the `Plot PSF Profile in Console` checkbox to see the x and z profiles of the PSF.\\n   They will show up in  the viewer console when `Calculate PSF` is executed.\\n\\n![raw](https://github.com/andreabassi78/napari-psf-simulator/raw/main/images/Plot.png)\\n**Plot profile of the PSF, shown in the Console**\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request. \\nThe plugin has been concived to be modular allowing the insertion of new aberations and pupils. Please contact the developers on github for adding new propagations and aberrations types. \\nAny suggestions or contributions are welcome.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-psf-simulator\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/andreabassi78/napari-psf-simulator/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-psf-simulator\\n\\n\\n\\n\\n\\n\\nA plugin for the simulation of the 3D Point Spread Function of an optical systen, particularly a microscope objective.\\nCalculates the PSF as the squared Fourier Transform of the pupil and uses the angular spectrum to propagate the PSF in 3D.\\nThe following aberrations are included:\\n- phase aberration described by a Zernike polynomials with n-m coefficients\\n- aberration induced by a slab, with a refractive index different from the one at the object (only scalar approximation is used, polarization not considered yet).  \\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-psf-simulator via pip:\\npip install napari-psf-simulator\\n\\nTo install latest development version :\\npip install git+https://github.com/andreabassi78/napari-psf-simulator.git\\n\\nUsage\\n1) Lauch the plugin and select the parameters of the microscope: NA (numerical aperture), wavelenght, n (refractive index at the object),\\n   Nxy (number of pixels), Nz (number of slices), dxy (pixel size, transverse sampling), dz (voxel depth, axial sampling)\\n2) Select an aberration type (if needed) and press Calculate PSF to run the simulator. This will create a new image layer with the 3D PSF.\\nThe option Show Airy disk creates 3 ellipses in a Shapes layer, showing the boundaries of the diffraction limited blob.\\n\\nNapari viewer with the psf-simulator widget showing the in-focus plane of an aberrated PSF\\n\\nSlicing through a PSF aberrated with Zernike polynomials of order N=3, M=1 (coma)\\n3) Click on the Plot PSF Profile in Console checkbox to see the x and z profiles of the PSF.\\n   They will show up in  the viewer console when Calculate PSF is executed.\\n\\nPlot profile of the PSF, shown in the Console\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request. \\nThe plugin has been concived to be modular allowing the insertion of new aberations and pupils. Please contact the developers on github for adding new propagations and aberrations types. \\nAny suggestions or contributions are welcome.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-psf-simulator\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"PSF simulator\",\"documentation\":\"https://github.com/andreabassi78/napari-psf-simulator#README.md\",\"first_released\":\"2022-04-14T20:30:28.449098Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-psf-simulator\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/andreabassi78/napari-psf-simulator\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-06-08T21:52:42.447846Z\",\"report_issues\":\"https://github.com/andreabassi78/napari-psf-simulator/issues\",\"requirements\":[\"numpy\",\"magicgui\",\"qtpy\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\"],\"summary\":\"A plugin for simulations of the Point Spread Function, with aberrations\",\"support\":\"https://github.com/andreabassi78/napari-psf-simulator/issues\",\"twitter\":\"\",\"version\":\"0.1.5\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Jordao Bragantini\"}],\"code_repository\":\"https://github.com/royerlab/napari-umap\",\"description\":\"# napari-umap\\n\\n[![License BSD-3](https://img.shields.io/pypi/l/napari-umap.svg?color=green)](https://github.com/royerlab/napari-umap/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-umap.svg?color=green)](https://pypi.org/project/napari-umap)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-umap.svg?color=green)](https://python.org)\\n[![tests](https://github.com/royerlab/napari-umap/workflows/tests/badge.svg)](https://github.com/royerlab/napari-umap/actions)\\n[![codecov](https://codecov.io/gh/royerlab/napari-umap/branch/main/graph/badge.svg)](https://codecov.io/gh/royerlab/napari-umap)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-umap)](https://napari-hub.org/plugins/napari-umap)\\n\\nA simple plugin to use with napari\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/stable/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-umap` via [pip]:\\n\\n    pip install napari-umap\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/royerlab/napari-umap.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-umap\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/royerlab/napari-umap/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-umap\\n\\n\\n\\n\\n\\n\\nA simple plugin to use with napari\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-umap via pip:\\npip install napari-umap\\n\\nTo install latest development version :\\npip install git+https://github.com/royerlab/napari-umap.git\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-umap\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"UMAP\",\"documentation\":\"https://github.com/royerlab/napari-umap#README.md\",\"first_released\":\"2022-11-01T11:50:17.650265Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-umap\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/royerlab/napari-umap\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[],\"release_date\":\"2022-11-01T11:50:17.650265Z\",\"report_issues\":\"https://github.com/royerlab/napari-umap/issues\",\"requirements\":[\"numpy\",\"magicgui\",\"qtpy\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\",\"pytest-qt ; extra == 'testing'\",\"napari ; extra == 'testing'\",\"pyqt5 ; extra == 'testing'\"],\"summary\":\"A simple plugin to use with napari\",\"support\":\"https://github.com/royerlab/napari-umap/issues\",\"twitter\":\"\",\"version\":\"0.0.1\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"email\":\"h.kawai888@gmail.com\",\"name\":\"Hiroki Kawai\"}],\"code_repository\":\"https://github.com/neurobiology-ut/PHILOW\",\"conda\":[],\"description\":\"# napari-PHILOW\\n\\n[![License](https://img.shields.io/pypi/l/napari-PHILOW.svg?color=green)](https://github.com/neurobiology-ut/PHILOW/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-PHILOW.svg?color=green)](https://pypi.org/project/napari-PHILOW)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-PHILOW.svg?color=green)](https://python.org)\\n[![tests](https://github.com/neurobiology-ut/napari-PHILOW/workflows/tests/badge.svg)](https://github.com/neurobiology-ut/PHILOW/actions)\\n[![codecov](https://codecov.io/gh/neurobiology-ut/napari-PHILOW/branch/main/graph/badge.svg)](https://codecov.io/gh/neurobiology-ut/PHILOW)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-PHILOW)](https://napari-hub.org/plugins/napari-PHILOW)\\n\\n# PHILOW <br>\\n***P***ython-based platform for ***h***uman-***i***n-the-***lo***op (HITL)  ***w***orkflow (PHILOW) <br>\\n\\nPHILOW is an interactive deep learning-based platform for 3D datasets implemented on top of [napari](https://github.com/napari/napari)\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/plugins/stable/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-PHILOW` via [pip]:\\n\\n    pip install napari-PHILOW\\n    \\nor clone this repository   \\nthen\\n```angular2\\ncd PHILOW\\npip install -e .\\n```\\n    \\n\\n## Usage\\n\\nLaunch napari \\n\\n```angular2\\nnapari\\n```\\n\\n\\n#### load dataset\\n\\n\\n1) Plugins > napari-PHILOW > Annotation Mode\\n\\n2) Select original dir : all slices must be in separate PNG and must be sequentially numbered (e.g. 000.png, 001.png ...)\\n\\n3) Select mask dir : To resume from the middle of the annotation, specify here the name of the directory containing the mask image. The directory must contain the same number of files with the same name as the original image.   \\n If you are starting a completely new annotation, you do not need to specify a directory. The directory for mask is automatically created and blank images are generated and stored.\\n\\n4) Enter a name for the label or model you want to create (e.g. mito, cristae, ...)   \\nThis name will be used as the directory name of the newly created mask dir if no mask dir is specified, \\nand as the name of the csv file for training dataset management.\\n\\n5) Check if you want to create new dataset (new model)\\nWhen checked, if there is already a csv file for training dataset management, a new csv file with one sequential number will be generated.\\n\\n6) Start tracing\\n\\n\\n#### create labels\\nCreate a label with the brush function.\\nmore information → https://napari.org/tutorials/fundamentals/labels.html\\n\\n#### Orthogonal view\\nIf you want to see orthogonal view, click on the location you want to see while holding down the Shift button.    \\nThe image from xy, yz, and zx will be displayed on the right side of the screen.\\n\\n#### Low confident layer\\nIf you are in the second iteration and you are loading the prediction results, you will see a low confidence layer.    \\nThis shows the area where the confidence of the prediction result is low.    \\nUse this as a reference for correction.   \\n\\n#### Small object layer\\nWe provide a small object layer to find small painted areas.   \\nThis is a layer for displaying small objects.   \\nThe slider widget on the left allows you to change the maximum object size to be displayed.   \\n\\n#### save labels\\nIf you want to save your label, click the \\\"save\\\" button on the bottom right.\\n\\n#### select training dataset\\nWe are providing a way to manage the dataset for use in training.   \\nIf you want to use the currently displayed slice as your training data, click the 'Not Checked' button near the center left to display 'Checked'.\\n\\n\\n### Train and pred with your gpu machine\\n#### Train\\nTo train on your GPU machine (or with CPU), \\n\\n1) Plugins > napari-PHILOW > Trainer\\n   \\n2) Select original dir : all slices must be in separate PNG and must be sequentially numbered (e.g. 000.png, 001.png ...)   \\n   \\n3) Select labels dir : all label images should be named same as original images and contains data management csv file   \\n   \\n4) Select dir for save trained model   \\n   \\n5) Click on the \\\"start training\\\" button   \\n\\n6) Dice score and dice loss are displayed. For more detail, check the command line for the progress of training. If you want to stop in the middle, click stop button.   \\n   \\n#### Predict\\nTo predict labels on your machine,  \\n\\n1) Plugins > napari-PHILOW > Predicter\\n   \\n2) Select original dir : all slices must be in separate PNG and must be sequentially numbered (e.g. 000.png, 001.png ...)   \\n   \\n3) (Optional) Select labels dir if you want to keep labels witch were used on training, and data management csv file   \\n   \\n4) Select model dir contains hdf5 file   \\n   \\n5) Select output dir for predicted labels   \\n\\n6) Uncheck the box if you DO NOT want to use TAP (Three-Axis-Prediction)   \\n   \\n7) Click on the \\\"predict\\\" button  \\n\\n8) Check the command line for the progress of prediction. If you want to stop in the middle, use ctrl+C.   \\n\\n9) You can start the next round of annotation by selecting the merged_prediction directory as the mask dir in Annotation mode.\\n\\n### Train and predict with Google Colab   \\nIf you don't have a GPU machine, you can use Google Colab to perform GPU-based training and prediction for free.    \\n\\n1) Open [train and predict notebook](https://github.com/neurobiology-ut/PHILOW/blob/develop/notebooks/train_and_pred_using_PHILOW.ipynb) and click \\\"Open in Colab\\\" button\\n\\n2) You can upload your own dataset to train and predict, or try it on demo data   \\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [GNU GPL v3.0] license,\\n\\\"napari-PHILOW\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n# Authors <br>\\n\\nShogo Suga <br>\\nHiroki Kawai <br>\\n<a href=\\\"http://park.itc.u-tokyo.ac.jp/Hirabayashi/WordPress/\\\">Yusuke Hirabayashi</a> \\n\\n\\n# How to Cite <br>\\nShogo Suga, Koki Nakamura, Bruno M Humbel, Hiroki Kawai, Yusuke Hirabayashi, An interactive deep learning-based approach reveals mitochondrial cristae topologies\\n<a href=\\\"https://doi.org/10.1101/2021.06.11.448083\\\">https://doi.org/10.1101/2021.06.11.448083</a>\\n\\n\\n```\\n@article {Suga2021.06.11.448083,\\n\\tauthor = {Suga, Shogo and Nakamura, Koki and Humbel, Bruno M and Kawai, Hiroki and Hirabayashi, Yusuke},\\n\\ttitle = {An interactive deep learning-based approach reveals mitochondrial cristae topologies},\\n\\telocation-id = {2021.06.11.448083},\\n\\tyear = {2021},\\n\\tdoi = {10.1101/2021.06.11.448083},\\n\\tpublisher = {Cold Spring Harbor Laboratory},\\n\\tURL = {https://www.biorxiv.org/content/early/2021/06/11/2021.06.11.448083},\\n\\teprint = {https://www.biorxiv.org/content/early/2021/06/11/2021.06.11.448083.full.pdf},\\n\\tjournal = {bioRxiv}\\n}\\n```\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-PHILOW\\n\\n\\n\\n\\n\\n\\nPHILOW \\nPython-based platform for human-in-the-loop (HITL)  workflow (PHILOW) \\nPHILOW is an interactive deep learning-based platform for 3D datasets implemented on top of napari\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-PHILOW via pip:\\npip install napari-PHILOW\\n\\nor clone this repository \\nthen\\nangular2\\ncd PHILOW\\npip install -e .\\nUsage\\nLaunch napari \\nangular2\\nnapari\\nload dataset\\n1) Plugins > napari-PHILOW > Annotation Mode\\n2) Select original dir : all slices must be in separate PNG and must be sequentially numbered (e.g. 000.png, 001.png ...)\\n3) Select mask dir : To resume from the middle of the annotation, specify here the name of the directory containing the mask image. The directory must contain the same number of files with the same name as the original image. \\n If you are starting a completely new annotation, you do not need to specify a directory. The directory for mask is automatically created and blank images are generated and stored.\\n4) Enter a name for the label or model you want to create (e.g. mito, cristae, ...) \\nThis name will be used as the directory name of the newly created mask dir if no mask dir is specified, \\nand as the name of the csv file for training dataset management.\\n5) Check if you want to create new dataset (new model)\\nWhen checked, if there is already a csv file for training dataset management, a new csv file with one sequential number will be generated.\\n6) Start tracing\\ncreate labels\\nCreate a label with the brush function.\\nmore information → https://napari.org/tutorials/fundamentals/labels.html\\nOrthogonal view\\nIf you want to see orthogonal view, click on the location you want to see while holding down the Shift button.  \\nThe image from xy, yz, and zx will be displayed on the right side of the screen.\\nLow confident layer\\nIf you are in the second iteration and you are loading the prediction results, you will see a low confidence layer.  \\nThis shows the area where the confidence of the prediction result is low.  \\nUse this as a reference for correction.   \\nSmall object layer\\nWe provide a small object layer to find small painted areas. \\nThis is a layer for displaying small objects. \\nThe slider widget on the left allows you to change the maximum object size to be displayed.   \\nsave labels\\nIf you want to save your label, click the \\\"save\\\" button on the bottom right.\\nselect training dataset\\nWe are providing a way to manage the dataset for use in training. \\nIf you want to use the currently displayed slice as your training data, click the 'Not Checked' button near the center left to display 'Checked'.\\nTrain and pred with your gpu machine\\nTrain\\nTo train on your GPU machine (or with CPU), \\n1) Plugins > napari-PHILOW > Trainer\\n2) Select original dir : all slices must be in separate PNG and must be sequentially numbered (e.g. 000.png, 001.png ...)   \\n3) Select labels dir : all label images should be named same as original images and contains data management csv file   \\n4) Select dir for save trained model   \\n5) Click on the \\\"start training\\\" button   \\n6) Dice score and dice loss are displayed. For more detail, check the command line for the progress of training. If you want to stop in the middle, click stop button.   \\nPredict\\nTo predict labels on your machine,  \\n1) Plugins > napari-PHILOW > Predicter\\n2) Select original dir : all slices must be in separate PNG and must be sequentially numbered (e.g. 000.png, 001.png ...)   \\n3) (Optional) Select labels dir if you want to keep labels witch were used on training, and data management csv file   \\n4) Select model dir contains hdf5 file   \\n5) Select output dir for predicted labels   \\n6) Uncheck the box if you DO NOT want to use TAP (Three-Axis-Prediction)   \\n7) Click on the \\\"predict\\\" button  \\n8) Check the command line for the progress of prediction. If you want to stop in the middle, use ctrl+C.   \\n9) You can start the next round of annotation by selecting the merged_prediction directory as the mask dir in Annotation mode.\\nTrain and predict with Google Colab\\nIf you don't have a GPU machine, you can use Google Colab to perform GPU-based training and prediction for free.    \\n1) Open train and predict notebook and click \\\"Open in Colab\\\" button\\n2) You can upload your own dataset to train and predict, or try it on demo data   \\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the GNU GPL v3.0 license,\\n\\\"napari-PHILOW\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\nAuthors \\nShogo Suga \\nHiroki Kawai \\nYusuke Hirabayashi \\nHow to Cite \\nShogo Suga, Koki Nakamura, Bruno M Humbel, Hiroki Kawai, Yusuke Hirabayashi, An interactive deep learning-based approach reveals mitochondrial cristae topologies\\nhttps://doi.org/10.1101/2021.06.11.448083\\n@article {Suga2021.06.11.448083,\\n    author = {Suga, Shogo and Nakamura, Koki and Humbel, Bruno M and Kawai, Hiroki and Hirabayashi, Yusuke},\\n    title = {An interactive deep learning-based approach reveals mitochondrial cristae topologies},\\n    elocation-id = {2021.06.11.448083},\\n    year = {2021},\\n    doi = {10.1101/2021.06.11.448083},\\n    publisher = {Cold Spring Harbor Laboratory},\\n    URL = {https://www.biorxiv.org/content/early/2021/06/11/2021.06.11.448083},\\n    eprint = {https://www.biorxiv.org/content/early/2021/06/11/2021.06.11.448083.full.pdf},\\n    journal = {bioRxiv}\\n}\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari-PHILOW\",\"documentation\":\"\",\"first_released\":\"2022-05-02T06:05:33.653275Z\",\"license\":\"GPL-3.0\",\"name\":\"napari-PHILOW\",\"npe2\":false,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"widget\"],\"project_site\":\"https://github.com/neurobiology-ut/PHILOW\",\"python_version\":\">=3.7\",\"reader_file_extensions\":[],\"release_date\":\"2022-05-02T06:05:33.653275Z\",\"report_issues\":\"\",\"requirements\":[\"napari-plugin-engine (>=0.1.4)\",\"tensorflow\",\"numpy\",\"scikit-image\",\"dask-image\",\"opencv-python\",\"matplotlib\",\"napari-tools-menu\",\"pandas\"],\"summary\":\"PHILOW is an interactive deep learning-based platform for 3D datasets\",\"support\":\"\",\"twitter\":\"\",\"version\":\"0.0.1\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}",
  "{\"authors\":[{\"name\":\"Ruben Lopez\"}],\"code_repository\":\"https://github.com/rjlopez2/napari-mat-file-reader\",\"description\":\"# napari-mat-file-reader\\n\\n[![License BSD-3](https://img.shields.io/pypi/l/napari-mat-file-reader.svg?color=green)](https://github.com/rjlopez2/napari-mat-file-reader/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-mat-file-reader.svg?color=green)](https://pypi.org/project/napari-mat-file-reader)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-mat-file-reader.svg?color=green)](https://python.org)\\n[![tests](https://github.com/rjlopez2/napari-mat-file-reader/workflows/tests/badge.svg)](https://github.com/rjlopez2/napari-mat-file-reader/actions)\\n[![codecov](https://codecov.io/gh/rjlopez2/napari-mat-file-reader/branch/main/graph/badge.svg)](https://codecov.io/gh/rjlopez2/napari-mat-file-reader)\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-mat-file-reader)](https://napari-hub.org/plugins/napari-mat-file-reader)\\n\\nThis is a simple wraper to read .mat files from Matlab\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon't miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/stable/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-mat-file-reader` via [pip]:\\n\\n    pip install napari-mat-file-reader\\n\\n\\n\\nTo install latest development version :\\n\\n    pip install git+https://github.com/rjlopez2/napari-mat-file-reader.git\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\\"napari-mat-file-reader\\\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/rjlopez2/napari-mat-file-reader/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\",\"description_content_type\":\"text/markdown\",\"description_text\":\"napari-mat-file-reader\\n\\n\\n\\n\\n\\n\\nThis is a simple wraper to read .mat files from Matlab\\n\\nThis napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.\\n\\nInstallation\\nYou can install napari-mat-file-reader via pip:\\npip install napari-mat-file-reader\\n\\nTo install latest development version :\\npip install git+https://github.com/rjlopez2/napari-mat-file-reader.git\\n\\nContributing\\nContributions are very welcome. Tests can be run with tox, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the BSD-3 license,\\n\\\"napari-mat-file-reader\\\" is free and open source software\\nIssues\\nIf you encounter any problems, please file an issue along with a detailed description.\",\"development_status\":[\"Development Status :: 2 - Pre-Alpha\"],\"display_name\":\"napari mat file reader\",\"documentation\":\"https://github.com/rjlopez2/napari-mat-file-reader#README.md\",\"first_released\":\"2022-11-01T22:14:35.861855Z\",\"license\":\"BSD-3-Clause\",\"name\":\"napari-mat-file-reader\",\"npe2\":true,\"operating_system\":[\"Operating System :: OS Independent\"],\"plugin_types\":[\"reader\",\"sample_data\"],\"project_site\":\"https://github.com/rjlopez2/napari-mat-file-reader\",\"python_version\":\">=3.8\",\"reader_file_extensions\":[\"*.mat\"],\"release_date\":\"2022-11-01T22:14:35.861855Z\",\"report_issues\":\"https://github.com/rjlopez2/napari-mat-file-reader/issues\",\"requirements\":[\"numpy\",\"mat73\",\"tox ; extra == 'testing'\",\"pytest ; extra == 'testing'\",\"pytest-cov ; extra == 'testing'\"],\"summary\":\"This is a simple wraper to read .mat files from Matlab\",\"support\":\"https://github.com/rjlopez2/napari-mat-file-reader/issues\",\"twitter\":\"\",\"version\":\"0.0.2\",\"visibility\":\"public\",\"writer_file_extensions\":[],\"writer_save_layers\":[]}"
]

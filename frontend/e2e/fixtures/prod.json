[
  {
    "authors": [],
    "category": { "Workflow step": ["Image registration"] },
    "category_hierarchy": {
      "Workflow step": [
        ["Image registration"],
        ["Image registration", "Affine registration"]
      ]
    },
    "code_repository": "https://github.com/brainglobe/brainreg-napari",
    "description": "[![Python Version](https://img.shields.io/pypi/pyversions/brainreg-napari.svg)](https://pypi.org/project/brainreg-napari) [![PyPI](https://img.shields.io/pypi/v/brainreg-napari.svg)](https://pypi.org/project/brainreg-napari) [![Wheel](https://img.shields.io/pypi/wheel/brainreg-napari.svg)](https://pypi.org/project/brainreg-napari) [![Development Status](https://img.shields.io/pypi/status/brainreg-napari.svg)](https://github.com/brainglobe/brainreg-napari) [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/python/black) [![codecov](https://codecov.io/gh/brainglobe/brainreg-napari/branch/master/graph/badge.svg?token=HEBXJPLD2S)](https://codecov.io/gh/brainglobe/brainreg-napari)  # brainreg-napari Napari plugin to run [brainreg](https://github.com/brainglobe/brainreg), developed by [Stephen Lenzi](https://github.com/stephenlenzi).  ## Installation ```bash pip install brainreg-napari ```  ## Usage Documentation for the plugin can be found [here](https://docs.brainglobe.info/brainreg-napari/introduction), and for more detail, documentation for the original brainreg can be found [here](https://docs.brainglobe.info/brainreg/introduction) and a tutorial is [here](https://docs.brainglobe.info/brainreg/tutorial).  For segmentation of bulk structures in 3D space (e.g. injection sites, Neuropixels probes), please see [brainreg-segment](https://github.com/brainglobe/brainreg-segment).  This software is at a very early stage, and was written with our data in mind. Over time we hope to support other data types/formats. If you have any issues, please get in touch [on the forum](https://forum.image.sc/tag/brainglobe) or by [raising an issue](https://github.com/brainglobe/brainreg/issues).  ## Details brainreg is an update to [amap](https://github.com/SainsburyWellcomeCentre/amap-python) (itself a port of the [original Java software](https://www.nature.com/articles/ncomms11879)) to include multiple registration backends, and to support the many atlases provided by [bg-atlasapi](https://github.com/brainglobe/bg-atlasapi).  The aim of brainreg is to register the template brain  (e.g. from the [Allen Reference Atlas](https://mouse.brain-map.org/static/atlas))   to the sample image. Once this is complete, any other image in the template   space can be aligned with the sample (such as region annotations, for   segmentation of the sample image). The template to sample transformation   can also be inverted, allowing sample images to be aligned in a common   coordinate space.  To do this, the template and sample images are filtered, and then registered in a three step process (reorientation, affine registration, and freeform registration.) The resulting transform from template to standard space is then applied to the atlas.  Full details of the process are in the [original aMAP paper](https://www.nature.com/articles/ncomms11879). ![reg_process](https://user-images.githubusercontent.com/13147259/143553945-a046e918-7614-4211-814c-fc840bb0159d.png) **Overview of the registration process**  ## Contributing Contributions to brainreg-napari are more than welcome. Please see the [contributing guide](https://github.com/brainglobe/.github/blob/main/CONTRIBUTING.md).  ### Citing brainreg  If you find brainreg useful, and use it in your research, please let us know and also cite the paper:  > Tyson, A. L., V&eacute;lez-Fort, M.,  Rousseau, C. V., Cossell, L., Tsitoura, C., Lenzi, S. C., Obenhaus, H. A., Claudi, F., Branco, T.,  Margrie, T. W. (2022). Accurate determination of marker location within whole-brain microscopy images. Scientific Reports, 12, 867 [doi.org/10.1038/s41598-021-04676-9](https://doi.org/10.1038/s41598-021-04676-9)  Please also cite aMAP (the original pipeline from which this software is based):  >Niedworok, C.J., Brown, A.P.Y., Jorge Cardoso, M., Osten, P., Ourselin, S., Modat, M. and Margrie, T.W., (2016). AMAP is a validated pipeline for registration and segmentation of high-resolution mouse brain data. Nature Communications. 7, 1–9. https://doi.org/10.1038/ncomms11879  Lastly, if you can, please cite the BrainGlobe Atlas API that provided the atlas:  >Claudi, F., Petrucco, L., Tyson, A. L., Branco, T., Margrie, T. W. and Portugues, R. (2020). BrainGlobe Atlas API: a common interface for neuroanatomical atlases. Journal of Open Source Software, 5(54), 2668, https://doi.org/10.21105/joss.02668  **Don't forget to cite the developers of the atlas that you used (e.g. the Allen Brain Atlas)!**   --- The BrainGlobe project is generously supported by the Sainsbury Wellcome Centre and the Institute of Neuroscience, Technical University of Munich, with funding from Wellcome, the Gatsby Charitable Foundation and the Munich Cluster for Systems Neurology - Synergy.  <img src='https://brainglobe.info/images/logos_combined.png' width=\"550\"> ",
    "description_content_type": "text/markdown",
    "description_text": "      brainreg-napari Napari plugin to run brainreg, developed by Stephen Lenzi. Installation bash pip install brainreg-napari Usage Documentation for the plugin can be found here, and for more detail, documentation for the original brainreg can be found here and a tutorial is here. For segmentation of bulk structures in 3D space (e.g. injection sites, Neuropixels probes), please see brainreg-segment. This software is at a very early stage, and was written with our data in mind. Over time we hope to support other data types/formats. If you have any issues, please get in touch on the forum or by raising an issue. Details brainreg is an update to amap (itself a port of the original Java software) to include multiple registration backends, and to support the many atlases provided by bg-atlasapi. The aim of brainreg is to register the template brain  (e.g. from the Allen Reference Atlas)   to the sample image. Once this is complete, any other image in the template   space can be aligned with the sample (such as region annotations, for   segmentation of the sample image). The template to sample transformation   can also be inverted, allowing sample images to be aligned in a common   coordinate space. To do this, the template and sample images are filtered, and then registered in a three step process (reorientation, affine registration, and freeform registration.) The resulting transform from template to standard space is then applied to the atlas. Full details of the process are in the original aMAP paper.  Overview of the registration process Contributing Contributions to brainreg-napari are more than welcome. Please see the contributing guide. Citing brainreg If you find brainreg useful, and use it in your research, please let us know and also cite the paper:  Tyson, A. L., Vélez-Fort, M.,  Rousseau, C. V., Cossell, L., Tsitoura, C., Lenzi, S. C., Obenhaus, H. A., Claudi, F., Branco, T.,  Margrie, T. W. (2022). Accurate determination of marker location within whole-brain microscopy images. Scientific Reports, 12, 867 doi.org/10.1038/s41598-021-04676-9  Please also cite aMAP (the original pipeline from which this software is based):  Niedworok, C.J., Brown, A.P.Y., Jorge Cardoso, M., Osten, P., Ourselin, S., Modat, M. and Margrie, T.W., (2016). AMAP is a validated pipeline for registration and segmentation of high-resolution mouse brain data. Nature Communications. 7, 1–9. https://doi.org/10.1038/ncomms11879  Lastly, if you can, please cite the BrainGlobe Atlas API that provided the atlas:  Claudi, F., Petrucco, L., Tyson, A. L., Branco, T., Margrie, T. W. and Portugues, R. (2020). BrainGlobe Atlas API: a common interface for neuroanatomical atlases. Journal of Open Source Software, 5(54), 2668, https://doi.org/10.21105/joss.02668  Don't forget to cite the developers of the atlas that you used (e.g. the Allen Brain Atlas)!  The BrainGlobe project is generously supported by the Sainsbury Wellcome Centre and the Institute of Neuroscience, Technical University of Munich, with funding from Wellcome, the Gatsby Charitable Foundation and the Munich Cluster for Systems Neurology - Synergy. ",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "brainreg-napari",
    "documentation": "",
    "first_released": "2021-07-13T17:35:34.578786Z",
    "license": "MIT",
    "name": "brainreg-napari",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget", "sample_data"],
    "project_site": "",
    "python_version": ">=3.8.0",
    "reader_file_extensions": [],
    "release_date": "2022-12-06T11:08:38.449213Z",
    "report_issues": "",
    "requirements": [
      "napari",
      "napari-plugin-engine (>=0.1.4)",
      "magicgui",
      "qtpy",
      "brainglobe-napari-io",
      "brainreg",
      "brainreg-segment",
      "pooch (>1)",
      "black ; extra == 'dev'",
      "pytest-cov ; extra == 'dev'",
      "pytest ; extra == 'dev'",
      "pytest-qt ; extra == 'dev'",
      "napari ; extra == 'dev'",
      "pyqt5 ; extra == 'dev'",
      "tox ; extra == 'dev'",
      "gitpython ; extra == 'dev'",
      "coverage (>=5.0.3) ; extra == 'dev'",
      "bump2version ; extra == 'dev'",
      "pre-commit ; extra == 'dev'",
      "flake8 ; extra == 'dev'",
      "check-manifest ; extra == 'dev'",
      "setuptools-scm ; extra == 'dev'"
    ],
    "summary": "Multi-atlas whole-brain microscopy registration",
    "support": "",
    "twitter": "",
    "version": "0.1.1",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Graham Dellaire" }, { "name": "Robert Haase" }],
    "code_repository": "https://github.com/gdellaire/bbii-decon",
    "conda": [{ "channel": "conda-forge", "package": "bbii-decon" }],
    "description": "# BBii-Decon  [![License](https://img.shields.io/pypi/l/bbii-decon.svg?color=green)](https://github.com/gdellaire/bbii-decon/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/bbii-decon.svg?color=green)](https://pypi.org/project/bbii-decon) [![Python Version](https://img.shields.io/pypi/pyversions/bbii-decon.svg?color=green)](https://python.org) [![tests](https://github.com/gdellaire/bbii-decon/workflows/tests/badge.svg)](https://github.com/gdellaire/bbii-decon/actions) [![codecov](https://codecov.io/gh/gdellaire/bbii-decon/branch/main/graph/badge.svg)](https://codecov.io/gh/gdellaire/bbii-decon) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/bbii-decon)](https://napari-hub.org/plugins/bbii-decon)  Projected Barzilai-Borwein Image Deconvolution with Infeasible Iterates (BBii-Decon)   The projected Barzilai-Borwein method of image deconvolution utilizing infeasible iterates (BBii-Decon), utilizes Barzilai-Borwein (BB) or projected BB (PBB) method and enforces a nonnegativity constraint, but allows for infeasible iterates between projections. This algorithm (BBii) results in faster convergence than the basic PBB method, while achieving better quality images, with reduced background than the unconstrained BB method (1).   The code represented is based on the original BBii algorithm written in MatLab by Kathleen Fraser and Dirk Arnold, which was ported to python 3.8 by Graham Dellaire, Dirk Arnold and Kathleen Fraser for non-commercial use.  The first implementation shown here is for 2D deconvolution using a known 2D PSF of 256 X 256 pixels, and images of at least 256 pixels in one dimension. One file implements just the deconvolution of a blurred image, while the second file contains a modification of the BBii-Decon algorithm that has a built in heuristic for measuring image reconstruction error relative to a ground truth image. For general 2D deconvolution, either a theoretical 2D PSF (if you know the optical properties of your system) or the central in focus image of a fluorescent bead taken with the same imaging setup (lens, magnification, camera) can produce a suitable PSF.  ### GPU-acceleration  For most 2D deconvolution, optimal results are obtained with 10 iterations of the algorithm. However, if processing takes too long, acceleration using graphics processing units (GPUs) may make sense, especially for processing larger images with >10 iterations or 3D images. (Note: At this time BBii-Decon is optimized for 2D deconvolution, with a 3D implementation planned in future).   This plugin supports accelerated processing using the [cupy](https://cupy.dev) library. To make use of it, please follow  [the instructions](https://docs.cupy.dev/en/stable/install.html#installing-cupy-from-conda-forge) to install cupy.  Installation may look like this: ``` conda create --name cupy_p38 python=3.8 conda activate cupy_p38 conda install -c conda-forge cupy cudatoolkit=10.2 ```  If cupy installation worked out, you will find another checkbox in the user interface. By activating it, processing  should become faster by factor 5-10, depending on processed image data and use GPU hardware.  ![img.png](https://github.com/gdellaire/BBii-Decon/raw/main/demo/use_GPU_checkbox.png)  ## Usage - napari  You can use the BBii deconvolution from within napari by clicking the menu `Plugins > bbii-decon > bbii deconvolution`.  In the dialog, select the PSF, the image to process (a) and click on `Run`. After a moment, the deconvolved image (b)  will show up.  ![img.png](https://github.com/gdellaire/BBii-Decon/raw/main/demo/screenshot_napari.png)  ## Usage from python  You can also call the function from python. There is a full working example in [this notebook](demo/BBii_Decon_2D_2021.ipynb).  ``` from bbii_decon import bbii  bbii(PSF, image, number_of_iterations = 15, tau = 1.0e-08, rho = 0.98) ```   ## Citation 1) [Kathleen Fraser, Dirk V. Arnold, and Graham Dellaire (2014). Projected Barzilai-Borwein method with infeasible iterates for nonnegative least-squares image deblurring. In Proceedings of the Eleventh Conference on Computer and Robot Vision (CRV 2014), Montreal, Canada, pp. 189--194.](https://ieeexplore.ieee.org/abstract/document/6816842)  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/plugins/stable/index.html -->  ## Installation  You can install `bbii-decon` via [pip]:      pip install bbii-decon   ## Installation for developers  Clone the github repository:  ``` conda install git  git clone https://github.com/gdellaire/BBii-Decon.git  cd BBii-Decon  pip install -e . ```  ## Deployment to pypi  For deploying the plugin to the python package index (pypi), one needs a [pypi user account](https://pypi.org/account/register/)  first. For deploying the plugin to pypi, one needs to install some tools:  ``` python -m pip install --user --upgrade setuptools wheel python -m pip install --user --upgrade twine ```  The following command allows us to package the souce code as a python wheel. Make sure that the 'dist' and 'build' folders are deleted before doing this:  ``` python setup.py sdist bdist_wheel ```  This command ships the just generated to pypi:  ``` python -m twine upload --repository pypi dist/* ```  [Read more about distributing your python package via pypi](https://realpython.com/pypi-publish-python-package/#publishing-to-pypi).   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"bbii-decon\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/gdellaire/bbii-decon/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "BBii-Decon       Projected Barzilai-Borwein Image Deconvolution with Infeasible Iterates (BBii-Decon) The projected Barzilai-Borwein method of image deconvolution utilizing infeasible iterates (BBii-Decon), utilizes Barzilai-Borwein (BB) or projected BB (PBB) method and enforces a nonnegativity constraint, but allows for infeasible iterates between projections. This algorithm (BBii) results in faster convergence than the basic PBB method, while achieving better quality images, with reduced background than the unconstrained BB method (1).  The code represented is based on the original BBii algorithm written in MatLab by Kathleen Fraser and Dirk Arnold, which was ported to python 3.8 by Graham Dellaire, Dirk Arnold and Kathleen Fraser for non-commercial use. The first implementation shown here is for 2D deconvolution using a known 2D PSF of 256 X 256 pixels, and images of at least 256 pixels in one dimension. One file implements just the deconvolution of a blurred image, while the second file contains a modification of the BBii-Decon algorithm that has a built in heuristic for measuring image reconstruction error relative to a ground truth image. For general 2D deconvolution, either a theoretical 2D PSF (if you know the optical properties of your system) or the central in focus image of a fluorescent bead taken with the same imaging setup (lens, magnification, camera) can produce a suitable PSF. GPU-acceleration For most 2D deconvolution, optimal results are obtained with 10 iterations of the algorithm. However, if processing takes too long, acceleration using graphics processing units (GPUs) may make sense, especially for processing larger images with >10 iterations or 3D images. (Note: At this time BBii-Decon is optimized for 2D deconvolution, with a 3D implementation planned in future).  This plugin supports accelerated processing using the cupy library. To make use of it, please follow  the instructions to install cupy.  Installation may look like this: conda create --name cupy_p38 python=3.8 conda activate cupy_p38 conda install -c conda-forge cupy cudatoolkit=10.2 If cupy installation worked out, you will find another checkbox in the user interface. By activating it, processing  should become faster by factor 5-10, depending on processed image data and use GPU hardware.  Usage - napari You can use the BBii deconvolution from within napari by clicking the menu Plugins > bbii-decon > bbii deconvolution.  In the dialog, select the PSF, the image to process (a) and click on Run. After a moment, the deconvolved image (b)  will show up.  Usage from python You can also call the function from python. There is a full working example in this notebook. ``` from bbii_decon import bbii bbii(PSF, image, number_of_iterations = 15, tau = 1.0e-08, rho = 0.98) ``` Citation 1) Kathleen Fraser, Dirk V. Arnold, and Graham Dellaire (2014). Projected Barzilai-Borwein method with infeasible iterates for nonnegative least-squares image deblurring. In Proceedings of the Eleventh Conference on Computer and Robot Vision (CRV 2014), Montreal, Canada, pp. 189--194.  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install bbii-decon via pip: pip install bbii-decon  Installation for developers Clone the github repository: ``` conda install git git clone https://github.com/gdellaire/BBii-Decon.git cd BBii-Decon pip install -e . ``` Deployment to pypi For deploying the plugin to the python package index (pypi), one needs a pypi user account  first. For deploying the plugin to pypi, one needs to install some tools: python -m pip install --user --upgrade setuptools wheel python -m pip install --user --upgrade twine The following command allows us to package the souce code as a python wheel. Make sure that the 'dist' and 'build' folders are deleted before doing this: python setup.py sdist bdist_wheel This command ships the just generated to pypi: python -m twine upload --repository pypi dist/* Read more about distributing your python package via pypi. Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"bbii-decon\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "bbii-decon",
    "documentation": "https://github.com/gdellaire/bbii-decon#README.md",
    "first_released": "2021-12-13T21:08:13.613383Z",
    "license": "BSD-3-Clause",
    "name": "bbii-decon",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/gdellaire/bbii-decon",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2021-12-13T21:08:13.613383Z",
    "report_issues": "https://github.com/gdellaire/bbii-decon/issues",
    "requirements": ["napari-plugin-engine (>=0.1.4)", "numpy", "pypher"],
    "summary": "Projected Barzilai-Borwein Image Deconvolution with Infeasible Iterates (BBii-Decon)",
    "support": "https://github.com/gdellaire/bbii-decon/issues",
    "twitter": "",
    "version": "0.0.1",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Ryan Conrad" }],
    "category": {
      "Supported data": ["2D", "3D"],
      "Workflow step": ["Image Segmentation"]
    },
    "category_hierarchy": {
      "Supported data": [["2D"], ["3D"]],
      "Workflow step": [["Image Segmentation"]]
    },
    "code_repository": "https://github.com/volume-em/empanada-napari",
    "conda": [],
    "description": "# empanada-napari  **The preprint describing this work is now available [on bioRxiv](https://www.biorxiv.org/content/10.1101/2022.03.17.484806v1).**  **Documentation for the plugin, including more detailed installation instructions, can be found [here](https://empanada.readthedocs.io/en/latest/empanada-napari.html).**  [empanada](https://github.com/volume-em/empanada) is a tool for deep learning-based panoptic segmentation of 2D and 3D electron microscopy images of cells. This plugin allows the running of panoptic segmentation models trained in empanada within [napari](https://napari.org). For help with this plugin please open an [issue](https://github.com/volume-em/empanada-napari/issues), for issues with napari specifically raise an [issue here instead](https://github.com/napari/napari/issues).  ## Implemented Models    - *MitoNet*: A generalist mitochondrial instance segmentation model.  ## Example Datasets  Volume EM datasets for benchmarking mitochondrial instance segmentation are available from [EMPIAR-10982](https://www.ebi.ac.uk/empiar/EMPIAR-10982/).  ## Installation  It's recommended to have installed napari through [conda](https://docs.conda.io/en/latest/miniconda.html). Then to install this plugin:  ```shell pip install empanada-napari ```  Launch napari:  ```shell napari ```  Look for empanada-napari under the \"Plugins\" menu.  ![empanada](images/demo.gif)  ## GPU Support  **Note: Mac doesn't support NVIDIA GPUS. This section only applies to Windows and Linux systems.**  As for any deep learning models, having a GPU installed on your system will significantly increase model throughput (although we ship CPU optimized versions of all models with the plugin).  This plugin relies on torch for running models. If a GPU was found on your system, then you will see that the \"Use GPU\" checkbox is checked by default in the \"2D Inference\" and \"3D Inference\" plugin widgets. Or if when running inference you see a message that says \"Using CPU\" in the terminal that means a GPU is not being used.  Make sure that GPU drivers are correctly installed. In terminal or command prompt:  ```shell nvidia-smi ```  If this returns \"command not found\" then you need to [install the driver from NVIDIA](https://www.nvidia.com/download/index.aspx). Instead, if if the driver is installed correctly, you may need to switch to the GPU enabled version of torch.  First, uninstall the current version of torch:  ```shell pip uninstall torch ```  Then [install torch >= 1.10 using conda for your system](https://pytorch.org/get-started/locally/). This command should work:  ```shell conda install pytorch cudatoolkit=11.3 -c pytorch ```  ## Citing this work  If you use results generated by this plugin in a publication, please cite:  ```bibtex @article {Conrad2022.03.17.484806, \\tauthor = {Conrad, Ryan and Narayan, Kedar}, \\ttitle = {Instance segmentation of mitochondria in electron microscopy images with a generalist deep learning model}, \\telocation-id = {2022.03.17.484806}, \\tyear = {2022}, \\tdoi = {10.1101/2022.03.17.484806}, \\tpublisher = {Cold Spring Harbor Laboratory}, \\tURL = {https://www.biorxiv.org/content/early/2022/03/18/2022.03.17.484806}, \\teprint = {https://www.biorxiv.org/content/early/2022/03/18/2022.03.17.484806.full.pdf}, \\tjournal = {bioRxiv} } ``` ",
    "description_content_type": "text/markdown",
    "description_text": "empanada-napari The preprint describing this work is now available on bioRxiv. Documentation for the plugin, including more detailed installation instructions, can be found here. empanada is a tool for deep learning-based panoptic segmentation of 2D and 3D electron microscopy images of cells. This plugin allows the running of panoptic segmentation models trained in empanada within napari. For help with this plugin please open an issue, for issues with napari specifically raise an issue here instead. Implemented Models  MitoNet: A generalist mitochondrial instance segmentation model.  Example Datasets Volume EM datasets for benchmarking mitochondrial instance segmentation are available from EMPIAR-10982. Installation It's recommended to have installed napari through conda. Then to install this plugin: shell pip install empanada-napari Launch napari: shell napari Look for empanada-napari under the \"Plugins\" menu.  GPU Support Note: Mac doesn't support NVIDIA GPUS. This section only applies to Windows and Linux systems. As for any deep learning models, having a GPU installed on your system will significantly increase model throughput (although we ship CPU optimized versions of all models with the plugin). This plugin relies on torch for running models. If a GPU was found on your system, then you will see that the \"Use GPU\" checkbox is checked by default in the \"2D Inference\" and \"3D Inference\" plugin widgets. Or if when running inference you see a message that says \"Using CPU\" in the terminal that means a GPU is not being used. Make sure that GPU drivers are correctly installed. In terminal or command prompt: shell nvidia-smi If this returns \"command not found\" then you need to install the driver from NVIDIA. Instead, if if the driver is installed correctly, you may need to switch to the GPU enabled version of torch. First, uninstall the current version of torch: shell pip uninstall torch Then install torch >= 1.10 using conda for your system. This command should work: shell conda install pytorch cudatoolkit=11.3 -c pytorch Citing this work If you use results generated by this plugin in a publication, please cite: bibtex @article {Conrad2022.03.17.484806,     author = {Conrad, Ryan and Narayan, Kedar},     title = {Instance segmentation of mitochondria in electron microscopy images with a generalist deep learning model},     elocation-id = {2022.03.17.484806},     year = {2022},     doi = {10.1101/2022.03.17.484806},     publisher = {Cold Spring Harbor Laboratory},     URL = {https://www.biorxiv.org/content/early/2022/03/18/2022.03.17.484806},     eprint = {https://www.biorxiv.org/content/early/2022/03/18/2022.03.17.484806.full.pdf},     journal = {bioRxiv} }",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "empanada-napari",
    "documentation": "https://github.com/volume-em/empanada-napari#README.md",
    "first_released": "2022-03-04T16:37:05.562410Z",
    "license": "BSD-3-Clause",
    "name": "empanada-napari",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/volume-em/empanada-napari",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-10-04T16:16:04.635717Z",
    "report_issues": "https://github.com/volume-em/empanada-napari/issues",
    "requirements": [
      "napari (>=0.4.15)",
      "numpy (<1.23)",
      "napari-plugin-engine (>=0.1.4)",
      "scikit-image (>=0.19)",
      "empanada-dl (>=0.1.7)",
      "imagecodecs"
    ],
    "summary": "Napari plugin of algorithms for Panoptic Segmentation of organelles in EM",
    "support": "https://github.com/volume-em/empanada-napari/issues",
    "twitter": "",
    "version": "0.2.3",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Johannes Elferich" }],
    "code_repository": "https://github.com/jojoelfe/napari-cryofibsem-monitor",
    "description": "# napari-cryofibsem-monitor  [![License](https://img.shields.io/pypi/l/napari-cryofibsem-monitor.svg?color=green)](https://github.com/jojoelfe/napari-cryofibsem-monitor/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-cryofibsem-monitor.svg?color=green)](https://pypi.org/project/napari-cryofibsem-monitor) [![Python Version](https://img.shields.io/pypi/pyversions/napari-cryofibsem-monitor.svg?color=green)](https://python.org) [![tests](https://github.com/jojoelfe/napari-cryofibsem-monitor/workflows/tests/badge.svg)](https://github.com/jojoelfe/napari-cryofibsem-monitor/actions) [![codecov](https://codecov.io/gh/jojoelfe/napari-cryofibsem-monitor/branch/main/graph/badge.svg)](https://codecov.io/gh/jojoelfe/napari-cryofibsem-monitor) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-cryofibsem-monitor)](https://napari-hub.org/plugins/napari-cryofibsem-monitor)  A plugin to monitor the creation of lamella using AutoTEM on a TFS Acquilos instrument   https://user-images.githubusercontent.com/6081039/201448228-fdd8b429-8ff6-4934-ad58-e80fbfcbaef0.mp4  ## Changelog  - **v0.0.3**      - Update data during milling     - Align images to keep lamella in the center     - Monitor thickness  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/docs/plugins/index.html -->  ## Installation  You can install `napari-cryofibsem-monitor` via [pip]:      pip install napari-cryofibsem-monitor    To install latest development version :      pip install git+https://github.com/jojoelfe/napari-cryofibsem-monitor.git   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [MIT] license, \"napari-cryofibsem-monitor\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/jojoelfe/napari-cryofibsem-monitor/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-cryofibsem-monitor       A plugin to monitor the creation of lamella using AutoTEM on a TFS Acquilos instrument https://user-images.githubusercontent.com/6081039/201448228-fdd8b429-8ff6-4934-ad58-e80fbfcbaef0.mp4 Changelog  v0.0.3  Update data during milling Align images to keep lamella in the center Monitor thickness     This napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.  Installation You can install napari-cryofibsem-monitor via pip: pip install napari-cryofibsem-monitor  To install latest development version : pip install git+https://github.com/jojoelfe/napari-cryofibsem-monitor.git  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the MIT license, \"napari-cryofibsem-monitor\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari-cryofibsem-monitor",
    "documentation": "https://github.com/jojoelfe/napari-cryofibsem-monitor#README.md",
    "first_released": "2021-11-05T16:10:31.864413Z",
    "license": "MIT",
    "name": "napari-cryofibsem-monitor",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/jojoelfe/napari-cryofibsem-monitor",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-11-12T01:03:51.584753Z",
    "report_issues": "https://github.com/jojoelfe/napari-cryofibsem-monitor/issues",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "tifffile",
      "imreg-dft",
      "matplotlib"
    ],
    "summary": "A plugin to monitor the creation of lamella using AutoTEM on a TFS Acquilos instrument",
    "support": "https://github.com/jojoelfe/napari-cryofibsem-monitor/issues",
    "twitter": "",
    "version": "0.0.3",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      { "name": "Pradeep Rajasekhar" },
      { "name": "Lachlan Whitehead" },
      { "name": "Robert Haase" }
    ],
    "code_repository": "https://github.com/BioimageAnalysisCoreWEHI/napari_lattice",
    "description": "# napari-lattice  [![License](https://img.shields.io/pypi/l/napari-lattice.svg?color=green)](https://github.com/githubuser/napari_lattice/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-lattice.svg?color=green)](https://pypi.org/project/napari_lattice) [![Python Version](https://img.shields.io/pypi/pyversions/napari-lattice.svg?color=green)](https://python.org) [![tests](https://github.com/BioimageAnalysisCoreWEHI/napari_lattice/actions/workflows/test_and_deploy.yml/badge.svg)](https://github.com/BioimageAnalysisCoreWEHI/napari_lattice/actions/workflows/test_and_deploy.yml) [![PyPI - Downloads](https://img.shields.io/pypi/dm/napari-lattice)](https://pypistats.org/packages/napari-lattice) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-lattice)](https://napari-hub.org/plugins/napari-lattice)  This napari plugin allows deskewing, cropping, visualisation and designing custom analysis pipelines for lattice lightsheet data, particularly from the Zeiss Lattice Lightsheet. The plugin has also been otpimixed to run in headless mode.   ## **Documentation**  Check the [Wiki page](https://github.com/BioimageAnalysisCoreWEHI/napari_lattice/wiki) for documentation on how to get started.   *************   <p align=\"left\"> <img src=\"https://raw.githubusercontent.com/BioimageAnalysisCoreWEHI/napari_lattice/master/resources/LLSZ_window.png\" alt=\"LLSZ_overview\" width=\"500\" > </p>  **Functions**  * Deskewing and deconvolution of Zeiss lattice lightsheet images   * Ability to preview deskewed image at channel or timepoint of interest * Crop and process only a small portion of the image  * Import ImageJ ROIs for cropping * Create image processing workflows using napari-workflows * Run deskewing, deconvolution and custom image processing workflows from the terminal * Files can be saved as h5 (BigDataViewer/BigStitcher) or tiff files * Run in terminal without napari, enabling processing workflows on the HPC  **Key Features**  Apply custom image processing workflows using `napari-workflows`.  * [Interactive workflow generation (no coding experience needed)](https://github.com/BioimageAnalysisCoreWEHI/napari_lattice/wiki/5.-Workflows-(Interactive:-no-coding)#workflow) * [Use custom python functions/modules within workflows](https://github.com/BioimageAnalysisCoreWEHI/napari_lattice/wiki/5.1-Workflows-(Custom-workflow)) * [How to use Cellpose for cell segmentation](https://github.com/BioimageAnalysisCoreWEHI/napari_lattice/wiki/5.1-Workflows-(Custom-workflow)#cellpose)   Support will be added for more file formats in the future.  Sample lattice lightsheet data download: https://doi.org/10.5281/zenodo.7117784  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/plugins/index.html -->   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [GPL-3.0 License] license, \"napari_lattice\" is free and open source software  ## Acknowledgment   This project was supported by funding from the [Rogers Lab at the Centre for Dynamic Imaging at the Walter and Eliza Hall Institute of Medical Research](https://imaging.wehi.edu.au/). This project has been made possible in part by [Napari plugin accelerator grant](https://chanzuckerberg.com/science/programs-resources/imaging/napari/lattice-light-sheet-data-analysis-toolset/) from the Chan Zuckerberg Initiative DAF, an advised fund of the Silicon Valley Community Foundation.   Thanks to the developers and maintainers of the amazing open-source plugins such as [pyclesperanto](https://github.com/clEsperanto/pyclesperanto_prototype), [aicsimageio](https://github.com/AllenCellModeling/aicsimageio), [dask](https://github.com/dask/dask) and [pycudadecon](https://github.com/tlambert03/pycudadecon).  Thanks in particular to the developers of open source projects: [LLSpy](https://github.com/tlambert03/LLSpy) and [lls_dd](https://github.com/VolkerH/Lattice_Lightsheet_Deskew_Deconv) as they were referred to extensively for developing napari-lattice.  Thanks to the imagesc forum!  ## Issues  If you encounter any problems, please [file an issue](https://github.com/BioimageAnalysisCoreWEHI/napari_lattice/issues) along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GGPL-3.0 License]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-lattice       This napari plugin allows deskewing, cropping, visualisation and designing custom analysis pipelines for lattice lightsheet data, particularly from the Zeiss Lattice Lightsheet. The plugin has also been otpimixed to run in headless mode. Documentation Check the Wiki page for documentation on how to get started.     Functions  Deskewing and deconvolution of Zeiss lattice lightsheet images Ability to preview deskewed image at channel or timepoint of interest Crop and process only a small portion of the image  Import ImageJ ROIs for cropping Create image processing workflows using napari-workflows Run deskewing, deconvolution and custom image processing workflows from the terminal Files can be saved as h5 (BigDataViewer/BigStitcher) or tiff files Run in terminal without napari, enabling processing workflows on the HPC  Key Features Apply custom image processing workflows using napari-workflows.  * Interactive workflow generation (no coding experience needed) * Use custom python functions/modules within workflows * How to use Cellpose for cell segmentation Support will be added for more file formats in the future. Sample lattice lightsheet data download: https://doi.org/10.5281/zenodo.7117784  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the [GPL-3.0 License] license, \"napari_lattice\" is free and open source software Acknowledgment This project was supported by funding from the Rogers Lab at the Centre for Dynamic Imaging at the Walter and Eliza Hall Institute of Medical Research. This project has been made possible in part by Napari plugin accelerator grant from the Chan Zuckerberg Initiative DAF, an advised fund of the Silicon Valley Community Foundation. Thanks to the developers and maintainers of the amazing open-source plugins such as pyclesperanto, aicsimageio, dask and pycudadecon.  Thanks in particular to the developers of open source projects: LLSpy and lls_dd as they were referred to extensively for developing napari-lattice.  Thanks to the imagesc forum! Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "Lattice Lightsheet Analysis",
    "documentation": "https://github.com/BioimageAnalysisCoreWEHI/napari_lattice#readme",
    "first_released": "2022-07-19T06:02:29.231754Z",
    "license": "GPL-3.0",
    "name": "napari-lattice",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader", "widget"],
    "project_site": "https://github.com/BioimageAnalysisCoreWEHI/napari_lattice",
    "python_version": ">=3.8",
    "reader_file_extensions": ["*.h5"],
    "release_date": "2022-12-22T03:18:01.288110Z",
    "report_issues": "https://github.com/BioimageAnalysisCoreWEHI/napari_lattice/issues",
    "requirements": [
      "aicsimageio (>=4.9.1)",
      "aicspylibczi (>=3.0.5)",
      "dask",
      "dask-image",
      "dask[distributed]",
      "magic-class (>=0.6.13)",
      "magicgui",
      "napari[all]",
      "pyopencl",
      "read-roi",
      "gputools",
      "pyclesperanto-prototype (>=0.20.0)",
      "napari-aicsimageio (>=0.7.2)",
      "napari-spreadsheet",
      "napari-workflows (>=0.2.8)",
      "napari-workflow-inspector",
      "npy2bdv",
      "redlionfish",
      "tifffile",
      "fsspec (>=2022.8.2)",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'"
    ],
    "summary": "Napari plugin for analysing and visualizing lattice lightsheet and Oblique Plane Microscopy data.",
    "support": "https://github.com/BioimageAnalysisCoreWEHI/napari_lattice/issues",
    "twitter": "",
    "version": "0.2.6",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      {
        "email": "juan.nunez-iglesias@monash.edu",
        "name": "Juan Nunez-Iglesias"
      }
    ],
    "category": {
      "Supported data": ["2D"],
      "Workflow step": ["Image registration"]
    },
    "category_hierarchy": {
      "Supported data": [["2D"]],
      "Workflow step": [
        ["Image registration"],
        ["Image registration", "Affine registration"],
        ["Image registration", "Affine registration", "Rigid registration"]
      ]
    },
    "code_repository": "https://github.com/jni/affinder",
    "description": "# Description  This GUI plugin allows you to quickly find the affine matrix mapping one image to another using manual correspondence points annotation.  More simply, this plugin allows you to select corresponding points on an image, and a second image you wish to transform. It computes  the requisite transformation matrix using Affine Transform, Euclidean Transform,  or Similarity Transform, and performs this transformation on the moving image, aligning it to the reference image.  https://user-images.githubusercontent.com/17995243/120086403-f1d0b300-c121-11eb-8000-a44a2ac54339.mp4   # Who is This For?  This is a simple plugin which can be used on any 2D images, provided they can be loaded as layers into napari. The images need not be the same file format and this plugin also works with labels layers.  No prior understanding of the transformation methods is required, as they perform in the background based on the reference points selected.  # How to Guide  You will need a combination of two or more 2D image and/or labels layers  loaded into napari. Once you have installed affinder, you can find it in the dock widgets menu.  ![Affinder widget in the Plugins->Add Dock Widget menu](https://i.imgur.com/w7MCXQy.png)  The first two dropdown boxes will be populated with the layers currently loaded into napari. Select a layer to use as reference, and another to transform.  ![Dropdowns allow you to select the reference and moving layers](https://i.imgur.com/Tdbm1sX.png)  Next, you can select the transformation model to use (affine is selected by default and is the least rigid transformation of those available). See [below](#transformation-models) for a description of the different models.  Finally, you can optionally select a path to a text file for saving out the resulting transformation matrix.  When you click Start, affinder will add two points layers to napari.  The plugin will also bring your reference image in focus, and its associated points layer. You can then start adding reference points by clicking on your image.  ![Adding reference points to layer](https://i.imgur.com/WPzNtyy.png)  Once three points are added, affinder will switch focus to the moving image, and you should then proceed to select the corresponding three points.  ![Adding corresponding points to newly focused layer](https://i.imgur.com/JVZCvmp.png)  affinder will immediately transform the moving image to align the points you've selected when you add your third corresponding point to your moving image.  ![The moving image is transformed once three points are added](https://i.imgur.com/NTne9fj.png)  From there, you can continue iteratively adding points until you  are happy with the alignment. Affinder will switch focus between reference and moving image with each point.  Click Finish to exit affinder.  ## Transformation Models  There are three transformation models available for use with affinder. They are listed here in order of increasing rigidity in the types of transforms they will allow. The eponymous Affine Transform is the  least rigid and is the default choice.  - [**Affine Transform**](https://en.wikipedia.org/wiki/Affine_transformation):  the least rigid transformation, it preserves lines and parallelism, but not necessarily distance and angles. Translation, scaling, similarity, reflection, rotation and shearing are all valid affine transformations.  - [**Similarity Transform**](https://en.wikipedia.org/wiki/Similarity_(geometry)):  this is a \"shape preserving\" transformation, producing objects which are  geometrically similar. Translation, rotation, reflection and uniform scaling are  valid similarity transforms. Shearing is not.  - [**Euclidean Transform**](https://en.wikipedia.org/wiki/Rigid_transformation): Also known as a rigid transformation, this transform preserves the Euclidean distance between each pair of points on the image. This includes rotation, translation and reflection but not scaling or shearing.  # Getting Help  If you find a bug with affinder, or would like support with using it, please raise an issue on the [GitHub repository](https://github.com/jni/affinder).  # How to Cite  Many plugins may be used in the course of published (or publishable) research, as well as during conference talks and other public facing events. If you'd like to be cited in a particular format, or have a DOI you'd like used, you should provide that information here. ",
    "description_content_type": "text/markdown",
    "description_text": "Description This GUI plugin allows you to quickly find the affine matrix mapping one image to another using manual correspondence points annotation. More simply, this plugin allows you to select corresponding points on an image, and a second image you wish to transform. It computes  the requisite transformation matrix using Affine Transform, Euclidean Transform,  or Similarity Transform, and performs this transformation on the moving image, aligning it to the reference image. https://user-images.githubusercontent.com/17995243/120086403-f1d0b300-c121-11eb-8000-a44a2ac54339.mp4 Who is This For? This is a simple plugin which can be used on any 2D images, provided they can be loaded as layers into napari. The images need not be the same file format and this plugin also works with labels layers. No prior understanding of the transformation methods is required, as they perform in the background based on the reference points selected. How to Guide You will need a combination of two or more 2D image and/or labels layers  loaded into napari. Once you have installed affinder, you can find it in the dock widgets menu.  The first two dropdown boxes will be populated with the layers currently loaded into napari. Select a layer to use as reference, and another to transform.  Next, you can select the transformation model to use (affine is selected by default and is the least rigid transformation of those available). See below for a description of the different models. Finally, you can optionally select a path to a text file for saving out the resulting transformation matrix. When you click Start, affinder will add two points layers to napari.  The plugin will also bring your reference image in focus, and its associated points layer. You can then start adding reference points by clicking on your image.  Once three points are added, affinder will switch focus to the moving image, and you should then proceed to select the corresponding three points.  affinder will immediately transform the moving image to align the points you've selected when you add your third corresponding point to your moving image.  From there, you can continue iteratively adding points until you  are happy with the alignment. Affinder will switch focus between reference and moving image with each point. Click Finish to exit affinder. Transformation Models There are three transformation models available for use with affinder. They are listed here in order of increasing rigidity in the types of transforms they will allow. The eponymous Affine Transform is the  least rigid and is the default choice.   Affine Transform:  the least rigid transformation, it preserves lines and parallelism, but not necessarily distance and angles. Translation, scaling, similarity, reflection, rotation and shearing are all valid affine transformations.   Similarity Transform:  this is a \"shape preserving\" transformation, producing objects which are  geometrically similar. Translation, rotation, reflection and uniform scaling are  valid similarity transforms. Shearing is not.   Euclidean Transform: Also known as a rigid transformation, this transform preserves the Euclidean distance between each pair of points on the image. This includes rotation, translation and reflection but not scaling or shearing.   Getting Help If you find a bug with affinder, or would like support with using it, please raise an issue on the GitHub repository. How to Cite Many plugins may be used in the course of published (or publishable) research, as well as during conference talks and other public facing events. If you'd like to be cited in a particular format, or have a DOI you'd like used, you should provide that information here.",
    "development_status": ["Development Status :: 3 - Alpha"],
    "display_name": "affinder",
    "documentation": "",
    "first_released": "2021-02-04T10:12:07.298699Z",
    "license": "BSD-3-Clause",
    "name": "affinder",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/jni/affinder",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-04-08T01:03:20.850508Z",
    "report_issues": "",
    "requirements": [
      "napari (>=0.4.12)",
      "npe2 (>=0.1.2)",
      "numpy",
      "scikit-image",
      "magicgui (>=0.3.7)",
      "toolz",
      "furo ; extra == 'docs'",
      "myst-parser ; extra == 'docs'",
      "coverage ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "scikit-image[data] ; extra == 'testing'",
      "napari[pyqt5] ; extra == 'testing'"
    ],
    "summary": "Quickly find the affine matrix mapping one image to another using manual correspondence points annotation",
    "support": "",
    "twitter": "",
    "version": "0.2.3",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      { "email": "talley.lambert@gmail.com", "name": "Talley Lambert" }
    ],
    "code_repository": "https://github.com/tlambert03/napari-dv",
    "description": "# napari-dv  [![License](https://img.shields.io/pypi/l/napari-dv.svg?color=green)](https://github.com/tlambert03/napari-dv/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-dv.svg?color=green)](https://pypi.org/project/napari-dv) [![Python Version](https://img.shields.io/pypi/pyversions/napari-dv.svg?color=green)](https://python.org) [![tests](https://github.com/tlambert03/napari-dv/workflows/tests/badge.svg)](https://github.com/tlambert03/napari-dv/actions) [![codecov](https://codecov.io/gh/tlambert03/napari-dv/branch/master/graph/badge.svg)](https://codecov.io/gh/tlambert03/napari-dv)  Deltavision/MRC file reader for napari.  This wraps the [mrc](https://github.com/tlambert03/mrc) library.  See also [napari-aicsimageio](https://github.com/AllenCellModeling/napari-aicsimageio), which also uses the [mrc](https://github.com/tlambert03/mrc) to provide dv file support, along with many other common file formats.  ## Installation  You can install `napari-dv` via [pip]:      pip install napari-dv  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [MIT] license, \"napari-dv\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [file an issue]: https://github.com/tlambert03/napari-dv/issues [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-dv      Deltavision/MRC file reader for napari. This wraps the mrc library. See also napari-aicsimageio, which also uses the mrc to provide dv file support, along with many other common file formats. Installation You can install napari-dv via pip: pip install napari-dv  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the MIT license, \"napari-dv\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "napari-dv",
    "documentation": "https://github.com/tlambert03/napari-dv#README.md",
    "first_released": "2020-02-02T21:12:45.927865Z",
    "license": "MIT",
    "name": "napari-dv",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader", "writer"],
    "project_site": "https://github.com/tlambert03/napari-dv",
    "python_version": ">=3.7",
    "reader_file_extensions": ["*.dv", "*.mrc"],
    "release_date": "2022-03-19T15:58:33.601081Z",
    "report_issues": "https://github.com/tlambert03/napari-dv/issues",
    "requirements": [
      "mrc (>=0.2.0)",
      "napari-plugin-engine (>=0.1.4)",
      "numpy ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'"
    ],
    "summary": "Deltavision/MRC file reader for napari",
    "support": "https://github.com/tlambert03/napari-dv/issues",
    "twitter": "",
    "version": "0.3.0",
    "visibility": "public",
    "writer_file_extensions": [".mrc", ".dv"],
    "writer_save_layers": ["image"]
  },
  {
    "authors": [
      {
        "name": "=?utf-8?q?Johannes_M=C3=BCller=2C_Ben_J=2E_Gross=2C_Robert_Haase=2C_Elijah_Shelton=2C_Carlos_Gomez=2C_Otger_Campas?="
      }
    ],
    "code_repository": "https://github.com/BiAPoL/napari-stress",
    "description": "[![License](https://img.shields.io/pypi/l/napari-stress.svg?color=green)](https://github.com/biapol/napari-stress/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-stress.svg?color=green)](https://pypi.org/project/napari-stress) [![Python Version](https://img.shields.io/pypi/pyversions/napari-stress.svg?color=green)](https://python.org) [![tests](https://github.com/BiAPoL/napari-stress/actions/workflows/test_and_deploy.yml/badge.svg)](https://github.com/BiAPoL/napari-stress/actions/workflows/test_and_deploy.yml) [![codecov](https://codecov.io/gh/BiAPoL/napari-stress/branch/main/graph/badge.svg?token=ZXQGREJAT9)](https://codecov.io/gh/BiAPoL/napari-stress) [![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&logoColor=white)](https://github.com/pre-commit/pre-commit) [![PyPI - Downloads](https://img.shields.io/pypi/dm/napari-stress.svg)](https://pypistats.org/packages/napari-stress) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-stress)](https://www.napari-hub.org/plugins/napari-stress) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.6607329.svg)](https://doi.org/10.5281/zenodo.6607329)  # napari-stress  This plugin provides tools for the analysis of surfaces in Napari, such as utilities to determine and refine the surface-representation of objects using a ray-casting approach and calculate the curvature of surfaces.  It re-implements code in Napari that was written for [Gross et al. (2021): STRESS, an automated geometrical characterization of deformable particles for in vivo measurements of cell and tissue mechanical stresses](https://www.biorxiv.org/content/10.1101/2021.03.26.437148v1)  and has been made open source in [this repository](https://github.com/campaslab/STRESS).  ![](https://github.com/BiAPoL/napari-stress/raw/main/docs/imgs/function_gifs/spherical_harmonics.gif)  ## Usage  For documentation on how to use napari-stress both interactively from the napari-viewer or from code, please visit the [**documentation**](https://biapol.github.io/napari-stress/intro.html)   ## Installation  Create a new conda environment with the following command.  If you have never used conda before, please [read this guide first](https://biapol.github.io/blog/johannes_mueller/anaconda_getting_started/).  ``` conda create -n napari-stress Python=3.9 napari jupyterlab -c conda-forge conda activate napari-stress ```  You can then install napari-stress using pip:  ``` pip install napari-stress ```  ## Issues  To report bugs, request new features or get in touch, please [open an issue](https://github.com/BiAPoL/napari-stress/issues) or tag `@EL_Pollo_Diablo` on [image.sc](https://forum.image.sc/).  ## See also  There are other napari plugins with similar / overlapping functionality  * [morphometrics](https://www.napari-hub.org/plugins/morphometrics) * [napari-pymeshlab](https://www.napari-hub.org/plugins/napari-pymeshlab) * [napari-process-points-and-surfaces](https://www.napari-hub.org/plugins/napari-process-points-and-surfaces)  ## Contributing  Contributions are very welcome. Tests can be run with [pytest], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-stress\" is free and open source software  ## Acknowledgements This project was supported by the Deutsche Forschungsgemeinschaft under Germany’s Excellence Strategy – EXC2068 - Cluster of Excellence \"Physics of Life\" of TU Dresden.  [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [pytest]: https://docs.pytest.org/en/7.0.x/ ",
    "description_content_type": "text/markdown",
    "description_text": "         napari-stress This plugin provides tools for the analysis of surfaces in Napari, such as utilities to determine and refine the surface-representation of objects using a ray-casting approach and calculate the curvature of surfaces.  It re-implements code in Napari that was written for Gross et al. (2021): STRESS, an automated geometrical characterization of deformable particles for in vivo measurements of cell and tissue mechanical stresses  and has been made open source in this repository.  Usage For documentation on how to use napari-stress both interactively from the napari-viewer or from code, please visit the documentation Installation Create a new conda environment with the following command.  If you have never used conda before, please read this guide first. conda create -n napari-stress Python=3.9 napari jupyterlab -c conda-forge conda activate napari-stress You can then install napari-stress using pip: pip install napari-stress Issues To report bugs, request new features or get in touch, please open an issue or tag @EL_Pollo_Diablo on image.sc. See also There are other napari plugins with similar / overlapping functionality  morphometrics napari-pymeshlab napari-process-points-and-surfaces  Contributing Contributions are very welcome. Tests can be run with pytest, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-stress\" is free and open source software Acknowledgements This project was supported by the Deutsche Forschungsgemeinschaft under Germany’s Excellence Strategy – EXC2068 - Cluster of Excellence \"Physics of Life\" of TU Dresden.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari-stress",
    "documentation": "https://biapol.github.io/napari-stress",
    "first_released": "2022-06-02T08:39:03.961427Z",
    "license": "BSD-3-Clause",
    "name": "napari-stress",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["sample_data"],
    "project_site": "",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-11-08T11:08:43.294960Z",
    "report_issues": "https://github.com/BiAPoL/napari-stress/issues",
    "requirements": [
      "numpy",
      "vedo (>=2022.4.1)",
      "napari",
      "vispy",
      "matplotlib",
      "tqdm",
      "scipy",
      "pandas",
      "scikit-image",
      "napari-tools-menu (>=0.1.15)",
      "napari-process-points-and-surfaces (>=0.3.0)",
      "aicsimageio",
      "napari-segment-blobs-and-things-with-membranes",
      "mpmath",
      "pyshtools (<=4.10.0)",
      "napari-matplotlib",
      "pygeodesic"
    ],
    "summary": "Interactive surface analysis in napari for measuring mechanical stresses in biological tissues",
    "support": "https://github.com/BiAPoL/napari-stress/issues",
    "twitter": "",
    "version": "0.1.2",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Meizhu Liang" }],
    "code_repository": "https://github.com/Meizhu-Liang/napari-generic-SIMulator",
    "conda": [
      { "channel": "conda-forge", "package": "napari-generic-simulator" }
    ],
    "description": "# napari-generic-SIMulator  [![License BSD-3](https://img.shields.io/pypi/l/napari-generic-SIMulator.svg?color=green)](https://github.com/Meizhu-Liang/napari-generic-SIMulator/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-generic-SIMulator.svg?color=green)](https://pypi.org/project/napari-generic-SIMulator) [![Python Version](https://img.shields.io/pypi/pyversions/napari-generic-SIMulator.svg?color=green)](https://python.org) [![tests](https://github.com/Meizhu-Liang/napari-generic-SIMulator/workflows/tests/badge.svg)](https://github.com/Meizhu-Liang/napari-generic-SIMulator/actions) [![codecov](https://codecov.io/gh/Meizhu-Liang/napari-generic-SIMulator/branch/main/graph/badge.svg)](https://codecov.io/gh/Meizhu-Liang/napari-generic-SIMulator) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-generic-SIMulator)](https://napari-hub.org/plugins/napari-generic-SIMulator)  A napari plugin to simulate raw-image stacks of Structured illumination microscopy (SIM) with light sheet.   The simulation is originally based on the paper <strong>GPU-accelerated real-time reconstruction in Python of three-dimensional datasets from structured illumination microscopy with hexagonal patterns</strong> by Hai Gong, Wenjun Guo and Mark A. A. Neil (https://doi.org/10.1098/rsta.2020.0162).   The calculation can be GPU-accelerated if the CUPY (tested with cupy 8.3.0) is installed. In addition, the TORCH package can complete the acceleration both on CPU if TORCH is installed, and on GPU if TORCH is compiled with the CUDA (tested with torch v1.12.0+cu116) enabled.  Currently applies to: - conventional 2-beam SIM data with 3 angles and 3 phases - 3-beam hexagonal SIM data with 7 phases, as described in the paper - 3-beam hexagonal SIM data with 7 phases at right-angles  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/plugins/index.html -->  ## Installation  You can install `napari-generic-SIMulator` via [pip]:      pip install napari-generic-SIMulator    To install latest development version :      pip install git+https://github.com/Meizhu-Liang/napari-generic-SIMulator.git  ## Usage  1) Open napari and create the viewer.   2) Launch the widget in ***Plugin***     ![raw](https://github.com/Meizhu-Liang/napari-generic-SIMulator/raw/main/images/img.png)     ![raw](https://github.com/Meizhu-Liang/napari-generic-SIMulator/raw/main/images/img_1.png)   3) Adjust the parameters in the widget and calculate the raw-image stack.     ![raw](https://github.com/Meizhu-Liang/napari-generic-SIMulator/raw/main/images/img_2.png)   4) The sum, psf and otf can be showed. Note the all of these correspond the generated raw-image stack, so keep the parameters the same before showing the sum (or psf and otf).     ![raw](https://github.com/Meizhu-Liang/napari-generic-SIMulator/raw/main/images/img_3.png)   5) The raw image stacks can be then processed by napari-sim-processor (https://www.napari-hub.org/plugins/napari-sim-processor). ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-generic-SIMulator\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/Meizhu-Liang/napari-generic-SIMulator/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-generic-SIMulator       A napari plugin to simulate raw-image stacks of Structured illumination microscopy (SIM) with light sheet.  The simulation is originally based on the paper GPU-accelerated real-time reconstruction in Python of three-dimensional datasets from structured illumination microscopy with hexagonal patterns by Hai Gong, Wenjun Guo and Mark A. A. Neil (https://doi.org/10.1098/rsta.2020.0162).  The calculation can be GPU-accelerated if the CUPY (tested with cupy 8.3.0) is installed. In addition, the TORCH package can complete the acceleration both on CPU if TORCH is installed, and on GPU if TORCH is compiled with the CUDA (tested with torch v1.12.0+cu116) enabled. Currently applies to: - conventional 2-beam SIM data with 3 angles and 3 phases - 3-beam hexagonal SIM data with 7 phases, as described in the paper - 3-beam hexagonal SIM data with 7 phases at right-angles  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install napari-generic-SIMulator via pip: pip install napari-generic-SIMulator  To install latest development version : pip install git+https://github.com/Meizhu-Liang/napari-generic-SIMulator.git  Usage 1) Open napari and create the viewer. 2) Launch the widget in Plugin   3) Adjust the parameters in the widget and calculate the raw-image stack.      4) The sum, psf and otf can be showed. Note the all of these correspond the generated raw-image stack, so keep the parameters the same before showing the sum (or psf and otf).      5) The raw image stacks can be then processed by napari-sim-processor (https://www.napari-hub.org/plugins/napari-sim-processor). Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-generic-SIMulator\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari generic SIMulator",
    "documentation": "https://github.com/Meizhu-Liang/napari-generic-SIMulator#README.md",
    "first_released": "2022-06-30T18:20:27.340885Z",
    "license": "BSD-3-Clause",
    "name": "napari-generic-SIMulator",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/Meizhu-Liang/napari-generic-SIMulator",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-09-06T19:48:03.153316Z",
    "report_issues": "https://github.com/Meizhu-Liang/napari-generic-SIMulator/issues",
    "requirements": [
      "numpy",
      "magicgui",
      "qtpy",
      "tifffile",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'",
      "tifffile ; extra == 'testing'"
    ],
    "summary": "A simple plugin to use with napari to simulate raw image stacks in Structured illumination microscopy (SIM) with napari.",
    "support": "https://github.com/Meizhu-Liang/napari-generic-SIMulator/issues",
    "twitter": "",
    "version": "0.0.18",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      { "email": "talley.lambert@gmail.com", "name": "Talley Lambert" }
    ],
    "code_repository": "https://github.com/tlambert03/napari-manual-transforms",
    "description": "# napari-manual-transforms  [![License](https://img.shields.io/pypi/l/napari-manual-transforms.svg?color=green)](https://github.com/tlambert03/napari-manual-transforms/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-manual-transforms.svg?color=green)](https://pypi.org/project/napari-manual-transforms) [![Python Version](https://img.shields.io/pypi/pyversions/napari-manual-transforms.svg?color=green)](https://python.org) [![tests](https://github.com/tlambert03/napari-manual-transforms/workflows/tests/badge.svg)](https://github.com/tlambert03/napari-manual-transforms/actions) [![codecov](https://codecov.io/gh/tlambert03/napari-manual-transforms/branch/main/graph/badge.svg)](https://codecov.io/gh/tlambert03/napari-manual-transforms)  Interface to manually edit layer affine transforms.  - express rotations as quaternion, euler angle, or axis + angle. - allows rotation around arbitrary origin - currently, focusing on rigid rotations - Alt-Drag to rotate a layer independently of the rest. - image resampling (i.e. \"apply\" the transformation to create new dataset that can be saved)  ![Plugin Preview](/preview.jpeg)  caveats:  - only works on 3D Image layers for now, open a feature request for other dims/layers. - will likely result in \"Non-orthogonal slicing is being requested\" warnings in 2D view.  ## Try it out  ```python  import napari  v = napari.Viewer() v.dims.ndisplay = 3 v.open_sample('napari', 'cells3d') v.window.add_plugin_dock_widget('napari-manual-transforms')  napari.run()  ```  ----------------------------------  ## Installation  You can install `napari-manual-transforms` via [pip]:  ```sh pip install napari-manual-transforms ```  To install latest development version :  ```sh pip install git+https://github.com/tlambert03/napari-manual-transforms.git ```  ## License  Distributed under the terms of the [BSD-3] license, \"napari-manual-transforms\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/tlambert03/napari-manual-transforms/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-manual-transforms      Interface to manually edit layer affine transforms.  express rotations as quaternion, euler angle, or axis + angle. allows rotation around arbitrary origin currently, focusing on rigid rotations Alt-Drag to rotate a layer independently of the rest. image resampling (i.e. \"apply\" the transformation to create new dataset that can be saved)   caveats:  only works on 3D Image layers for now, open a feature request for other dims/layers. will likely result in \"Non-orthogonal slicing is being requested\" warnings in 2D view.  Try it out ```python import napari v = napari.Viewer() v.dims.ndisplay = 3 v.open_sample('napari', 'cells3d') v.window.add_plugin_dock_widget('napari-manual-transforms') napari.run() ```  Installation You can install napari-manual-transforms via pip: sh pip install napari-manual-transforms To install latest development version : sh pip install git+https://github.com/tlambert03/napari-manual-transforms.git License Distributed under the terms of the BSD-3 license, \"napari-manual-transforms\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "Manual Transforms",
    "documentation": "https://github.com/tlambert03/napari-manual-transforms#README.md",
    "first_released": "2022-04-28T17:16:04.435019Z",
    "license": "BSD-3-Clause",
    "name": "napari-manual-transforms",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/tlambert03/napari-manual-transforms",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-04-29T20:47:53.695702Z",
    "report_issues": "https://github.com/tlambert03/napari-manual-transforms/issues",
    "requirements": [
      "magicgui",
      "napari",
      "numpy",
      "pytransform3d",
      "qtpy",
      "scipy",
      "vispy",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "tox ; extra == 'testing'"
    ],
    "summary": "Interface to manually edit layer affine transforms",
    "support": "https://github.com/tlambert03/napari-manual-transforms/issues",
    "twitter": "",
    "version": "0.0.3",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Marc Boucsein" }, { "name": "Robin Koch" }],
    "code_repository": "https://github.com/MBPhys/Image-Composer",
    "conda": [{ "channel": "conda-forge", "package": "image-composer" }],
    "description": "# Image-Composer  [![License](https://img.shields.io/pypi/l/napari-medical-image-formats.svg?color=green)](https://github.com/MBPhys/Image-Composer/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/Image-Composer.svg?color=green)](https://pypi.org/project/Image-Composer) [![Python Version](https://img.shields.io/pypi/pyversions/Image-Composer.svg?color=green)](https://python.org)   A napari plugin in order to compose a background image with a foreground image.  ----------------------------------  ## Installation  You can install `Image-Composer` via [pip]:      pip install Image-Composer  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"Image-Composer\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin [file an issue]: https://github.com/MBPhys/Image-Composer/issues [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "Image-Composer    A napari plugin in order to compose a background image with a foreground image.  Installation You can install Image-Composer via pip: pip install Image-Composer  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"Image-Composer\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "Image-Composer",
    "documentation": "",
    "first_released": "2022-01-10T19:11:03.062277Z",
    "license": "BSD-3-Clause",
    "name": "Image-Composer",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/MBPhys/Image-Composer",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-01-12T12:56:51.937813Z",
    "report_issues": "",
    "requirements": ["napari-plugin-engine (>=0.1.4)", "numpy"],
    "summary": "A napari plugin in order to compose a background image with a foreground image",
    "support": "",
    "twitter": "",
    "version": "0.0.19",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Marc Boucsein" }, { "name": "Robin Koch" }],
    "code_repository": "https://github.com/MBPhys/Image-Part-Selecter",
    "conda": [{ "channel": "conda-forge", "package": "image-part-selecter" }],
    "description": "# Image-Part-Selecter  [![License](https://img.shields.io/pypi/l/napari-medical-image-formats.svg?color=green)](https://github.com/MBPhys/Image-Part-Selecter/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/Image-Part-Selecter.svg?color=green)](https://pypi.org/project/Image-Part-Selecter) [![Python Version](https://img.shields.io/pypi/pyversions/Image-Part-Selecter.svg?color=green)](https://python.org)   A napari plugin in order to select parts of images  ----------------------------------  ## Installation  You can install `Image-Part-Selecter` via [pip]:      pip install Image-Part-Selecter  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"Image-Part-Selecter\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin [file an issue]: https://github.com/MBPhys/Image-Part-Selecter/issues [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "Image-Part-Selecter    A napari plugin in order to select parts of images  Installation You can install Image-Part-Selecter via pip: pip install Image-Part-Selecter  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"Image-Part-Selecter\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "Image-Part-Selecter",
    "documentation": "",
    "first_released": "2022-01-12T10:42:55.967575Z",
    "license": "BSD-3-Clause",
    "name": "Image-Part-Selecter",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/MBPhys/Image-Part-Selecter",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-01-12T10:42:55.967575Z",
    "report_issues": "",
    "requirements": ["napari-plugin-engine (>=0.1.4)", "numpy"],
    "summary": "A napari plugin in order to select parts of images",
    "support": "",
    "twitter": "",
    "version": "0.0.7",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Marc Boucsein" }, { "name": "Robin Koch" }],
    "code_repository": "https://github.com/MBPhys/Offset-Subtraction",
    "conda": [{ "channel": "conda-forge", "package": "offset-subtraction" }],
    "description": "# Offset-Subtraction  [![License](https://img.shields.io/pypi/l/napari-medical-image-formats.svg?color=green)](https://github.com/MBPhys/Offset-Subtraction/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/Offset-Subtraction.svg?color=green)](https://pypi.org/project/Offset-Subtraction) [![Python Version](https://img.shields.io/pypi/pyversions/Offset-Subtraction.svg?color=green)](https://python.org)   A napari plugin in oder to subtract an intensity offset such as autofluorescence  ----------------------------------  ## Installation  You can install `Offset-Subtraction` via [pip]:      pip install Offset-Subtraction  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"Offset-Subtraction\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin [file an issue]: https://github.com/MBPhys/Offset-Subtraction/issues [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "Offset-Subtraction    A napari plugin in oder to subtract an intensity offset such as autofluorescence  Installation You can install Offset-Subtraction via pip: pip install Offset-Subtraction  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"Offset-Subtraction\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "Offset-Subtraction",
    "documentation": "",
    "first_released": "2022-01-13T11:52:17.512835Z",
    "license": "BSD-3-Clause",
    "name": "Offset-Subtraction",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/MBPhys/Offset-Subtraction",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-01-13T11:52:17.512835Z",
    "report_issues": "",
    "requirements": ["napari-plugin-engine (>=0.1.4)", "numpy", "dask"],
    "summary": "A napari plugin in oder to subtract an intensity offset such as autofluorescence",
    "support": "",
    "twitter": "",
    "version": "0.0.5",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Marc Boucsein" }, { "name": "Robin Koch" }],
    "code_repository": "https://github.com/MBPhys/Label-Creator",
    "conda": [{ "channel": "conda-forge", "package": "label-creator" }],
    "description": "# Label-Creator  [![License](https://img.shields.io/pypi/l/napari-medical-image-formats.svg?color=green)](https://github.com/MBPhys/Label-Creator/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/Label-Creator.svg?color=green)](https://pypi.org/project/Label-Creator) [![Python Version](https://img.shields.io/pypi/pyversions/Label-Creator.svg?color=green)](https://python.org)   A napari plugin for generation of Label-Layers according to selected image data shapes.  ----------------------------------  ## Installation  You can install `Label-Creator` via [pip]:      pip install Label-Creator  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"Label-Creator\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin [file an issue]: https://github.com/MBPhys/Label-Creator/issues [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "Label-Creator    A napari plugin for generation of Label-Layers according to selected image data shapes.  Installation You can install Label-Creator via pip: pip install Label-Creator  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"Label-Creator\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "Label-Creator",
    "documentation": "",
    "first_released": "2022-01-12T18:26:04.391894Z",
    "license": "BSD-3-Clause",
    "name": "Label-Creator",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/MBPhys/Label-Creator",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-01-21T12:02:30.775041Z",
    "report_issues": "",
    "requirements": ["napari-plugin-engine (>=0.1.4)", "numpy", "dask"],
    "summary": "A napari plugin for generation of Label-Layers according to selected image data shapes",
    "support": "",
    "twitter": "",
    "version": "0.0.9",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Marc Boucsein" }, { "name": "Robin Koch" }],
    "code_repository": "https://github.com/MBPhys/Layer-Data-Replace",
    "conda": [{ "channel": "conda-forge", "package": "layer-data-replace" }],
    "description": "# Layer-Data-Replace  [![License](https://img.shields.io/pypi/l/napari-medical-image-formats.svg?color=green)](https://github.com/MBPhys/Layer-Data-Replace/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/Layer-Data-Replace.svg?color=green)](https://pypi.org/project/Layer-Data-Replace) [![Python Version](https://img.shields.io/pypi/pyversions/Layer-Data-Replace.svg?color=green)](https://python.org)   A napari plugin in order to replace parts of the data of a layer by another one.  ----------------------------------  ## Installation  You can install `Layer-Data-Replace` via [pip]:      pip install Layer-Data-Replace  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"Layer-Data-Replace\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin [file an issue]: https://github.com/MBPhys/Layer-Data-Replace/issues [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "Layer-Data-Replace    A napari plugin in order to replace parts of the data of a layer by another one.  Installation You can install Layer-Data-Replace via pip: pip install Layer-Data-Replace  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"Layer-Data-Replace\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "Layer-Data-Replace",
    "documentation": "",
    "first_released": "2022-01-13T15:35:24.841872Z",
    "license": "BSD-3-Clause",
    "name": "Layer-Data-Replace",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/MBPhys/Layer-Data-Replace",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-01-13T15:35:24.841872Z",
    "report_issues": "",
    "requirements": ["napari-plugin-engine (>=0.1.4)", "numpy", "dask"],
    "summary": "A napari plugin in order to replace parts of the data of a layer by another one",
    "support": "",
    "twitter": "",
    "version": "0.0.5",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      { "email": "v.o.van_der_valk@lumc.nl", "name": "Viktor van der Valk" }
    ],
    "code_repository": "https://github.com/SuperElastix/elastix_napari",
    "description": "# Description  This plugin makes the elastix toolbox for rigid and nonrigid registration of images available in napari. elastix is open source software, based on the well-known Insight Segmentation and Registration Toolkit (ITK). The software consists of a collection of algorithms that are commonly used to solve (medical) image registration problems. The modular design of elastix allows the user to quickly configure, test, and compare different registration methods for a specific application.  # Who is This For?  With this plugin both 2D and 3D images in all file formats available in ITK can be registered. The plugin supports various transformations including rigid, affine and bspline.  Registration within the plugin is done based on user defined parameters, but for novice users defaults for each transformation model are available.  # How to Guide  Load the images you want to register into napari and select them in the fixed (or reference) and moving image dropdowns of plugin interface.  For fast and easy registration only the preferred transformation (rigid, affine or bspline) has to be selected (see Transformations section for explanation).  For more advanced registrations the following adjustments can be made in the plugin:  - Masks for both the fixed and the moving images can be selected to let elastix only include certain areas in the registration. These masks have to be loaded into napari and selected in the correct mask dropdown menus, which appear when the masks box is ticked. - Point sets for both the fixed and the moving images can de selected to use certain points to aid registration. These point set files have to be .txt files in the following format:    index/point\\\\   #points\\\\   point1 x point1 y [point1 z]\\\\   point2 x point2 y [point2 z]    The first line indicates whether the points are given as “indices” (of the fixed image), or as “points” (in   physical coordinates). The second line stores the number of points that will be specified. After that the   point data is given. For example:    point\\\\   3\\\\   2.32 5.34 -4.12\\\\   -1.56 0.12 9.23\\\\   1.00 7.34 -0.23  - An initial transform file that specifies a transform that is applied before the registration is done, can be uploaded as a .txt file. For the latest file and transform formats that are supported, see the [elastix manual](https://elastix.lumc.nl/doxygen/index.html)  - For the most common registration parameters adjustments can be made in the plugin GUI  - Other, less common registration parameters can be adjusted by uploading custom transform parameter file(s). (Select 'custom' in the preset dropdown).   <img width=\"1438\" alt=\"Screenshot 2021-05-12 at 15 07 24\" src=\"https://user-images.githubusercontent.com/33719474/117980045-d6009b00-b333-11eb-9976-f64d34f4f7cc.png\">  # Transformations  In the plugin 3 common transformations are available as presets, other transformations can be done with the 'custom' option in the preset dropdown. The plugin then has the ability to upload custom parameter files in which other transformations can be specified.  The three common transformations are:  - [Rigid Transform](https://en.wikipedia.org/wiki/Rigid_transformation): Also known as a Euclidean transformation, this transform preserves the Euclidean distance between each pair of points on the image. This includes rotation, translation and reflection but not scaling or shearing.   - [Affine Transform](https://en.wikipedia.org/wiki/Affine_transformation): This transfrom preserves lines and parallelism, but not necessarily distance and angles. Translation, scaling, similarity, reflection, rotation and shearing are all valid affine transformations.  - [BSpline Transform](https://en.wikipedia.org/wiki/B-spline): This is a deformable transformation that preserves none of the properties mentioned in the transforms describe above.  # Getting Help If you find a bug in the elastix napari plugin, or would like support with using it, please raise an issue on the [GitHub repository](https://github.com/SuperElastix/elastix_napari).  For question specifically about the elastix toolbox we have a [mailing list](https://groups.google.com/forum/#!forum/elastix-imageregistration).  # Contributions Contributions to the elastix_napari plugin, [itkelastix](https://github.com/InsightSoftwareConsortium/ITKElastix) (the python wrapper) or [elastix](https://github.com/SuperElastix/elastix) (the C++ core) on which the plugin is build, are welcome. ",
    "description_content_type": "text/markdown",
    "description_text": "Description This plugin makes the elastix toolbox for rigid and nonrigid registration of images available in napari. elastix is open source software, based on the well-known Insight Segmentation and Registration Toolkit (ITK). The software consists of a collection of algorithms that are commonly used to solve (medical) image registration problems. The modular design of elastix allows the user to quickly configure, test, and compare different registration methods for a specific application. Who is This For? With this plugin both 2D and 3D images in all file formats available in ITK can be registered. The plugin supports various transformations including rigid, affine and bspline. Registration within the plugin is done based on user defined parameters, but for novice users defaults for each transformation model are available. How to Guide Load the images you want to register into napari and select them in the fixed (or reference) and moving image dropdowns of plugin interface. For fast and easy registration only the preferred transformation (rigid, affine or bspline) has to be selected (see Transformations section for explanation). For more advanced registrations the following adjustments can be made in the plugin:  Masks for both the fixed and the moving images can be selected to let elastix only include certain areas in the registration. These masks have to be loaded into napari and selected in the correct mask dropdown menus, which appear when the masks box is ticked. Point sets for both the fixed and the moving images can de selected to use certain points to aid registration. These point set files have to be .txt files in the following format:  index/point\\\\   #points\\\\   point1 x point1 y [point1 z]\\\\   point2 x point2 y [point2 z] The first line indicates whether the points are given as “indices” (of the fixed image), or as “points” (in   physical coordinates). The second line stores the number of points that will be specified. After that the   point data is given. For example: point\\\\   3\\\\   2.32 5.34 -4.12\\\\   -1.56 0.12 9.23\\\\   1.00 7.34 -0.23   An initial transform file that specifies a transform that is applied before the registration is done, can be uploaded as a .txt file. For the latest file and transform formats that are supported, see the elastix manual   For the most common registration parameters adjustments can be made in the plugin GUI   Other, less common registration parameters can be adjusted by uploading custom transform parameter file(s). (Select 'custom' in the preset dropdown).    Transformations In the plugin 3 common transformations are available as presets, other transformations can be done with the 'custom' option in the preset dropdown. The plugin then has the ability to upload custom parameter files in which other transformations can be specified. The three common transformations are:   Rigid Transform: Also known as a Euclidean transformation, this transform preserves the Euclidean distance between each pair of points on the image. This includes rotation, translation and reflection but not scaling or shearing.   Affine Transform: This transfrom preserves lines and parallelism, but not necessarily distance and angles. Translation, scaling, similarity, reflection, rotation and shearing are all valid affine transformations.   BSpline Transform: This is a deformable transformation that preserves none of the properties mentioned in the transforms describe above.   Getting Help If you find a bug in the elastix napari plugin, or would like support with using it, please raise an issue on the GitHub repository. For question specifically about the elastix toolbox we have a mailing list. Contributions Contributions to the elastix_napari plugin, itkelastix (the python wrapper) or elastix (the C++ core) on which the plugin is build, are welcome.",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "elastix-napari",
    "documentation": "",
    "first_released": "2021-03-24T08:31:46.514582Z",
    "license": "Apache-2.0",
    "name": "elastix-napari",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://elastix.lumc.nl/",
    "python_version": ">=3.6",
    "reader_file_extensions": [],
    "release_date": "2021-07-05T08:34:36.293228Z",
    "report_issues": "",
    "requirements": [
      "itk-elastix (>=0.11.1)",
      "numpy (>=1.19.0)",
      "napari (>=0.4.6)",
      "napari-plugin-engine (>=0.1.4)",
      "magicgui (>=0.2.6)",
      "itk-napari-conversion (>=0.3.1)",
      "napari-itk-io (>=0.1.0)"
    ],
    "summary": "A toolbox for rigid and nonrigid registration of images.",
    "support": "https://groups.google.com/g/elastix-imageregistration",
    "twitter": "",
    "version": "0.1.7",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "email": "dev@biberger.xyz", "name": "Simon Biberger" }],
    "code_repository": "https://github.com/biberger/napari-ccp4map",
    "description": "# napari-ccp4map  [![License](https://img.shields.io/pypi/l/napari-ccp4map.svg?color=green)](https://github.com/biberger/napari-ccp4map/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-ccp4map.svg?color=green)](https://pypi.org/project/napari-ccp4map) [![Python Version](https://img.shields.io/pypi/pyversions/napari-ccp4map.svg?color=green)](https://python.org) [![tests](https://github.com/biberger/napari-ccp4map/workflows/tests/badge.svg)](https://github.com/biberger/napari-ccp4map/actions) [![codecov](https://codecov.io/gh/biberger/napari-ccp4map/branch/master/graph/badge.svg)](https://codecov.io/gh/biberger/napari-ccp4map)  Enables napari to read .map files in the ccp4 format.  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/docs/plugins/index.html -->  ## Installation  You can install `napari-ccp4map` via [pip]:      pip install napari-ccp4map  ## Usage If the plugin was installed correctly, it will pop up in a napari window under Plugins->Install/Uninstall Plugins. You can either drag&drop filed into the window to read them, or search for a folder/file using Ctrl+O.  ## How it works This plugin simply reads a file and allows [gemmi](https://github.com/project-gemmi/gemmi) to interact with it. Then, numpy turns the file into an array.  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-ccp4map\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/biberger/napari-ccp4map/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-ccp4map      Enables napari to read .map files in the ccp4 format.  This napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.  Installation You can install napari-ccp4map via pip: pip install napari-ccp4map  Usage If the plugin was installed correctly, it will pop up in a napari window under Plugins->Install/Uninstall Plugins. You can either drag&drop filed into the window to read them, or search for a folder/file using Ctrl+O. How it works This plugin simply reads a file and allows gemmi to interact with it. Then, numpy turns the file into an array. Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-ccp4map\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari-ccp4map",
    "documentation": "https://github.com/biberger/napari-ccp4map#README.md",
    "first_released": "2021-10-04T17:00:03.473074Z",
    "license": "BSD-3-Clause",
    "name": "napari-ccp4map",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader"],
    "project_site": "https://github.com/biberger/napari-ccp4map",
    "python_version": ">=3.7",
    "reader_file_extensions": ["*"],
    "release_date": "2021-10-04T17:49:51.073658Z",
    "report_issues": "https://github.com/biberger/napari-ccp4map/issues",
    "requirements": ["napari-plugin-engine (>=0.1.4)", "numpy", "gemmi"],
    "summary": "Enables napari to read .map files in the ccp4 format. Drag&Drop or press Ctrl+O to read files.",
    "support": "https://github.com/biberger/napari-ccp4map/issues",
    "twitter": "",
    "version": "1.0",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Benjamin Grädel" }],
    "code_repository": "https://github.com/bgraedel/arcos-gui",
    "conda": [{ "channel": "conda-forge", "package": "arcos-gui" }],
    "description": "# arcos-gui  [![License](https://img.shields.io/pypi/l/arcos-gui.svg?color=green)](https://github.com/bgraedel/arcos-gui/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/arcos-gui.svg?color=green)](https://pypi.org/project/arcos-gui) [![Python Version](https://img.shields.io/pypi/pyversions/arcos-gui.svg?color=green)](https://python.org) [![tests](https://github.com/bgraedel/arcos-gui/workflows/tests/badge.svg)](https://github.com/bgraedel/arcos-gui/actions) [![codecov](https://codecov.io/gh/bgraedel/arcos-gui/branch/main/graph/badge.svg)](https://codecov.io/gh/bgraedel/arcos-gui) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/arcos-gui)](https://napari-hub.org/plugins/arcos-gui)  A napari plugin to detect and visualize collective signaling events  ---------------------------------- - Package specific Documentation: <https://bgraedel.github.io/arcos-gui> - ARCOS documentation: <https://arcos.gitbook.io>  **A**utomated **R**ecognition of **C**ollective **S**ignalling (ARCOS) is an algorithm to identify collective spatial events in time series data, that was written by Maciej Dobrzynski (https://github.com/dmattek). It is available as an R (ARCOS) and python (arcos4py) package. ARCOS can identify and visualize collective protein activation in 2- and 3D cell cultures over time.  This plugin integrates ARCOS into napari. Users can import tracked time-series data in CSV format. The plugin provides GUI elements to process this data with ARCOS. Layers containing the detected collective events are subsequently added to the viewer.  Following analysis, the user can export the output as a CSV file with the detected collective events or as a sequence of images to generate a movie.   ![](https://user-images.githubusercontent.com/100028238/177808096-399abe6c-37a7-473b-96d2-afda5042a51e.gif)  [Watch full demo on youtube](https://www.youtube.com/watch?v=hG_z_BFcAiQ)   # Installation  You can install `arcos-gui` via [pip]:      pip install arcos-gui  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"arcos-gui\" is free and open-source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/bgraedel/arcos-gui/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/arcos-gui/ [PyPI]: https://pypi.org/  # Credits  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template. ",
    "description_content_type": "text/markdown",
    "description_text": "arcos-gui       A napari plugin to detect and visualize collective signaling events   Package specific Documentation: https://bgraedel.github.io/arcos-gui ARCOS documentation: https://arcos.gitbook.io  Automated Recognition of Collective Signalling (ARCOS) is an algorithm to identify collective spatial events in time series data, that was written by Maciej Dobrzynski (https://github.com/dmattek). It is available as an R (ARCOS) and python (arcos4py) package. ARCOS can identify and visualize collective protein activation in 2- and 3D cell cultures over time. This plugin integrates ARCOS into napari. Users can import tracked time-series data in CSV format. The plugin provides GUI elements to process this data with ARCOS. Layers containing the detected collective events are subsequently added to the viewer. Following analysis, the user can export the output as a CSV file with the detected collective events or as a sequence of images to generate a movie.  Watch full demo on youtube Installation You can install arcos-gui via pip: pip install arcos-gui  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"arcos-gui\" is free and open-source software Issues If you encounter any problems, please file an issue along with a detailed description. Credits This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "napari ARCOS",
    "documentation": "https://bgraedel.github.io/arcos-gui/",
    "first_released": "2022-02-24T16:00:50.612224Z",
    "license": "BSD-3-Clause",
    "name": "arcos-gui",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget", "sample_data"],
    "project_site": "https://github.com/bgraedel/arcos-gui",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-07-07T15:33:53.627299Z",
    "report_issues": "https://github.com/bgraedel/arcos-gui/issues",
    "requirements": [
      "arcos4py (>=0.1.4)",
      "magicgui (>=0.3.0)",
      "matplotlib (>=3.3.4)",
      "napari (>=0.4.14)",
      "numpy (>=1.21.5)",
      "pandas (>=1.3.5)",
      "scikit-image (>=0.18.1)",
      "scipy (>=1.7.3)",
      "mkdocs ; extra == 'doc'",
      "mkdocs-include-markdown-plugin ; extra == 'doc'",
      "mkdocs-material ; extra == 'doc'",
      "mkdocs-material-extensions ; extra == 'doc'",
      "pyqt5 ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'"
    ],
    "summary": "A napari plugin to detect and visualize collective signaling events",
    "support": "https://github.com/bgraedel/arcos-gui/issues",
    "twitter": "",
    "version": "0.0.6",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Alan R. Lowe" }],
    "category": {
      "Supported data": ["2D", "3D", "Time series", "Multi-channel"],
      "Workflow step": ["Object tracking"]
    },
    "category_hierarchy": {
      "Supported data": [["2D"], ["3D"], ["Time series"], ["Multi-channel"]],
      "Workflow step": [
        ["Object tracking", "Isolated object tracking", "Cell tracking"],
        ["Object tracking", "Cell lineage extraction"],
        ["Object tracking"]
      ]
    },
    "code_repository": "https://github.com/quantumjot/BayesianTracker",
    "description": "[![PyPI](https://img.shields.io/pypi/v/btrack)](https://pypi.org/project/btrack) [![Supported Python versions](https://img.shields.io/pypi/pyversions/btrack.svg)](https://python.org) [![Downloads](https://pepy.tech/badge/btrack/month)](https://pepy.tech/project/btrack) [![Black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black) [![Tests](https://github.com/quantumjot/BayesianTracker/actions/workflows/test.yml/badge.svg)](https://github.com/quantumjot/BayesianTracker/actions/workflows/test.yml) [![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&logoColor=white)](https://github.com/pre-commit/pre-commit) [![Documentation](https://readthedocs.org/projects/btrack/badge/?version=latest)](https://btrack.readthedocs.io/en/latest/?badge=latest) [![codecov](https://codecov.io/gh/quantumjot/BayesianTracker/branch/main/graph/badge.svg?token=QCFC9AWK0R)](https://codecov.io/gh/quantumjot/BayesianTracker) [![doi:10.3389/fcomp.2021.734559](https://img.shields.io/badge/doi-10.3389%2Ffcomp.2021.734559-blue)](https://doi.org/10.3389/fcomp.2021.734559)  ![logo](https://btrack.readthedocs.io/en/latest/_images/btrack_logo.png)   BayesianTracker (`btrack`) is a multi object tracking algorithm, specifically used to reconstruct trajectories in crowded fields.  New observations are assigned to tracks by evaluating the posterior probability of each potential linkage from a Bayesian belief matrix for all possible linkages.  We developed `btrack` for cell tracking in time-lapse microscopy data.  ![](https://raw.githubusercontent.com/lowe-lab-ucl/arboretum/master/examples/arboretum.gif)  <!-- ## tutorials  * https://napari.org/tutorials/tracking/cell_tracking.html -->   ## associated plugins  * [napari-arboretum](https://www.napari-hub.org/plugins/napari-arboretum) - Napari plugin to enable track graph and lineage tree visualization. * [napari-btrack](https://github.com/lowe-lab-ucl/napari-btrack) - (Experimental) Napari plugin to provide a frontend GUI for `btrack`. ",
    "description_content_type": "text/markdown",
    "description_text": "          BayesianTracker (btrack) is a multi object tracking algorithm, specifically used to reconstruct trajectories in crowded fields.  New observations are assigned to tracks by evaluating the posterior probability of each potential linkage from a Bayesian belief matrix for all possible linkages. We developed btrack for cell tracking in time-lapse microscopy data.   associated plugins  napari-arboretum - Napari plugin to enable track graph and lineage tree visualization. napari-btrack - (Experimental) Napari plugin to provide a frontend GUI for btrack. ",
    "development_status": [],
    "display_name": "btrack",
    "documentation": "https://btrack.readthedocs.io/en/stable/",
    "first_released": "2020-05-27T07:49:37.422791Z",
    "license": "MIT",
    "name": "btrack",
    "npe2": true,
    "operating_system": [
      "Operating System :: MacOS",
      "Operating System :: Microsoft :: Windows",
      "Operating System :: POSIX",
      "Operating System :: Unix"
    ],
    "plugin_types": ["reader"],
    "project_site": "https://github.com/quantumjot/BayesianTracker",
    "python_version": ">=3.7",
    "reader_file_extensions": ["*.hdf5", "*.hdf", "*.h5"],
    "release_date": "2022-12-05T16:52:14.480984Z",
    "report_issues": "https://github.com/quantumjot/BayesianTracker/issues",
    "requirements": [
      "cvxopt (>=1.2.0)",
      "h5py (>=2.10.0)",
      "numpy (>=1.17.3)",
      "pooch (>=1.0.0)",
      "pydantic (>=1.9.0)",
      "scikit-image (>=0.16.2)",
      "scipy (>=1.3.1)",
      "numpydoc ; extra == 'docs'",
      "sphinx ; extra == 'docs'",
      "sphinx-automodapi ; extra == 'docs'",
      "sphinx-panels ; extra == 'docs'",
      "sphinx-rtd-theme ; extra == 'docs'",
      "napari (>=0.4.16) ; extra == 'napari'"
    ],
    "summary": "A framework for Bayesian multi-object tracking",
    "support": "https://github.com/quantumjot/BayesianTracker/issues",
    "twitter": "",
    "version": "0.5.0",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Eduardo Gonzalez Solares" }],
    "code_repository": null,
    "description": "# IMAXT multiscale napari plugin  [![License GNU LGPL v3.0](https://img.shields.io/pypi/l/imaxt-multiscale-plugin.svg?color=green)](https://github.com/eg266/imaxt-multiscale-plugin/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/imaxt-multiscale-plugin.svg?color=green)](https://pypi.org/project/imaxt-multiscale-plugin) [![Python Version](https://img.shields.io/pypi/pyversions/imaxt-multiscale-plugin.svg?color=green)](https://python.org) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/imaxt-multiscale-plugin)](https://napari-hub.org/plugins/imaxt-multiscale-plugin)  A napari plugin to visualize multi-resolution images created with the IMAXT mosaic pipeline.  ----------------------------------------------------  ## Installation  You can install `imaxt-multiscale-plugin` via [pip]:      pip install imaxt-multiscale-plugin   ## Usage  Run [napari] with the name of the sample to visualize either a local path:      napari /storage/imaxt/eglez/processed/stpt/20220606_PDX_AB559_GFP_005503_100x15um  or a sample in S3 storage:      napari s3://imaxtgw/stpt/20220608_DI_PDX_SA535_Tum_5223_04280_100x15um      ## Screenshots  ![Alt text](https://gitlab.developers.cam.ac.uk/astronomy/camcead/imaxt/imaxt-multiscale-plugin/-/raw/main/assets/napari1.png \"a title\") ![Alt text](https://gitlab.developers.cam.ac.uk/astronomy/camcead/imaxt/imaxt-multiscale-plugin/-/raw/main/assets/napari2.png \"a title\") ![Alt text](https://gitlab.developers.cam.ac.uk/astronomy/camcead/imaxt/imaxt-multiscale-plugin/-/raw/main/assets/napari3.png \"a title\") ![Alt text](https://gitlab.developers.cam.ac.uk/astronomy/camcead/imaxt/imaxt-multiscale-plugin/-/raw/main/assets/napari4.png \"a title\")  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [GNU LGPL v3.0] license, \"imaxt-multiscale-plugin\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "IMAXT multiscale napari plugin     A napari plugin to visualize multi-resolution images created with the IMAXT mosaic pipeline.  Installation You can install imaxt-multiscale-plugin via pip: pip install imaxt-multiscale-plugin  Usage Run napari with the name of the sample to visualize either a local path: napari /storage/imaxt/eglez/processed/stpt/20220606_PDX_AB559_GFP_005503_100x15um  or a sample in S3 storage: napari s3://imaxtgw/stpt/20220608_DI_PDX_SA535_Tum_5223_04280_100x15um  Screenshots     Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the GNU LGPL v3.0 license, \"imaxt-multiscale-plugin\" is free and open source software Issues If you encounter any problems, please [file an issue] along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "IMAXT Multiscale Image Napari Plugin",
    "documentation": "",
    "first_released": "2022-07-27T15:03:33.793891Z",
    "license": "LGPL-3.0-only",
    "name": "imaxt-multiscale-plugin",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader", "writer", "widget"],
    "project_site": "https://gitlab.developers.cam.ac.uk/astronomy/camcead/imaxt/imaxt-multiscale-plugin",
    "python_version": ">=3.8",
    "reader_file_extensions": ["*"],
    "release_date": "2022-12-11T09:43:26.452179Z",
    "report_issues": "",
    "requirements": [
      "numpy",
      "magicgui",
      "qtpy",
      "imaxt-image",
      "xarray",
      "dask",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "summary": "A simple plugin to use with napari",
    "support": "",
    "twitter": "",
    "version": "0.2.2",
    "writer_file_extensions": [".tif", ".tiff", ".npy"],
    "writer_save_layers": ["image", "labels*", "image*"]
  },
  {
    "authors": [
      { "email": "robert.haase@tu-dresden.de", "name": "Robert Haase" }
    ],
    "code_repository": "https://github.com/haesleinhuepf/beetlesafari",
    "conda": [{ "channel": "conda-forge", "package": "beetlesafari" }],
    "description": "A library for working with light sheet imaging data of developing embryos acquired using [ClearControl](https://github.com/ClearControl) at the [Center for Systems Biology Dresden](https://www.csbdresden.de/), e.g. _Tribolium castaneum_.  # Installation ``` conda install -c conda-forge pyopencl pip install beetlesafari ``` ",
    "description_content_type": "text/markdown",
    "description_text": "A library for working with light sheet imaging data of developing embryos acquired using ClearControl at the Center for Systems Biology Dresden, e.g. Tribolium castaneum. Installation conda install -c conda-forge pyopencl pip install beetlesafari",
    "development_status": ["Development Status :: 3 - Alpha"],
    "display_name": "beetlesafari",
    "documentation": "",
    "first_released": "2021-06-04T17:04:34.199080Z",
    "license": "BSD-3-Clause",
    "name": "beetlesafari",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": [],
    "project_site": "https://github.com/haesleinhuepf/beetlesafari",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-06-21T03:20:03.238550Z",
    "report_issues": "",
    "requirements": [
      "numpy",
      "pyopencl",
      "toolz",
      "scikit-image",
      "requests",
      "pyclesperanto-prototype",
      "napari",
      "magicgui",
      "dask",
      "cachetools",
      "napari-tools-menu"
    ],
    "summary": "A napari plugin for loading and working with light sheet imaging data of developing embryos acquired using ClearControl, e.g. _Tribolium castaneum_.",
    "support": "",
    "twitter": "",
    "version": "0.4.0",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "email": "code@adamltyson.com", "name": "Adam Tyson" }],
    "category": {
      "Supported data": ["3D"],
      "Workflow step": [
        "Visualization",
        "Image classification",
        "Image registration"
      ]
    },
    "category_hierarchy": {
      "Supported data": [["3D"]],
      "Workflow step": [
        ["Visualization", "Image visualisation"],
        ["Visualization", "Image visualisation", "Overlay"],
        ["Visualization", "Image visualisation", "Slice rendering"],
        ["Image classification"],
        ["Image registration"]
      ]
    },
    "code_repository": "https://github.com/brainglobe/brainglobe-napari-io",
    "description": "# napari-brainglobe-io  [![License](https://img.shields.io/pypi/l/brainglobe-napari-io.svg?color=green)](https://github.com/napari/brainglobe-napari-io/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/brainglobe-napari-io.svg?color=green)](https://pypi.org/project/brainglobe-napari-io) [![Python Version](https://img.shields.io/pypi/pyversions/brainglobe-napari-io.svg?color=green)](https://python.org) [![tests](https://github.com/brainglobe/brainglobe-napari-io/workflows/tests/badge.svg)](https://github.com/brainglobe/brainglobe-napari-io/actions) [![codecov](https://codecov.io/gh/brainglobe/brainglobe-napari-io/branch/master/graph/badge.svg)](https://codecov.io/gh/brainglobe/brainglobe-napari-io)  Visualise cellfinder and brainreg results with napari   ----------------------------------   ## Installation This package is likely already installed  (e.g. with cellfinder, brainreg or another napari plugin), but if you want to  install it again, either use the napari plugin install GUI or you can  install `brainglobe-napari-io` via [pip]:      pip install brainglobe-napari-io  ## Usage * Open napari (however you normally do it, but typically just type `napari` into your terminal, or click on your desktop icon)  ### brainreg #### Sample space Drag your [brainreg](https://github.com/brainglobe/brainreg) output directory (the one with the log file) onto the napari window.      Various images should then open, including: * `Registered image` - the image used for registration, downsampled to atlas resolution * `atlas_name` - e.g. `allen_mouse_25um` the atlas labels, warped to your sample brain * `Boundaries` - the boundaries of the atlas regions  If you downsampled additional channels, these will also be loaded.  Most of these images will not be visible by default. Click the little eye icon to toggle visibility.  _N.B. If you use a high resolution atlas (such as `allen_mouse_10um`), then the files can take a little while to load._  ![sample_space](https://raw.githubusercontent.com/brainglobe/brainglobe-napari-io/master/resources/sample_space.gif)   #### Atlas space `napari-brainreg` also comes with an additional plugin, for visualising your data  in atlas space.   This is typically only used in other software, but you can enable it yourself: * Open napari * Navigate to `Plugins` -> `Plugin Call Order` * In the `Plugin Sorter` window, select `napari_get_reader` from the `select hook...` dropdown box * Drag `brainreg_read_dir_standard_space` (the atlas space viewer plugin) above `brainreg_read_dir` (the normal plugin) to ensure that the atlas space plugin is used preferentially.   ### cellfinder #### Load cellfinder XML file * Load your raw data (drag and drop the data directories into napari, one at a time) * Drag and drop your cellfinder XML file (e.g. `cell_classification.xml`) into napari.  #### Load cellfinder directory * Load your raw data (drag and drop the data directories into napari, one at a time) * Drag and drop your cellfinder output directory into napari.  The plugin will then load your detected cells (in yellow) and the rejected cell  candidates (in blue). If you carried out registration, then these results will be  overlaid (similarly to the loading brainreg data, but transformed to the  coordinate space of your raw data).  ![load_data](https://raw.githubusercontent.com/brainglobe/brainglobe-napari-io/master/resources/load_data.gif) **Loading raw data**  ![load_data](https://raw.githubusercontent.com/brainglobe/brainglobe-napari-io/master/resources/load_results.gif) **Loading cellfinder results**    ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [MIT] license, \"brainglobe-napari-io\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin [file an issue]: https://github.com/brainglobe/brainglobe-napari-io/issues [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/    --- The BrainGlobe project is generously supported by the Sainsbury Wellcome Centre and the Institute of Neuroscience, Technical University of Munich, with funding from Wellcome, the Gatsby Charitable Foundation and the Munich Cluster for Systems Neurology - Synergy.  <img src='https://brainglobe.info/images/logos_combined.png' width=\"550\">   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-brainglobe-io      Visualise cellfinder and brainreg results with napari  Installation This package is likely already installed  (e.g. with cellfinder, brainreg or another napari plugin), but if you want to  install it again, either use the napari plugin install GUI or you can  install brainglobe-napari-io via pip: pip install brainglobe-napari-io  Usage  Open napari (however you normally do it, but typically just type napari into your terminal, or click on your desktop icon)  brainreg Sample space Drag your brainreg output directory (the one with the log file) onto the napari window. Various images should then open, including: * Registered image - the image used for registration, downsampled to atlas resolution * atlas_name - e.g. allen_mouse_25um the atlas labels, warped to your sample brain * Boundaries - the boundaries of the atlas regions If you downsampled additional channels, these will also be loaded. Most of these images will not be visible by default. Click the little eye icon to toggle visibility. N.B. If you use a high resolution atlas (such as allen_mouse_10um), then the files can take a little while to load.  Atlas space napari-brainreg also comes with an additional plugin, for visualising your data  in atlas space.  This is typically only used in other software, but you can enable it yourself: * Open napari * Navigate to Plugins -> Plugin Call Order * In the Plugin Sorter window, select napari_get_reader from the select hook... dropdown box * Drag brainreg_read_dir_standard_space (the atlas space viewer plugin) above brainreg_read_dir (the normal plugin) to ensure that the atlas space plugin is used preferentially. cellfinder Load cellfinder XML file  Load your raw data (drag and drop the data directories into napari, one at a time) Drag and drop your cellfinder XML file (e.g. cell_classification.xml) into napari.  Load cellfinder directory  Load your raw data (drag and drop the data directories into napari, one at a time) Drag and drop your cellfinder output directory into napari.  The plugin will then load your detected cells (in yellow) and the rejected cell  candidates (in blue). If you carried out registration, then these results will be  overlaid (similarly to the loading brainreg data, but transformed to the  coordinate space of your raw data).  Loading raw data  Loading cellfinder results Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the MIT license, \"brainglobe-napari-io\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.  The BrainGlobe project is generously supported by the Sainsbury Wellcome Centre and the Institute of Neuroscience, Technical University of Munich, with funding from Wellcome, the Gatsby Charitable Foundation and the Munich Cluster for Systems Neurology - Synergy. ",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "brainglobe-napari-io",
    "documentation": "https://docs.brainglobe.info",
    "first_released": "2021-03-12T12:52:23.068881Z",
    "license": "BSD-3-Clause",
    "name": "brainglobe-napari-io",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader", "writer"],
    "project_site": "https://brainglobe.info",
    "python_version": ">=3.6",
    "reader_file_extensions": ["*.xml", "*.tif", "*.tiff"],
    "release_date": "2022-03-18T11:54:20.055771Z",
    "report_issues": "https://github.com/brainglobe/brainglobe-napari-io/issues",
    "requirements": [
      "napari",
      "napari-plugin-engine (>=0.1.4)",
      "napari-ndtiffs",
      "tifffile (>=2020.8.13)",
      "imlib",
      "bg-space",
      "bg-atlasapi",
      "black ; extra == 'dev'",
      "pytest-cov ; extra == 'dev'",
      "pytest ; extra == 'dev'",
      "gitpython ; extra == 'dev'",
      "coverage (>=5.0.3) ; extra == 'dev'",
      "bump2version ; extra == 'dev'",
      "pre-commit ; extra == 'dev'",
      "flake8 ; extra == 'dev'"
    ],
    "summary": "Read and write files from the BrainGlobe neuroanatomy suite",
    "support": "https://forum.image.sc/tag/brainglobe",
    "twitter": "https://twitter.com/brain_globe",
    "version": "0.1.5",
    "visibility": "public",
    "writer_file_extensions": [".xml"],
    "writer_save_layers": ["points", "points+"]
  },
  {
    "authors": [{ "name": "Marc Boucsein" }, { "name": "Robin Koch" }],
    "code_repository": "https://github.com/MBPhys/World2Data",
    "conda": [{ "channel": "conda-forge", "package": "world2data" }],
    "description": "# World2Data  [![License](https://img.shields.io/pypi/l/napari-medical-image-formats.svg?color=green)](https://github.com/MBPhys/World2Data/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/World2Data.svg?color=green)](https://pypi.org/project/World2Data) [![Python Version](https://img.shields.io/pypi/pyversions/World2Data.svg?color=green)](https://python.org)   A napari plugin in order to convert the world information to the data of a 2D/3D layer.  ----------------------------------  ## Installation  You can install `World2Data` via [pip]:      pip install World2Data  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"World2Data\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin [file an issue]: https://github.com/MBPhys/World2Data/issues [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "World2Data    A napari plugin in order to convert the world information to the data of a 2D/3D layer.  Installation You can install World2Data via pip: pip install World2Data  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"World2Data\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "World2Data",
    "documentation": "",
    "first_released": "2022-01-14T14:59:38.742805Z",
    "license": "BSD-3-Clause",
    "name": "World2Data",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/MBPhys/World2Data",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-01-14T14:59:38.742805Z",
    "report_issues": "",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "gryds",
      "dask",
      "scikit-image"
    ],
    "summary": "A napari plugin in order to convert the world information to the data of a 2D/3D layer",
    "support": "",
    "twitter": "",
    "version": "0.0.3",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Grzegorz Bokota" }],
    "code_repository": "https://github.com/4DNucleome/PartSeg-smfish",
    "description": "# PartSeg-smfish  [![License BSD-3](https://img.shields.io/pypi/l/PartSeg-smfish.svg?color=green)](https://github.com/4DNucleome/PartSeg-smfish/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/PartSeg-smfish.svg?color=green)](https://pypi.org/project/PartSeg-smfish) [![Python Version](https://img.shields.io/pypi/pyversions/PartSeg-smfish.svg?color=green)](https://python.org) [![tests](https://github.com/4DNucleome/PartSeg-smfish/workflows/tests/badge.svg)](https://github.com/4DNucleome/PartSeg-smfish/actions) [![codecov](https://codecov.io/gh/4DNucleome/PartSeg-smfish/branch/main/graph/badge.svg)](https://codecov.io/gh/4DNucleome/PartSeg-smfish) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/PartSeg-smfish)](https://napari-hub.org/plugins/PartSeg-smfish)  PartSeg and napari plugin for smfish data  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/stable/plugins/index.html -->  ## Installation  You can install `PartSeg-smfish` via [pip]:      pip install PartSeg-smfish    To install latest development version :      pip install git+https://github.com/4DNucleome/PartSeg-smfish.git   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"PartSeg-smfish\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/4DNucleome/PartSeg-smfish/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "PartSeg-smfish       PartSeg and napari plugin for smfish data  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install PartSeg-smfish via pip: pip install PartSeg-smfish  To install latest development version : pip install git+https://github.com/4DNucleome/PartSeg-smfish.git  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"PartSeg-smfish\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "PartSeg-smfish",
    "documentation": "https://github.com/4DNucleome/PartSeg-smfish#README.md",
    "first_released": "2022-10-24T11:04:32.268909Z",
    "license": "BSD-3-Clause",
    "name": "PartSeg-smfish",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/4DNucleome/PartSeg-smfish",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-12-06T10:08:29.060888Z",
    "report_issues": "https://github.com/4DNucleome/PartSeg-smfish/issues",
    "requirements": [
      "PartSeg (>=0.13.0)",
      "numpy",
      "napari",
      "qtpy",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'"
    ],
    "summary": "PartSeg and napari plugin for smfish data",
    "support": "https://github.com/4DNucleome/PartSeg-smfish/issues",
    "twitter": "",
    "version": "0.1.3",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Riley M Shea" }],
    "code_repository": null,
    "conda": [{ "channel": "conda-forge", "package": "avidaq" }],
    "description": "# avidaq  [![PyPI](https://img.shields.io/pypi/v/avidaq.svg?color=green)](https://pypi.org/project/avidaq) [![Python Version](https://img.shields.io/pypi/pyversions/avidaq.svg?color=green)](https://python.org) [![tests](https://github.com/optimax/avidaq/workflows/tests/badge.svg)](https://github.com/optimax/avidaq/actions) [![codecov](https://codecov.io/gh/optimax/avidaq/branch/main/graph/badge.svg)](https://codecov.io/gh/optimax/avidaq) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/avidaq)](https://napari-hub.org/plugins/avidaq)  controls for napari and micromanger  ---  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/plugins/index.html -->  ## Installation  ### Standard installation  You can install `avidaq` via [pip]:  ```shell pip install napari[all] avidaq ```  ### Install from plugin menu  Alternatively you can install `avidaq` via the [napari] plugin menu:  ## ![napari-add-plugin](napari-add-plugin.png)  ## Running  First start micromanager.  Make sure the server port checkbox is activated.  Then to start napari with the avidaq plugin active run: `napari -w avidaq`  ![](screenshot.png)  ## Updating presets  MDA presets are stored in a json file in the user's home directory.  ```shell  `C:\\\\\\\\Users\\\\YourName\\\\.avidaq\\\\mda_presets.json` ```  This file should exist after plugin installation with some defaults. You do not need to create the file yourself.  Add or modify the values and reload napari to see the changes.  All parameter entries are optional, if not provided the default value will be used.  The parameter names and their descriptions can be found [here] (https://github.com/micro-manager/pycro-manager/blob/main/pycromanager/acq_util.py#L102-L115)  The format is as follows:  ```json {     \"gui_display_name\": {         \"parameter_name\": value,         \"parameter_name\": value,         ...     },     \"gui_display_name\": {         \"parameter_name\": value,         \"parameter_name\": value,         ...     },     ... } ```  defaults:  ```json {   \"Basic\": {     \"num_time_points\": 5,     \"z_start\": 0,     \"z_end\": 6,     \"z_step\": 0.4   },   \"Simple\": {     \"num_time_points\": 2,     \"z_start\": 0,     \"z_end\": 2,     \"z_step\": 0.1   },   \"Detailed\": {     \"num_time_points\": 10,     \"z_start\": 0,     \"z_end\": 12,     \"z_step\": 0.2   } } ```  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  ## Development  You should have python3.8 or higher installed.  1. clone this repo 2. create a virtual environment `python -m venv .venv && source .venv/bin/activate` 3. run `pip install -e '.[testing,build]'` 4. run `pre-commit install`  ### To run unit tests  `pytest`  ### typical workflow  1. edit code in `/src` 2. run napari -w avidaq 3. repeat  ### Releasing to pypi   Project is automically built and deployed to pypi upon   ---  [napari]: https://github.com/napari/napari [cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [mit]: http://opensource.org/licenses/MIT [bsd-3]: http://opensource.org/licenses/BSD-3-Clause [gnu gpl v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [gnu lgpl v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [apache software license 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [mozilla public license 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [pypi]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "avidaq      controls for napari and micromanger  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation Standard installation You can install avidaq via pip: shell pip install napari[all] avidaq Install from plugin menu Alternatively you can install avidaq via the napari plugin menu:  Running First start micromanager.  Make sure the server port checkbox is activated. Then to start napari with the avidaq plugin active run: napari -w avidaq  Updating presets MDA presets are stored in a json file in the user's home directory. ```shell C:\\\\\\\\Users\\\\YourName\\\\.avidaq\\\\mda_presets.json ``` This file should exist after plugin installation with some defaults. You do not need to create the file yourself. Add or modify the values and reload napari to see the changes. All parameter entries are optional, if not provided the default value will be used. The parameter names and their descriptions can be found [here] (https://github.com/micro-manager/pycro-manager/blob/main/pycromanager/acq_util.py#L102-L115) The format is as follows: json {     \"gui_display_name\": {         \"parameter_name\": value,         \"parameter_name\": value,         ...     },     \"gui_display_name\": {         \"parameter_name\": value,         \"parameter_name\": value,         ...     },     ... } defaults: json {   \"Basic\": {     \"num_time_points\": 5,     \"z_start\": 0,     \"z_end\": 6,     \"z_step\": 0.4   },   \"Simple\": {     \"num_time_points\": 2,     \"z_start\": 0,     \"z_end\": 2,     \"z_step\": 0.1   },   \"Detailed\": {     \"num_time_points\": 10,     \"z_start\": 0,     \"z_end\": 12,     \"z_step\": 0.2   } } Issues If you encounter any problems, please [file an issue] along with a detailed description. Development You should have python3.8 or higher installed.  clone this repo create a virtual environment python -m venv .venv && source .venv/bin/activate run pip install -e '.[testing,build]' run pre-commit install  To run unit tests pytest typical workflow  edit code in /src run napari -w avidaq repeat  Releasing to pypi Project is automically built and deployed to pypi upon ",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari avidaq",
    "documentation": "",
    "first_released": "2022-07-21T16:20:38.686709Z",
    "license": "BSD-3-Clause",
    "name": "avidaq",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader", "writer", "widget", "sample_data"],
    "project_site": "",
    "python_version": ">=3.8",
    "reader_file_extensions": ["*.npy"],
    "release_date": "2022-08-25T15:58:05.024483Z",
    "report_issues": "",
    "requirements": [
      "magicgui",
      "numpy",
      "pycromanager",
      "qtpy",
      "twine ; extra == 'build'",
      "black ; extra == 'testing'",
      "ipykernel ; extra == 'testing'",
      "matplotlib ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'",
      "pyright ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "tox ; extra == 'testing'",
      "yappi ; extra == 'testing'"
    ],
    "summary": "controls for napari and micromanger",
    "support": "",
    "twitter": "",
    "version": "0.0.5",
    "writer_file_extensions": [".npy"],
    "writer_save_layers": ["image", "image*", "labels*"]
  },
  {
    "authors": [
      { "email": "stringerc@janelia.hhmi.org", "name": "Carsen Stringer" }
    ],
    "code_repository": "https://github.com/Mouseland/cellpose-napari",
    "conda": [{ "channel": "conda-forge", "package": "cellpose-napari" }],
    "description": "# cellpose-napari <img src=\"docs/_static/favicon.ico\" width=\"50\" title=\"cellpose\" alt=\"cellpose\" align=\"right\" vspace = \"50\">  [![Documentation Status](https://readthedocs.org/projects/cellpose-napari/badge/?version=latest)](https://cellpose-napari.readthedocs.io/en/latest/?badge=latest) [![tests](https://github.com/mouseland/cellpose-napari/workflows/tests/badge.svg)](https://github.com/mouseland/cellpose-napari/actions) [![codecov](https://codecov.io/gh/Mouseland/cellpose-napari/branch/main/graph/badge.svg)](https://codecov.io/gh/MouseLand/cellpose-napari) [![PyPI version](https://badge.fury.io/py/cellpose-napari.svg)](https://badge.fury.io/py/cellpose-napari) [![PyPI - Downloads](https://img.shields.io/pypi/dm/cellpose-napari)](https://pypistats.org/packages/cellpose-napari) [![Python version](https://img.shields.io/pypi/pyversions/cellpose-napari)](https://pypistats.org/packages/cellpose-napari) [![License](https://img.shields.io/pypi/l/cellpose-napari.svg?color=green)](https://github.com/mouseland/cellpose-napari/raw/master/LICENSE) [![Contributors](https://img.shields.io/github/contributors-anon/MouseLand/cellpose-napari)](https://github.com/MouseLand/cellpose-napari/graphs/contributors) [![website](https://img.shields.io/website?url=https%3A%2F%2Fwww.cellpose.org)](https://www.cellpose.org) [![GitHub stars](https://img.shields.io/github/stars/MouseLand/cellpose-napari?style=social)](https://github.com/MouseLand/cellpose-napari/) [![GitHub forks](https://img.shields.io/github/forks/MouseLand/cellpose-napari?style=social)](https://github.com/MouseLand/cellpose-napari/)  a napari plugin for anatomical segmentation of general cellular images  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.  The plugin code was written by Carsen Stringer, and the cellpose code was written by Carsen Stringer and Marius Pachitariu. To learn about Cellpose, read the [**paper**](https://t.co/kBMXmPp3Yn?amp=1) or watch this [**talk**](https://t.co/JChCsTD0SK?amp=1).   For support with the plugin, please open an [issue](https://github.com/MouseLand/cellpose-napari/issues). For support with cellpose, please open an [issue](https://github.com/MouseLand/cellpose/issues) on the cellpose repo.    If you use this plugin please cite the [paper](https://www.nature.com/articles/s41592-020-01018-x): ::            @article{stringer2021cellpose,       title={Cellpose: a generalist algorithm for cellular segmentation},       author={Stringer, Carsen and Wang, Tim and Michaelos, Michalis and Pachitariu, Marius},       journal={Nature Methods},       volume={18},       number={1},       pages={100--106},       year={2021},       publisher={Nature Publishing Group}       }   ![cellpose-napari_plugin](https://cellpose-napari.readthedocs.io/en/latest/_images/napari_main_demo_fast_small.gif?raw=true \"cellpose-napari\")  ## Installation  Install an [Anaconda](https://www.anaconda.com/download/) distribution of Python -- Choose **Python 3** and your operating system. Note you might need to use an anaconda prompt if you did not add anaconda to the path.  Install `napari` with pip: `pip install napari[all]`. Then install `cellpose-napari` via [pip]:      pip install cellpose-napari       Or install the plugin inside napari in the plugin window.  If install fails in your base environment, create a new environment: 1. Download the [`environment.yml`](https://github.com/MouseLand/cellpose-napari/blob/master/environment.yml?raw=true) file from the repository. You can do this by cloning the repository, or copy-pasting the text from the file into a text document on your local computer. 2. Open an anaconda prompt / command prompt with `conda` for **python 3** in the path 3. Change directories to where the `environment.yml` is and run `conda env create -f environment.yml` 4. To activate this new environment, run `conda activate cellpose-napari` 5. You should see `(cellpose-napari)` on the left side of the terminal line.   If you have **issues** with cellpose installation, see the [cellpose docs](https://cellpose.readthedocs.io/en/latest/installation.html) for more details, and then if the suggestions fail, open an issue.  ### Upgrading software  You can upgrade the plugin with ~~~ pip install cellpose-napari --upgrade ~~~  and you can upgrade cellpose with ~~~ pip install cellpose --upgrade ~~~  ### GPU version (CUDA) on Windows or Linux  If you plan on running many images, you may want to install a GPU version of *torch* (if it isn't already installed).  Before installing the GPU version, remove the CPU version: ~~~ pip uninstall torch ~~~  Follow the instructions [here](https://pytorch.org/get-started/locally/) to determine what version to install. The Anaconda install is recommended along with CUDA version 10.2. For instance this command will install the 10.2 version on Linux and Windows (note the `torchvision` and `torchaudio` commands are removed because cellpose doesn't require them):  ~~~ conda install pytorch cudatoolkit=10.2 -c pytorch ~~~~  When upgrading GPU Cellpose in the future, you will want to ignore dependencies (to ensure that the pip version of torch does not install): ~~~ pip install --no-deps cellpose --upgrade ~~~  ### Installation of github version  Follow steps from above to install the dependencies. In the github repository, run `pip install -e .` and the github version will be installed. If you want to go back to the pip version of cellpose-napari, then say `pip install cellpose-napari`.   ## Running the software   Open napari with the cellpose-napari dock widget open ``` napari -w cellpose-napari ```  There is sample data in the File menu, or get started with your own images!  ### Detailed usage [documentation](https://cellpose-napari.readthedocs.io/).  ## Contributing  Contributions are very welcome. Tests are run with pytest.  ## License  Distributed under the terms of the [BSD-3] license, \"cellpose-napari\" is free and open source software.  ## Dependencies cellpose-napari relies on the following excellent packages (which are automatically installed with conda/pip if missing): - [napari](https://napari.org) - [magicgui](https://napari.org/magicgui/)  cellpose relies on the following excellent packages (which are automatically installed with conda/pip if missing): - [torch](https://pytorch.org/) - [numpy](http://www.numpy.org/) (>=1.16.0) - [numba](http://numba.pydata.org/numba-doc/latest/user/5minguide.html) - [scipy](https://www.scipy.org/) - [natsort](https://natsort.readthedocs.io/en/master/) - [tifffile](https://pypi.org/project/tifffile/) - [opencv](https://opencv.org/)   [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin ",
    "description_content_type": "text/markdown",
    "description_text": "cellpose-napari             a napari plugin for anatomical segmentation of general cellular images  This napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template. The plugin code was written by Carsen Stringer, and the cellpose code was written by Carsen Stringer and Marius Pachitariu. To learn about Cellpose, read the paper or watch this talk.  For support with the plugin, please open an issue. For support with cellpose, please open an issue on the cellpose repo.  If you use this plugin please cite the paper: ::   @article{stringer2021cellpose,   title={Cellpose: a generalist algorithm for cellular segmentation},   author={Stringer, Carsen and Wang, Tim and Michaelos, Michalis and Pachitariu, Marius},   journal={Nature Methods},   volume={18},   number={1},   pages={100--106},   year={2021},   publisher={Nature Publishing Group}   }   Installation Install an Anaconda distribution of Python -- Choose Python 3 and your operating system. Note you might need to use an anaconda prompt if you did not add anaconda to the path. Install napari with pip: pip install napari[all]. Then install cellpose-napari via [pip]: pip install cellpose-napari  Or install the plugin inside napari in the plugin window. If install fails in your base environment, create a new environment: 1. Download the environment.yml file from the repository. You can do this by cloning the repository, or copy-pasting the text from the file into a text document on your local computer. 2. Open an anaconda prompt / command prompt with conda for python 3 in the path 3. Change directories to where the environment.yml is and run conda env create -f environment.yml 4. To activate this new environment, run conda activate cellpose-napari 5. You should see (cellpose-napari) on the left side of the terminal line.  If you have issues with cellpose installation, see the cellpose docs for more details, and then if the suggestions fail, open an issue. Upgrading software You can upgrade the plugin with ~~~ pip install cellpose-napari --upgrade ~~~ and you can upgrade cellpose with ~~~ pip install cellpose --upgrade ~~~ GPU version (CUDA) on Windows or Linux If you plan on running many images, you may want to install a GPU version of torch (if it isn't already installed). Before installing the GPU version, remove the CPU version: ~~~ pip uninstall torch ~~~ Follow the instructions here to determine what version to install. The Anaconda install is recommended along with CUDA version 10.2. For instance this command will install the 10.2 version on Linux and Windows (note the torchvision and torchaudio commands are removed because cellpose doesn't require them): ~~~ conda install pytorch cudatoolkit=10.2 -c pytorch ~~~~ When upgrading GPU Cellpose in the future, you will want to ignore dependencies (to ensure that the pip version of torch does not install): ~~~ pip install --no-deps cellpose --upgrade ~~~ Installation of github version Follow steps from above to install the dependencies. In the github repository, run pip install -e . and the github version will be installed. If you want to go back to the pip version of cellpose-napari, then say pip install cellpose-napari. Running the software Open napari with the cellpose-napari dock widget open napari -w cellpose-napari There is sample data in the File menu, or get started with your own images! Detailed usage documentation. Contributing Contributions are very welcome. Tests are run with pytest. License Distributed under the terms of the BSD-3 license, \"cellpose-napari\" is free and open source software. Dependencies cellpose-napari relies on the following excellent packages (which are automatically installed with conda/pip if missing): - napari - magicgui cellpose relies on the following excellent packages (which are automatically installed with conda/pip if missing): - torch - numpy (>=1.16.0) - numba - scipy - natsort - tifffile - opencv",
    "development_status": [],
    "display_name": "cellpose-napari",
    "documentation": "",
    "first_released": "2021-04-26T03:13:32.237206Z",
    "license": "BSD-3-Clause",
    "name": "cellpose-napari",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget", "sample_data"],
    "project_site": "https://github.com/Mouseland/cellpose-napari",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-06-06T22:52:18.964369Z",
    "report_issues": "",
    "requirements": [
      "napari",
      "napari-plugin-engine (>=0.1.4)",
      "cellpose (>0.6.3)",
      "imagecodecs",
      "sphinx (>=3.0) ; extra == 'docs'",
      "sphinxcontrib-apidoc ; extra == 'docs'",
      "sphinx-rtd-theme ; extra == 'docs'",
      "sphinx-prompt ; extra == 'docs'",
      "sphinx-autodoc-typehints ; extra == 'docs'",
      "pytest ; extra == 'tests_require'",
      "pytest-qt ; extra == 'tests_require'"
    ],
    "summary": "a generalist algorithm for anatomical segmentation",
    "support": "",
    "twitter": "",
    "version": "0.1.5",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Marc Boucsein" }, { "name": "Robin Koch" }],
    "code_repository": "https://github.com/DKFZ-TMTRR/Partial-Aligner",
    "conda": [{ "channel": "conda-forge", "package": "partial-aligner" }],
    "description": "# Partial-Aligner  [![License](https://img.shields.io/pypi/l/napari-medical-image-formats.svg?color=green)](https://github.com/MBPhys/Partial-Aligner/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/Partial-Aligner.svg?color=green)](https://pypi.org/project/Partial-Aligner) [![Python Version](https://img.shields.io/pypi/pyversions/Partial-Aligner.svg?color=green)](https://python.org)   A napari plugin to affine transform images and parts of images in 2D and 3D. It was developed in the context of brain slice registration and solves multiple, related problems when working with histology slices.  ----------------------------------  ## Installation  You can install `Partial-Aligner` via [pip]:      pip install Partial-Aligner      To make full use of this plugin, please also install the sister plugins:      pip install Label-Creator     pip install Layer-Data-Replace     pip install World2Data  ## Usage  It is important to note that this plugin is part of a group of plugins ([Label-Creator](https://github.com/DKFZ-TMTRR/Label-Creator, \"Creates Labels\"), [Layer-Data-Replace](https://github.com/DKFZ-TMTRR/Layer-Data-Replace, \"Replaces the data of a layer with other data\"), [World2Data](https://github.com/DKFZ-TMTRR/World2Data, \"Applies a transformation to an image\")) which are intended to be used together.   The principle workflow with this plugin is as follows:  1. Load an image of interest (ioi) using standard napari. 2. Find out meaningful transformation parameters for the ioi (or part of it) based on what you see in the viewer. 3. (optional) Save the affine transformation matrix (can later be applied to other modalities) 4. Apply the transformation to create a new, altered version of the ioi (use plugin [World2Data](https://github.com/DKFZ-TMTRR/World2Data, \"Applies a transformation to an image\"))  Decisions on the parameters (step 2) are made based on the problem at hand:  - Registration: You have a second (fixed) image and you want to align your ioi to that image? Transform your whole ioi! Just play with the transformation parameters until you are happy with the alignment of ioi and fixed image.  <p align=\"center\">     <img src=\"https://user-images.githubusercontent.com/36212786/149524198-9a25b6dc-4169-4546-85b3-7c2f57fccc97.png\" width=\"50%\" height=\"50%\">  <br />       <i>DAPI staining (red) before (left) and after (right) manual registration on an MRI image (green).</i>  </p>  - Histology artifact repair: Parts of your histology slice are misplaced? Transform the misplaced parts! Label them and change the transformation parameters for the misplaced parts until you are happy with their alignment with the rest of the image.  <p align=\"center\"> <img src=\"https://user-images.githubusercontent.com/36212786/149526385-09aeebe2-d03e-4dd4-a424-d0f3af207529.png\" width=\"50%\" height=\"50%\">  <br />       <i> Original slice with misplaced region (left), marked using the label function (middle) and after manual adjustment (right), where the misplaced region (green) was cut and newly positioned.</i>  </p>  To make this plugin run reasonably fast, the affine transformations are not applied to the image data in real time. Instead, the internal napari viewing parameters are changed according to the transformation parameters. Therefore, to save transformed image data, the [World2Data](https://github.com/DKFZ-TMTRR/World2Data, \"Applies a transformation to an image\") plugin is used, which calculates and saves the resulting image based on the internal napari viewing parameters.   Here we showcase a resulting multimodal 3D alignment of a whole mouse brain. The modalities are CT, MRI, simulated radiation dose distributions, DAPI staining and DNA-damage repair foci, with a Nissl-staining mouse atlas as template.  https://user-images.githubusercontent.com/36212786/149530462-51a53631-bf74-459b-ab4e-572c52cf2692.mov        ## Contributing  Contributions are very welcome. Tests can be run with [tox].  ## License  Distributed under the terms of the [BSD-3] license, \"Partial-Aligner\" is free and open source software.  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin [file an issue]: https://github.com/MBPhys/Partial-Aligner/issues [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "Partial-Aligner    A napari plugin to affine transform images and parts of images in 2D and 3D. It was developed in the context of brain slice registration and solves multiple, related problems when working with histology slices.  Installation You can install Partial-Aligner via pip: pip install Partial-Aligner  To make full use of this plugin, please also install the sister plugins: pip install Label-Creator pip install Layer-Data-Replace pip install World2Data  Usage It is important to note that this plugin is part of a group of plugins (Label-Creator, Layer-Data-Replace, World2Data) which are intended to be used together.  The principle workflow with this plugin is as follows:  Load an image of interest (ioi) using standard napari. Find out meaningful transformation parameters for the ioi (or part of it) based on what you see in the viewer. (optional) Save the affine transformation matrix (can later be applied to other modalities) Apply the transformation to create a new, altered version of the ioi (use plugin World2Data)  Decisions on the parameters (step 2) are made based on the problem at hand:  Registration: You have a second (fixed) image and you want to align your ioi to that image? Transform your whole ioi! Just play with the transformation parameters until you are happy with the alignment of ioi and fixed image.     DAPI staining (red) before (left) and after (right) manual registration on an MRI image (green).   Histology artifact repair: Parts of your histology slice are misplaced? Transform the misplaced parts! Label them and change the transformation parameters for the misplaced parts until you are happy with their alignment with the rest of the image.      Original slice with misplaced region (left), marked using the label function (middle) and after manual adjustment (right), where the misplaced region (green) was cut and newly positioned.  To make this plugin run reasonably fast, the affine transformations are not applied to the image data in real time. Instead, the internal napari viewing parameters are changed according to the transformation parameters. Therefore, to save transformed image data, the World2Data plugin is used, which calculates and saves the resulting image based on the internal napari viewing parameters. Here we showcase a resulting multimodal 3D alignment of a whole mouse brain. The modalities are CT, MRI, simulated radiation dose distributions, DAPI staining and DNA-damage repair foci, with a Nissl-staining mouse atlas as template. https://user-images.githubusercontent.com/36212786/149530462-51a53631-bf74-459b-ab4e-572c52cf2692.mov Contributing Contributions are very welcome. Tests can be run with tox. License Distributed under the terms of the BSD-3 license, \"Partial-Aligner\" is free and open source software. Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "Partial-Aligner",
    "documentation": "",
    "first_released": "2022-01-14T15:10:00.208187Z",
    "license": "BSD-3-Clause",
    "name": "Partial-Aligner",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/DKFZ-TMTRR/Partial-Aligner",
    "python_version": ">=3.9",
    "reader_file_extensions": [],
    "release_date": "2022-01-14T15:10:00.208187Z",
    "report_issues": "",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "packaging",
      "dask"
    ],
    "summary": "A napari plugin for manual registration of (a part of) an image",
    "support": "",
    "twitter": "",
    "version": "0.0.1",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Kevin Yamauchi" }],
    "code_repository": "https://github.com/kevinyamauchi/morphometrics",
    "conda": [{ "channel": "conda-forge", "package": "morphometrics" }],
    "description": "# morphometrics  [![License](https://img.shields.io/pypi/l/morphometrics.svg?color=green)](https://github.com/morphometrics/morphometrics/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/morphometrics.svg?color=green)](https://pypi.org/project/morphometrics) [![Python Version](https://img.shields.io/pypi/pyversions/morphometrics.svg?color=green)](https://python.org) [![tests](https://github.com/morphometrics/morphometrics/workflows/tests/badge.svg)](https://github.com/morphometrics/morphometrics/actions) [![codecov](https://codecov.io/gh/morphometrics/morphometrics/branch/main/graph/badge.svg)](https://codecov.io/gh/morphometrics/morphometrics) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/morphometrics)](https://napari-hub.org/plugins/morphometrics)  A plugin for quantifying shape and neighborhoods from images.  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/plugins/stable/index.html -->  ## Installation  ### conda environment file You can install `morphometrics` via our conda environment file. To do so, first install anaconda or miniconda on your computer. Then, download the [`environment.yml file`](https://raw.githubusercontent.com/kevinyamauchi/morphometrics/master/environment.yml) (right click the link and \"Save as...\"). In your terminal, navigate to the directory you downloaded the `environment.yml` file to:  ```bash cd <path/to/downloaded/environment.yml> ```  Then create the `morphometrics` environment using  ```bash conda env create -f environment.yml ```  Once the environment has been created, you can activate it and use `morphometrics` as described below.  ```bash conda activate morphometrics ```  If you are on Mac OS or Linux install the following:  Mac:  ```bash conda install -c conda-forge ocl_icd_wrapper_apple ```  Linux:  ```bash conda install -c conda-forge ocl-icd-system ```   ### Development installation  To install latest development version :      pip install git+https://github.com/kevinyamauchi/morphometrics.git  ## Example applications <table border=\"0\"> <tr><td>   <img src=\"https://github.com/kevinyamauchi/morphometrics/raw/main/resources/surface_distance_measurement.gif\" width=\"300\"/>  </td><td>  [measure the distance between surfaces](https://github.com/kevinyamauchi/morphometrics/blob/main/examples/surface_distance_measurement.ipynb)  </td></tr><tr><td>  <img src=\"https://github.com/kevinyamauchi/morphometrics/raw/main/resources/region_props_plugin.png\" width=\"300\"/>  </td><td>  [napari plugin for measuring properties of segmented objects (regionprops)](https://github.com/kevinyamauchi/morphometrics/blob/main/examples/measure_with_widget.py)  </td></tr><tr><td>  <img src=\"https://github.com/kevinyamauchi/morphometrics/raw/main/resources/object_classification.png\" width=\"300\"/>  </td><td>  [object classification](https://github.com/kevinyamauchi/morphometrics/blob/main/examples/object_classification.ipynb)  </td></tr><tr><td>  <img src=\"https://github.com/kevinyamauchi/morphometrics/raw/main/resources/mesh_object.png\" width=\"300\"/>  </td><td>  [mesh binary mask](https://github.com/kevinyamauchi/morphometrics/blob/main/examples/mesh_binary_mask.ipynb)   </td></tr></table>   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"morphometrics\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/kevinyamauchi/morphometrics/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "morphometrics       A plugin for quantifying shape and neighborhoods from images.  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation conda environment file You can install morphometrics via our conda environment file. To do so, first install anaconda or miniconda on your computer. Then, download the environment.yml file (right click the link and \"Save as...\"). In your terminal, navigate to the directory you downloaded the environment.yml file to: bash cd <path/to/downloaded/environment.yml> Then create the morphometrics environment using bash conda env create -f environment.yml Once the environment has been created, you can activate it and use morphometrics as described below. bash conda activate morphometrics If you are on Mac OS or Linux install the following: Mac: bash conda install -c conda-forge ocl_icd_wrapper_apple Linux: bash conda install -c conda-forge ocl-icd-system Development installation To install latest development version : pip install git+https://github.com/kevinyamauchi/morphometrics.git  Example applications      [measure the distance between surfaces](https://github.com/kevinyamauchi/morphometrics/blob/main/examples/surface_distance_measurement.ipynb)      [napari plugin for measuring properties of segmented objects (regionprops)](https://github.com/kevinyamauchi/morphometrics/blob/main/examples/measure_with_widget.py)      [object classification](https://github.com/kevinyamauchi/morphometrics/blob/main/examples/object_classification.ipynb)      [mesh binary mask](https://github.com/kevinyamauchi/morphometrics/blob/main/examples/mesh_binary_mask.ipynb)    Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"morphometrics\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "morphometrics",
    "documentation": "https://github.com/kevinyamauchi/morphometrics#README.md",
    "first_released": "2022-03-17T16:28:24.113225Z",
    "license": "BSD-3-Clause",
    "name": "morphometrics",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget", "sample_data"],
    "project_site": "https://github.com/kevinyamauchi/morphometrics",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-10-15T07:27:42.246704Z",
    "report_issues": "https://github.com/kevinyamauchi/morphometrics/issues",
    "requirements": [
      "leidenalg",
      "napari-skimage-regionprops",
      "napari",
      "qtpy",
      "numba (>=0.55.2)",
      "numpy (>=1.21)",
      "pandas",
      "pyclesperanto-prototype (>=0.8.0)",
      "pymeshfix",
      "scanpy",
      "scikit-image (>0.19.0)",
      "scikit-learn (>=0.24.2)",
      "tqdm",
      "trimesh[easy]",
      "pre-commit ; extra == 'dev'",
      "pytest ; extra == 'dev'",
      "pytest-qt ; extra == 'dev'"
    ],
    "summary": "A plugin for quantifying shape and neighborhoods from images.",
    "support": "https://github.com/kevinyamauchi/morphometrics/issues",
    "twitter": "",
    "version": "0.0.6",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      { "email": "cqzhang@g.ecc.u-tokyo.ac.jp", "name": "Chenqi Zhang" }
    ],
    "code_repository": "https://github.com/zcqwh/disease-classifier",
    "conda": [],
    "description": "# disease-classifier  [![License](https://img.shields.io/pypi/l/disease-classifier.svg?color=green)](https://github.com/zcqwh/disease-classifier/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/disease-classifier.svg?color=green)](https://pypi.org/project/disease-classifier) [![Python Version](https://img.shields.io/pypi/pyversions/disease-classifier.svg?color=green)](https://python.org) [![tests](https://github.com/zcqwh/disease-classifier/workflows/tests/badge.svg)](https://github.com/zcqwh/disease-classifier/actions) [![codecov](https://codecov.io/gh/zcqwh/disease-classifier/branch/main/graph/badge.svg)](https://codecov.io/gh/zcqwh/disease-classifier) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/disease-classifier)](https://napari-hub.org/plugins/disease-classifier)  A napari plugin for disease classification based on iPAC images.    ## Installation  You can install `disease-classifier` via [pip]:      pip install disease-classifier    To install latest development version :      pip install git+https://github.com/zcqwh/disease-classifier.git  ## Introduction #### Load data (.rtdc or .bin) * Drag and drop the data in .rtdc or .bin into the files table. * Click eye button to preview images. ![](https://github.com/zcqwh/disease-classifier/blob/main/Tutorial/Gif/01_Load_preview.gif?raw=true)   #### Choose model and classify  * Choose the model folder including CNN and RF/PLDA. * Check the data. * Click classify. ![](https://github.com/zcqwh/disease-classifier/blob/main/Tutorial/Gif/02_model_classify.gif?raw=true)  #### Preview classification results * Click the eye button to preview the result. * Click the header to show all. ![](https://github.com/zcqwh/disease-classifier/blob/main/Tutorial/Gif/03_preview_result.gif?raw=true)   #### Save results * Click “Add classification to .rtdc file” button to save results. ![](https://github.com/zcqwh/disease-classifier/blob/main/Tutorial/Gif/04_save.gif?raw=true)   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"disease-classifier\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/plugins/stable/index.html -->  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/zcqwh/disease-classifier/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/  ",
    "description_content_type": "text/markdown",
    "description_text": "disease-classifier       A napari plugin for disease classification based on iPAC images. Installation You can install disease-classifier via pip: pip install disease-classifier  To install latest development version : pip install git+https://github.com/zcqwh/disease-classifier.git  Introduction Load data (.rtdc or .bin)  Drag and drop the data in .rtdc or .bin into the files table. Click eye button to preview images.   Choose model and classify  Choose the model folder including CNN and RF/PLDA. Check the data. Click classify.   Preview classification results  Click the eye button to preview the result. Click the header to show all.   Save results  Click “Add classification to .rtdc file” button to save results.   Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"disease-classifier\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template. ",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "Disease classifier",
    "documentation": "https://github.com/zcqwh/disease-classifier#README.md",
    "first_released": "2022-06-07T06:07:03.112636Z",
    "license": "BSD-3-Clause",
    "name": "disease-classifier",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/zcqwh/disease-classifier",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-06-07T06:07:03.112636Z",
    "report_issues": "https://github.com/zcqwh/disease-classifier/issues",
    "requirements": [
      "numpy",
      "magicgui",
      "qtpy",
      "matplotlib",
      "h5py (>=3.6.0)",
      "napari (>=0.4.15)",
      "numpy (>=1.22.4)",
      "opencv-contrib-python-headless (>=4.5.5.64)",
      "pytranskit (>=0.2.3)",
      "statsmodels (>=0.13.2)",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "summary": "A disease classifier based on iPAC images.",
    "support": "https://github.com/zcqwh/disease-classifier/issues",
    "twitter": "",
    "version": "0.0.1",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Adam Tyson" }],
    "category": {
      "Image modality": ["Multi-photon microscopy", "Fluorescence microscopy"],
      "Supported data": ["3D"],
      "Workflow step": [
        "Image feature detection",
        "Image registration",
        "Image Segmentation",
        "Image annotation",
        "Object classification",
        "Object feature extraction"
      ]
    },
    "category_hierarchy": {
      "Image modality": [
        ["Multi-photon microscopy"],
        ["Fluorescence microscopy", "Light-sheet microscopy"]
      ],
      "Supported data": [["3D"]],
      "Workflow step": [
        ["Image feature detection"],
        ["Image registration"],
        ["Image Segmentation"],
        ["Image Segmentation", "Image thresholding"],
        ["Image annotation"],
        ["Object classification"],
        ["Object feature extraction"]
      ]
    },
    "code_repository": "https://github.com/brainglobe/cellfinder-napari",
    "conda": [{ "channel": "conda-forge", "package": "cellfinder-napari" }],
    "description": "# cellfinder-napari  [![License](https://img.shields.io/pypi/l/cellfinder-napari.svg?color=green)](https://github.com/napari/cellfinder-napari/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/cellfinder-napari.svg?color=green)](https://pypi.org/project/cellfinder-napari) [![Python Version](https://img.shields.io/pypi/pyversions/cellfinder-napari.svg?color=green)](https://python.org) [![tests](https://github.com/brainglobe/cellfinder-napari/workflows/tests/badge.svg)](https://github.com/brainglobe/cellfinder-napari/actions) [![codecov](https://codecov.io/gh/brainglobe/cellfinder-napari/branch/main/graph/badge.svg?token=C4uzd0cm2u)](https://codecov.io/gh/brainglobe/cellfinder-napari) [![Downloads](https://pepy.tech/badge/cellfinder-napari)](https://pepy.tech/project/cellfinder-napari) [![Wheel](https://img.shields.io/pypi/wheel/cellfinder.svg)](https://pypi.org/project/cellfinder) [![Development Status](https://img.shields.io/pypi/status/cellfinder-napari.svg)](https://github.com/brainglobe/cellfinder-napari) [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/python/black) [![Imports: isort](https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat&labelColor=ef8336)](https://pycqa.github.io/isort/) [![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&logoColor=white)](https://github.com/pre-commit/pre-commit) [![Contributions](https://img.shields.io/badge/Contributions-Welcome-brightgreen.svg)](https://docs.brainglobe.info/cellfinder/contributing) [![Website](https://img.shields.io/website?up_message=online&url=https%3A%2F%2Fbrainglobe.info/cellfinder)](https://brainglobe.info/cellfinder) [![Twitter](https://img.shields.io/twitter/follow/brain_globe?style=social)](https://twitter.com/brain_globe)  ### Efficient cell detection in large images (e.g. whole mouse brain images)  `cellfinder-napari` is a front-end to [cellfinder-core](https://github.com/brainglobe/cellfinder-core) to allow ease of use within the [napari](https://napari.org/index.html) multidimensional image viewer. For more details on this approach, please see [Tyson, Rousseau & Niedworok et al. (2021)](https://doi.org/10.1371/journal.pcbi.1009074). This algorithm can also be used within the original [cellfinder](https://github.com/brainglobe/cellfinder) software for whole-brain microscopy analysis.  `cellfinder-napari`, `cellfinder` and `cellfinder-core` were developed by [Charly Rousseau](https://github.com/crousseau) and [Adam Tyson](https://github.com/adamltyson) in the [Margrie Lab](https://www.sainsburywellcome.org/web/groups/margrie-lab), based on previous work by [Christian Niedworok](https://github.com/cniedwor), generously supported by the [Sainsbury Wellcome Centre](https://www.sainsburywellcome.org/web/).  ---- ![raw](https://raw.githubusercontent.com/brainglobe/cellfinder-napari/master/resources/cellfinder-napari.gif)  **Visualising detected cells in the cellfinder napari plugin**  ---- ## Instructions  ### Installation Once you have [installed napari](https://napari.org/index.html#installation). You can install napari either through the napari plugin installation tool, or directly from PyPI with: ```bash pip install cellfinder-napari ```  ### Usage Full documentation can be found [here](https://docs.brainglobe.info/cellfinder-napari).  This software is at a very early stage, and was written with our data in mind. Over time we hope to support other data types/formats. If you have any questions or issues, please get in touch [on the forum](https://forum.image.sc/tag/brainglobe) or by [raising an issue](https://github.com/brainglobe/cellfinder-napari/issues).   --- ## Illustration  ### Introduction cellfinder takes a stitched, but otherwise raw dataset with at least two channels:  * Background channel (i.e. autofluorescence)  * Signal channel, the one with the cells to be detected:  ![raw](https://raw.githubusercontent.com/brainglobe/cellfinder/master/resources/raw.png) **Raw coronal serial two-photon mouse brain image showing labelled cells**   ### Cell candidate detection Classical image analysis (e.g. filters, thresholding) is used to find cell-like objects (with false positives):  ![raw](https://raw.githubusercontent.com/brainglobe/cellfinder/master/resources/detect.png) **Candidate cells (including many artefacts)**   ### Cell candidate classification A deep-learning network (ResNet) is used to classify cell candidates as true cells or artefacts:  ![raw](https://raw.githubusercontent.com/brainglobe/cellfinder/master/resources/classify.png) **Cassified cell candidates. Yellow - cells, Blue - artefacts**  ## Contributing Contributions to cellfinder-napari are more than welcome. Please see the [contributing guide](https://github.com/brainglobe/.github/blob/main/CONTRIBUTING.md).  ## Citing cellfinder  If you find this plugin useful, and use it in your research, please cite the preprint outlining the cell detection algorithm: > Tyson, A. L., Rousseau, C. V., Niedworok, C. J., Keshavarzi, S., Tsitoura, C., Cossell, L., Strom, M. and Margrie, T. W. (2021) “A deep learning algorithm for 3D cell detection in whole mouse brain image datasets’ PLOS Computational Biology, 17(5), e1009074 [https://doi.org/10.1371/journal.pcbi.1009074](https://doi.org/10.1371/journal.pcbi.1009074)   **If you use this, or any other tools in the brainglobe suite, please  [let us know](mailto:code@adamltyson.com?subject=cellfinder-napari), and  we'd be happy to promote your paper/talk etc.**  --- The BrainGlobe project is generously supported by the Sainsbury Wellcome Centre and the Institute of Neuroscience, Technical University of Munich, with funding from Wellcome, the Gatsby Charitable Foundation and the Munich Cluster for Systems Neurology - Synergy.  <img src='https://brainglobe.info/images/logos_combined.png' width=\"550\"> ",
    "description_content_type": "text/markdown",
    "description_text": "cellfinder-napari               Efficient cell detection in large images (e.g. whole mouse brain images) cellfinder-napari is a front-end to cellfinder-core to allow ease of use within the napari multidimensional image viewer. For more details on this approach, please see Tyson, Rousseau & Niedworok et al. (2021). This algorithm can also be used within the original cellfinder software for whole-brain microscopy analysis. cellfinder-napari, cellfinder and cellfinder-core were developed by Charly Rousseau and Adam Tyson in the Margrie Lab, based on previous work by Christian Niedworok, generously supported by the Sainsbury Wellcome Centre.   Visualising detected cells in the cellfinder napari plugin  Instructions Installation Once you have installed napari. You can install napari either through the napari plugin installation tool, or directly from PyPI with: bash pip install cellfinder-napari Usage Full documentation can be found here. This software is at a very early stage, and was written with our data in mind. Over time we hope to support other data types/formats. If you have any questions or issues, please get in touch on the forum or by raising an issue.  Illustration Introduction cellfinder takes a stitched, but otherwise raw dataset with at least two channels:  * Background channel (i.e. autofluorescence)  * Signal channel, the one with the cells to be detected:  Raw coronal serial two-photon mouse brain image showing labelled cells Cell candidate detection Classical image analysis (e.g. filters, thresholding) is used to find cell-like objects (with false positives):  Candidate cells (including many artefacts) Cell candidate classification A deep-learning network (ResNet) is used to classify cell candidates as true cells or artefacts:  Cassified cell candidates. Yellow - cells, Blue - artefacts Contributing Contributions to cellfinder-napari are more than welcome. Please see the contributing guide. Citing cellfinder If you find this plugin useful, and use it in your research, please cite the preprint outlining the cell detection algorithm:  Tyson, A. L., Rousseau, C. V., Niedworok, C. J., Keshavarzi, S., Tsitoura, C., Cossell, L., Strom, M. and Margrie, T. W. (2021) “A deep learning algorithm for 3D cell detection in whole mouse brain image datasets’ PLOS Computational Biology, 17(5), e1009074 https://doi.org/10.1371/journal.pcbi.1009074  If you use this, or any other tools in the brainglobe suite, please  let us know, and  we'd be happy to promote your paper/talk etc.  The BrainGlobe project is generously supported by the Sainsbury Wellcome Centre and the Institute of Neuroscience, Technical University of Munich, with funding from Wellcome, the Gatsby Charitable Foundation and the Munich Cluster for Systems Neurology - Synergy. ",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "cellfinder-napari",
    "documentation": "https://docs.brainglobe.info/cellfinder-napari/",
    "first_released": "2021-01-25T16:40:18.651958Z",
    "license": "BSD-3-Clause",
    "name": "cellfinder-napari",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget", "sample_data"],
    "project_site": "https://brainglobe.info/cellfinder",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-10-21T15:12:05.089449Z",
    "report_issues": "https://github.com/brainglobe/cellfinder-napari/issues",
    "requirements": [
      "napari",
      "napari-plugin-engine (>=0.1.4)",
      "napari-ndtiffs",
      "brainglobe-napari-io",
      "cellfinder-core (>=0.3)",
      "pooch (>=1)",
      "black ; extra == 'dev'",
      "bump2version ; extra == 'dev'",
      "gitpython ; extra == 'dev'",
      "pre-commit ; extra == 'dev'",
      "pytest ; extra == 'dev'",
      "pytest-cov ; extra == 'dev'",
      "pytest-qt ; extra == 'dev'"
    ],
    "summary": "Efficient cell detection in large images",
    "support": "https://forum.image.sc/tag/brainglobe",
    "twitter": "",
    "version": "0.0.20",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Adam Tyson" }, { "name": "Horst Obenhaus" }],
    "category": {
      "Supported data": ["3D"],
      "Workflow step": ["Image Segmentation", "Image annotation"]
    },
    "category_hierarchy": {
      "Supported data": [["3D"]],
      "Workflow step": [
        ["Image Segmentation", "Manual segmentation"],
        ["Image annotation", "Dense image annotation", "Manual segmentation"],
        ["Image Segmentation"]
      ]
    },
    "code_repository": "https://github.com/brainglobe/brainreg-segment",
    "conda": [{ "channel": "conda-forge", "package": "brainreg-segment" }],
    "description": "[![Python Version](https://img.shields.io/pypi/pyversions/brainreg-segment.svg)](https://pypi.org/project/brainreg-segment) [![PyPI](https://img.shields.io/pypi/v/brainreg-segment.svg)](https://pypi.org/project/brainreg-segment) [![Wheel](https://img.shields.io/pypi/wheel/brainreg-segment.svg)](https://pypi.org/project/brainreg-segment) [![Development Status](https://img.shields.io/pypi/status/brainreg-segment.svg)](https://github.com/brainglobe/brainreg-segment) [![Tests](https://img.shields.io/github/workflow/status/brainglobe/brainreg-segment/tests)](     https://github.com/brainglobe/brainreg-segment/actions) [![Coverage Status](https://coveralls.io/repos/github/brainglobe/brainreg-segment/badge.svg?branch=master)](https://coveralls.io/github/brainglobe/brainreg-segment?branch=master) [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/python/black) [![Twitter](https://img.shields.io/twitter/follow/brain_globe?style=social)](https://twitter.com/brain_globe)  # brainreg-segment Segmentation of 1/2/3D brain structures in a common anatomical space  `brainreg-segment` is a companion to [`brainreg`](https://github.com/brainglobe/brainreg) allowing manual segmentation of regions/objects within the brain (e.g. injection sites, probes etc.) allowing for automated analysis of brain region distribution, and visualisation (e.g. in [brainrender](https://github.com/BrancoLab/brainrender)).  `brainreg-segment` and `brainreg` were developed by [Adam Tyson](https://github.com/adamltyson) and [Charly Rousseau](https://github.com/crousseau) in the [Margrie Lab](https://www.sainsburywellcome.org/web/groups/margrie-lab), based on [aMAP](https://doi.org/10.1038/ncomms11879) by [Christian Niedworok](https://github.com/cniedwor). The work was generously supported by the [Sainsbury Wellcome Centre](https://www.sainsburywellcome.org/web/).  ## Installation  brainreg-segment comes bundled with [`brainreg`](https://github.com/brainglobe/brainreg), so see the [brainreg installation instructions](https://docs.brainglobe.info/brainreg/installation).  brainreg-segment can be installed on it's own (`pip install brainreg-segment`), but you will need to register your data with brainreg first.  ## Usage  See [user guide](https://docs.brainglobe.info/brainreg-segment/user-guide).  If you have any questions, head over to the [image.sc forum](https://forum.image.sc/tag/brainglobe).  ## Contributing Contributions are very welcome. Please see the [contributing guide](https://github.com/brainglobe/.github/blob/main/CONTRIBUTING.md).  ### Citing brainreg-segment  If you find brainreg-segment useful, and use it in your research, please let us know and also cite the paper:  > Tyson, A. L., V&eacute;lez-Fort, M.,  Rousseau, C. V., Cossell, L., Tsitoura, C., Lenzi, S. C., Obenhaus, H. A., Claudi, F., Branco, T.,  Margrie, T. W. (2022). Accurate determination of marker location within whole-brain microscopy images. Scientific Reports, 12, 867 [doi.org/10.1038/s41598-021-04676-9](https://doi.org/10.1038/s41598-021-04676-9)  --- The BrainGlobe project is generously supported by the Sainsbury Wellcome Centre and the Institute of Neuroscience, Technical University of Munich, with funding from Wellcome, the Gatsby Charitable Foundation and the Munich Cluster for Systems Neurology - Synergy.  <img src='https://brainglobe.info/images/logos_combined.png' width=\"550\">   ",
    "description_content_type": "text/markdown",
    "description_text": "        brainreg-segment Segmentation of 1/2/3D brain structures in a common anatomical space brainreg-segment is a companion to brainreg allowing manual segmentation of regions/objects within the brain (e.g. injection sites, probes etc.) allowing for automated analysis of brain region distribution, and visualisation (e.g. in brainrender). brainreg-segment and brainreg were developed by Adam Tyson and Charly Rousseau in the Margrie Lab, based on aMAP by Christian Niedworok. The work was generously supported by the Sainsbury Wellcome Centre. Installation brainreg-segment comes bundled with brainreg, so see the brainreg installation instructions. brainreg-segment can be installed on it's own (pip install brainreg-segment), but you will need to register your data with brainreg first. Usage See user guide. If you have any questions, head over to the image.sc forum. Contributing Contributions are very welcome. Please see the contributing guide. Citing brainreg-segment If you find brainreg-segment useful, and use it in your research, please let us know and also cite the paper:  Tyson, A. L., Vélez-Fort, M.,  Rousseau, C. V., Cossell, L., Tsitoura, C., Lenzi, S. C., Obenhaus, H. A., Claudi, F., Branco, T.,  Margrie, T. W. (2022). Accurate determination of marker location within whole-brain microscopy images. Scientific Reports, 12, 867 doi.org/10.1038/s41598-021-04676-9   The BrainGlobe project is generously supported by the Sainsbury Wellcome Centre and the Institute of Neuroscience, Technical University of Munich, with funding from Wellcome, the Gatsby Charitable Foundation and the Munich Cluster for Systems Neurology - Synergy. ",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "brainreg-segment",
    "documentation": "https://docs.brainglobe.info/brainreg-segment",
    "first_released": "2020-08-26T09:04:27.878441Z",
    "license": "BSD-3-Clause",
    "name": "brainreg-segment",
    "npe2": false,
    "operating_system": [
      "Operating System :: Microsoft :: Windows :: Windows 10",
      "Operating System :: POSIX :: Linux"
    ],
    "plugin_types": ["widget", "sample_data"],
    "project_site": "https://brainglobe.info/",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-06-15T12:30:32.795955Z",
    "report_issues": "https://github.com/brainglobe/brainreg-segment/issues",
    "requirements": [
      "numpy",
      "tables",
      "scikit-image",
      "pandas",
      "napari (>=0.4.5)",
      "napari-plugin-engine (>=0.1.4)",
      "imlib (>=0.0.26)",
      "dask (>=2.15.0)",
      "imio",
      "brainglobe-napari-io",
      "black ; extra == 'dev'",
      "pytest-cov ; extra == 'dev'",
      "pytest ; extra == 'dev'",
      "pytest-qt ; extra == 'dev'",
      "coverage ; extra == 'dev'",
      "bump2version ; extra == 'dev'",
      "pre-commit ; extra == 'dev'",
      "flake8 ; extra == 'dev'"
    ],
    "summary": "Manual segmentation of 3D brain structures in a common anatomical space",
    "support": "https://forum.image.sc/tag/brainglobe",
    "twitter": "",
    "version": "0.2.16",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      { "email": "jordao.bragantini@gmail.com", "name": "Jordão Bragantini" }
    ],
    "code_repository": null,
    "description": "# Grabber: A Tool to Improve Convergence in Interactive Image Segmentation  [![License](https://img.shields.io/pypi/l/grabber.svg?color=green)](https://github.com/LIDS-UNICAMP/grabber/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/grabber.svg?color=green)](https://pypi.org/project/grabber) [![Python Version](https://img.shields.io/pypi/pyversions/grabber.svg?color=green)](https://python.org) [![tests](https://github.com/LIDS-UNICAMP/grabber/workflows/tests/badge.svg)](https://github.com/LIDS-UNICAMP/grabber/actions) [![codecov](https://codecov.io/gh/LIDS-UNICAMP/grabber/branch/master/graph/badge.svg)](https://codecov.io/gh/LIDS-UNICAMP/grabber)  A tool for contour-based segmentation correction (2D only).  This repository provides a demo code of the paper: > **Grabber: A Tool to Improve Convergence in Interactive Image Segmentation** > [Jordão Bragantini](https://jookuma.github.io/), Bruno Moura, [Alexandre X. Falcão](http://lids.ic.unicamp.br/), [Fábio A. M. Cappabianco](https://scholar.google.com/citations?user=qmH9VEEAAAAJ&hl=en&oi=ao)  https://user-images.githubusercontent.com/21022743/145699960-57da06a5-668f-4e81-82b5-7f3d3ddf8ee3.mp4  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/docs/plugins/index.html -->  ## Installation  You can install `grabber-ift` via [pip]:      pip install grabber-ift   ## Known Limitations  This implementation doesn't support the items below, feel free to open a PR to add them.  - It only support 2D image, supporting 3D images isn't trivial, but it could be applied per slice with minor changes.  ## Citation  If this work was useful for your research, please cite our paper:  ``` @article{bragantini2020grabber,   title={Grabber: A Tool to Improve Convergence in Interactive Image Segmentation,   author={Bragantini, Jord{\\\\~a}o and Bruno Moura, Falc{\\\\~a}o, Alexandre Xavier and Cappabianco, F{\\\\'a}bio AM,   journal={Pattern Recognition Letters},   year={2020} } ```  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [MIT] license, \"grabber\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "Grabber: A Tool to Improve Convergence in Interactive Image Segmentation      A tool for contour-based segmentation correction (2D only). This repository provides a demo code of the paper:  Grabber: A Tool to Improve Convergence in Interactive Image Segmentation Jordão Bragantini, Bruno Moura, Alexandre X. Falcão, Fábio A. M. Cappabianco  https://user-images.githubusercontent.com/21022743/145699960-57da06a5-668f-4e81-82b5-7f3d3ddf8ee3.mp4  This napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.  Installation You can install grabber-ift via pip: pip install grabber-ift  Known Limitations This implementation doesn't support the items below, feel free to open a PR to add them.  It only support 2D image, supporting 3D images isn't trivial, but it could be applied per slice with minor changes.  Citation If this work was useful for your research, please cite our paper: @article{bragantini2020grabber,   title={Grabber: A Tool to Improve Convergence in Interactive Image Segmentation,   author={Bragantini, Jord{\\\\~a}o and Bruno Moura, Falc{\\\\~a}o, Alexandre Xavier and Cappabianco, F{\\\\'a}bio AM,   journal={Pattern Recognition Letters},   year={2020} } Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the MIT license, \"grabber\" is free and open source software Issues If you encounter any problems, please [file an issue] along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "grabber-ift",
    "documentation": "",
    "first_released": "2021-12-12T06:26:35.907956Z",
    "license": "MIT",
    "name": "grabber-ift",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2021-12-14T03:26:55.818612Z",
    "report_issues": "",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "pyift (>=0.0.4)",
      "opencv-python-headless (>=4.4.0)",
      "scipy (>=1.7.2)"
    ],
    "summary": "A tool for contour-based segmentation correction (2D only).",
    "support": "",
    "twitter": "",
    "version": "0.2.2",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "jhnnsrs" }],
    "code_repository": "https://github.com/jhnnsrs/mikro-napari",
    "conda": [],
    "description": "# mikro-napari  [![codecov](https://codecov.io/gh/jhnnsrs/mikro-napari/branch/master/graph/badge.svg?token=UGXEA2THBV)](https://codecov.io/gh/jhnnsrs/mikro-napari) [![PyPI version](https://badge.fury.io/py/mikro-napari.svg)](https://pypi.org/project/mikro-napari/) [![Maintenance](https://img.shields.io/badge/Maintained%3F-yes-green.svg)](https://pypi.org/project/mikro-napari/) ![Maintainer](https://img.shields.io/badge/maintainer-jhnnsrs-blue) [![PyPI pyversions](https://img.shields.io/pypi/pyversions/mikro-napari.svg)](https://pypi.python.org/pypi/mikro-napari/) [![PyPI status](https://img.shields.io/pypi/status/mikro-napari.svg)](https://pypi.python.org/pypi/mikro-napari/)  mikro napari enables napari on the mikro/arkitekt platform  # DEVELOPMENT  ## Idea  This is a napari plugin, that provides a simple user interface to use napari with mikro you can view and annotate data on the mikro platform (synchronised between all of your napari instances) and use napari within arkitekt workflows (can be extended with other plugins)  ## Install  Simple install this plugin via naparis plugin-manager and enable it.  Login with your local mikro/arkitekt platform and start using it in workflows  You can also install mikro-napari directly in your enviroment   ```bash pip install mikro-napari napari[pyqt5] ```  ",
    "description_content_type": "text/markdown",
    "description_text": "mikro-napari       mikro napari enables napari on the mikro/arkitekt platform DEVELOPMENT Idea This is a napari plugin, that provides a simple user interface to use napari with mikro you can view and annotate data on the mikro platform (synchronised between all of your napari instances) and use napari within arkitekt workflows (can be extended with other plugins) Install Simple install this plugin via naparis plugin-manager and enable it.  Login with your local mikro/arkitekt platform and start using it in workflows You can also install mikro-napari directly in your enviroment  bash pip install mikro-napari napari[pyqt5]",
    "development_status": [],
    "display_name": "mikro-napari",
    "documentation": "https://jhnnsrs.github.io/doks/",
    "first_released": "2021-09-30T09:39:08.169115Z",
    "license": "CC BY-NC 3.0",
    "name": "mikro-napari",
    "npe2": true,
    "operating_system": [],
    "plugin_types": ["widget"],
    "project_site": "https://jhnnsrs.github.io/doks/",
    "python_version": ">=3.8,<4.0",
    "reader_file_extensions": [],
    "release_date": "2022-10-22T17:04:38.835694Z",
    "report_issues": "https://github.com/jhnnsrs/mikro-napari/issues",
    "requirements": ["arkitekt (==0.3.16)"],
    "summary": "A napari plugin to interact with and provide functionality for a connected arkitekt server",
    "support": "https://jhnnsrs.github.io/doks/",
    "twitter": "https://twitter.com/jhnnsrs",
    "version": "0.1.49",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "email": "nick.schaub@nih.gov", "name": "Nick Schaub" }],
    "code_repository": "https://github.com/labshare/bfio",
    "description": "# **B**io**F**ormats **I**nput/**O**utput utility (bfio 2.3.0)  [![Documentation Status](https://readthedocs.org/projects/bfio/badge/?version=latest)](https://bfio.readthedocs.io/en/latest/?badge=latest) [![PyPI](https://img.shields.io/pypi/v/bfio)](https://pypi.org/project/filepattern/) ![PyPI - Downloads](https://img.shields.io/pypi/dm/bfio) ![Bower](https://img.shields.io/bower/l/MI)  This tool is a simplified but powerful interface to [Bioformats](https://www.openmicroscopy.org/bio-formats/) using jpype for direct access to the library. This tool is designed with scalable image analysis in mind, with a simple interface to treat any image like a memory mapped array.  Docker containers with all necessary components are available (see **Docker Containers** section).  ## Summary  - [Installation](#installation) - [Docker](#docker) - [Documentation](#documentation) - [Contributing](#contributing) - [Versioning](#versioning) - [Authors](#authors) - [License](#license) - [Acknowledgments](#acknowledgments)  ## Installation  ### Setting up Java  **Note:** `bfio` can be used without Java, but only the `python` and `zarr` backends will be useable. Only files in tiled OME Tiff or OME Zarr format can be read/written.  In order to use the `Java` backend, it is necessary to first install the JDK. The `bfio` package is generally tested with [JDK 8](https://docs.oracle.com/javase/8/docs/technotes/guides/install/install_overview.html), but JDK 11 and later also appear to work.  ### Installing bfio  The `bfio` package and the core dependencies (numpy, tifffile, imagecodecs) can be installed using pip:  `pip install bfio`  Additionally, `bfio` with other dependencies can be installed:  1. `pip install bfio[bioformats]` - Adds support for BioFormats/Java. See [License](#license) for additional information. 2. `pip install bfio[zarr]` - Adds support for OME Zarr 3. `pip install bfio[all]` - Installs all dependencies.  ## Docker  ### labshare/polus-bfio-util:2.3.0  Ubuntu based container with bfio and all dependencies (including Java).  ### labshare/polus-bfio-util:2.3.0-imagej  Same as above, except comes with ImageJ and PyImageJ.  ### labshare/polus-bfio-util:2.3.0-tensorflow  Tensorflow container with bfio isntalled.  ## Documentation  Documentation and examples are available on [Read the Docs](https://bfio.readthedocs.io/en/latest/).  ## Versioning  We use [SemVer](http://semver.org/) for versioning. For the versions available, see the [tags on this repository](https://github.com/PurpleBooth/a-good-readme-template/tags).  ## Authors  Nick Schaub (nick.schaub@nih.gov, nick.schaub@labshare.org)  ## License  This project is licensed under the [MIT License](LICENSE) Creative Commons License - see the [LICENSE](LICENSE) file for details.  **NOTE**  Bioformats is licensed under GPL, and as a consequence so is the `bioformats_jar`  package. These packages and libraries are installed when using the `bfio[bioformats]` option.  ## Acknowledgments  - Parts of this code were written/modified from existing code found in     `tifffile`.   ",
    "description_content_type": "text/markdown",
    "description_text": "BioFormats Input/Output utility (bfio 2.3.0)     This tool is a simplified but powerful interface to Bioformats using jpype for direct access to the library. This tool is designed with scalable image analysis in mind, with a simple interface to treat any image like a memory mapped array. Docker containers with all necessary components are available (see Docker Containers section). Summary  Installation Docker Documentation Contributing Versioning Authors License Acknowledgments  Installation Setting up Java Note: bfio can be used without Java, but only the python and zarr backends will be useable. Only files in tiled OME Tiff or OME Zarr format can be read/written. In order to use the Java backend, it is necessary to first install the JDK. The bfio package is generally tested with JDK 8, but JDK 11 and later also appear to work. Installing bfio The bfio package and the core dependencies (numpy, tifffile, imagecodecs) can be installed using pip: pip install bfio Additionally, bfio with other dependencies can be installed:  pip install bfio[bioformats] - Adds support for BioFormats/Java. See License for additional information. pip install bfio[zarr] - Adds support for OME Zarr pip install bfio[all] - Installs all dependencies.  Docker labshare/polus-bfio-util:2.3.0 Ubuntu based container with bfio and all dependencies (including Java). labshare/polus-bfio-util:2.3.0-imagej Same as above, except comes with ImageJ and PyImageJ. labshare/polus-bfio-util:2.3.0-tensorflow Tensorflow container with bfio isntalled. Documentation Documentation and examples are available on Read the Docs. Versioning We use SemVer for versioning. For the versions available, see the tags on this repository. Authors Nick Schaub (nick.schaub@nih.gov, nick.schaub@labshare.org) License This project is licensed under the MIT License Creative Commons License - see the LICENSE file for details. NOTE Bioformats is licensed under GPL, and as a consequence so is the bioformats_jar  package. These packages and libraries are installed when using the bfio[bioformats] option. Acknowledgments  Parts of this code were written/modified from existing code found in     tifffile. ",
    "development_status": ["Development Status :: 5 - Production/Stable"],
    "display_name": "bfio",
    "documentation": "https://bfio.readthedocs.io/en/latest/",
    "first_released": "2020-07-22T21:02:33.612752Z",
    "license": "MIT",
    "name": "bfio",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader", "writer"],
    "project_site": "",
    "python_version": ">=3.6",
    "reader_file_extensions": ["*"],
    "release_date": "2022-04-07T18:49:53.674962Z",
    "report_issues": "",
    "requirements": [
      "tifffile (>=2021.8.30)",
      "imagecodecs (>=2021.2.26)",
      "numpy (>=1.20.1)",
      "ome-types (>=0.2.10)",
      "lxml",
      "zarr (>=2.6.1) ; extra == 'all'",
      "bioformats-jar (==6.7.0.post2) ; extra == 'all'",
      "bioformats-jar (==6.7.0.post2) ; extra == 'bioformats'",
      "zarr (>=2.6.1) ; extra == 'dev'",
      "requests (>=2.26.0) ; extra == 'dev'",
      "bioformats-jar (==6.7.0.post2) ; extra == 'dev'",
      "zarr (>=2.6.1) ; extra == 'zarr'"
    ],
    "summary": "Simple reading and writing classes for tiled tiffs using Bioformats.",
    "support": "",
    "twitter": "",
    "version": "2.3.0",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": ["image"]
  },
  {
    "authors": [{ "name": "Nicholas Sofroniew" }],
    "code_repository": "https://github.com/napari/napari-console",
    "description": "# napari-console (WIP, under active development)  [![License](https://img.shields.io/pypi/l/napari-console.svg?color=green)](https://github.com/napari/napari-console/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-console.svg?color=green)](https://pypi.org/project/napari-console) [![Python Version](https://img.shields.io/pypi/pyversions/napari-console.svg?color=green)](https://python.org) [![tests](https://github.com/sofroniewn/napari-console/workflows/tests/badge.svg)](https://github.com/sofroniewn/napari-console/actions) [![codecov](https://codecov.io/gh/sofroniewn/napari-console/branch/master/graph/badge.svg)](https://codecov.io/gh/sofroniewn/napari-console)  A plugin that adds a console to napari  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/docs/plugins/index.html -->  ## Installation  You can install `napari-console` via [pip]:      pip install napari-console  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-console\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin [file an issue]: https://github.com/sofroniewn/napari-console/issues [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-console (WIP, under active development)      A plugin that adds a console to napari  This napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.  Installation You can install napari-console via pip: pip install napari-console  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-console\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "",
    "documentation": "",
    "first_released": "2021-01-21T04:42:40.342150Z",
    "license": "BSD-3-Clause",
    "name": "napari-console",
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": [],
    "project_site": "https://github.com/napari/napari-console",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-12-13T08:55:51.822464Z",
    "report_issues": "",
    "requirements": [
      "ipykernel (>=5.2.0)",
      "IPython (>=7.7.0)",
      "napari-plugin-engine (>=0.1.9)",
      "qtconsole (!=4.7.6,>=4.5.1)",
      "qtpy (>=1.7.0)"
    ],
    "summary": "A plugin that adds a console to napari",
    "support": "",
    "twitter": "",
    "version": "0.0.7",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      { "name": "S. Panigrahi" },
      { "name": "L. Espinosa" },
      { "name": "IAM" },
      { "name": "LCB" }
    ],
    "code_repository": "https://github.com/pswap/misic",
    "description": "# misic-napari  <!-- [![License](https://img.shields.io/pypi/l/misic-napari-plugin.svg?color=green)](https://github.com/pswap/misic-napari-plugin/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/misic-napari-plugin.svg?color=green)](https://pypi.org/project/misic-napari-plugin) [![Python Version](https://img.shields.io/pypi/pyversions/misic-napari-plugin.svg?color=green)](https://python.org) [![tests](https://github.com/pswap/misic-napari-plugin/workflows/tests/badge.svg)](https://github.com/pswap/misic-napari-plugin/actions) [![codecov](https://codecov.io/gh/pswap/misic-napari-plugin/branch/master/graph/badge.svg)](https://codecov.io/gh/pswap/misic-napari-plugin) -->  ----------------------------------  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/docs/plugins/index.html -->  A napari plugin for [MiSiC](https://elifesciences.org/articles/65151). Segmentation of bacteria in dense colonies.  The plugin provides acces to preprocessing of the image like scaling, gamma correction, sharpness and noise variance that can improve the segmentation of bacteria irrespective of the imaging modality.  ## Install Napari Install napari either the bundled app or through [pip/conda] https://napari.org/#installation  ## Installation  Install `misic-napari` through plugin manager in napari.  Or  You can install `misic-napari` via [pip] in the napari console:      pip install misic-napari  ## Tutorial Note:  The image should be in the format [n,row,col] or [row,col], i.e., a single image or a stack. Hyper-stacks are not supported yet.   #### get_width   Creates a Shapes layer with name 'cell-width' where the cell width can be hand drawn using line drawing tools in the shapes layer. This need not be precise and can be adjusted later. Click `get_cell_width` to obtain the desired mean cell width. This will be used to scale the image accordingly before segmentation.   #### segment  This can be used to quickly set the parameters that can be later used to segment the whole stack.  ``` use_roi ``` A square ROI of side 256 is created by default for quickly checking adjusting the segmentation parameters. The roi can be resized or moved in the `roi` shapes layer.  ``` light_background ``` True; for phase-contrast images.  False; for bright-field and fluorescence images.  ``` use_local_noise ``` If checked, this adds noise to image with local variance. In this case, a noise_var of around 0.1 works well. If unchecked, this adds noise with global variance of noise_var/100. Adding may help in removing false positives.  ``` gaussian_laplace ``` Useful when segmenting fluorescence images.   ``` adjust_scale ``` Fine-tuning the scale around ([0.8,1.2]) the scale obtained from cell-width determined in `get_cell_width`.  ``` noise_var ``` Amount of noise to be added to the image at the preprocessing step. This helps reduce the False Positives and, in many cases, to separate cells effectively.  ``` gamma ``` gamma correction   ``` sharpness_scale and sharpness_amount ``` Unsharp mask based sharpness with sigma = sharpness_scale and amount = sharpness_amount    ### segment_stack Segments the entire stack using the parameters that were obtained in \"segment\".   ### save The parameters can be saved in a json file.   ## License  Distributed under the terms of the [MIT] license, \"misic-napari\" is free and open source software  ## Cite ``` @article {10.7554/eLife.65151, article_type = {journal}, title = {Misic, a general deep learning-based method for the high-throughput cell segmentation of complex bacterial communities}, author = {Panigrahi, Swapnesh and Murat, Dorothée and Le Gall, Antoine and Martineau, Eugénie and Goldlust, Kelly and Fiche, Jean-Bernard and Rombouts, Sara and Nöllmann, Marcelo and Espinosa, Leon and Mignot, Tâm}, editor = {Xiao, Jie and Storz, Gisela and Hensel, Zach}, volume = 10, year = 2021, month = {sep}, pub_date = {2021-09-09}, pages = {e65151}, citation = {eLife 2021;10:e65151}, doi = {10.7554/eLife.65151}, url = {https://doi.org/10.7554/eLife.65151}, abstract = {Studies of bacterial communities, biofilms and microbiomes, are multiplying due to their impact on health and ecology. Live imaging of microbial communities requires new tools for the robust identification of bacterial cells in dense and often inter-species populations, sometimes over very large scales. Here, we developed MiSiC, a general deep-learning-based 2D segmentation method that automatically segments single bacteria in complex images of interacting bacterial communities with very little parameter adjustment, independent of the microscopy settings and imaging modality. Using a bacterial predator-prey interaction model, we demonstrate that MiSiC enables the analysis of interspecies interactions, resolving processes at subcellular scales and discriminating between species in millimeter size datasets. The simple implementation of MiSiC and the relatively low need in computing power make its use broadly accessible to fields interested in bacterial interactions and cell biology.}, keywords = {Deep learning, image analysis, microscopy, myxococcus xanthus, biofilms}, journal = {eLife}, issn = {2050-084X}, publisher = {eLife Sciences Publications, Ltd}, } ``` ",
    "description_content_type": "text/markdown",
    "description_text": "misic-napari    A napari plugin for MiSiC. Segmentation of bacteria in dense colonies.  The plugin provides acces to preprocessing of the image like scaling, gamma correction, sharpness and noise variance that can improve the segmentation of bacteria irrespective of the imaging modality. Install Napari Install napari either the bundled app or through [pip/conda] https://napari.org/#installation Installation Install misic-napari through plugin manager in napari. Or You can install misic-napari via [pip] in the napari console: pip install misic-napari  Tutorial Note:  The image should be in the format [n,row,col] or [row,col], i.e., a single image or a stack. Hyper-stacks are not supported yet.  get_width Creates a Shapes layer with name 'cell-width' where the cell width can be hand drawn using line drawing tools in the shapes layer. This need not be precise and can be adjusted later. Click get_cell_width to obtain the desired mean cell width. This will be used to scale the image accordingly before segmentation. segment This can be used to quickly set the parameters that can be later used to segment the whole stack. use_roi A square ROI of side 256 is created by default for quickly checking adjusting the segmentation parameters. The roi can be resized or moved in the roi shapes layer. light_background True; for phase-contrast images. False; for bright-field and fluorescence images. use_local_noise If checked, this adds noise to image with local variance. In this case, a noise_var of around 0.1 works well. If unchecked, this adds noise with global variance of noise_var/100. Adding may help in removing false positives. gaussian_laplace Useful when segmenting fluorescence images.  adjust_scale Fine-tuning the scale around ([0.8,1.2]) the scale obtained from cell-width determined in get_cell_width. noise_var Amount of noise to be added to the image at the preprocessing step. This helps reduce the False Positives and, in many cases, to separate cells effectively.  gamma gamma correction  sharpness_scale and sharpness_amount Unsharp mask based sharpness with sigma = sharpness_scale and amount = sharpness_amount segment_stack Segments the entire stack using the parameters that were obtained in \"segment\". save The parameters can be saved in a json file.  License Distributed under the terms of the [MIT] license, \"misic-napari\" is free and open source software Cite @article {10.7554/eLife.65151, article_type = {journal}, title = {Misic, a general deep learning-based method for the high-throughput cell segmentation of complex bacterial communities}, author = {Panigrahi, Swapnesh and Murat, Dorothée and Le Gall, Antoine and Martineau, Eugénie and Goldlust, Kelly and Fiche, Jean-Bernard and Rombouts, Sara and Nöllmann, Marcelo and Espinosa, Leon and Mignot, Tâm}, editor = {Xiao, Jie and Storz, Gisela and Hensel, Zach}, volume = 10, year = 2021, month = {sep}, pub_date = {2021-09-09}, pages = {e65151}, citation = {eLife 2021;10:e65151}, doi = {10.7554/eLife.65151}, url = {https://doi.org/10.7554/eLife.65151}, abstract = {Studies of bacterial communities, biofilms and microbiomes, are multiplying due to their impact on health and ecology. Live imaging of microbial communities requires new tools for the robust identification of bacterial cells in dense and often inter-species populations, sometimes over very large scales. Here, we developed MiSiC, a general deep-learning-based 2D segmentation method that automatically segments single bacteria in complex images of interacting bacterial communities with very little parameter adjustment, independent of the microscopy settings and imaging modality. Using a bacterial predator-prey interaction model, we demonstrate that MiSiC enables the analysis of interspecies interactions, resolving processes at subcellular scales and discriminating between species in millimeter size datasets. The simple implementation of MiSiC and the relatively low need in computing power make its use broadly accessible to fields interested in bacterial interactions and cell biology.}, keywords = {Deep learning, image analysis, microscopy, myxococcus xanthus, biofilms}, journal = {eLife}, issn = {2050-084X}, publisher = {eLife Sciences Publications, Ltd}, }",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "misic-napari",
    "documentation": "",
    "first_released": "2021-12-07T20:17:57.756112Z",
    "license": "MIT",
    "name": "misic-napari",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/pswap/misic",
    "python_version": ">=3.6",
    "reader_file_extensions": [],
    "release_date": "2022-12-15T15:36:30.601282Z",
    "report_issues": "",
    "requirements": ["tensorflow", "termcolor"],
    "summary": "Segmentation of bacteria agnostic to imaging modality",
    "support": "",
    "twitter": "",
    "version": "0.2.3",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      { "email": "jonas.windhager@uzh.ch", "name": "Jonas Windhager" }
    ],
    "code_repository": "https://github.com/BodenmillerGroup/napari-czifile2",
    "description": "# napari-czifile2  <a href=\"https://pypi.org/project/napari-czifile2/\">     <img src=\"https://img.shields.io/pypi/v/napari-czifile2\" alt=\"PyPI\" /> </a> <a href=\"https://github.com/BodenmillerGroup/napari-czifile2/blob/main/LICENSE.md\">     <img src=\"https://img.shields.io/pypi/l/napari-czifile2\" alt=\"License\" /> </a> <a href=\"https://www.python.org/\">     <img src=\"https://img.shields.io/pypi/pyversions/napari-czifile2\" alt=\"Python\" /> </a> <a href=\"https://github.com/BodenmillerGroup/napari-czifile2/issues\">     <img src=\"https://img.shields.io/github/issues/BodenmillerGroup/napari-czifile2\" alt=\"Issues\" /> </a> <a href=\"https://github.com/BodenmillerGroup/napari-czifile2/pulls\">     <img src=\"https://img.shields.io/github/issues-pr/BodenmillerGroup/napari-czifile2\" alt=\"Pull requests\" /> </a>  Carl Zeiss Image (.czi) file type support for napari  Open .czi files and interactively view scenes co-registered in the machine's coordinate system using napari  ## Installation  You can install napari-czifile2 via [pip](https://pypi.org/project/pip/):      pip install napari-czifile2  Alternatively, you can install napari-czifile2 via [conda](https://conda.io/):      conda install -c conda-forge napari-czifile2  ## Authors  Created and maintained by Jonas Windhager [jonas.windhager@uzh.ch](mailto:jonas.windhager@uzh.ch)  ## Contributing  [Contributing](https://github.com/BodenmillerGroup/napari-czifile2/blob/main/CONTRIBUTING.md)  ## Changelog  [Changelog](https://github.com/BodenmillerGroup/napari-czifile2/blob/main/CHANGELOG.md)  ## License  [MIT](https://github.com/BodenmillerGroup/napari-czifile2/blob/main/LICENSE.md) ",
    "description_content_type": "text/markdown",
    "description_text": "napari-czifile2                Carl Zeiss Image (.czi) file type support for napari Open .czi files and interactively view scenes co-registered in the machine's coordinate system using napari Installation You can install napari-czifile2 via pip: pip install napari-czifile2  Alternatively, you can install napari-czifile2 via conda: conda install -c conda-forge napari-czifile2  Authors Created and maintained by Jonas Windhager jonas.windhager@uzh.ch Contributing Contributing Changelog Changelog License MIT",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "napari-czifile2",
    "documentation": "https://github.com/BodenmillerGroup/napari-czifile2#README.md",
    "first_released": "2021-02-02T03:07:14.849183Z",
    "license": "MIT",
    "name": "napari-czifile2",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader"],
    "project_site": "https://github.com/BodenmillerGroup/napari-czifile2",
    "python_version": ">=3.8",
    "reader_file_extensions": ["*.czi"],
    "release_date": "2022-06-01T12:21:38.927913Z",
    "report_issues": "https://github.com/BodenmillerGroup/napari-czifile2/issues",
    "requirements": ["czifile", "imagecodecs", "numpy", "tifffile"],
    "summary": "Carl Zeiss Image (.czi) file support for napari",
    "support": "https://github.com/BodenmillerGroup/napari-czifile2/issues",
    "twitter": "",
    "version": "0.2.7",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Tristan Cotte" }],
    "code_repository": "https://github.com/tcotte/napari-calibration",
    "conda": [],
    "description": "# napari-calibration\\r \\r [![License](https://img.shields.io/pypi/l/napari-calibration.svg?color=green)](https://github.com/tcotte/napari-calibration/raw/main/LICENSE)\\r [![PyPI](https://img.shields.io/pypi/v/napari-calibration.svg?color=green)](https://pypi.org/project/napari-calibration)\\r [![Python Version](https://img.shields.io/pypi/pyversions/napari-calibration.svg?color=green)](https://python.org)\\r [![tests](https://github.com/tcotte/napari-calibration/workflows/tests/badge.svg)](https://github.com/tcotte/napari-calibration/actions)\\r [![codecov](https://codecov.io/gh/tcotte/napari-calibration/branch/main/graph/badge.svg)](https://codecov.io/gh/tcotte/napari-calibration)\\r [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-calibration)](https://napari-hub.org/plugins/napari-calibration)\\r \\r Plug in which enables to make camera calibration\\r \\r ----------------------------------\\r \\r This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\r \\r <!--\\r Don't miss the full getting started guide to set up your new package:\\r https://github.com/napari/cookiecutter-napari-plugin#getting-started\\r \\r and review the napari docs for plugin developers:\\r https://napari.org/plugins/stable/index.html\\r -->\\r \\r ## Installation\\r \\r You can install `napari-calibration` via [pip]:\\r \\r     pip install napari-calibration\\r \\r \\r \\r To install latest development version :\\r \\r     pip install git+https://github.com/tcotte/napari-calibration.git\\r \\r \\r ## Contributing\\r \\r Contributions are very welcome. Tests can be run with [tox], please ensure\\r the coverage at least stays the same before you submit a pull request.\\r \\r ## License\\r \\r Distributed under the terms of the [BSD-3] license,\\r \"napari-calibration\" is free and open source software\\r \\r ## Issues\\r \\r If you encounter any problems, please [file an issue] along with a detailed description.\\r \\r [napari]: https://github.com/napari/napari\\r [Cookiecutter]: https://github.com/audreyr/cookiecutter\\r [@napari]: https://github.com/napari\\r [MIT]: http://opensource.org/licenses/MIT\\r [BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\r [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\r [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\r [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\r [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\r [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\r \\r [file an issue]: https://github.com/tcotte/napari-calibration/issues\\r \\r [napari]: https://github.com/napari/napari\\r [tox]: https://tox.readthedocs.io/en/latest/\\r [pip]: https://pypi.org/project/pip/\\r [PyPI]: https://pypi.org/\\r \\r \\r ",
    "description_content_type": "text/markdown",
    "description_text": "napari-calibration       Plug in which enables to make camera calibration  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install napari-calibration via pip: pip install napari-calibration  To install latest development version : pip install git+https://github.com/tcotte/napari-calibration.git  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-calibration\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari Calibration",
    "documentation": "https://github.com/tcotte/napari-calibration#README.md",
    "first_released": "2022-08-23T15:02:59.873198Z",
    "license": "BSD-3-Clause",
    "name": "napari-calibration",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/tcotte/napari-calibration",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-08-29T12:09:19.883590Z",
    "report_issues": "https://github.com/tcotte/napari-calibration/issues",
    "requirements": null,
    "summary": "Plug in which enables to make camera calibration",
    "support": "https://github.com/tcotte/napari-calibration/issues",
    "twitter": "",
    "version": "0.0.14",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "email": "dibrov@mpi-cbg.de", "name": "Alexandr Dibrov" }],
    "code_repository": null,
    "description": "# napari-kics  ![napari-kics](https://github.com/mpicbg-csbd/napari-kics/raw/main/docs/banner.png?sanitize=true&raw=true)  [![standard-readme compliant](https://img.shields.io/badge/readme%20style-standard-brightgreen.svg)](https://github.com/RichardLitt/standard-readme) [![License](https://img.shields.io/pypi/l/napari-kics.svg?color=green)](./LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-kics.svg?color=green)](https://pypi.org/project/napari-kics) [![Python Version](https://img.shields.io/pypi/pyversions/napari-kics.svg?color=green)](https://python.org) [![Python package](https://github.com/mpicbg-csbd/napari-kics/actions/workflows/python-package.yml/badge.svg)](https://github.com/mpicbg-csbd/napari-kics/actions/workflows/python-package.yml)   > A plugin to estimate chromosome sizes from karyotype images.  <small>*This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.*</small>   ## Table of Contents  - Install - Usage - Example - Citation - Maintainer - Contributing - License   ## Install  You can install `napari-kics` via [pip]:  ```sh pip install napari-kics ```  This will install all required dependencies as well. We recommend installing it in a virtual environment, e.g. using [conda]:  ```sh conda create -n kics python conda activate kics pip install napari-kics ```  We recommend using [mamba] as a faster alternative to conda.   ## Usage  1. Launch Napari via command line (`napari`). 2. Activate the plugin via menu `Plugins -> napari-kics: Karyotype Widget`. 3. Select file via `File -> Open File`. 4. Follow instructions in the panel on the right.  You may use the interactive analysis plots directly via command line:  ```sh karyotype-analysis-plots ```   ## Example  1. Launch Napari via command line (`napari`). 2. Activate the plugin via menu `Plugins -> napari-kics: Karyotype Widget`. 3. Select file via `File -> Open Sample -> napari-kics: sample`. 4. Follow instructions in the panel on the right.  Try out the interactive analysis plots directly via command line:  ```sh karyotype-analysis-plots --example ```   ## Citation  > Arne Ludwig, Alexandr Dibrov, Gene Myers, Martin Pippel. > Estimating chromosome sizes from karyotype images enables validation of > *de novo* assemblies. To be published.   ## License  Distributed under the terms of the [BSD-3] license, \"napari-kics\" is free and open source software   ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.   ## Contributing  Contributions are very welcome. Please [file a pull request] with your contribution.  You can setup a local development environment for `napari-kics` via [pip]:  ```sh git clone https://github.com/mpicbg-csbd/napari-kics.git cd napari-kics pip install -e . ```   [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin [@napari]: https://github.com/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [conda]: https://www.anaconda.com/products/distribution [mamba]: https://github.com/mamba-org/mamba [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ [file an issue]: https://github.com/mpicbg-csbd/napari-kics/issues [file a pull request]: https://github.com/mpicbg-csbd/napari-kics/pulls  ## Overview https://user-images.githubusercontent.com/17703905/139654249-685703b5-2196-4a73-a036-d40d578ebcdf.mp4     ",
    "description_content_type": "text/markdown",
    "description_text": "napari-kics        A plugin to estimate chromosome sizes from karyotype images.  This napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template. Table of Contents  Install Usage Example Citation Maintainer Contributing License  Install You can install napari-kics via pip: sh pip install napari-kics This will install all required dependencies as well. We recommend installing it in a virtual environment, e.g. using conda: sh conda create -n kics python conda activate kics pip install napari-kics We recommend using mamba as a faster alternative to conda. Usage  Launch Napari via command line (napari). Activate the plugin via menu Plugins -> napari-kics: Karyotype Widget. Select file via File -> Open File. Follow instructions in the panel on the right.  You may use the interactive analysis plots directly via command line: sh karyotype-analysis-plots Example  Launch Napari via command line (napari). Activate the plugin via menu Plugins -> napari-kics: Karyotype Widget. Select file via File -> Open Sample -> napari-kics: sample. Follow instructions in the panel on the right.  Try out the interactive analysis plots directly via command line: sh karyotype-analysis-plots --example Citation  Arne Ludwig, Alexandr Dibrov, Gene Myers, Martin Pippel. Estimating chromosome sizes from karyotype images enables validation of de novo assemblies. To be published.  License Distributed under the terms of the BSD-3 license, \"napari-kics\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description. Contributing Contributions are very welcome. Please file a pull request with your contribution. You can setup a local development environment for napari-kics via pip: sh git clone https://github.com/mpicbg-csbd/napari-kics.git cd napari-kics pip install -e . Overview https://user-images.githubusercontent.com/17703905/139654249-685703b5-2196-4a73-a036-d40d578ebcdf.mp4",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari-kics",
    "documentation": "",
    "first_released": "2022-05-02T11:07:16.697044Z",
    "license": "BSD-3-Clause",
    "name": "napari-kics",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget", "sample_data"],
    "project_site": "",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-05-22T16:27:15.233710Z",
    "report_issues": "",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "napari[all]",
      "scikit-image",
      "pandas",
      "pulp",
      "pyqtgraph"
    ],
    "summary": "A plugin to estimate chromosome sizes from karyotype images.",
    "support": "",
    "twitter": "",
    "version": "0.0.3rc6",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Constantin Aronssohn" }],
    "category": {
      "Image modality": ["Electron microscopy"],
      "Supported data": ["3D"],
      "Workflow step": [
        "Visualization",
        "Image annotation",
        "Image Segmentation",
        "Clustering"
      ]
    },
    "category_hierarchy": {
      "Image modality": [
        [
          "Electron microscopy",
          "Cryo electron microscopy",
          "Electron cryotomography"
        ],
        [
          "Electron microscopy",
          "Transmission electron microscopy",
          "Electron tomography",
          "Electron cryotomography"
        ]
      ],
      "Supported data": [["3D"]],
      "Workflow step": [
        ["Visualization"],
        ["Image annotation"],
        ["Image Segmentation"],
        ["Image Segmentation", "Model-based segmentation"],
        ["Clustering"]
      ]
    },
    "code_repository": "https://github.com/deep-finder/napari-deepfinder",
    "conda": [],
    "description": "# napari-deepfinder  [![License GNU GPL v3.0](https://img.shields.io/pypi/l/napari-deepfinder.svg?color=green)](https://github.com/deep-finder/napari-deepfinder/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-deepfinder.svg?color=green)](https://pypi.org/project/napari-deepfinder) [![Python Version](https://img.shields.io/pypi/pyversions/napari-deepfinder.svg?color=green)](https://python.org) [![tests](https://github.com/deep-finder/napari-deepfinder/workflows/tests/badge.svg)](https://github.com/deep-finder/napari-deepfinder/actions) [![codecov](https://codecov.io/gh/deep-finder/napari-deepfinder/branch/main/graph/badge.svg)](https://codecov.io/gh/deep-finder/napari-deepfinder) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-deepfinder)](https://napari-hub.org/plugins/napari-deepfinder)  A napari plugin for the DeepFinder library which includes display, annotation, target generation, segmentation and clustering functionalities. An orthoslice view has been added for an easier visualisation and annotation process.  **The documentation for users is available [here](https://deep-finder.github.io/napari-deepfinder/).**  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/plugins/index.html -->  ## Installation  You can install `napari-deepfinder` via [pip]:      pip install napari-deepfinder    To install latest development version :      pip install git+https://github.com/deep-finder/napari-deepfinder.git   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [GNU GPL v3.0] license, \"napari-deepfinder\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/deep-finder/napari-deepfinder/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-deepfinder       A napari plugin for the DeepFinder library which includes display, annotation, target generation, segmentation and clustering functionalities. An orthoslice view has been added for an easier visualisation and annotation process. The documentation for users is available here.  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install napari-deepfinder via pip: pip install napari-deepfinder  To install latest development version : pip install git+https://github.com/deep-finder/napari-deepfinder.git  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the GNU GPL v3.0 license, \"napari-deepfinder\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 3 - Alpha"],
    "display_name": "Napari DeepFinder",
    "documentation": "https://deep-finder.github.io/napari-deepfinder/",
    "first_released": "2022-07-25T13:01:19.322626Z",
    "license": "GPL-3.0",
    "name": "napari-deepfinder",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader", "writer", "widget"],
    "project_site": "https://github.com/deep-finder/napari-deepfinder",
    "python_version": ">=3.8",
    "reader_file_extensions": [
      "*.mrc",
      "*.rec",
      "*.xlsx",
      "*.ods",
      "*.xls",
      "*.h5",
      "*.xml",
      "*.map",
      "*.TIF",
      "*.tif"
    ],
    "release_date": "2022-07-25T13:01:19.322626Z",
    "report_issues": "https://github.com/deep-finder/napari-deepfinder/issues",
    "requirements": [
      "em-deepfinder",
      "numpy",
      "magicgui",
      "qtpy",
      "napari",
      "scikit-image",
      "typing",
      "pandas",
      "lxml",
      "pillow",
      "h5py",
      "mrcfile",
      "scipy",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'"
    ],
    "summary": "A napari plugin for the DeepFinder library which includes display, annotation, target generation, segmentation and clustering functionalities. An orthoslice view has been added for an easier visualisation and annotation process.",
    "support": "https://github.com/deep-finder/napari-deepfinder/issues",
    "twitter": "",
    "version": "0.0.1",
    "visibility": "public",
    "writer_file_extensions": [".xml", ".mrc"],
    "writer_save_layers": ["labels", "points*", "image"]
  },
  {
    "authors": [
      { "name": "Cyril Achard" },
      { "name": "Maxime Vidal" },
      { "name": "Jessy Lauer" },
      { "name": "Mackenzie Mathis" }
    ],
    "category": {
      "Image modality": ["Fluorescence microscopy"],
      "Supported data": ["3D"],
      "Workflow step": ["Image Segmentation", "Visualization"]
    },
    "category_hierarchy": {
      "Image modality": [["Fluorescence microscopy", "Light-sheet microscopy"]],
      "Supported data": [["3D"]],
      "Workflow step": [
        ["Image Segmentation", "Cell segmentation"],
        ["Visualization", "Image visualisation"],
        ["Image Segmentation", "Region growing", "Watershed segmentation"],
        ["Visualization", "Plotting"]
      ]
    },
    "code_repository": "https://github.com/AdaptiveMotorControlLab/CellSeg3d",
    "conda": [{ "channel": "conda-forge", "package": "napari-cellseg3d" }],
    "description": "# napari-cellseg3D: a napari plug-in for direct 3D cell segmentation with deep learning   <img src=\"https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/838605d0-9723-4e43-83cd-6dbfe4adf36b/cellseg-logo.png?format=1500w\" title=\"cellseg3d\" alt=\"cellseg3d logo\" width=\"350\" align=\"right\" vspace = \"80\"/>  <a href=\"https://github.com/psf/black\"><img alt=\"Code style: black\" src=\"https://img.shields.io/badge/code%20style-black-000000.svg\"></a> [![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://github.com/AdaptiveMotorControlLab/CellSeg3d/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-cellseg3d.svg?color=green)](https://pypi.org/project/napari-cellseg3d) [![Python Version](https://img.shields.io/pypi/pyversions/napari-cellseg-annotator.svg?color=green)](https://python.org) [![codecov](https://codecov.io/gh/AdaptiveMotorControlLab/CellSeg3d/branch/main/graph/badge.svg?token=hzUcn3XN8F)](https://codecov.io/gh/AdaptiveMotorControlLab/CellSeg3d) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-cellseg3d)](https://www.napari-hub.org/plugins/napari-cellseg3d)   A napari plugin for 3D cell segmentation: training, inference, and data review. In particular, this project was developed for analysis of mesoSPIM-acquired (cleared tissue + lightsheet) datasets.  ----------------------------------  ## News  **June 2022: This is an alpha version, please expect bugs and issues, and help us make the code better by reporting them as an issue!**    ## Installation  Note : we recommend using conda to create a new environment for the plugin.      conda create --name python=3.8 napari-cellseg3d     conda activate napari-cellseg3d  You can install `napari-cellseg3d` via [pip]:        pip install napari-cellseg3d  OR directly via [napari-hub]:  - Install napari from pip with `pip install \"napari[all]\"`, then from the “Plugins” menu within the napari application, select “Install/Uninstall Package(s)...” - Copy `napari-cellseg3d` and paste it where it says “Install by name/url…” - Click “Install”  ## Documentation  Available at https://AdaptiveMotorControlLab.github.io/CellSeg3d  You can also generate docs by running ``make html`` in the docs folder.  ## Usage  To use the plugin, please run: ``` napari ``` Then go into Plugins > napari-cellseg3d, and choose which tool to use.  - **Review**: This module allows you to review your labels, from predictions or manual labeling, and correct them if needed. It then saves the status of each file in a csv, for easier monitoring. - **Inference**: This module allows you to use pre-trained segmentation algorithms on volumes to automatically label cells and compute statistics. - **Train**:  This module allows you to train segmentation algorithms from labeled volumes. - **Utilities**: This module allows you to perform several actions like cropping your volumes and labels dynamically, by selecting a fixed size volume and moving it around the image; computing prediction scores from ground truth and predicition labels; or converting labels from instance to segmentation and the opposite.   ## Requirements **Python >= 3.8 required**  Requires **pytorch** and **MONAI**. For PyTorch, please see [PyTorch's website for installation instructions]. A CUDA-capable GPU is not needed but very strongly recommended, especially for training. If you get errors from MONAI regarding missing readers, please see [MONAI's optional dependencies] page for instructions on getting the readers required by your images.   ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.   ## Testing  To run tests locally:  - Locally : run ``pytest`` in the plugin folder - Locally with coverage : In the plugin folder, run ``coverage run --source=napari_cellseg3d -m pytest`` then ``coverage xml`` to generate a .xml coverage file. - With tox : run ``tox`` in the plugin folder (will simulate tests with several python and OS configs, requires substantial storage space)  ## Contributing  Contributions are very welcome.  Please ensure the coverage at least stays the same before you submit a pull request.  For local installation from Github cloning, please run:  ``` pip install -e . ```  ## License  Distributed under the terms of the [MIT] license.  \"napari-cellseg3d\" is free and open source software.  [napari-hub]: https://www.napari-hub.org/plugins/napari-cellseg3d  [file an issue]: https://github.com/AdaptiveMotorControlLab/CellSeg3d/issues [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/  [PyTorch's website for installation instructions]: https://pytorch.org/get-started/locally/ [MONAI's optional dependencies]: https://docs.monai.io/en/stable/installation.html#installing-the-recommended-dependencies  ## Acknowledgements  This plugin was developed by Cyril Achard, Maxime Vidal, Mackenzie Mathis. This work was funded, in part, from the Wyss Center to the [Mathis Laboratory of Adaptive Motor Control](https://www.mackenziemathislab.org/).   ## Plugin base This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/plugins/stable/index.html --> ",
    "description_content_type": "text/markdown",
    "description_text": "napari-cellseg3D: a napari plug-in for direct 3D cell segmentation with deep learning        A napari plugin for 3D cell segmentation: training, inference, and data review. In particular, this project was developed for analysis of mesoSPIM-acquired (cleared tissue + lightsheet) datasets.  News June 2022: This is an alpha version, please expect bugs and issues, and help us make the code better by reporting them as an issue! Installation Note : we recommend using conda to create a new environment for the plugin. conda create --name python=3.8 napari-cellseg3d conda activate napari-cellseg3d  You can install napari-cellseg3d via pip:   pip install napari-cellseg3d  OR directly via napari-hub:  Install napari from pip with pip install \"napari[all]\", then from the “Plugins” menu within the napari application, select “Install/Uninstall Package(s)...” Copy napari-cellseg3d and paste it where it says “Install by name/url…” Click “Install”  Documentation Available at https://AdaptiveMotorControlLab.github.io/CellSeg3d You can also generate docs by running make html in the docs folder. Usage To use the plugin, please run: napari Then go into Plugins > napari-cellseg3d, and choose which tool to use.  Review: This module allows you to review your labels, from predictions or manual labeling, and correct them if needed. It then saves the status of each file in a csv, for easier monitoring. Inference: This module allows you to use pre-trained segmentation algorithms on volumes to automatically label cells and compute statistics. Train:  This module allows you to train segmentation algorithms from labeled volumes. Utilities: This module allows you to perform several actions like cropping your volumes and labels dynamically, by selecting a fixed size volume and moving it around the image; computing prediction scores from ground truth and predicition labels; or converting labels from instance to segmentation and the opposite.  Requirements Python >= 3.8 required Requires pytorch and MONAI. For PyTorch, please see PyTorch's website for installation instructions. A CUDA-capable GPU is not needed but very strongly recommended, especially for training. If you get errors from MONAI regarding missing readers, please see MONAI's optional dependencies page for instructions on getting the readers required by your images. Issues If you encounter any problems, please file an issue along with a detailed description. Testing To run tests locally:  Locally : run pytest in the plugin folder Locally with coverage : In the plugin folder, run coverage run --source=napari_cellseg3d -m pytest then coverage xml to generate a .xml coverage file. With tox : run tox in the plugin folder (will simulate tests with several python and OS configs, requires substantial storage space)  Contributing Contributions are very welcome. Please ensure the coverage at least stays the same before you submit a pull request. For local installation from Github cloning, please run: pip install -e . License Distributed under the terms of the MIT license. \"napari-cellseg3d\" is free and open source software. Acknowledgements This plugin was developed by Cyril Achard, Maxime Vidal, Mackenzie Mathis. This work was funded, in part, from the Wyss Center to the Mathis Laboratory of Adaptive Motor Control. Plugin base This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template. ",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "Cell Segmentation Annotator",
    "documentation": "https://adaptivemotorcontrollab.github.io/CellSeg3d/res/welcome.html",
    "first_released": "2022-06-25T17:39:48.308233Z",
    "license": "MIT",
    "name": "napari-cellseg3d",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/AdaptiveMotorControlLab/CellSeg3d",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-08-19T16:11:07.693839Z",
    "report_issues": "https://github.com/AdaptiveMotorControlLab/CellSeg3d/issues",
    "requirements": [
      "numpy",
      "napari[all] (>=0.4.14)",
      "QtPy",
      "opencv-python (>=4.5.5)",
      "dask-image (>=0.6.0)",
      "scikit-image (>=0.19.2)",
      "matplotlib (>=3.4.1)",
      "tifffile (>=2022.2.9)",
      "imageio-ffmpeg (>=0.4.5)",
      "torch (>=1.11)",
      "monai[einops,itk,nibabel,scikit-image] (>=0.9.0)",
      "tqdm",
      "monai (>=0.9.0)",
      "nibabel",
      "scikit-image",
      "pillow",
      "matplotlib",
      "vispy (>=0.9.6)"
    ],
    "summary": "plugin for cell segmentation",
    "support": "https://github.com/AdaptiveMotorControlLab/CellSeg3d/issues",
    "twitter": "",
    "version": "0.0.1rc4",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Sean Martin" }],
    "code_repository": "https://github.com/seankmartin/napari-cookiecut",
    "conda": [],
    "description": "# napari-cookiecut  [![License BSD-3](https://img.shields.io/pypi/l/napari-cookiecut.svg?color=green)](https://github.com/seankmartin/napari-cookiecut/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-cookiecut.svg?color=green)](https://pypi.org/project/napari-cookiecut) [![Python Version](https://img.shields.io/pypi/pyversions/napari-cookiecut.svg?color=green)](https://python.org) [![tests](https://github.com/seankmartin/napari-cookiecut/workflows/tests/badge.svg)](https://github.com/seankmartin/napari-cookiecut/actions) [![codecov](https://codecov.io/gh/seankmartin/napari-cookiecut/branch/main/graph/badge.svg)](https://codecov.io/gh/seankmartin/napari-cookiecut) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-cookiecut)](https://napari-hub.org/plugins/napari-cookiecut)  A fixed version of a cookiecut napari plugin template that has been set up with all the basic functionality following the README for reference.  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/stable/plugins/index.html -->  ## Installation  You can install `napari-cookiecut` via [pip]:      pip install napari-cookiecut    To install latest development version :      pip install git+https://github.com/seankmartin/napari-cookiecut.git   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-cookiecut\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/seankmartin/napari-cookiecut/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-cookiecut       A fixed version of a cookiecut napari plugin template that has been set up with all the basic functionality following the README for reference.  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install napari-cookiecut via pip: pip install napari-cookiecut  To install latest development version : pip install git+https://github.com/seankmartin/napari-cookiecut.git  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-cookiecut\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "Cookiecut",
    "documentation": "https://github.com/seankmartin/napari-cookiecut#README.md",
    "first_released": "2022-10-21T11:45:28.481005Z",
    "license": "BSD-3-Clause",
    "name": "napari-cookiecut",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader", "writer", "widget", "sample_data"],
    "project_site": "https://github.com/seankmartin/napari-cookiecut",
    "python_version": ">=3.8",
    "reader_file_extensions": ["*.npy"],
    "release_date": "2022-10-24T14:00:12.254912Z",
    "report_issues": "https://github.com/seankmartin/napari-cookiecut/issues",
    "requirements": [
      "numpy",
      "magicgui",
      "qtpy",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "summary": "Fixed cut",
    "support": "https://github.com/seankmartin/napari-cookiecut/issues",
    "twitter": "",
    "version": "0.1.1",
    "visibility": "public",
    "writer_file_extensions": [".npy"],
    "writer_save_layers": ["image", "image*", "labels*"]
  },
  {
    "authors": [{ "email": "trevor.j.manz@gmail.com", "name": "Trevor Manz" }],
    "code_repository": "https://github.com/manzt/napari-lazy-openslide",
    "description": "# napari-lazy-openslide  [![License](https://img.shields.io/pypi/l/napari-lazy-openslide.svg?color=green)](https://github.com/manzt/napari-lazy-openslide/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-lazy-openslide.svg?color=green)](https://pypi.org/project/napari-lazy-openslide) [![Python Version](https://img.shields.io/pypi/pyversions/napari-lazy-openslide.svg?color=green)](https://python.org) [![tests](https://github.com/manzt/napari-lazy-openslide/workflows/tests/badge.svg)](https://github.com/manzt/napari-lazy-openslide/actions)  An experimental plugin to lazily load multiscale whole-slide tiff images with openslide and dask.  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.  ## Installation  **Step 1.)** Make sure you have OpenSlide installed. Download instructions [here](https://openslide.org/download/).  > NOTE: Installation on macOS is easiest via Homebrew: `brew install openslide`. Up-to-date and multiplatform  > binaries for `openslide` are also avaiable via `conda`: `conda install -c sdvillal openslide-python`  **Step 2.)** Install `napari-lazy-openslide` via `pip`:      pip install napari-lazy-openslide  ## Usage  ### Napari plugin  ```bash $ napari tumor_004.tif ``` By installing this package via `pip`, the plugin should be recognized by `napari`. The plugin attempts to read image formats recognized by `openslide` that are multiscale  (`openslide.OpenSlide.level_count > 1`).   It should be noted that `napari-lazy-openslide` is experimental and has primarily  been tested with `CAMELYON16` and `CAMELYON17` datasets, which can be  downloaded [here](https://camelyon17.grand-challenge.org/Data/).  ![Interactive deep zoom of whole-slide image](tumor_004.gif)   ### Using `OpenSlideStore` with Zarr and Dask  The `OpenSlideStore` class wraps an `openslide.OpenSlide` object as a valid Zarr store.  The underlying `openslide` image pyramid is translated to the Zarr multiscales extension, where each level of the pyramid is a separate 3D `zarr.Array` with shape `(y, x, 4)`.  ```python import dask.array as da import zarr  from napari_lazy_openslide import OpenSlideStore  store = OpenSlideStore('tumor_004.tif') grp = zarr.open(store, mode=\"r\")  # The OpenSlideStore implements the multiscales extension # https://forum.image.sc/t/multiscale-arrays-v0-1/37930 datasets = grp.attrs[\"multiscales\"][0][\"datasets\"]  pyramid = [grp.get(d[\"path\"]) for d in datasets] print(pyramid) # [ #   <zarr.core.Array '/0' (23705, 29879, 4) uint8 read-only>, #   <zarr.core.Array '/1' (5926, 7469, 4) uint8 read-only>, #   <zarr.core.Array '/2' (2963, 3734, 4) uint8 read-only>, # ]  pyramid = [da.from_zarr(store, component=d[\"path\"]) for d in datasets] print(pyramid) # [ #   dask.array<from-zarr, shape=(23705, 29879, 4), dtype=uint8, chunksize=(512, 512, 4), chunktype=numpy.ndarray>, #   dask.array<from-zarr, shape=(5926, 7469, 4), dtype=uint8, chunksize=(512, 512, 4), chunktype=numpy.ndarray>, #   dask.array<from-zarr, shape=(2963, 3734, 4), dtype=uint8, chunksize=(512, 512, 4), chunktype=numpy.ndarray>, # ]  # Now you can use numpy-like indexing with openslide, reading data into memory lazily! low_res = pyramid[-1][:] region = pyramid[0][y_start:y_end, x_start:x_end] ```  ## Contributing  Contributions are very welcome. Tests can be run with `tox`, please ensure the coverage at least stays the same before you submit a pull request.  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin [file an issue]: https://github.com/manzt/napari-lazy-openslide/issues [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-lazy-openslide     An experimental plugin to lazily load multiscale whole-slide tiff images with openslide and dask.  This napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template. Installation Step 1.) Make sure you have OpenSlide installed. Download instructions here.  NOTE: Installation on macOS is easiest via Homebrew: brew install openslide. Up-to-date and multiplatform  binaries for openslide are also avaiable via conda: conda install -c sdvillal openslide-python  Step 2.) Install napari-lazy-openslide via pip: pip install napari-lazy-openslide  Usage Napari plugin bash $ napari tumor_004.tif By installing this package via pip, the plugin should be recognized by napari. The plugin attempts to read image formats recognized by openslide that are multiscale  (openslide.OpenSlide.level_count > 1).  It should be noted that napari-lazy-openslide is experimental and has primarily  been tested with CAMELYON16 and CAMELYON17 datasets, which can be  downloaded here.  Using OpenSlideStore with Zarr and Dask The OpenSlideStore class wraps an openslide.OpenSlide object as a valid Zarr store.  The underlying openslide image pyramid is translated to the Zarr multiscales extension, where each level of the pyramid is a separate 3D zarr.Array with shape (y, x, 4). ```python import dask.array as da import zarr from napari_lazy_openslide import OpenSlideStore store = OpenSlideStore('tumor_004.tif') grp = zarr.open(store, mode=\"r\") The OpenSlideStore implements the multiscales extension https://forum.image.sc/t/multiscale-arrays-v0-1/37930 datasets = grp.attrs[\"multiscales\"][0][\"datasets\"] pyramid = [grp.get(d[\"path\"]) for d in datasets] print(pyramid) [ , , , ] pyramid = [da.from_zarr(store, component=d[\"path\"]) for d in datasets] print(pyramid) [ dask.array, dask.array, dask.array, ] Now you can use numpy-like indexing with openslide, reading data into memory lazily! low_res = pyramid[-1][:] region = pyramid[0][y_start:y_end, x_start:x_end] ``` Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "napari-lazy-openslide",
    "documentation": "",
    "first_released": "2020-07-14T17:50:55.269908Z",
    "license": "BSD-3-Clause",
    "name": "napari-lazy-openslide",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader"],
    "project_site": "https://github.com/manzt/napari-lazy-openslide",
    "python_version": ">=3.6",
    "reader_file_extensions": ["*"],
    "release_date": "2022-05-19T13:32:58.189644Z",
    "report_issues": "",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "zarr (>=2.11.0)",
      "numpy",
      "dask[array]",
      "openslide-python"
    ],
    "summary": "A plugin to lazily load multiscale whole-slide images with openslide and dask",
    "support": "",
    "twitter": "",
    "version": "0.3.0",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [],
    "code_repository": "https://github.com/emilmelnikov/napari-cilia-beating-frequency",
    "conda": [],
    "description": "# Cilia beating frequency  > Cilia beating frequency detection. ",
    "description_content_type": "text/markdown",
    "description_text": "Cilia beating frequency  Cilia beating frequency detection. ",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "Cilia beating frequency detection",
    "documentation": "",
    "first_released": "2022-07-13T08:44:09.675558Z",
    "license": "MIT",
    "name": "napari-cilia-beating-frequency",
    "npe2": true,
    "operating_system": [],
    "plugin_types": ["widget"],
    "project_site": "",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-07-13T08:44:09.675558Z",
    "report_issues": "",
    "requirements": [
      "magicgui",
      "napari[all]",
      "numpy (>=1.20)",
      "qtpy",
      "scipy"
    ],
    "summary": "Cilia beating frequency detection",
    "support": "",
    "twitter": "",
    "version": "0.1",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Robert Haase" }],
    "category": {
      "Supported data": ["2D", "3D"],
      "Workflow step": [
        "Image Segmentation",
        "Image correction",
        "Image reconstruction",
        "Image enhancement",
        "Morphological operations",
        "Image feature detection"
      ]
    },
    "category_hierarchy": {
      "Supported data": [["2D"], ["3D"]],
      "Workflow step": [
        ["Image Segmentation", "Connected-component analysis"],
        ["Image correction", "Illumination correction"],
        ["Image correction"],
        ["Image reconstruction", "Image denoising"],
        ["Image enhancement", "Image denoising"],
        ["Image Segmentation", "Image thresholding"],
        ["Image Segmentation"],
        ["Image enhancement", "Smoothing"],
        ["Morphological operations"],
        ["Image Segmentation", "Semi-automatic segmentation"],
        ["Image feature detection", "Edge detection"],
        ["Morphological operations", "Top-hat transform"],
        ["Morphological operations", "Closing"],
        ["Morphological operations", "Dilation"],
        ["Morphological operations", "Opening"],
        ["Morphological operations", "Erosion"],
        ["Image enhancement"]
      ]
    },
    "code_repository": "https://github.com/haesleinhuepf/napari-cupy-image-processing",
    "conda": [
      { "channel": "conda-forge", "package": "napari-cupy-image-processing" }
    ],
    "description": "# napari-cupy-image-processing  [![License](https://img.shields.io/pypi/l/napari-cupy-image-processing.svg?color=green)](https://github.com/haesleinhuepf/napari-cupy-image-processing/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-cupy-image-processing.svg?color=green)](https://pypi.org/project/napari-cupy-image-processing) [![Python Version](https://img.shields.io/pypi/pyversions/napari-cupy-image-processing.svg?color=green)](https://python.org) [![tests](https://github.com/haesleinhuepf/napari-cupy-image-processing/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-cupy-image-processing/actions) [![codecov](https://codecov.io/gh/haesleinhuepf/napari-cupy-image-processing/branch/master/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-cupy-image-processing) [![Development Status](https://img.shields.io/pypi/status/napari-cupy-image-processing.svg)](https://en.wikipedia.org/wiki/Software_release_life_cycle#Alpha) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-cupy-image-processing)](https://napari-hub.org/plugins/napari-cupy-image-processing)   GPU-accelerated image processing using [cupy](https://cupy.dev) and [CUDA](https://en.wikipedia.org/wiki/CUDA)  ## Usage  This napari plugin adds some menu entries to the Tools menu. You can recognize them with their suffix `(n-cupy)` in brackets. Furthermore, it can be used from the [napari-assistant](https://www.napari-hub.org/plugins/napari-assistant) graphical user interface.  Therefore, just click the menu `Tools > Utilities > Assistant (na)` or run `naparia` from the command line.  ![img.png](https://github.com/haesleinhuepf/napari-cupy-image-processing/raw/main/docs/screenshot-with-tools-menu.png)  You can also call operations from python, e.g. as shown in this [demo notebook](https://github.com/haesleinhuepf/napari-cupy-image-processing/raw/main/docs/demo.ipynb).  ## Installation  Follow the [instructions for installing cupy](https://docs.cupy.dev/en/stable/install.html#installing-cupy-from-conda-forge) on your computer first.      conda install -c conda-forge cupy  Afterwards, you can install `napari-cupy-image-processing` via [pip]:      pip install napari-cupy-image-processing  A more detailed example for installation (change 11.2 to your desired CUDA version): ``` conda create --name cupy_p39 python=3.9 conda activate cupy_p39 conda install -c conda-forge cupy cudatoolkit=11.2 napari pip install napari-cupy-image-processing ```  ## Contributing  Contributions are very welcome. Adding [cupy ndimage](https://docs.cupy.dev/en/stable/reference/ndimage.html) functions is quite easy as you can see in the  [implementation of the current operations](https://github.com/haesleinhuepf/napari-cupy-image-processing/blob/main/napari_cupy_image_processing/_cupy_image_processing.py#L48).  If you need another function in napari, just send a PR. Please make sure the tests pass locally before submitting a PR.  ``` pip install pytest-cov pytest-qt pytest --cov=napari_cupy_image_processing ```  This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.  ## License  Distributed under the terms of the [MIT] license, \"napari-cupy-image-processing\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/haesleinhuepf/napari-cupy-image-processing/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-cupy-image-processing        GPU-accelerated image processing using cupy and CUDA Usage This napari plugin adds some menu entries to the Tools menu. You can recognize them with their suffix (n-cupy) in brackets. Furthermore, it can be used from the napari-assistant graphical user interface.  Therefore, just click the menu Tools > Utilities > Assistant (na) or run naparia from the command line.  You can also call operations from python, e.g. as shown in this demo notebook. Installation Follow the instructions for installing cupy on your computer first. conda install -c conda-forge cupy  Afterwards, you can install napari-cupy-image-processing via pip: pip install napari-cupy-image-processing  A more detailed example for installation (change 11.2 to your desired CUDA version): conda create --name cupy_p39 python=3.9 conda activate cupy_p39 conda install -c conda-forge cupy cudatoolkit=11.2 napari pip install napari-cupy-image-processing Contributing Contributions are very welcome. Adding cupy ndimage functions is quite easy as you can see in the  implementation of the current operations.  If you need another function in napari, just send a PR. Please make sure the tests pass locally before submitting a PR. pip install pytest-cov pytest-qt pytest --cov=napari_cupy_image_processing This napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template. License Distributed under the terms of the MIT license, \"napari-cupy-image-processing\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 3 - Alpha"],
    "display_name": "napari-cupy-image-processing",
    "documentation": "https://github.com/haesleinhuepf/napari-cupy-image-processing#README.md",
    "first_released": "2021-10-22T19:41:19.679750Z",
    "license": "MIT",
    "name": "napari-cupy-image-processing",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/haesleinhuepf/napari-cupy-image-processing",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-08-27T12:39:35.723835Z",
    "report_issues": "https://github.com/haesleinhuepf/napari-cupy-image-processing/issues",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "toolz",
      "cupy",
      "napari-tools-menu",
      "scikit-image",
      "napari-time-slicer (>=0.4.8)",
      "napari-skimage-regionprops",
      "napari-assistant",
      "stackview (>=0.3.2)"
    ],
    "summary": "GPU-accelerated image processing using CUDA",
    "support": "https://github.com/haesleinhuepf/napari-cupy-image-processing/issues",
    "twitter": "",
    "version": "0.3.1",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "email": "lefevreedg@gmail.com", "name": "Edgar Lefevre" }],
    "code_repository": "https://github.com/EdgarLefevre/napari-deepmeta",
    "conda": [{ "channel": "conda-forge", "package": "napari-deepmeta" }],
    "description": "# napari-deepmeta  [![License MIT](https://img.shields.io/pypi/l/napari-deepmeta.svg?color=green)](https://github.com/EdgarLefevre/napari-deepmeta/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-deepmeta.svg?color=green)](https://pypi.org/project/napari-deepmeta) [![Python Version](https://img.shields.io/pypi/pyversions/napari-deepmeta.svg?color=green)](https://python.org) [![tests](https://github.com/EdgarLefevre/napari-deepmeta/workflows/tests/badge.svg)](https://github.com/EdgarLefevre/napari-deepmeta/actions) [![codecov](https://codecov.io/gh/EdgarLefevre/napari-deepmeta/branch/main/graph/badge.svg)](https://codecov.io/gh/EdgarLefevre/napari-deepmeta) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-deepmeta)](https://napari-hub.org/plugins/napari-deepmeta)  Mice lungs and metastases segmentation tool. This tool is a demo tool for DeepMeta network.  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  ## Installation  You can install `napari-deepmeta` via [pip]:      pip install napari-deepmeta    To install latest development version :      pip install git+https://github.com/EdgarLefevre/napari-deepmeta.git   ## Usage  This plugin is designed to process your mouse MRI images with our dataset. It comes with a demo, including one of our test images.  By opening the deepmeta demo plugin, you will see an interface with one unique button, by clicking on it, it will load an image, run prediction and then draw the masks contours on each slice.  If you open the deepmeta plugin, you will see an interface with one button and 3 checkboxes. By checking the checkboxes, you add steps to the pipeline (enhance contrast, do postprocessing, segment metastases). Once everything is setup, just click on the button and wait (the waiting time depends on your computer performance.)  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [MIT] license, \"napari-deepmeta\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/EdgarLefevre/napari-deepmeta/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-deepmeta       Mice lungs and metastases segmentation tool. This tool is a demo tool for DeepMeta network.  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template. Installation You can install napari-deepmeta via pip: pip install napari-deepmeta  To install latest development version : pip install git+https://github.com/EdgarLefevre/napari-deepmeta.git  Usage This plugin is designed to process your mouse MRI images with our dataset. It comes with a demo, including one of our test images. By opening the deepmeta demo plugin, you will see an interface with one unique button, by clicking on it, it will load an image, run prediction and then draw the masks contours on each slice. If you open the deepmeta plugin, you will see an interface with one button and 3 checkboxes. By checking the checkboxes, you add steps to the pipeline (enhance contrast, do postprocessing, segment metastases). Once everything is setup, just click on the button and wait (the waiting time depends on your computer performance.) Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the MIT license, \"napari-deepmeta\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari DeepMeta",
    "documentation": "https://github.com/EdgarLefevre/napari-deepmeta#README.md",
    "first_released": "2021-06-02T14:41:03.386781Z",
    "license": "MIT",
    "name": "napari-deepmeta",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/EdgarLefevre/napari-deepmeta",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-07-07T08:55:23.291200Z",
    "report_issues": "https://github.com/EdgarLefevre/napari-deepmeta/issues",
    "requirements": [
      "connected-components-3d",
      "magicgui",
      "napari",
      "numpy",
      "opencv-python",
      "qtpy",
      "scikit-image",
      "torch",
      "connected-components-3d ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "opencv-python ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "tox ; extra == 'testing'"
    ],
    "summary": "Mice lungs and metastases segmentation tool.",
    "support": "https://github.com/EdgarLefevre/napari-deepmeta/issues",
    "twitter": "",
    "version": "2.1",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "David Bauer" }],
    "code_repository": "https://github.com/bauerdavid/napari-input-visualizer",
    "conda": [],
    "description": "# napari-input-visualizer  [![License BSD-3](https://img.shields.io/pypi/l/napari-input-visualizer.svg?color=green)](https://github.com/bauerdavid/napari-input-visualizer/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-input-visualizer.svg?color=green)](https://pypi.org/project/napari-input-visualizer) [![Python Version](https://img.shields.io/pypi/pyversions/napari-input-visualizer.svg?color=green)](https://python.org) [![tests](https://github.com/bauerdavid/napari-input-visualizer/workflows/tests/badge.svg)](https://github.com/bauerdavid/napari-input-visualizer/actions) [![codecov](https://codecov.io/gh/bauerdavid/napari-input-visualizer/branch/main/graph/badge.svg)](https://codecov.io/gh/bauerdavid/napari-input-visualizer) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-input-visualizer)](https://napari-hub.org/plugins/napari-input-visualizer)  Visualize keyboard and mouse button presses  A simple tool to visualize input events like keyboard presses or mouse clicking and scrolling. Use it to create tutorial videos or to demonstrate a bug you encountered!  ## Demo:   https://user-images.githubusercontent.com/36735863/194586424-1e6288d3-2c2f-412c-a1cb-91d139f787bd.mp4    ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/stable/plugins/index.html -->  ## Installation  You can install `napari-input-visualizer` via [pip]:      pip install napari-input-visualizer    To install latest development version :      pip install git+https://github.com/bauerdavid/napari-input-visualizer.git   ## Contributing  Contributions are very welcome.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-input-visualizer\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/bauerdavid/napari-input-visualizer/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-input-visualizer       Visualize keyboard and mouse button presses A simple tool to visualize input events like keyboard presses or mouse clicking and scrolling. Use it to create tutorial videos or to demonstrate a bug you encountered! Demo: https://user-images.githubusercontent.com/36735863/194586424-1e6288d3-2c2f-412c-a1cb-91d139f787bd.mp4  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install napari-input-visualizer via pip: pip install napari-input-visualizer  To install latest development version : pip install git+https://github.com/bauerdavid/napari-input-visualizer.git  Contributing Contributions are very welcome. License Distributed under the terms of the BSD-3 license, \"napari-input-visualizer\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "Input Visualizer",
    "documentation": "https://github.com/bauerdavid/napari-input-visualizer#README.md",
    "first_released": "2022-10-10T07:00:43.684882Z",
    "license": "BSD-3-Clause",
    "name": "napari-input-visualizer",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/bauerdavid/napari-input-visualizer",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-10-10T07:00:43.684882Z",
    "report_issues": "https://github.com/bauerdavid/napari-input-visualizer/issues",
    "requirements": [
      "numpy",
      "magicgui",
      "qtpy",
      "napari",
      "imageio-ffmpeg",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "summary": "Visualize keyboard and mouse button presses",
    "support": "https://github.com/bauerdavid/napari-input-visualizer/issues",
    "twitter": "",
    "version": "0.0.1",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Marc Boucsein" }, { "name": "Robin Koch" }],
    "code_repository": "https://github.com/MBPhys/napari-elementary-numpy-operations",
    "conda": [
      {
        "channel": "conda-forge",
        "package": "napari-elementary-numpy-operations"
      }
    ],
    "description": "# napari-elementary-numpy-operations  [![License](https://img.shields.io/pypi/l/napari-medical-image-formats.svg?color=green)](https://github.com/MBPhys/napari-elementary-numpy-operations/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-elementary-numpy-operations.svg?color=green)](https://pypi.org/project/napari-elementary-numpy-operations) [![Python Version](https://img.shields.io/pypi/pyversions/napari-elementary-numpy-operations.svg?color=green)](https://python.org)   A napari plugin covers elementary numpy operations like swap axes, flip, sqeeze an array or rotate an arrays by 90° steps.  ----------------------------------  ## Installation  You can install `napari-elementary-numpy-operations` via [pip]:      napari-elementary-numpy-operations  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-elementary-numpy-operations\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin [file an issue]: https://github.com/MBPhys/napari-elementary-numpy-operations/issues [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-elementary-numpy-operations    A napari plugin covers elementary numpy operations like swap axes, flip, sqeeze an array or rotate an arrays by 90° steps.  Installation You can install napari-elementary-numpy-operations via pip: napari-elementary-numpy-operations  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-elementary-numpy-operations\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "napari-elementary-numpy-operations",
    "documentation": "",
    "first_released": "2022-01-12T12:15:11.182134Z",
    "license": "BSD-3-Clause",
    "name": "napari-elementary-numpy-operations",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/MBPhys/napari-elementary-numpy-operations",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-01-21T16:42:21.676529Z",
    "report_issues": "",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "qtpy",
      "superqt"
    ],
    "summary": "A napari plugin covers elementary numpy operations like swap axes, flip, sqeeze an array or rotate an arrays by 90° steps",
    "support": "",
    "twitter": "",
    "version": "0.0.5",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      { "email": "jordao.bragantini@czbiohub.org", "name": "Jordao Bragantini" }
    ],
    "category": {
      "Image modality": ["Fluorescence microscopy"],
      "Supported data": ["2D", "3D", "Time series"],
      "Workflow step": [
        "Image reconstruction",
        "Image enhancement",
        "Image registration",
        "Image fusion",
        "Image correction",
        "Visualization"
      ]
    },
    "category_hierarchy": {
      "Image modality": [["Fluorescence microscopy", "Light-sheet microscopy"]],
      "Supported data": [["2D"], ["3D"], ["Time series"]],
      "Workflow step": [
        ["Image reconstruction", "Image denoising"],
        ["Image enhancement", "Image denoising"],
        ["Image registration"],
        ["Image fusion"],
        ["Image correction"],
        ["Image reconstruction", "Image deconvolution"],
        ["Visualization", "Image visualisation"]
      ]
    },
    "code_repository": "https://github.com/royerlab/napari-dexp",
    "conda": [{ "channel": "conda-forge", "package": "napari-dexp" }],
    "description": "# napari-DEXP  [![License](https://img.shields.io/pypi/l/napari-dexp.svg?color=green)](https://github.com/royerlab/napari-dexp/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-dexp.svg?color=green)](https://pypi.org/project/napari-dexp) [![Python Version](https://img.shields.io/pypi/pyversions/napari-dexp.svg?color=green)](https://python.org) [![tests](https://github.com/royerlab/napari-dexp/workflows/tests/badge.svg)](https://github.com/royerlab/napari-dexp/actions) [![codecov](https://codecov.io/gh/royerlab/napari-dexp/branch/master/graph/badge.svg)](https://codecov.io/gh/royerlab/napari-dexp)  A plugin to interface [DEXP](https://github.com/royerlab/dexp) with [napari](https://github.com/napari/napari).  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/docs/plugins/index.html -->  ## Installation  You can install `napari-dexp` via [pip]:      pip install napari-dexp  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-dexp\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin [file an issue]: https://github.com/royerlab/napari-dexp/issues [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-DEXP      A plugin to interface DEXP with napari.  This napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.  Installation You can install napari-dexp via pip: pip install napari-dexp  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-dexp\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "DEXP",
    "documentation": "",
    "first_released": "2021-07-01T18:14:41.391237Z",
    "license": "BSD-3-Clause",
    "name": "napari-dexp",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader", "writer"],
    "project_site": "https://github.com/royerlab/napari-dexp",
    "python_version": ">=3.7",
    "reader_file_extensions": ["*.zarr.zip", "*.zarr"],
    "release_date": "2022-06-23T19:50:58.841329Z",
    "report_issues": "",
    "requirements": [
      "napari",
      "napari-plugin-engine (>=0.1.4)",
      "dexp",
      "numpy"
    ],
    "summary": "A simple plugin to use with napari",
    "support": "",
    "twitter": "",
    "version": "0.0.7",
    "visibility": "public",
    "writer_file_extensions": [".zarr"],
    "writer_save_layers": ["labels", "image"]
  },
  {
    "authors": [{ "name": "Tom Burke" }, { "name": "Joran Deschamps" }],
    "code_repository": "https://github.com/juglab/napari_denoiseg",
    "conda": [],
    "description": "# napari-denoiseg  [![License](https://img.shields.io/pypi/l/napari-denoiseg.svg?color=green)](https://github.com/juglab/napari-denoiseg/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-denoiseg.svg?color=green)](https://pypi.org/project/napari-denoiseg) [![Python Version](https://img.shields.io/pypi/pyversions/napari-denoiseg.svg?color=green)](https://python.org) [![tests](https://github.com/juglab/napari-denoiseg/workflows/build/badge.svg)](https://github.com/juglab/napari-denoiseg/actions) [![codecov](https://codecov.io/gh/juglab/napari-denoiseg/branch/main/graph/badge.svg)](https://codecov.io/gh/juglab/napari-denoiseg) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-denoiseg)](https://napari-hub.org/plugins/napari-denoiseg)  A napari plugin performing joint denoising and segmentation of microscopy images using [DenoiSeg](https://github.com/juglab/DenoiSeg).  <img src=\"https://raw.githubusercontent.com/juglab/napari-denoiseg/master/docs/images/example.png\" width=\"800\" /> ----------------------------------  ## Installation  Check out the [documentation](https://juglab.github.io/napari-denoiseg/installation.html) for more detailed installation  instructions.    ## Quick demo  You can try out a demo by loading the `DenoiSeg Demo prediction` plugin and directly clicking on `Predict`.   <img src=\"https://raw.githubusercontent.com/juglab/napari-denoiseg/master/docs/images/prediction.gif\" width=\"800\" />   ## Documentation  Documentation is available on the [project website](https://juglab.github.io/napari-denoiseg/).   ## Contributing and feedback  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request. You can also  help us improve by [filing an issue] along with a detailed description or contact us through the [image.sc](https://forum.image.sc/) forum (tag @jdeschamps).   ## Cite us   Tim-Oliver Buchholz, Mangal Prakash, Alexander Krull and Florian Jug, \"[DenoiSeg: Joint Denoising and Segmentation](https://arxiv.org/abs/2005.02987)\" _arxiv_ (2020)   ## Acknowledgements  This plugin was developed thanks to the support of the Silicon Valley Community Foundation (SCVF) and the  Chan-Zuckerberg Initiative (CZI) with the napari Plugin Accelerator grant _2021-239867_.   Distributed under the terms of the [BSD-3] license, \"napari-denoiseg\" is a free and open source software.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [filing an issue]: https://github.com/juglab/napari-denoiseg/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-denoiseg       A napari plugin performing joint denoising and segmentation of microscopy images using DenoiSeg.  Installation Check out the documentation for more detailed installation  instructions.  Quick demo You can try out a demo by loading the DenoiSeg Demo prediction plugin and directly clicking on Predict.  Documentation Documentation is available on the project website. Contributing and feedback Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. You can also  help us improve by filing an issue along with a detailed description or contact us through the image.sc forum (tag @jdeschamps). Cite us Tim-Oliver Buchholz, Mangal Prakash, Alexander Krull and Florian Jug, \"DenoiSeg: Joint Denoising and Segmentation\" arxiv (2020) Acknowledgements This plugin was developed thanks to the support of the Silicon Valley Community Foundation (SCVF) and the  Chan-Zuckerberg Initiative (CZI) with the napari Plugin Accelerator grant 2021-239867. Distributed under the terms of the BSD-3 license, \"napari-denoiseg\" is a free and open source software.",
    "development_status": ["Development Status :: 3 - Alpha"],
    "display_name": "DenoiSeg",
    "documentation": "https://juglab.github.io/napari-denoiseg/",
    "first_released": "2022-10-31T21:39:50.344176Z",
    "license": "BSD-3-Clause",
    "name": "napari-denoiseg",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget", "sample_data"],
    "project_site": "https://github.com/juglab/napari_denoiseg",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-11-01T16:23:33.870728Z",
    "report_issues": "https://github.com/juglab/napari_denoiseg/issues",
    "requirements": [
      "numpy",
      "pyqtgraph",
      "denoiseg (>=0.3.0)",
      "bioimageio.core",
      "magicgui",
      "qtpy",
      "napari-time-slicer (>=0.4.9)",
      "napari (<=0.4.15)",
      "vispy (<=0.9.6)",
      "imageio (!=2.11.0,!=2.22.1,>=2.5.0)",
      "tensorflow ; platform_system != \"Darwin\" or platform_machine != \"arm64\"",
      "tensorflow-macos ; platform_system == \"Darwin\" and platform_machine == \"arm64\"",
      "tensorflow-metal ; platform_system == \"Darwin\" and platform_machine == \"arm64\"",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "summary": "A napari plugin performing joint denoising and segmentation of microscopy images using DenoiSeg.",
    "support": "https://github.com/juglab/napari_denoiseg/issues",
    "twitter": "",
    "version": "0.0.1rc2",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      { "name": "Robert Haase" },
      { "name": "Tim Morello" },
      { "name": "Marcelo Leomil Zoccoler" }
    ],
    "code_repository": "https://github.com/biapol/napari-crop",
    "conda": [{ "channel": "conda-forge", "package": "napari-crop" }],
    "description": "# napari-crop  [![License](https://img.shields.io/pypi/l/napari-crop.svg?color=green)](https://github.com/haesleinhuepf/napari-crop/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-crop.svg?color=green)](https://pypi.org/project/napari-crop) [![Python Version](https://img.shields.io/pypi/pyversions/napari-crop.svg?color=green)](https://python.org) [![tests](https://github.com/haesleinhuepf/napari-crop/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-crop/actions) [![codecov](https://codecov.io/gh/haesleinhuepf/napari-crop/branch/master/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-crop)  Crop regions in napari manually  ![](https://github.com/haesleinhuepf/napari-crop/raw/main/images/screencast.gif)  Crop in any dimension  ![](https://github.com/haesleinhuepf/napari-crop/blob/main/images/side_crop.gif)  ## Usage Create a new shapes layer to annotate the region you would like to crop:  ![](https://github.com/haesleinhuepf/napari-crop/raw/main/images/shapes.png)  Use the rectangle tool to annotate a region. Start the `crop` tool from the `Tools > Utilities > Crop region` menu.  Click the `Run` button to crop the region.  ![](https://github.com/haesleinhuepf/napari-crop/raw/main/images/draw_rectangle.png)  You can also use the `Select shapes` tool to move the rectangle to a new place and crop another region by clicking on `Run`.  ![](https://github.com/haesleinhuepf/napari-crop/raw/main/images/move_rectangle.png)  Hint: You can also use the [napari-tabu](https://www.napari-hub.org/plugins/napari-tabu) plugin to send all your cropped images to a new napari window.  ![](https://github.com/haesleinhuepf/napari-crop/raw/main/images/new_window.gif)  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.  ## Installation  You can install `napari-crop` via [pip]:      pip install napari-crop  ## Contributing  Contributions are very welcome.   ## License  Distributed under the terms of the [BSD-3] license, \"napari-crop\" is free and open source software  ## Issues  If you encounter any problems, please create a thread on [image.sc] along with a detailed description and tag [@haesleinhuepf].  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin [file an issue]: https://github.com/haesleinhuepf/napari-crop/issues [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ [image.sc]: https://image.sc [@haesleinhuepf]: https://twitter.com/haesleinhuepf    ",
    "description_content_type": "text/markdown",
    "description_text": "napari-crop      Crop regions in napari manually  Crop in any dimension  Usage Create a new shapes layer to annotate the region you would like to crop:  Use the rectangle tool to annotate a region. Start the crop tool from the Tools > Utilities > Crop region menu.  Click the Run button to crop the region.  You can also use the Select shapes tool to move the rectangle to a new place and crop another region by clicking on Run.  Hint: You can also use the napari-tabu plugin to send all your cropped images to a new napari window.   This napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template. Installation You can install napari-crop via pip: pip install napari-crop  Contributing Contributions are very welcome.  License Distributed under the terms of the BSD-3 license, \"napari-crop\" is free and open source software Issues If you encounter any problems, please create a thread on image.sc along with a detailed description and tag @haesleinhuepf.",
    "development_status": ["Development Status :: 3 - Alpha"],
    "display_name": "napari-crop",
    "documentation": "https://github.com/biapol/napari-crop#README.md",
    "first_released": "2021-10-21T18:34:09.956294Z",
    "license": "BSD-3-Clause",
    "name": "napari-crop",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/biapol/napari-crop",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-01-05T18:25:40.159587Z",
    "report_issues": "https://github.com/biapol/napari-crop/issues",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "scikit-image",
      "napari-tools-menu"
    ],
    "summary": "Crop regions in napari manually",
    "support": "https://github.com/biapol/napari-crop/issues",
    "twitter": "",
    "version": "0.1.5",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      { "email": "matt.mccormick@kitware.com", "name": "Matt McCormick" }
    ],
    "code_repository": "https://github.com/InsightSoftwareConsortium/napari-itk-io",
    "description": "# napari-itk-io  [![License](https://img.shields.io/pypi/l/napari-itk-io.svg?color=green)](https://github.com/InsightSoftwareConsortium/napari-itk-io/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-itk-io.svg?color=green)](https://pypi.org/project/napari-itk-io) [![Python Version](https://img.shields.io/pypi/pyversions/napari-itk-io.svg?color=green)](https://python.org) [![tests](https://github.com/InsightSoftwareConsortium/napari-itk-io/workflows/tests/badge.svg)](https://github.com/InsightSoftwareConsortium/napari-itk-io/actions) [![codecov](https://codecov.io/gh/InsightSoftwareConsortium/napari-itk-io/branch/master/graph/badge.svg)](https://codecov.io/gh/InsightSoftwareConsortium/napari-itk-io)  File IO with [itk](https://itk.org) for [napari](https://napari.org).  Image metadata, e.g. the pixel spacing, origin, and metadata tags, are preserved and passed into napari.  Supported image file formats:  - [BioRad](http://www.bio-rad.com/) - [BMP](https://en.wikipedia.org/wiki/BMP_file_format) - [DICOM](http://dicom.nema.org/) - [DICOM Series](http://dicom.nema.org/) - [ITK HDF5](https://support.hdfgroup.org/HDF5/) - [JPEG](https://en.wikipedia.org/wiki/JPEG_File_Interchange_Format) - [GE4,GE5,GEAdw](http://www3.gehealthcare.com) - [Gipl (Guys Image Processing Lab)](https://www.ncbi.nlm.nih.gov/pubmed/12956259) - [LSM](http://www.openwetware.org/wiki/Dissecting_LSM_files) - [MetaImage](https://itk.org/Wiki/ITK/MetaIO/Documentation) - [MINC 2.0](https://en.wikibooks.org/wiki/MINC/SoftwareDevelopment/MINC2.0_File_Format_Reference) - [MGH](https://surfer.nmr.mgh.harvard.edu/fswiki/FsTutorial/MghFormat) - [MRC](http://www.ccpem.ac.uk/mrc_format/mrc_format.php) - [NifTi](https://nifti.nimh.nih.gov/nifti-1) - [NRRD](http://teem.sourceforge.net/nrrd/format.html) - [Portable Network Graphics (PNG)](https://en.wikipedia.org/wiki/Portable_Network_Graphics) - [Tagged Image File Format (TIFF)](https://en.wikipedia.org/wiki/TIFF) - [VTK legacy file format for images](http://www.vtk.org/VTK/img/file-formats.pdf)  For DICOM Series, select the folder containing the series with *File -> Open Folder...*. The first series will be selected and sorted spatially.  ## Installation  You can install `napari-itk-io` via [pip]:      pip install napari-itk-io  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  Follow the [itk contributing guidelines](https://github.com/InsightSoftwareConsortium/ITK/blob/master/CONTRIBUTING.md) and the [itk code of conduct](https://github.com/InsightSoftwareConsortium/ITK/blob/master/CODE_OF_CONDUCT.md).  ## License  Distributed under the terms of the [Apache Software License 2.0] license, \"napari-itk-io\" is free and open source software.  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin [file an issue]: https://github.com/InsightSoftwareConsortium/napari-itk-io/issues [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/  ",
    "description_content_type": "text/markdown",
    "description_text": "napari-itk-io      File IO with itk for napari. Image metadata, e.g. the pixel spacing, origin, and metadata tags, are preserved and passed into napari. Supported image file formats:  BioRad BMP DICOM DICOM Series ITK HDF5 JPEG GE4,GE5,GEAdw Gipl (Guys Image Processing Lab) LSM MetaImage MINC 2.0 MGH MRC NifTi NRRD Portable Network Graphics (PNG) Tagged Image File Format (TIFF) VTK legacy file format for images  For DICOM Series, select the folder containing the series with File -> Open Folder.... The first series will be selected and sorted spatially. Installation You can install napari-itk-io via pip: pip install napari-itk-io  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. Follow the itk contributing guidelines and the itk code of conduct. License Distributed under the terms of the Apache Software License 2.0 license, \"napari-itk-io\" is free and open source software. Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "napari-itk-io",
    "documentation": "",
    "first_released": "2021-04-28T22:49:15.780944Z",
    "license": "Apache-2.0",
    "name": "napari-itk-io",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader", "writer"],
    "project_site": "https://github.com/InsightSoftwareConsortium/napari-itk-io",
    "python_version": ">=3.8",
    "reader_file_extensions": ["*"],
    "release_date": "2022-03-28T13:57:53.681568Z",
    "report_issues": "",
    "requirements": [
      "numpy",
      "napari-plugin-engine (>=0.2.0)",
      "itk-io (>=5.2.0)",
      "itk-napari-conversion"
    ],
    "summary": "File IO with itk for napari",
    "support": "",
    "twitter": "",
    "version": "0.2.0",
    "visibility": "public",
    "writer_file_extensions": [".nii"],
    "writer_save_layers": ["image", "labels*", "labels", "image*"]
  },
  {
    "authors": [
      { "email": "jacopo.abramo@gmail.com", "name": "Jacopo Abramo" }
    ],
    "code_repository": "https://github.com/jacopoabramo/napari-live-recording",
    "description": "# napari-live-recording  [![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://github.com/jacopoabramo/napari-live-recording/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-live-recording.svg?color=green)](https://pypi.org/project/napari-live-recording) [![Python Version](https://img.shields.io/pypi/pyversions/napari-live-recording.svg?color=green)](https://python.org) [![tests](https://github.com/jethro33/napari-live-recording/workflows/tests/badge.svg)](https://github.com/jacopoabramo/napari-live-recording/actions) [![codecov](https://codecov.io/gh/jethro33/napari-live-recording/branch/master/graph/badge.svg)](https://codecov.io/gh/jacopoabramo/napari-live-recording)  A napari plugin for live video recording with a generic camera device and generate a stack of TIFF images from said device.  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/docs/plugins/index.html -->  ## Installation  You can install `napari-live-recording` via [pip]:      pip install napari-live-recording  ## Documentation  You can review the documentation of this plugin [here](./docs/README.md)  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [MIT] license, \"napari-live-recording\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/jacopoabramo/napari-live-recording/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-live-recording      A napari plugin for live video recording with a generic camera device and generate a stack of TIFF images from said device.  This napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.  Installation You can install napari-live-recording via pip: pip install napari-live-recording  Documentation You can review the documentation of this plugin here Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the MIT license, \"napari-live-recording\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari-live-recording",
    "documentation": "https://github.com/jacopoabramo/napari-live-recording#README.md",
    "first_released": "2021-10-05T19:20:55.265189Z",
    "license": "MIT",
    "name": "napari-live-recording",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/jethro33/napari-live-recording",
    "python_version": ">=3.6",
    "reader_file_extensions": [],
    "release_date": "2022-05-02T11:24:37.421410Z",
    "report_issues": "https://github.com/jacopoabramo/napari-live-recording/issues",
    "requirements": [
      "superqt",
      "numpy",
      "opencv-python",
      "opencv-contrib-python",
      "dask-image",
      "napari",
      "qtpy"
    ],
    "summary": "A napari plugin for live video recording with a generic camera device.",
    "support": "https://github.com/jacopoabramo/napari-live-recording/issues",
    "twitter": "",
    "version": "0.2.1",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "email": "burke@mpi-cbg.de", "name": "Tom Burke" }],
    "code_repository": "https://github.com/Labelings/napari-labeling",
    "description": "# napari-labeling  [![License](https://img.shields.io/pypi/l/napari-labeling.svg?color=green)](https://github.com/tomburke-rse/napari-labeling/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-labeling.svg?color=green)](https://pypi.org/project/napari-labeling) [![Python Version](https://img.shields.io/pypi/pyversions/napari-labeling.svg?color=green)](https://python.org) [![tests](https://github.com/tomburke-rse/napari-labeling/workflows/tests/badge.svg)](https://github.com/tomburke-rse/napari-labeling/actions) [![codecov](https://codecov.io/gh/tomburke-rse/napari-labeling/branch/main/graph/badge.svg)](https://codecov.io/gh/tomburke-rse/napari-labeling) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-labeling)](https://napari-hub.org/plugins/napari-labeling)  This is a napari-plugin based on the [labeling project].  It allows the generation of overlapping labels in one layer, save and load of this layer in a json-based file format and it contains a widget to explore the overlapping labels layer and select specific segments with a mouse click .  Please note that currently, the widget part only works by adding it through code with:      from napari_labeling import edit_widget     viewer = napari.Viewer()     viewer.window.add_dock_widget(edit_widget)  An example on how to achieve this can be found in the [main.py] on GitHub.  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/plugins/stable/index.html -->  ## Installation  You can install `napari-labeling` via [pip]:      pip install napari-labeling     ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-labeling\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/  [labeling project]: https://github.com/Labelings/Labeling [main.py]: https://github.com/Labelings/Labeling/blob/main/main.py [file an issue]: https://github.com/Labelings/napari-labeling/issues   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-labeling       This is a napari-plugin based on the labeling project. It allows the generation of overlapping labels in one layer, save and load of this layer in a json-based file format and it contains a widget to explore the overlapping labels layer and select specific segments with a mouse click . Please note that currently, the widget part only works by adding it through code with: from napari_labeling import edit_widget viewer = napari.Viewer() viewer.window.add_dock_widget(edit_widget)  An example on how to achieve this can be found in the main.py on GitHub.  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install napari-labeling via pip: pip install napari-labeling  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-labeling\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "napari Labeling",
    "documentation": "",
    "first_released": "2022-05-09T14:46:55.976557Z",
    "license": "BSD-3-Clause",
    "name": "napari-labeling",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader", "writer", "widget"],
    "project_site": "https://github.com/Labelings/napari-labeling",
    "python_version": ">=3.7",
    "reader_file_extensions": ["*.lbl.json"],
    "release_date": "2022-05-10T09:34:17.445511Z",
    "report_issues": "",
    "requirements": ["numpy", "labeling"],
    "summary": "A napari plugin for handling overlapping labeling data",
    "support": "",
    "twitter": "",
    "version": "0.1.2",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": ["labels"]
  },
  {
    "authors": [{ "name": "Federico Gasparoli" }, { "name": "Talley Lambert" }],
    "code_repository": "https://github.com/tlambert03/napari-micromanager",
    "conda": [{ "channel": "conda-forge", "package": "napari-micromanager" }],
    "description": "# napari-micromanager  [![License](https://img.shields.io/pypi/l/napari-micromanager.svg?color=green)](https://github.com/napari/napari-micromanager/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-micromanager.svg?color=green)](https://pypi.org/project/napari-micromanager) [![Python Version](https://img.shields.io/pypi/pyversions/napari-micromanager.svg?color=green)](https://python.org) [![Tests](https://github.com/tlambert03/napari-micromanager/actions/workflows/test.yml/badge.svg)](https://github.com/tlambert03/napari-micromanager/actions/workflows/test.yml) [![codecov](https://codecov.io/gh/tlambert03/napari-micromanager/branch/main/graph/badge.svg?token=tf6lYDWV1s)](https://codecov.io/gh/tlambert03/napari-micromanager)  GUI interface between napari and micromanager powered by [pymmcore-plus](https://pymmcore-plus.readthedocs.io/).  🚧 Experimental!  Work in progress!  Here be 🐲 🚧  ---------------------------------- <img width=\"1797\" alt=\"mm\" src=\"https://user-images.githubusercontent.com/1609449/138457506-787b7bec-7f30-4d92-b5cf-6e218c87225a.png\">   This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/docs/plugins/index.html -->  ## Installation  You can install `napari-micromanager` via [pip]:      pip install napari-micromanager  ### Getting micromanager adapters:  The easiest way to get the micromanager adapters is to use:  ``` python -m pymmcore_plus.install ```  this will install micromanager to the pymmcore_plus folder in your site-package; use this to see where:  ``` python -c \"from pymmcore_plus import find_micromanager; print(find_micromanager())\" ```  alternatively, you can direct pymmcore_plus to your own micromanager installation with the `MICROMANAGER_PATH` environment variable:  ``` export MICROMANAGER_PATH='/path/to/Micro-Manager-...' ```  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ### Launching napari with plugin You can launch napari and automatically load this plugin using the `launch-dev.py` script:  ```bash python launch-dev.py ```  Alternatively you can run:  ```bash napari -w napari-micromanager ```  ## License  Distributed under the terms of the [BSD-3] license, \"napari-micromanager\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin [file an issue]: https://github.com/tlambert03/napari-micromanager/issues [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-micromanager      GUI interface between napari and micromanager powered by pymmcore-plus. 🚧 Experimental!  Work in progress!  Here be 🐲 🚧   This napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.  Installation You can install napari-micromanager via pip: pip install napari-micromanager  Getting micromanager adapters: The easiest way to get the micromanager adapters is to use: python -m pymmcore_plus.install this will install micromanager to the pymmcore_plus folder in your site-package; use this to see where: python -c \"from pymmcore_plus import find_micromanager; print(find_micromanager())\" alternatively, you can direct pymmcore_plus to your own micromanager installation with the MICROMANAGER_PATH environment variable: export MICROMANAGER_PATH='/path/to/Micro-Manager-...' Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. Launching napari with plugin You can launch napari and automatically load this plugin using the launch-dev.py script: bash python launch-dev.py Alternatively you can run: bash napari -w napari-micromanager License Distributed under the terms of the BSD-3 license, \"napari-micromanager\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 3 - Alpha"],
    "display_name": "napari-micromanager",
    "documentation": "",
    "first_released": "2021-08-15T00:48:42.041377Z",
    "license": "BSD-3-Clause",
    "name": "napari-micromanager",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/tlambert03/napari-micromanager",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-05-19T23:49:28.692371Z",
    "report_issues": "",
    "requirements": [
      "napari (>=0.4.13)",
      "pymmcore-plus (>=0.4.0)",
      "useq-schema (>=0.1.0)",
      "superqt (>=0.3.1)",
      "fonticon-materialdesignicons6",
      "tifffile",
      "PyQt5 ; extra == 'pyqt5'",
      "PySide2 ; extra == 'pyside2'",
      "pytest ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'"
    ],
    "summary": "GUI interface between napari and micromanager",
    "support": "",
    "twitter": "",
    "version": "0.0.1rc7",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "email": "alan.watson@pitt.edu", "name": "Alan M Watson" }],
    "code_repository": "https://github.com/AlanMWatson/napari-imaris-loader",
    "description": "  # Description  This plugin enables viewing of Bitplane Imaris files, including very large datasets.  The GIFs below demonstrate rendering of a ~2TB IMS file containing a 2 color whole mouse brain.  The plugin has been tested on datasets as large as 20TB.  **NOTE: For this plugin to work \"File/Preferences/Experimental/Render Images Asynchronously\" must be selected.**  ### Open IMS file:  ![Open IMS file GIF](https://i.imgur.com/ByHb0wI.gif \"Open IMS file\")    ### Render in 3D:  A plugin is provided to dynamically reload the data after selecting the lowest resolution level to be included in the viewer.  Since napari only renders the lowest resolution, the user can use this plugin to control the quality of 3D rendering.  See features and limitations for tips on suggested usage.  ![3D Rendering and Quality Adjustment GIF](https://i.imgur.com/MZNlWtM.gif \"3D Rendering and Quality Adjustment\")  ### Features  * Multiscale Rendering   * Image pyramids which are present in the native IMS format are automatically added to napari during file loading. * Chunks are implemented by dask and matched to the chunk sizes stored in each dataset.  (Napari appears to only ask for 2D chunks - unclear how helpful this feature is currently) * Successfully handles multi-terabyte multi-timepoint multi-channel datasets. * Tested with all sample files provided by Bitplane. * Higher 3D rendering quality is enabled by a widget that reloads data after specifying the lowest resolution level (higher number = lower resolution) to be included in the multiscale series.  ### Known Issues / limitations  * Currently, this is **only an image loader**, and there are no features for loading or viewing objects * Napari sometimes throws errors indicating that it expected a 3D or 5D array but receives the other.   * This sometimes *but relatively rarely* causes napari to crash   * Would like to enable Asynchronous Tiling of Images, but this results in more instability and causes crashes.   ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/docs/plugins/index.html -->  ## Installation  You can install `napari-imaris-loader` via [pip]:      pip install napari-imaris-loader  ## Change Log:  ##### <u>v0.1.2:</u>  **Fixed:** Issue where .ims files containing a single color 2D image would not open.  **Fixed:** Issue where using the widget to change resolutions while in 3D rendering would cause a crash.  Now the viewer is automatically forced into 2D rendering mode when the widget is used.  **Dependency change:** The loader is now dependent in a separate package for loading IMS files.  https://pypi.org/project/imaris-ims-file-reader/  **v0.1.3:**  Documentation  **v0.1.4:**  Add napari to install requirements for plugin compatibility  **v0.1.5:**  Changes to napari:  - now requires napari[all] upon install. - requires >=v0.1.5 of imaris-ims-file-reader  **v0.1.6:**  - Fix issue #7 where contrastLimits possibly unbound in reader  **v0.1.7:**  - For squeeze_output=False when opening .ims file for Napari compatibility  **v0.1.8:**  - Add automatic determination of contrast_limits - Fix bug in squeeze_output  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-imaris-loader\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/AlanMWatson/napari-imaris-loader/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "Description This plugin enables viewing of Bitplane Imaris files, including very large datasets.  The GIFs below demonstrate rendering of a ~2TB IMS file containing a 2 color whole mouse brain.  The plugin has been tested on datasets as large as 20TB. NOTE: For this plugin to work \"File/Preferences/Experimental/Render Images Asynchronously\" must be selected. Open IMS file:  Render in 3D: A plugin is provided to dynamically reload the data after selecting the lowest resolution level to be included in the viewer.  Since napari only renders the lowest resolution, the user can use this plugin to control the quality of 3D rendering.  See features and limitations for tips on suggested usage.  Features  Multiscale Rendering Image pyramids which are present in the native IMS format are automatically added to napari during file loading. Chunks are implemented by dask and matched to the chunk sizes stored in each dataset.  (Napari appears to only ask for 2D chunks - unclear how helpful this feature is currently) Successfully handles multi-terabyte multi-timepoint multi-channel datasets. Tested with all sample files provided by Bitplane. Higher 3D rendering quality is enabled by a widget that reloads data after specifying the lowest resolution level (higher number = lower resolution) to be included in the multiscale series.  Known Issues / limitations  Currently, this is only an image loader, and there are no features for loading or viewing objects Napari sometimes throws errors indicating that it expected a 3D or 5D array but receives the other. This sometimes but relatively rarely causes napari to crash Would like to enable Asynchronous Tiling of Images, but this results in more instability and causes crashes.   This napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.  Installation You can install napari-imaris-loader via pip: pip install napari-imaris-loader  Change Log: v0.1.2: Fixed: Issue where .ims files containing a single color 2D image would not open. Fixed: Issue where using the widget to change resolutions while in 3D rendering would cause a crash.  Now the viewer is automatically forced into 2D rendering mode when the widget is used. Dependency change: The loader is now dependent in a separate package for loading IMS files.  https://pypi.org/project/imaris-ims-file-reader/ v0.1.3: Documentation v0.1.4: Add napari to install requirements for plugin compatibility v0.1.5: Changes to napari:  now requires napari[all] upon install. requires >=v0.1.5 of imaris-ims-file-reader  v0.1.6:  Fix issue #7 where contrastLimits possibly unbound in reader  v0.1.7:  For squeeze_output=False when opening .ims file for Napari compatibility  v0.1.8:  Add automatic determination of contrast_limits Fix bug in squeeze_output  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-imaris-loader\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari-imaris-loader",
    "documentation": "https://github.com/AlanMWatson/napari-imaris-loader#README.md",
    "first_released": "2021-10-21T20:10:04.792830Z",
    "license": "BSD-3-Clause",
    "name": "napari-imaris-loader",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader", "widget"],
    "project_site": "https://github.com/AlanMWatson/napari-imaris-loader",
    "python_version": ">=3.8",
    "reader_file_extensions": ["*"],
    "release_date": "2022-04-13T18:03:21.139205Z",
    "report_issues": "https://github.com/AlanMWatson/napari-imaris-loader/issues",
    "requirements": [
      "napari[all]",
      "napari-plugin-engine (>=0.1.4)",
      "imaris-ims-file-reader (>=0.1.5)",
      "numpy",
      "h5py",
      "dask"
    ],
    "summary": "Napari plugin for loading Bitplane imaris files '.ims'",
    "support": "https://github.com/AlanMWatson/napari-imaris-loader/issues",
    "twitter": "",
    "version": "0.1.8",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "email": "brisvag@gmail.com", "name": "Lorenzo Gaifas" }],
    "code_repository": "https://github.com/brisvag/napari-em-reader",
    "description": "# napari-em-reader  [![License](https://img.shields.io/pypi/l/napari-em-reader.svg?color=green)](https://github.com/brisvag/napari-em-reader/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-em-reader.svg?color=green)](https://pypi.org/project/napari-em-reader) [![Python Version](https://img.shields.io/pypi/pyversions/napari-em-reader.svg?color=green)](https://python.org) [![tests](https://github.com/brisvag/napari-em-reader/workflows/tests/badge.svg)](https://github.com/brisvag/napari-em-reader/actions) [![codecov](https://codecov.io/gh/brisvag/napari-em-reader/branch/master/graph/badge.svg)](https://codecov.io/gh/brisvag/napari-em-reader)  A napari plugin to read .em files  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/docs/plugins/index.html -->  ## Installation  You can install `napari-em-reader` via [pip]:      pip install napari-em-reader  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-em-reader\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin [file an issue]: https://github.com/brisvag/napari-em-reader/issues [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-em-reader      A napari plugin to read .em files  This napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.  Installation You can install napari-em-reader via pip: pip install napari-em-reader  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-em-reader\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "napari-em-reader",
    "documentation": "",
    "first_released": "2021-02-23T14:51:15.118105Z",
    "license": "BSD-3-Clause",
    "name": "napari-em-reader",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader"],
    "project_site": "https://github.com/brisvag/napari-em-reader",
    "python_version": ">=3.6",
    "reader_file_extensions": ["*"],
    "release_date": "2021-02-23T14:51:15.118105Z",
    "report_issues": "",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "emfile (>=0.2)"
    ],
    "summary": "A napari plugin to read .em files",
    "support": "",
    "twitter": "",
    "version": "0.1.0",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Robert Haase" }],
    "code_repository": "https://github.com/haesleinhuepf/napari-layer-details-display",
    "conda": [
      { "channel": "conda-forge", "package": "napari-layer-details-display" }
    ],
    "description": "# napari-layer-details-display  [![License](https://img.shields.io/pypi/l/napari-layer-details-display.svg?color=green)](https://github.com/haesleinhuepf/napari-layer-details-display/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-layer-details-display.svg?color=green)](https://pypi.org/project/napari-layer-details-display) [![Python Version](https://img.shields.io/pypi/pyversions/napari-layer-details-display.svg?color=green)](https://python.org) [![tests](https://github.com/haesleinhuepf/napari-layer-details-display/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-layer-details-display/actions) [![codecov](https://codecov.io/gh/haesleinhuepf/napari-layer-details-display/branch/main/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-layer-details-display) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-layer-details-display)](https://napari-hub.org/plugins/napari-layer-details-display)  A display for layer information and properties  ![img.png](https://github.com/haesleinhuepf/napari-layer-details-display/raw/main/images/screenshot.png)  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  ## Installation  You can install `napari-layer-details-display` via [pip]:      pip install napari-layer-details-display    To install latest development version :      pip install git+https://github.com/haesleinhuepf/napari-layer-details-display.git   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-layer-details-display\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/haesleinhuepf/napari-layer-details-display/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-layer-details-display       A display for layer information and properties   This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template. Installation You can install napari-layer-details-display via pip: pip install napari-layer-details-display  To install latest development version : pip install git+https://github.com/haesleinhuepf/napari-layer-details-display.git  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-layer-details-display\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari-layer-details-display",
    "documentation": "https://github.com/haesleinhuepf/napari-layer-details-display#README.md",
    "first_released": "2021-11-13T09:03:13.359953Z",
    "license": "BSD-3-Clause",
    "name": "napari-layer-details-display",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/haesleinhuepf/napari-layer-details-display",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-07-10T14:03:10.876447Z",
    "report_issues": "https://github.com/haesleinhuepf/napari-layer-details-display/issues",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "napari-tools-menu"
    ],
    "summary": "A display for layer information and properties",
    "support": "https://github.com/haesleinhuepf/napari-layer-details-display/issues",
    "twitter": "",
    "version": "0.1.4",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      { "email": "jonas.windhager@uzh.ch", "name": "Jonas Windhager" }
    ],
    "category": { "Supported data": ["Multi-channel"] },
    "category_hierarchy": { "Supported data": [["Multi-channel"]] },
    "code_repository": "https://github.com/BodenmillerGroup/napari-imc",
    "description": "**Supported file formats**:  - Fluidigm MCD  - Fluidigm TXT  **Supported image types**:   - Panoramas (single-channel, color)   - Acquisitions (multi-channel, grayscale)  All images are loaded co-registered within the machine's coordinate system. ",
    "description_content_type": "text/markdown",
    "description_text": "Supported file formats:  - Fluidigm MCD  - Fluidigm TXT Supported image types:   - Panoramas (single-channel, color)   - Acquisitions (multi-channel, grayscale) All images are loaded co-registered within the machine's coordinate system.",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "napari-imc",
    "documentation": "https://github.com/BodenmillerGroup/napari-imc#README.md",
    "first_released": "2021-01-19T00:32:41.040698Z",
    "license": "MIT",
    "name": "napari-imc",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader", "widget"],
    "project_site": "https://github.com/BodenmillerGroup/napari-imc",
    "python_version": ">=3.8",
    "reader_file_extensions": ["*.mcd", "*.txt"],
    "release_date": "2022-06-01T12:53:34.138474Z",
    "report_issues": "https://github.com/BodenmillerGroup/napari-imc/issues",
    "requirements": ["numpy", "qtpy", "readimc", "superqt"],
    "summary": "Imaging Mass Cytometry (IMC) file type support for napari",
    "support": "https://github.com/BodenmillerGroup/napari-imc/issues",
    "twitter": "",
    "version": "0.6.5",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Kyle Harrington" }],
    "code_repository": "https://github.com/kephale/napari-stable-diffusion",
    "conda": [],
    "description": "# napari-stable-diffusion  [![License BSD-3](https://img.shields.io/pypi/l/napari-stable-diffusion.svg?color=green)](https://github.com/kephale/napari-stable-diffusion/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-stable-diffusion.svg?color=green)](https://pypi.org/project/napari-stable-diffusion) [![Python Version](https://img.shields.io/pypi/pyversions/napari-stable-diffusion.svg?color=green)](https://python.org) [![tests](https://github.com/kephale/napari-stable-diffusion/workflows/tests/badge.svg)](https://github.com/kephale/napari-stable-diffusion/actions) [![codecov](https://codecov.io/gh/kephale/napari-stable-diffusion/branch/main/graph/badge.svg)](https://codecov.io/gh/kephale/napari-stable-diffusion) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-stable-diffusion)](https://napari-hub.org/plugins/napari-stable-diffusion)  A demo of stable diffusion in napari.  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  ![demo image of napari-stable-diffusion of the prompt \"a unicorn and a dinosaur eating cookies and drinking tea\"](./napari_stable_diffusion_demo.png)  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/stable/plugins/index.html -->  ## Installation  You can install `napari-stable-diffusion` via [pip]:      pip install napari-stable-diffusion  To install latest development version :      pip install git+https://github.com/kephale/napari-stable-diffusion.git  You will also need to sign up with HuggingFace and [generate an access token](https://huggingface.co/docs/hub/security-tokens) to get access to the Stable Diffusion model we use.  When you have generated your access token you can either permanently set the `HF_TOKEN_SD` environment variable in your `.bashrc` or whichever file your OS uses, or you can include it on the command line  ``` HF_TOKEN_SD=\"hf_aaaAaaaasdadsadsaoaoaoasoidijo\" napari ```  For more information on the Stable Diffusion model itself, please see https://huggingface.co/CompVis/stable-diffusion-v1-4.  ### Apple M1 specific instructions  To utilize the M1 GPU, the nightly version of PyTorch needs to be installed. Consider using `conda` or `mamba` like this:  ``` mamba create -c pytorch-nightly -n napari-stable-diffusion python=3.9 pip pyqt pytorch torchvision pip install git+https://github.com/kephale/napari-stable-diffusion.git ```  ## Next steps  - Image 2 Image support - Inpainting support  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-stable-diffusion\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/kephale/napari-stable-diffusion/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-stable-diffusion       A demo of stable diffusion in napari.  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.   Installation You can install napari-stable-diffusion via pip: pip install napari-stable-diffusion  To install latest development version : pip install git+https://github.com/kephale/napari-stable-diffusion.git  You will also need to sign up with HuggingFace and generate an access token to get access to the Stable Diffusion model we use. When you have generated your access token you can either permanently set the HF_TOKEN_SD environment variable in your .bashrc or whichever file your OS uses, or you can include it on the command line HF_TOKEN_SD=\"hf_aaaAaaaasdadsadsaoaoaoasoidijo\" napari For more information on the Stable Diffusion model itself, please see https://huggingface.co/CompVis/stable-diffusion-v1-4. Apple M1 specific instructions To utilize the M1 GPU, the nightly version of PyTorch needs to be installed. Consider using conda or mamba like this: mamba create -c pytorch-nightly -n napari-stable-diffusion python=3.9 pip pyqt pytorch torchvision pip install git+https://github.com/kephale/napari-stable-diffusion.git Next steps  Image 2 Image support Inpainting support  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-stable-diffusion\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "Stable Diffusion",
    "documentation": "https://github.com/kephale/napari-stable-diffusion#README.md",
    "first_released": "2022-10-27T21:23:10.927779Z",
    "license": "BSD-3-Clause",
    "name": "napari-stable-diffusion",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/kephale/napari-stable-diffusion",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-10-27T21:23:10.927779Z",
    "report_issues": "https://github.com/kephale/napari-stable-diffusion/issues",
    "requirements": [
      "napari",
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "magicgui",
      "qtpy",
      "diffusers",
      "transformers",
      "torch",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "summary": "A demo of stable diffusion in napari",
    "support": "https://github.com/kephale/napari-stable-diffusion/issues",
    "twitter": "",
    "version": "0.0.1",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      { "email": "meriadec.prigent@gmail.com", "name": "Sylvain Prigent" }
    ],
    "code_repository": "https://github.com/sylvainprigent/napari-sdeconv",
    "conda": [{ "channel": "conda-forge", "package": "napari-sdeconv" }],
    "description": "# Description  `napari-sdeconv` is a suite of **Napari** plugins for the `sdeconv library <https://sylvainprigent.github.io/sdeconv/>`_ dedicated to 2D and 3D images deconvolution. It contains multiple deconvolution algorithms and PSF Generators.  * Example of deconvolution with Spitfire  ![img](https://raw.githubusercontent.com/sylvainprigent/napari-sdeconv/main/docs/images/spitfire3D.png)   * Example of how to generate a 3D PSF  ![img](https://raw.githubusercontent.com/sylvainprigent/napari-sdeconv/main/docs/images/gibson_lanni.png)   ## Installation  You can install `napari-sdeconv` via [pip]:      pip install napari-sdeconv       Note that the current version of the package only support python 3.9      ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [GNU GPL v3.0] license, \"napari-tracks-reader\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin [file an issue]: https://github.com/sylvainprigent/napari-sdeconv/issues [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "Description napari-sdeconv is a suite of Napari plugins for the sdeconv library <https://sylvainprigent.github.io/sdeconv/>_ dedicated to 2D and 3D images deconvolution. It contains multiple deconvolution algorithms and PSF Generators.  Example of deconvolution with Spitfire    Example of how to generate a 3D PSF   Installation You can install napari-sdeconv via pip: pip install napari-sdeconv  Note that the current version of the package only support python 3.9     Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the GNU GPL v3.0 license, \"napari-tracks-reader\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari sdeconv",
    "documentation": "https://github.com/sylvainprigent/napari-sdeconv#README.md",
    "first_released": "2021-09-02T14:23:19.264744Z",
    "license": "BSD-3-Clause",
    "name": "napari-sdeconv",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget", "sample_data"],
    "project_site": "https://github.com/sylvainprigent/napari-sdeconv",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-07-06T18:51:42.845316Z",
    "report_issues": "https://github.com/sylvainprigent/napari-sdeconv/issues",
    "requirements": [
      "numpy",
      "magicgui",
      "qtpy",
      "sdeconv (>=1.0.1)",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'",
      "sdeconv (>=1.0.1) ; extra == 'testing'"
    ],
    "summary": "2D and 3D deconvolution",
    "support": "https://github.com/sylvainprigent/napari-sdeconv/issues",
    "twitter": "",
    "version": "1.0.1",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      { "email": "sylvain.prigent@inria.fr", "name": "Sylvain Prigent" }
    ],
    "category": {
      "Supported data": ["2D", "3D", "Time series"],
      "Workflow step": ["Object tracking"]
    },
    "category_hierarchy": {
      "Supported data": [["2D"], ["3D"], ["Time series"]],
      "Workflow step": [
        ["Object tracking", "Isolated object tracking", "Particle tracking"]
      ]
    },
    "code_repository": "https://github.com/sylvainprigent/napari-stracking",
    "description": "# Description  The STracking suite provides a set of plugins for particles tracking in 2D+t and 3D+t images.  A classical particles tracking pipeline is made of 5 sequential steps:  <video width=\"512\" height=\"288\" controls>   <source src=\"https://raw.githubusercontent.com/sylvainprigent/napari-stracking/main/docs/images/intro.mp4\" type=\"video/mp4\"> </video>      A full documentation is available [here](<https://sylvainprigent.github.io/napari-stracking/guide.html>)         ## Installation  You can install `napari-stracking` via [pip]:      pip install napari-stracking  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-tracks-reader\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin [file an issue]: https://github.com/sylvainprigent/napari-strcking/issues [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/",
    "description_content_type": "text/markdown",
    "description_text": "Description The STracking suite provides a set of plugins for particles tracking in 2D+t and 3D+t images.  A classical particles tracking pipeline is made of 5 sequential steps:    A full documentation is available here  Installation You can install napari-stracking via pip: pip install napari-stracking  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-tracks-reader\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari-stracking",
    "documentation": "https://sylvainprigent.github.io/napari-stracking/description.html",
    "first_released": "2021-08-11T12:26:25.759055Z",
    "license": "BSD-3-Clause",
    "name": "napari-stracking",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://sylvainprigent.github.io/napari-stracking/",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-04-29T08:24:06.683008Z",
    "report_issues": "https://github.com/sylvainprigent/napari-stracking/issues",
    "requirements": [
      "napari",
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "stracking (>=0.1.9)"
    ],
    "summary": "Linking and tracks analysis",
    "support": "https://github.com/sylvainprigent/napari-stracking/issues",
    "twitter": "https://twitter.com/SylvainMPrigent",
    "version": "0.1.9",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      { "email": "sofroniewn@gmail.com", "name": "Nicholas Sofroniew" }
    ],
    "code_repository": "https://github.com/napari/napari-svg",
    "description": "# napari-svg  [![License](https://img.shields.io/pypi/l/napari-svg.svg?color=green)](https://github.com/napari/napari-svg/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-svg.svg?color=green)](https://pypi.org/project/napari-svg) [![Python Version](https://img.shields.io/pypi/pyversions/napari-svg.svg?color=green)](https://python.org) [![tests](https://github.com/napari/napari-svg/workflows/tests/badge.svg)](https://github.com/napari/napari-svg/actions) [![codecov](https://codecov.io/gh/napari/napari-svg/branch/master/graph/badge.svg)](https://codecov.io/gh/napari/napari-svg)  A plugin for reading and writing svg files with napari  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/docs/plugins/index.html -->  ## Installation  You can install `napari-svg` via [pip]:      pip install napari-svg  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-svg\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin [file an issue]: https://github.com/napari/napari-svg/issues [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-svg      A plugin for reading and writing svg files with napari  This napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.  Installation You can install napari-svg via pip: pip install napari-svg  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-svg\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "napari SVG",
    "documentation": "",
    "first_released": "2020-04-13T03:37:20.169990Z",
    "license": "BSD-3-Clause",
    "name": "napari-svg",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["writer"],
    "project_site": "https://github.com/napari/napari-svg",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-01-13T14:20:45.453540Z",
    "report_issues": "",
    "requirements": [
      "imageio (>=2.5.0)",
      "numpy (>=1.16.0)",
      "napari-plugin-engine (>=0.1.4)",
      "vispy (>=0.6.4)",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "napari (>=0.4) ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "summary": "A plugin for reading and writing svg files with napari",
    "support": "",
    "twitter": "",
    "version": "0.1.6",
    "visibility": "public",
    "writer_file_extensions": [".svg"],
    "writer_save_layers": [
      "shapes*",
      "vectors*",
      "image*",
      "labels*",
      "points*"
    ]
  },
  {
    "authors": [
      { "name": "Pranjal Dhole" },
      { "name": "Duway Nicolas Lesmes Leon" }
    ],
    "category": {
      "Workflow step": ["Image Segmentation", "Pixel classification"]
    },
    "category_hierarchy": {
      "Workflow step": [
        ["Image Segmentation", "Model-based segmentation"],
        ["Pixel classification"]
      ]
    },
    "code_repository": "https://github.com/yapic/napari-yapic-prediction",
    "description": "# napari-yapic-prediction  [![License](https://img.shields.io/pypi/l/napari-yapic-prediction.svg?color=green)](https://github.com/yapic/napari-yapic-prediction/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-yapic-prediction.svg?color=green)](https://pypi.org/project/napari-yapic-prediction) [![Python Version](https://img.shields.io/pypi/pyversions/napari-yapic-prediction.svg?color=green)](https://python.org) [![tests](https://github.com/yapic/napari-yapic-prediction/workflows/tests/badge.svg)](https://github.com/yapic/napari-yapic-prediction/actions) [![codecov](https://codecov.io/gh/yapic/napari-yapic-prediction/branch/master/graph/badge.svg?token=amah2YwOpx)](https://codecov.io/gh/yapic/napari-yapic-prediction)  A napari widget plugin to perform YAPiC model segmentation prediction in the napari window.   ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/docs/plugins/index.html -->  ## Description  This napari plugin provides a widget to upload a [YAPiC] trained model and perform segmentation over all the present images in the napari window. The segmentation results are uploaded as napari layers into the viewer automatically with the name structure of *imgename_prediction*.  ## Installation  1. Please install either GPU or CPU version of tensorflow that is compatible with your `cuda` and `cudnn` libraries before installing the plugin depending on your system. One of the plugin dependency is `yapic` that currently has sensitivity to tensorflow versions.  2. You can install `napari-yapic-prediction` via [pip]:      ```pip install napari-yapic-prediction```  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [GNU GPL v3.0] license, \"napari-yapic-prediction\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin [file an issue]: https://github.com/yapic/napari-yapic-prediction/issues [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ [YAPiC]: https://yapic.github.io/yapic/   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-yapic-prediction      A napari widget plugin to perform YAPiC model segmentation prediction in the napari window.   This napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.  Description This napari plugin provides a widget to upload a YAPiC trained model and perform segmentation over all the present images in the napari window. The segmentation results are uploaded as napari layers into the viewer automatically with the name structure of imgename_prediction. Installation   Please install either GPU or CPU version of tensorflow that is compatible with your cuda and cudnn libraries before installing the plugin depending on your system. One of the plugin dependency is yapic that currently has sensitivity to tensorflow versions.   You can install napari-yapic-prediction via pip: pip install napari-yapic-prediction   Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the GNU GPL v3.0 license, \"napari-yapic-prediction\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "napari-yapic-prediction",
    "documentation": "https://yapic.github.io/napari-yapic-prediction/",
    "first_released": "2021-04-19T08:42:54.339457Z",
    "license": "GPL-3.0",
    "name": "napari-yapic-prediction",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://yapic.github.io/napari-yapic-prediction/",
    "python_version": ">=3.6",
    "reader_file_extensions": [],
    "release_date": "2022-02-04T12:37:30.358543Z",
    "report_issues": "https://github.com/yapic/napari-yapic-prediction/issues",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "napari[all]",
      "yapic",
      "scikit-image"
    ],
    "summary": "napari widget that performs image segmentation with yapic model in the napari window. Install TENSORFLOW to use this plugin.",
    "support": "",
    "twitter": "",
    "version": "0.2.0",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      { "email": "robert.haase@tu-dresden.de", "name": "Robert Haase" }
    ],
    "code_repository": "https://github.com/haesleinhuepf/napari-tabu",
    "description": "# napari-tabu  [![License](https://img.shields.io/pypi/l/napari-tabu.svg?color=green)](https://github.com/haesleinhuepf/napari-tabu/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-tabu.svg?color=green)](https://pypi.org/project/napari-tabu) [![Python Version](https://img.shields.io/pypi/pyversions/napari-tabu.svg?color=green)](https://python.org) [![tests](https://github.com/haesleinhuepf/napari-tabu/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-tabu/actions) [![codecov](https://codecov.io/gh/haesleinhuepf/napari-tabu/branch/master/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-tabu)  A plugin for handling multiple napari windows ![](https://github.com/haesleinhuepf/napari-tabu/raw/main/docs/napari-tabu-screencast.gif)  ----------------------------------  ## Usage  To open a new window, first click the menu `Plugins > napari-tabu: open new window` ![](https://github.com/haesleinhuepf/napari-tabu/raw/main/docs/new_window_menu.png)  Afterwards, select the layer which should be opened in the new window and click on `Run`: ![](https://github.com/haesleinhuepf/napari-tabu/raw/main/docs/new_window_dialog.png)  When you're done with working with the new window, you can send back the result of your work using the `Send current layer back to main napari` butoon: ![](https://github.com/haesleinhuepf/napari-tabu/raw/main/docs/send_back.png)  This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.  ## Installation  You can install `napari-tabu` via [pip]:      pip install napari-tabu  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-tabu\" is free and open source software  ## Issues  If you encounter any problems, please open a thread on [image.sc](https://image.sc) along with a detailed description and tag [@haesleinhuepf](https://github.com/haesleinhuepf).  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/haesleinhuepf/napari-tabu/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-tabu      A plugin for handling multiple napari windows   Usage To open a new window, first click the menu Plugins > napari-tabu: open new window  Afterwards, select the layer which should be opened in the new window and click on Run:  When you're done with working with the new window, you can send back the result of your work using the Send current layer back to main napari butoon:  This napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template. Installation You can install napari-tabu via pip: pip install napari-tabu  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-tabu\" is free and open source software Issues If you encounter any problems, please open a thread on image.sc along with a detailed description and tag @haesleinhuepf.",
    "development_status": ["Development Status :: 3 - Alpha"],
    "display_name": "napari-tabu",
    "documentation": "https://github.com/haesleinhuepf/napari-tabu#README.md",
    "first_released": "2021-10-14T14:47:39.487563Z",
    "license": "BSD-3-Clause",
    "name": "napari-tabu",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/haesleinhuepf/napari-tabu",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-01-01T15:58:42.300925Z",
    "report_issues": "https://github.com/haesleinhuepf/napari-tabu/issues",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "napari",
      "napari-tools-menu"
    ],
    "summary": "A plugin for handling multiple napari windows",
    "support": "https://github.com/haesleinhuepf/napari-tabu/issues",
    "twitter": "",
    "version": "0.1.5",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Clément Cazorla" }],
    "code_repository": "https://bitbucket.org/koopa31/napari_svetlana/src/main/",
    "description": "# napari_svetlana  [![License](https://img.shields.io/pypi/l/napari_svetlana.svg?color=green)](https://bitbucket.org/koopa31/napari_svetlana/src/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari_svetlana.svg?color=green)](https://pypi.org/project/napari_svetlana) [![Python Version](https://img.shields.io/pypi/pyversions/napari_svetlana.svg?color=green)](https://python.org) [![tests](https://bitbucket.org/koopa31/napari_svetlana/workflows/tests/badge.svg)](https://bitbucket.org/koopa31/napari_svetlana/actions) [![codecov](https://codecov.io/gh/koopa31/napari_svetlana/branch/main/graph/badge.svg)](https://codecov.io/gh/koopa31/napari_svetlana) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-svetlana)](https://napari-hub.org/plugins/napari-svetlana) [![Documentation](https://readthedocs.org/projects/svetlana-documentation/badge/?version=latest)](https://svetlana-documentation.readthedocs.io/en/latest/)  The aim of this plugin is to classify the output of a segmentation algorithm. The inputs are : <ul>   <li>A folder of raw images</li>   <li>Their segmentation masks where each ROI has its own label.</li> </ul>  Svetlana can process 2D, 3D and multichannel image. If you want to use it to work on cell images, we strongly recommend the use of [Cellpose](https://www.cellpose.org) for the segmentation part, as it provides excellent quality results and a standard output format accepted by Svetlana (labels masks).   If you use this plugin please cite the paper:   ```bibtex @InProceedings{2022_cazorla801, \\tauthor = \"Clément Cazorla and Pierre Weiss and Renaud Morin\", \\ttitle = \"SVETLANA: UN CLASSIFIEUR DE SEGMENTATION POUR NAPARI\", \\tbooktitle = \"28° Colloque sur le traitement du signal et des images\", \\tyear = \"2022\", \\tpublisher = \"GRETSI - Groupe de Recherche en Traitement du Signal et des Images\", \\tnumber = \"001-0194\", \\tpages = \"p. 777-780\", \\tmonth = \"Sep # 6--9\", \\taddress = \"Nancy\", \\tdoi = \"\", \\tpdf = \"2022_cazorla801.pdf\", } ```   ![](https://bitbucket.org/koopa31/napari_svetlana/raw/bb1010b99e03cdbd94d1cd70cc63f93deb63a58e/images/svetlagif.gif)   ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/plugins/stable/index.html -->  ## Installation  First install Napari in a Python 3.9 Conda environment following the instructions provided in the official [documentation](https://napari.org/stable/tutorials/fundamentals/installation.html).  You can install `napari_svetlana` via [pip], or directly from the Napari plugin manager (see Napari documentation): ```bash pip install napari_svetlana ``` WARNING:  If you have a Cuda compatible GPU on your computer, some computations may be accelerated using [Cupy](https://pypi.org/project/cupy/). Unfortunately, Cupy needs Cudatoolkit to be installed. This library can only be installed via  Conda while the plugin is a pip plugin, so it must be installed manually for the moment: ```bash conda install cudatoolkit=10.2  ``` Also note that the library ([Cucim](https://pypi.org/project/cucim/)) that we use to improve these performances, computing morphological operations on GPU is unfortunately only available for Linux systems. Hence, if you are a Windows user, this installation is not necessary.  ## Tutorial  To learn more about the features of Svetlana and how to use it, please check our [Youtube tutorial](https://youtube.com) and our [documentation](https://svetlana-documentation.readthedocs.io/en/latest/). A folder in this repository called \"[Demo images](https://bitbucket.org/koopa31/napari_svetlana/src/main/Demo%20images/)\", contains two demo images, similar to the ones of the Youtube tutorial. Feel free to use them to test all the features of Sevtlana.  ## The data augmentation   It is possible to perform all the complex data augmentations proposed in the Albumentations library. To do so, please refer to the [documentation](https://albumentations.ai/docs/getting_started/transforms_and_targets/), and add all the needed parameters to the JSON configuration file.  **Example:**  Gaussian blurring in documentation :  ```python GaussianBlur(blur_limit=(3, 7), sigma_limit=0, always_apply=False, p=0.5) ```  Equivalent in JSON configuration file: ```json \"GaussianBlur\": {       \"apply\": \"False\",       \"blur_limit\": \"(3, 7)\",       \"sigma_limit\": \"0\",        \"p\": \"0.5\"   } ```  where _apply_ means you want this data augmentation to be applied or not.  ## Contributing  Contributions are very welcome.  ## License  Distributed under the terms of the [BSD-3] license, \"napari_svetlana\" is free and open source software  ## Acknowledgements  The method was developed by [Clément Cazorla](https://koopa31.github.io/), [Renaud Morin](https://www.linkedin.com/in/renaud-morin-6a42665b/?originalSubdomain=fr) and [Pierre Weiss](https://www.math.univ-toulouse.fr/~weiss/). And the plugin was written by Clément Cazorla. The project is co-funded by [Imactiv-3D](https://www.imactiv-3d.com/) and [CNRS](https://www.cnrs.fr/fr).  ## Issues  If you encounter any problems, please [file an issue](https://bitbucket.org/koopa31/napari_svetlana/issues?status=new&status=open) along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari_svetlana        The aim of this plugin is to classify the output of a segmentation algorithm. The inputs are :  A folder of raw images Their segmentation masks where each ROI has its own label.  Svetlana can process 2D, 3D and multichannel image. If you want to use it to work on cell images, we strongly recommend the use of Cellpose for the segmentation part, as it provides excellent quality results and a standard output format accepted by Svetlana (labels masks).  If you use this plugin please cite the paper:  bibtex @InProceedings{2022_cazorla801,     author = \"Clément Cazorla and Pierre Weiss and Renaud Morin\",     title = \"SVETLANA: UN CLASSIFIEUR DE SEGMENTATION POUR NAPARI\",     booktitle = \"28° Colloque sur le traitement du signal et des images\",     year = \"2022\",     publisher = \"GRETSI - Groupe de Recherche en Traitement du Signal et des Images\",     number = \"001-0194\",     pages = \"p. 777-780\",     month = \"Sep # 6--9\",     address = \"Nancy\",     doi = \"\",     pdf = \"2022_cazorla801.pdf\", }   This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation First install Napari in a Python 3.9 Conda environment following the instructions provided in the official documentation. You can install napari_svetlana via pip, or directly from the Napari plugin manager (see Napari documentation): bash pip install napari_svetlana WARNING: If you have a Cuda compatible GPU on your computer, some computations may be accelerated using Cupy. Unfortunately, Cupy needs Cudatoolkit to be installed. This library can only be installed via  Conda while the plugin is a pip plugin, so it must be installed manually for the moment: bash conda install cudatoolkit=10.2 Also note that the library (Cucim) that we use to improve these performances, computing morphological operations on GPU is unfortunately only available for Linux systems. Hence, if you are a Windows user, this installation is not necessary. Tutorial To learn more about the features of Svetlana and how to use it, please check our Youtube tutorial and our documentation. A folder in this repository called \"Demo images\", contains two demo images, similar to the ones of the Youtube tutorial. Feel free to use them to test all the features of Sevtlana. The data augmentation It is possible to perform all the complex data augmentations proposed in the Albumentations library. To do so, please refer to the documentation, and add all the needed parameters to the JSON configuration file. Example: Gaussian blurring in documentation : python GaussianBlur(blur_limit=(3, 7), sigma_limit=0, always_apply=False, p=0.5) Equivalent in JSON configuration file: json \"GaussianBlur\": {       \"apply\": \"False\",       \"blur_limit\": \"(3, 7)\",       \"sigma_limit\": \"0\",        \"p\": \"0.5\"   } where apply means you want this data augmentation to be applied or not. Contributing Contributions are very welcome. License Distributed under the terms of the BSD-3 license, \"napari_svetlana\" is free and open source software Acknowledgements The method was developed by Clément Cazorla, Renaud Morin and Pierre Weiss. And the plugin was written by Clément Cazorla. The project is co-funded by Imactiv-3D and CNRS. Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari-svetlana",
    "documentation": "https://svetlana-documentation.readthedocs.io/en/latest/",
    "first_released": "2022-11-22T10:45:31.312794Z",
    "license": "GPL-3.0-only",
    "name": "napari-svetlana",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader", "writer", "widget"],
    "project_site": "",
    "python_version": ">=3.9",
    "reader_file_extensions": ["*"],
    "release_date": "2022-12-16T13:36:27.222303Z",
    "report_issues": "https://bitbucket.org/koopa31/napari_svetlana/issues?status=new&status=open",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "albumentations (==1.0.3)",
      "joblib (==1.1.0)",
      "light-the-torch",
      "matplotlib",
      "opencv-python (==4.5.5.62)",
      "PyQt5",
      "cupy-cuda102 (==10.6.0)",
      "xlsxwriter",
      "pandas",
      "cucim (==22.6.0) ; platform_system == \"Linux\""
    ],
    "summary": "A classification plugin for the ROIs of a segmentation mask.",
    "support": "https://bitbucket.org/koopa31/napari_svetlana/issues?status=new&status=open",
    "twitter": "",
    "version": "0.2.1",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": ["image"]
  },
  {
    "authors": [{ "email": "alisterburt@gmail.com", "name": "Alister Burt" }],
    "code_repository": "https://github.com/alisterburt/napari-subboxer",
    "description": "# napari-subboxer  [![License](https://img.shields.io/pypi/l/napari-subboxer.svg?color=green)](https://github.com/alisterburt/napari-subboxer/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-subboxer.svg?color=green)](https://pypi.org/project/napari-subboxer) [![Python Version](https://img.shields.io/pypi/pyversions/napari-subboxer.svg?color=green)](https://python.org) [![tests](https://github.com/alisterburt/napari-subboxer/workflows/tests/badge.svg)](https://github.com/alisterburt/napari-subboxer/actions) [![codecov](https://codecov.io/gh/alisterburt/napari-subboxer/branch/master/graph/badge.svg)](https://codecov.io/gh/alisterburt/napari-subboxer)  A napari plugin for visualising and interacting with electron cryotomograms.   ## Installation  You can install `napari-subboxer` via [pip]:      pip install napari-subboxer  ## Usage  This plugin provides a user interface for opening electron cryotomograms in  napari as both volumes and slices through volumes.  ![demo](https://user-images.githubusercontent.com/7307488/138575305-b05c4735-9c03-4629-bfb0-9612ea8f26fd.gif)  The plugin can be opened from the `plugins` menu in napari, or with  `napari-subboxer` at the command line.  ![plugins-menu](https://user-images.githubusercontent.com/7307488/138575015-00ea78d9-02c1-44bc-9034-0c0a7fa8d973.png)  ```yaml Usage: napari-subboxer [TOMOGRAM_FILE]    An interactive tool for defining and applying relative transforms   on sets of particles in napari.  Arguments:   [TOMOGRAM_FILE]  Options:   --help                          Show this message and exit.  ```  ## Contributing  Contributions are very welcome.   ## License  Distributed under the terms of the [BSD-3] license, \"napari-subboxer\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/alisterburt/napari-subboxer/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-subboxer      A napari plugin for visualising and interacting with electron cryotomograms. Installation You can install napari-subboxer via pip: pip install napari-subboxer  Usage This plugin provides a user interface for opening electron cryotomograms in  napari as both volumes and slices through volumes.  The plugin can be opened from the plugins menu in napari, or with  napari-subboxer at the command line.  ```yaml Usage: napari-subboxer [TOMOGRAM_FILE] An interactive tool for defining and applying relative transforms   on sets of particles in napari. Arguments:   [TOMOGRAM_FILE] Options:   --help                          Show this message and exit. ``` Contributing Contributions are very welcome.  License Distributed under the terms of the BSD-3 license, \"napari-subboxer\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari-subboxer",
    "documentation": "https://github.com/alisterburt/napari-subboxer#README.md",
    "first_released": "2021-11-22T10:40:55.622000Z",
    "license": "BSD-3-Clause",
    "name": "napari-subboxer",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/alisterburt/napari-subboxer",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2021-11-22T10:40:55.622000Z",
    "report_issues": "https://github.com/alisterburt/napari-subboxer/issues",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "napari[pyqt5] (==0.4.12)",
      "mrcfile",
      "typer",
      "eulerangles",
      "starfile",
      "einops",
      "pydantic"
    ],
    "summary": "A napari plugin for interacting with electron cryotomograms",
    "support": "https://github.com/alisterburt/napari-subboxer/issues",
    "twitter": "",
    "version": "0.0.1",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Johannes Müller" }],
    "code_repository": "https://github.com/jo-mueller/napari-stl-exporter.git",
    "conda": [{ "channel": "conda-forge", "package": "napari-stl-exporter" }],
    "description": "# napari-stl-exporter  [![License](https://img.shields.io/pypi/l/napari-stl-exporter.svg?color=green)](https://github.com/jo-mueller/napari-stl-exporter/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-stl-exporter.svg?color=green)](https://pypi.org/project/napari-stl-exporter) [![Python Version](https://img.shields.io/pypi/pyversions/napari-stl-exporter.svg?color=green)](https://python.org) [![tests](https://github.com/jo-mueller/napari-stl-exporter/workflows/tests/badge.svg)](https://github.com/jo-mueller/napari-stl-exporter/actions) [![codecov](https://codecov.io/gh/jo-mueller/napari-stl-exporter/branch/main/graph/badge.svg?token=9zctLzazD9)](https://codecov.io/gh/jo-mueller/napari-stl-exporter)  This plugin allows to import and export surface data in Napari to common file formats. The generated file formats can be read by other common applications, and - in particular - allow *3D-printing*.  <img src=\"https://github.com/jo-mueller/napari-stl-exporter/blob/main/doc/model_and_printed_model.png\" width=80% height=80%>  ## Usage This section explains which data can be written with the napari-stl-exported and how you can do so.  ### Supported file formats: Currently supported file formats for export include and rely on the [vedo io API](https://vedo.embl.es/autodocs/content/vedo/io.html#vedo.io). * *.stl*: [Standard triangle language](https://en.wikipedia.org/wiki/STL_%28file_format%29) * *.ply*: [Polygon file format](https://en.wikipedia.org/wiki/PLY_(file_format)) * *.obj*: [Wavefront object](https://en.wikipedia.org/wiki/Wavefront_.obj_file)  ### Supported Napari layers:  Currently supported Napari layer types are: * [Surface layers](https://napari.org/howtos/layers/surface.html) * [Label layers](https://napari.org/howtos/layers/labels.html): The label data is converted to surface data under the hood using the [marching cubes algorithm](https://scikit-image.org/docs/dev/api/skimage.measure.html#skimage.measure.marching_cubes) implemented in [scikit-image](https://scikit-image.org/) and is then exported using [Vedo](https://vedo.embl.es/). Warning: This can be slow for large image data!  ### Import/export  **Interactively:** To export the data, simply save the selected layer with `File > Save Selected Layer(s)` and specify the file ending to be `some_file.[file_ending]`, for supported file types, see above. Similarly, supported file types can be imported into Napari with `File > `  **From code**: A [Napari Label layer](https://napari.org/api/napari.layers.Labels.html) can be added to the viewer as described in the [napari reference](https://napari.org/gallery/add_labels.html?highlight=add_labels) with this code snippet:  ```python import napari import numpy as np  # Load and binarize image label_image = np.zeros((100, 100, 100), dtype=int) label_image[25:75, 25:75, 25:75] = 1  # Add data to viewer viewer = napari.Viewer() label_layer = viewer.add_labels(data, name='3D object')  # save the layer as 3D printable file to disc napari.save_layers(r'/some/path/test.stl', [label_layer]) ```  ### Sample data You can create sample label/surface data for export using the built-in functions as shown here:  <img src=\"https://github.com/jo-mueller/napari-stl-exporter/blob/main/doc/1_sample_data.png\" width=45% height=45%>  ...or from code with  ```Python import napari_stl_exporter  pyramid = napari_stl_exporter.make_pyramid_surface()  ```  ### 3D-printing To actually send your object to a 3D-printer, it has to be further converted to the *.gcode* format with a Slicer program. The latter convert the 3D object to machine-relevant parameters (printing detail, motor trajectories, etc). Popular slicers are:  * [Slic3r](https://slic3r.org/): Documentation [here](https://manual.slic3r.org/intro/overview) * [Prusa Slicer](https://www.prusa3d.com/prusaslicer/): Tutorial [here](https://help.prusa3d.com/en/article/first-print-with-prusaslicer_1753)  *Note*: You can also upload the STL file to [github.com](https://github.com) and interact with it in the browser:  <img src=\"https://github.com/jo-mueller/napari-stl-exporter/blob/main/doc/pyramid_browser_screenshot.png\" width=45% height=45%>  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/docs/plugins/index.html -->  ## Installation  You can install `napari-stl-exporter` via [pip]:      pip install napari-stl-exporter  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-stl-exporter\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue](https://github.com/jo-mueller/napari-stl-exporter/issues) along with a detailed description or post to image.sc and tag ```El_Pollo_Diablo```  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-stl-exporter      This plugin allows to import and export surface data in Napari to common file formats. The generated file formats can be read by other common applications, and - in particular - allow 3D-printing.  Usage This section explains which data can be written with the napari-stl-exported and how you can do so. Supported file formats: Currently supported file formats for export include and rely on the vedo io API. * .stl: Standard triangle language * .ply: Polygon file format * .obj: Wavefront object Supported Napari layers: Currently supported Napari layer types are: * Surface layers * Label layers: The label data is converted to surface data under the hood using the marching cubes algorithm implemented in scikit-image and is then exported using Vedo. Warning: This can be slow for large image data! Import/export Interactively: To export the data, simply save the selected layer with File > Save Selected Layer(s) and specify the file ending to be some_file.[file_ending], for supported file types, see above. Similarly, supported file types can be imported into Napari with File > From code: A Napari Label layer can be added to the viewer as described in the napari reference with this code snippet: ```python import napari import numpy as np Load and binarize image label_image = np.zeros((100, 100, 100), dtype=int) label_image[25:75, 25:75, 25:75] = 1 Add data to viewer viewer = napari.Viewer() label_layer = viewer.add_labels(data, name='3D object') save the layer as 3D printable file to disc napari.save_layers(r'/some/path/test.stl', [label_layer]) ``` Sample data You can create sample label/surface data for export using the built-in functions as shown here:  ...or from code with ```Python import napari_stl_exporter pyramid = napari_stl_exporter.make_pyramid_surface() ``` 3D-printing To actually send your object to a 3D-printer, it has to be further converted to the .gcode format with a Slicer program. The latter convert the 3D object to machine-relevant parameters (printing detail, motor trajectories, etc). Popular slicers are:  Slic3r: Documentation here Prusa Slicer: Tutorial here  Note: You can also upload the STL file to github.com and interact with it in the browser:   This napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.  Installation You can install napari-stl-exporter via pip: pip install napari-stl-exporter  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-stl-exporter\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description or post to image.sc and tag El_Pollo_Diablo",
    "development_status": ["Development Status :: 3 - Alpha"],
    "display_name": "napari-stl-exporter",
    "documentation": "",
    "first_released": "2021-10-06T12:49:22.533995Z",
    "license": "BSD-3-Clause",
    "name": "napari-stl-exporter",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader", "writer", "sample_data"],
    "project_site": "https://github.com/jo-mueller/napari-stl-exporter.git",
    "python_version": ">=3.7",
    "reader_file_extensions": ["*.stl", "*.obj", "*.ply"],
    "release_date": "2022-08-02T11:09:54.445003Z",
    "report_issues": "",
    "requirements": [
      "napari",
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "scikit-image",
      "vedo",
      "npe2"
    ],
    "summary": "Exports label images to 3D-printable stl files.",
    "support": "",
    "twitter": "",
    "version": "0.0.11",
    "visibility": "public",
    "writer_file_extensions": [".stl", ".ply", ".obj"],
    "writer_save_layers": ["surface", "labels"]
  },
  {
    "authors": [{ "name": "Austin E. Y. T. Lefebvre" }],
    "code_repository": "https://github.com/aelefebv/snouty-viewer",
    "conda": [],
    "description": "  <!-- This file is designed to provide you with a starting template for documenting the functionality of your plugin. Its content will be rendered on your plugin's napari hub page.  The sections below are given as a guide for the flow of information only, and are in no way prescriptive. You should feel free to merge, remove, add and rename sections at will to make this document work best for your plugin.  ## Description  This should be a detailed description of the context of your plugin and its intended purpose.  If you have videos or screenshots of your plugin in action, you should include them here as well, to make them front and center for new users.  You should use absolute links to these assets, so that we can easily display them on the hub. The easiest way to include a video is to use a GIF, for example hosted on imgur. You can then reference this GIF as an image.  ![Example GIF hosted on Imgur](https://i.imgur.com/A5phCX4.gif)  Note that GIFs larger than 5MB won't be rendered by GitHub - we will however, render them on the napari hub.  The other alternative, if you prefer to keep a video, is to use GitHub's video embedding feature.  1. Push your `DESCRIPTION.md` to GitHub on your repository (this can also be done as part of a Pull Request) 2. Edit `.napari/DESCRIPTION.md` **on GitH ub**. 3. Drag and drop your video into its desired location. It will be uploaded and hosted on GitHub for you, but will not be placed in your repository. 4. We will take the resolved link to the video and render it on the hub.  Here is an example of an mp4 video embedded this way.  https://user-images.githubusercontent.com/17995243/120088305-6c093380-c132-11eb-822d-620e81eb5f0e.mp4  ## Intended Audience & Supported Data  This section should describe the target audience for this plugin (any knowledge, skills and experience required), as well as a description of the types of data supported by this plugin.  Try to make the data description as explicit as possible, so that users know the format your plugin expects. This applies both to reader plugins reading file formats and to function/dock widget plugins accepting layers and/or layer data. For example, if you know your plugin only works with 3D integer data in \"tyx\" order, make sure to mention this.  If you know of researchers, groups or labs using your plugin, or if it has been cited anywhere, feel free to also include this information here.  ## Quickstart  This section should go through step-by-step examples of how your plugin should be used. Where your plugin provides multiple dock widgets or functions, you should split these out into separate subsections for easy browsing. Include screenshots and videos wherever possible to elucidate your descriptions.  Ideally, this section should start with minimal examples for those who just want a quick overview of the plugin's functionality, but you should definitely link out to more complex and in-depth tutorials highlighting any intricacies of your plugin, and more detailed documentation if you have it.  ## Additional Install Steps (uncommon) We will be providing installation instructions on the hub, which will be sufficient for the majority of plugins. They will include instructions to pip install, and to install via napari itself.  Most plugins can be installed out-of-the-box by just specifying the package requirements over in `setup.cfg`. However, if your plugin has any more complex dependencies, or requires any additional preparation before (or after) installation, you should add this information here.  ## Getting Help  This section should point users to your preferred support tools, whether this be raising an issue on GitHub, asking a question on image.sc, or using some other method of contact. If you distinguish between usage support and bug/feature support, you should state that here.  ## How to Cite  Many plugins may be used in the course of published (or publishable) research, as well as during conference talks and other public facing events. If you'd like to be cited in a particular format, or have a DOI you'd like used, you should provide that information here. -->  ## Description Easy to use plugin for opening raw Snouty files and converting them to native view.  ![Example](https://i.imgur.com/VirE5DM.gif)  ## Intended Audience & Supported Data This plugin is intended for those using a SOLS (Snouty) microscope collected via [Alfred Millett-Sikking's code](https://github.com/amsikking/SOLS_microscope).  This plugin accepts a folder with at least subdirectories of data and metadata as an input. The metadata must have a 000000.txt file for the metadata to be properly parsed.  ## Quickstart  ### Getting the plugin working 1. pip install snouty-viewer (within a virtual environment of Python 3.8, 3.9, or 3.10 recommended) 2. Open up napari  ### Viewing raw Snouty data - Drag and drop a root folder of your Snouty data. This is the folder that includes the data and metadata subfolders. - Select \"Snouty Viewer\" for opening.  ### Converting raw Snouty data to its native view 1. Click plugins, snouty-viewer: Native View 2. Select the file you want to convert 3. Press Run  ### Saving your native view file 1. Select the file you want to save 2. File > Save Selected Layer(s)... 3. Select where you want to save your file 4. Write your file name (recommended to end in .tif) 5. Save 6. Wait (this could take a few minutes depending on your file's size)  ## Getting Help - Open up an issue on [GitHub](https://github.com/aelefebv/snouty-viewer/issues). - Start a thread on [image.sc](https://forum.image.sc/) ",
    "description_content_type": "text/markdown",
    "description_text": " Description Easy to use plugin for opening raw Snouty files and converting them to native view.  Intended Audience & Supported Data This plugin is intended for those using a SOLS (Snouty) microscope collected via Alfred Millett-Sikking's code. This plugin accepts a folder with at least subdirectories of data and metadata as an input. The metadata must have a 000000.txt file for the metadata to be properly parsed. Quickstart Getting the plugin working  pip install snouty-viewer (within a virtual environment of Python 3.8, 3.9, or 3.10 recommended) Open up napari  Viewing raw Snouty data  Drag and drop a root folder of your Snouty data. This is the folder that includes the data and metadata subfolders. Select \"Snouty Viewer\" for opening.  Converting raw Snouty data to its native view  Click plugins, snouty-viewer: Native View Select the file you want to convert Press Run  Saving your native view file  Select the file you want to save File > Save Selected Layer(s)... Select where you want to save your file Write your file name (recommended to end in .tif) Save Wait (this could take a few minutes depending on your file's size)  Getting Help  Open up an issue on GitHub. Start a thread on image.sc ",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "Snouty Viewer",
    "documentation": "https://github.com/aelefebv/snouty-viewer#README.md",
    "first_released": "2022-08-31T23:36:04.300528Z",
    "license": "MIT",
    "name": "snouty-viewer",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader", "widget"],
    "project_site": "https://github.com/aelefebv/snouty-viewer",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-09-08T22:33:29.469083Z",
    "report_issues": "https://github.com/aelefebv/snouty-viewer/issues",
    "requirements": [
      "magicgui",
      "napari",
      "numpy",
      "tifffile",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "tox ; extra == 'testing'"
    ],
    "summary": "A plugin to visualize and convert Snouty data.",
    "support": "https://github.com/aelefebv/snouty-viewer/issues",
    "twitter": "",
    "version": "0.0.4",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      { "email": "robert.haase@tu-dresden.de", "name": "Robert Haase" }
    ],
    "code_repository": "https://github.com/haesleinhuepf/napari-workflow-inspector",
    "description": "# napari-workflow-inspector  [![License](https://img.shields.io/pypi/l/napari-workflow-inspector.svg?color=green)](https://github.com/haesleinhuepf/napari-workflow-inspector/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-workflow-inspector.svg?color=green)](https://pypi.org/project/napari-workflow-inspector) [![Python Version](https://img.shields.io/pypi/pyversions/napari-workflow-inspector.svg?color=green)](https://python.org) [![tests](https://github.com/haesleinhuepf/napari-workflow-inspector/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-workflow-inspector/actions) [![codecov](https://codecov.io/gh/haesleinhuepf/napari-workflow-inspector/branch/main/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-workflow-inspector) [![Development Status](https://img.shields.io/pypi/status/napari-workflow-inspector.svg)](https://en.wikipedia.org/wiki/Software_release_life_cycle#Alpha) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-workflow-inspector)](https://napari-hub.org/plugins/napari-workflow-inspector)  Inspect relationships between image processing operations in active workflows in napari. Open the inspector by clicking the menu `Tools > Visualization > Workflow Inspector`.  ![img_1.png](https://github.com/haesleinhuepf/napari-workflow-inspector/raw/main/docs/screenshot_graph.png)  Also install the [napari-script-editor](https://www.napari-hub.org/plugins/napari-script-editor)  to generate code from active workflows.  ![img_2.png](https://github.com/haesleinhuepf/napari-workflow-inspector/raw/main/docs/screenshot_script_editor.png)  For recording workflows, all napari image processing plugins that use the `@time_slicer` interface are supported. See [napari-time-slicer](https://www.napari-hub.org/plugins/napari-time-slicer) for a list. More to come, stay tuned.  ## Installation  You can install `napari-workflow-inspector` via [pip]:  ``` pip install napari-workflow-inspector ```  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-workflow-inspector\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/haesleinhuepf/napari-workflow-inspector/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-workflow-inspector        Inspect relationships between image processing operations in active workflows in napari. Open the inspector by clicking the menu Tools > Visualization > Workflow Inspector.  Also install the napari-script-editor  to generate code from active workflows.  For recording workflows, all napari image processing plugins that use the @time_slicer interface are supported. See napari-time-slicer for a list. More to come, stay tuned. Installation You can install napari-workflow-inspector via pip: pip install napari-workflow-inspector Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-workflow-inspector\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 3 - Alpha"],
    "display_name": "napari-workflow-inspector",
    "documentation": "https://github.com/haesleinhuepf/napari-workflow-inspector#README.md",
    "first_released": "2021-12-04T14:21:01.302048Z",
    "license": "BSD-3-Clause",
    "name": "napari-workflow-inspector",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/haesleinhuepf/napari-workflow-inspector",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-05-15T12:00:02.019533Z",
    "report_issues": "https://github.com/haesleinhuepf/napari-workflow-inspector/issues",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "napari-tools-menu",
      "networkx",
      "matplotlib",
      "napari-workflows"
    ],
    "summary": "Inspect relationships between image processing operations in active workflows in napari",
    "support": "https://github.com/haesleinhuepf/napari-workflow-inspector/issues",
    "twitter": "",
    "version": "0.2.2",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      { "email": "robert.haase@tu-dresden.de", "name": "Robert Haase" }
    ],
    "code_repository": "https://github.com/haesleinhuepf/natari",
    "description": "# natari  [![License](https://img.shields.io/pypi/l/natari.svg?color=green)](https://github.com/haesleinhuepf/natari/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/natari.svg?color=green)](https://pypi.org/project/natari) [![Python Version](https://img.shields.io/pypi/pyversions/natari.svg?color=green)](https://python.org)  Napari gaming  ## Sliding puzzle  Restore the image by reordering the superpixels using the `W`, `A`, `S`, `D` keys!   ![](https://github.com/haesleinhuepf/natari/raw/master/images/sliding_puzzle.gif)  ## Cell counting arcade Commander! Cells are intruding our dish! Control your tiny space ship using `1` and `2` keys to move it left/right. Use the `9` key to shoot a anti-cell bullet.  ![](https://github.com/haesleinhuepf/natari/raw/master/images/cell_counting_arcade.gif)  The image originates from [BBBC022v1](https://bbbc.broadinstitute.org/BBBC022) (Gustafsdottir et al., PLOS ONE, 2013), available from the Broad Bioimage Benchmark Collection (Ljosa et al., Nature Methods, 2012).  ## Snake Two mitochondria navigating in a cell searching for stress granules.  The two players can control their mito using the `W`, `A`, `S`, `D` and `I`, `J`, `K`, `L`  keys, respectively.  ![](https://github.com/haesleinhuepf/natari/raw/master/images/snake.gif)  ## Ping pong Don't drop the organoid! Use your racket and hit it back to your colleague! The two players can use `W`, `S` and `I`, `K` to control their racket, respectively.  ![](https://github.com/haesleinhuepf/natari/raw/master/images/ping_pong.gif)   This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.  ## Installation  You can install `natari` via [pip]:      pip install natari  ## Known issues  * To make the keyboard buttons work, you sometimes have to click within the image after starting the game.  ## Contributing  Contributions are very welcome.   ## License  \"natari\" is free and open source software. The code is in the public domain.  [See also: unlicense.org](https://unlicense.org)  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/haesleinhuepf/natari/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "natari    Napari gaming Sliding puzzle Restore the image by reordering the superpixels using the W, A, S, D keys!   Cell counting arcade Commander! Cells are intruding our dish! Control your tiny space ship using 1 and 2 keys to move it left/right. Use the 9 key to shoot a anti-cell bullet.  The image originates from BBBC022v1 (Gustafsdottir et al., PLOS ONE, 2013), available from the Broad Bioimage Benchmark Collection (Ljosa et al., Nature Methods, 2012). Snake Two mitochondria navigating in a cell searching for stress granules.  The two players can control their mito using the W, A, S, D and I, J, K, L  keys, respectively.  Ping pong Don't drop the organoid! Use your racket and hit it back to your colleague! The two players can use W, S and I, K to control their racket, respectively.  This napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template. Installation You can install natari via pip: pip install natari  Known issues  To make the keyboard buttons work, you sometimes have to click within the image after starting the game.  Contributing Contributions are very welcome.  License \"natari\" is free and open source software. The code is in the public domain. See also: unlicense.org Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 3 - Alpha"],
    "display_name": "natari",
    "documentation": "https://github.com/haesleinhuepf/natari#README.md",
    "first_released": "2021-10-17T09:32:48.750117Z",
    "license": "Unlicense",
    "name": "natari",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/haesleinhuepf/natari",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2021-12-22T14:04:54.166677Z",
    "report_issues": "https://github.com/haesleinhuepf/natari/issues",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "napari",
      "scipy",
      "napari-tools-menu"
    ],
    "summary": "Napari gaming",
    "support": "https://github.com/haesleinhuepf/natari/issues",
    "twitter": "",
    "version": "0.2.7",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "email": "chris.havlin@gmail.com", "name": "Chris Havlin" }],
    "code_repository": "https://github.com/data-exp-lab/yt-napari",
    "description": "  <!-- This file is designed to provide you with a starting template for documenting the functionality of your plugin. Its content will be rendered on your plugin's napari hub page.  The sections below are given as a guide for the flow of information only, and are in no way prescriptive. You should feel free to merge, remove, add and rename sections at will to make this document work best for your plugin.  # Description  This should be a detailed description of the context of your plugin and its intended purpose.  If you have videos or screenshots of your plugin in action, you should include them here as well, to make them front and center for new users.  You should use absolute links to these assets, so that we can easily display them on the hub. The easiest way to include a video is to use a GIF, for example hosted on imgur. You can then reference this GIF as an image.  ![Example GIF hosted on Imgur](https://i.imgur.com/A5phCX4.gif)  Note that GIFs larger than 5MB won't be rendered by GitHub - we will however, render them on the napari hub.  The other alternative, if you prefer to keep a video, is to use GitHub's video embedding feature.  1. Push your `DESCRIPTION.md` to GitHub on your repository (this can also be done as part of a Pull Request) 2. Edit `.napari/DESCRIPTION.md` **on GitHub**. 3. Drag and drop your video into its desired location. It will be uploaded and hosted on GitHub for you, but will not be placed in your repository. 4. We will take the resolved link to the video and render it on the hub.  Here is an example of an mp4 video embedded this way.  https://user-images.githubusercontent.com/17995243/120088305-6c093380-c132-11eb-822d-620e81eb5f0e.mp4  # Intended Audience & Supported Data  This section should describe the target audience for this plugin (any knowledge, skills and experience required), as well as a description of the types of data supported by this plugin.  Try to make the data description as explicit as possible, so that users know the format your plugin expects. This applies both to reader plugins reading file formats and to function/dock widget plugins accepting layers and/or layer data. For example, if you know your plugin only works with 3D integer data in \"tyx\" order, make sure to mention this.  If you know of researchers, groups or labs using your plugin, or if it has been cited anywhere, feel free to also include this information here.  # Quickstart  This section should go through step-by-step examples of how your plugin should be used. Where your plugin provides multiple dock widgets or functions, you should split these out into separate subsections for easy browsing. Include screenshots and videos wherever possible to elucidate your descriptions.  Ideally, this section should start with minimal examples for those who just want a quick overview of the plugin's functionality, but you should definitely link out to more complex and in-depth tutorials highlighting any intricacies of your plugin, and more detailed documentation if you have it.  # Additional Install Steps (uncommon) We will be providing installation instructions on the hub, which will be sufficient for the majority of plugins. They will include instructions to pip install, and to install via napari itself.  Most plugins can be installed out-of-the-box by just specifying the package requirements over in `setup.cfg`. However, if your plugin has any more complex dependencies, or requires any additional preparation before (or after) installation, you should add this information here.  # Getting Help  This section should point users to your preferred support tools, whether this be raising an issue on GitHub, asking a question on image.sc, or using some other method of contact. If you distinguish between usage support and bug/feature support, you should state that here.  # How to Cite  Many plugins may be used in the course of published (or publishable) research, as well as during conference talks and other public facing events. If you'd like to be cited in a particular format, or have a DOI you'd like used, you should provide that information here. -->  yt-napari provides plugins to help load data from [yt](https://yt-project.org/) into napari. It includes a graphical data loader, json-based file loading and helper-functions for jupyter notebook interaction. See the [full documentation](https://yt-napari.readthedocs.io) for more details. ",
    "description_content_type": "text/markdown",
    "description_text": " yt-napari provides plugins to help load data from yt into napari. It includes a graphical data loader, json-based file loading and helper-functions for jupyter notebook interaction. See the full documentation for more details.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "yt-napari",
    "documentation": "https://github.com/data-exp-lab/yt-napari#README.md",
    "first_released": "2022-05-02T22:27:00.024562Z",
    "license": "BSD-3-Clause",
    "name": "yt-napari",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader", "widget"],
    "project_site": "https://github.com/data-exp-lab/yt-napari",
    "python_version": ">=3.7",
    "reader_file_extensions": ["*.json"],
    "release_date": "2022-05-02T22:27:00.024562Z",
    "report_issues": "https://github.com/data-exp-lab/yt-napari/issues",
    "requirements": [
      "magicgui (>=0.4.0)",
      "napari (>=0.4.13)",
      "npe2",
      "numpy",
      "pydantic",
      "yt (>=4.0.1)"
    ],
    "summary": "A napari plugin for loading data from yt",
    "support": "https://github.com/data-exp-lab/yt-napari/issues",
    "twitter": "",
    "version": "0.0.1",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Hanjin Liu" }],
    "code_repository": "https://github.com/hanjinliu/napari-spreadsheet",
    "conda": [],
    "description": "# napari-spreadsheet  [![License BSD-3](https://img.shields.io/pypi/l/napari-spreadsheet.svg?color=green)](https://github.com/hanjinliu/napari-spreadsheet/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-spreadsheet.svg?color=green)](https://pypi.org/project/napari-spreadsheet) [![Python Version](https://img.shields.io/pypi/pyversions/napari-spreadsheet.svg?color=green)](https://python.org) [![tests](https://github.com/hanjinliu/napari-spreadsheet/workflows/tests/badge.svg)](https://github.com/hanjinliu/napari-spreadsheet/actions) [![codecov](https://codecov.io/gh/hanjinliu/napari-spreadsheet/branch/main/graph/badge.svg)](https://codecov.io/gh/hanjinliu/napari-spreadsheet) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-spreadsheet)](https://napari-hub.org/plugins/napari-spreadsheet)  Let's replace Microsoft Excel or Google Spreadsheet with `napari-spreadsheet` for your daily image analysis.  ### Highlights  - Convert layer features to a spreadsheet. - Update layer features from a spreadsheet. - Send spreadsheet data to the namespace of napari's console directly.  ![](images/image.png)  This plugin is largely dependent on [tabulous](https://github.com/hanjinliu/tabulous). You can browse all the key-combos by pressing `Ctrl+K` &rarr; `Shift+?`.  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/stable/plugins/index.html -->  ## Installation  You can install `napari-spreadsheet` via [pip]:      pip install napari-spreadsheet    To install latest development version :      pip install git+https://github.com/hanjinliu/napari-spreadsheet.git   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-spreadsheet\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/hanjinliu/napari-spreadsheet/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-spreadsheet       Let's replace Microsoft Excel or Google Spreadsheet with napari-spreadsheet for your daily image analysis. Highlights  Convert layer features to a spreadsheet. Update layer features from a spreadsheet. Send spreadsheet data to the namespace of napari's console directly.   This plugin is largely dependent on tabulous. You can browse all the key-combos by pressing Ctrl+K → Shift+?.  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install napari-spreadsheet via pip: pip install napari-spreadsheet  To install latest development version : pip install git+https://github.com/hanjinliu/napari-spreadsheet.git  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-spreadsheet\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "Spreadsheet",
    "documentation": "https://github.com/hanjinliu/napari-spreadsheet#README.md",
    "first_released": "2022-08-28T02:06:43.203186Z",
    "license": "BSD-3-Clause",
    "name": "napari-spreadsheet",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader", "widget"],
    "project_site": "https://github.com/hanjinliu/napari-spreadsheet",
    "python_version": ">=3.8",
    "reader_file_extensions": ["*.txt", "*.xlsx", "*.dat", "*.csv"],
    "release_date": "2022-08-28T02:06:43.203186Z",
    "report_issues": "https://github.com/hanjinliu/napari-spreadsheet/issues",
    "requirements": [
      "magicgui",
      "numpy",
      "pandas",
      "qtpy",
      "tabulous (>=0.1.2)",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "tox ; extra == 'testing'"
    ],
    "summary": "A spreadsheet widget for napari",
    "support": "https://github.com/hanjinliu/napari-spreadsheet/issues",
    "twitter": "",
    "version": "0.0.1",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      { "email": "liuhanjin-sc@g.ecc.u-tokyo.ac.jp", "name": "Hanjin Liu" }
    ],
    "code_repository": "https://github.com/hanjinliu/napari-text-layer",
    "description": "# napari-text-layer  Napari text layer for bio-image annotation.  ![](https://github.com/hanjinliu/napari-text-layer/raw/main/GIFs/annot.gif)  ![](https://github.com/hanjinliu/napari-text-layer/raw/main/GIFs/age.gif)  ### Installation  You can install using pip:  ``` pip install napari-text-layer ```  ### Keybindings and mouse callbacks  - \"&rarr;\", \"&larr;\", \"&uarr;\", \"&darr;\" ... Move selected shapes by 1 pixel. - \"F2\" ... Enter edit mode at the selected shape (or the last one if no shape is selected). - \"Enter\" ... Finish edit mode or add a new shape at the same interval. - \"Ctrl\" + \"Shift\" + \"<\" or \">\" ... Change font size. - double click ... Enter edit mode at the clicked shape.   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-text-layer Napari text layer for bio-image annotation.   Installation You can install using pip: pip install napari-text-layer Keybindings and mouse callbacks  \"→\", \"←\", \"↑\", \"↓\" ... Move selected shapes by 1 pixel. \"F2\" ... Enter edit mode at the selected shape (or the last one if no shape is selected). \"Enter\" ... Finish edit mode or add a new shape at the same interval. \"Ctrl\" + \"Shift\" + \"<\" or \">\" ... Change font size. double click ... Enter edit mode at the clicked shape. ",
    "development_status": [],
    "display_name": "napari-text-layer",
    "documentation": "",
    "first_released": "2021-10-30T02:46:10.282131Z",
    "license": "BSD-3-Clause",
    "name": "napari-text-layer",
    "npe2": false,
    "operating_system": [],
    "plugin_types": ["widget"],
    "project_site": "",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2021-12-13T07:12:56.138020Z",
    "report_issues": "",
    "requirements": null,
    "summary": "Text layer for bio-image annotation.",
    "support": "",
    "twitter": "",
    "version": "0.1.2",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      { "email": "lennart.kowitz@isas.de", "name": "Lennart Kowitz" }
    ],
    "code_repository": "https://github.com/MMV-Lab/vessel-express-napari",
    "description": "# vessel-express-napari  [![License](https://img.shields.io/pypi/l/vessel-express-napari.svg?color=green)](https://github.com/MMV-Lab/vessel-express-napari/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/vessel-express-napari.svg?color=green)](https://pypi.org/project/vessel-express) [![Python Version](https://img.shields.io/pypi/pyversions/vessel-express-napari.svg?color=green)](https://python.org) [![tests](https://github.com/MMV-Lab/vessel-express-napari/workflows/tests/badge.svg)](https://github.com/MMV-Lab/vessel-express-napari/actions) [![codecov](https://codecov.io/gh/MMV-Lab/vessel-express-napari/branch/main/graph/badge.svg)](https://codecov.io/gh/MMV-Lab/vessel-express-napari) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/vessel-express-napari)](https://napari-hub.org/plugins/vessel-express)  A simple plugin for 3D vessel segmentation of LSFM images  This [napari] plugin can be used to optimize the segmentation parameters for the [main VesselExpress software platform](https://github.com/RUB-Bioinf/VesselExpress).  ----------------------------------   ## Installation  The easiest way to install the plugin is to open napari, go to Plugins, then Install/Uninstall plugins. You will be able to find the plugin by name \"vessel-express-napari\".   Or, you can install `vessel-express-napari` via [pip]:      pip install vessel-express-napari   To install latest development version :      pip install git+https://github.com/MMV-Lab/vessel-express-napari.git   ## Documentation  We provide a [quick start guide] to explain the important pieces of this plugin. Suggestions and feature quests are very welcomed.    ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"vessel-express-napari\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/MMV-Lab/vessel-express-napari/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ [quick start guide]: https://github.com/MMV-Lab/vessel-express-napari/quick_start.md ",
    "description_content_type": "text/markdown",
    "description_text": "vessel-express-napari       A simple plugin for 3D vessel segmentation of LSFM images This napari plugin can be used to optimize the segmentation parameters for the main VesselExpress software platform.  Installation The easiest way to install the plugin is to open napari, go to Plugins, then Install/Uninstall plugins. You will be able to find the plugin by name \"vessel-express-napari\".  Or, you can install vessel-express-napari via pip: pip install vessel-express-napari  To install latest development version : pip install git+https://github.com/MMV-Lab/vessel-express-napari.git  Documentation We provide a quick start guide to explain the important pieces of this plugin. Suggestions and feature quests are very welcomed.  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"vessel-express-napari\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "vessel-express-napari",
    "documentation": "https://github.com/MMV-Lab/vessel-express-napari#README.md",
    "first_released": "2022-05-17T17:42:39.105873Z",
    "license": "BSD-3-Clause",
    "name": "vessel-express-napari",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader", "widget"],
    "project_site": "https://github.com/MMV-Lab/vessel-express-napari",
    "python_version": ">=3.7",
    "reader_file_extensions": ["*"],
    "release_date": "2022-05-17T17:42:39.105873Z",
    "report_issues": "https://github.com/MMV-Lab/vessel-express-napari/issues",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "itk",
      "scikit-image",
      "aicssegmentation"
    ],
    "summary": "A simple plugin for 3D vessel segmentation",
    "support": "https://github.com/MMV-Lab/vessel-express-napari/issues",
    "twitter": "",
    "version": "0.0.8",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      { "name": "Abigail McGovern" },
      { "name": "Juan Nunez-Iglesias" }
    ],
    "category": { "Workflow step": ["Image Segmentation"] },
    "category_hierarchy": { "Workflow step": [["Image Segmentation"]] },
    "code_repository": "https://github.com/jni/zarpaint",
    "conda": [],
    "description": "# zarpaint  [![License](https://img.shields.io/pypi/l/zarpaint.svg?color=green)](https://github.com/jni/zarpaint/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/zarpaint.svg?color=green)](https://pypi.org/project/zarpaint) [![Python Version](https://img.shields.io/pypi/pyversions/zarpaint.svg?color=green)](https://python.org) [![tests](https://github.com/jni/zarpaint/workflows/tests/badge.svg)](https://github.com/jni/zarpaint/actions) [![codecov](https://codecov.io/gh/jni/zarpaint/branch/main/graph/badge.svg)](https://codecov.io/gh/jni/zarpaint)  Paint segmentations directly to on-disk/remote zarr arrays  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/docs/plugins/index.html -->  ## Installation  You can install `zarpaint` via [pip]:      pip install zarpaint  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"zarpaint\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin [file an issue]: https://github.com/jni/zarpaint/issues [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/",
    "description_content_type": "text/markdown",
    "description_text": "zarpaint      Paint segmentations directly to on-disk/remote zarr arrays  This napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.  Installation You can install zarpaint via pip: pip install zarpaint  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"zarpaint\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "zarpaint",
    "documentation": "",
    "first_released": "2021-06-17T00:46:55.252216Z",
    "license": "BSD-3-Clause",
    "name": "zarpaint",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader", "widget"],
    "project_site": "https://github.com/jni/zarpaint",
    "python_version": ">=3.7",
    "reader_file_extensions": ["*"],
    "release_date": "2021-08-10T05:40:33.725904Z",
    "report_issues": "",
    "requirements": null,
    "summary": "Paint segmentations directly to on-disk/remote zarr arrays",
    "support": "",
    "twitter": "",
    "version": "0.2.0",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Niklas Netter" }],
    "code_repository": "https://github.com/gatoniel/napari-timeseries-opener-plugin",
    "conda": [
      { "channel": "conda-forge", "package": "napari-timeseries-opener-plugin" }
    ],
    "description": "# napari-timeseries-opener-plugin  [![License](https://img.shields.io/pypi/l/napari-timeseries-opener-plugin.svg?color=green)](https://github.com/gatoniel/napari-timeseries-opener-plugin/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-timeseries-opener-plugin.svg?color=green)](https://pypi.org/project/napari-timeseries-opener-plugin) [![Python Version](https://img.shields.io/pypi/pyversions/napari-timeseries-opener-plugin.svg?color=green)](https://python.org) [![tests](https://github.com/gatoniel/napari-timeseries-opener-plugin/workflows/tests/badge.svg)](https://github.com/gatoniel/napari-timeseries-opener-plugin/actions) [![codecov](https://codecov.io/gh/gatoniel/napari-timeseries-opener-plugin/branch/main/graph/badge.svg)](https://codecov.io/gh/gatoniel/napari-timeseries-opener-plugin) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-timeseries-opener-plugin)](https://napari-hub.org/plugins/napari-timeseries-opener-plugin)  Simple plugin that opens separate .tif files as a 3-dimensional layer.  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/plugins/stable/index.html -->  ## Run  In powershell run when you do not have sufficient GPU support in your environment ``` $env:CUDA_VISIBLE_DEVICES=-1; napari ```  ## Installation  You can install `napari-timeseries-opener-plugin` via [pip]:      pip install napari-timeseries-opener-plugin    To install latest development version :      pip install git+https://github.com/gatoniel/napari-timeseries-opener-plugin.git   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-timeseries-opener-plugin\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/gatoniel/napari-timeseries-opener-plugin/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-timeseries-opener-plugin       Simple plugin that opens separate .tif files as a 3-dimensional layer.  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Run In powershell run when you do not have sufficient GPU support in your environment $env:CUDA_VISIBLE_DEVICES=-1; napari Installation You can install napari-timeseries-opener-plugin via pip: pip install napari-timeseries-opener-plugin  To install latest development version : pip install git+https://github.com/gatoniel/napari-timeseries-opener-plugin.git  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-timeseries-opener-plugin\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari-timeseries-opener-plugin",
    "documentation": "https://github.com/gatoniel/napari-timeseries-opener-plugin#README.md",
    "first_released": "2022-02-28T09:50:37.838408Z",
    "license": "BSD-3-Clause",
    "name": "napari-timeseries-opener-plugin",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/gatoniel/napari-timeseries-opener-plugin",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-10-04T13:28:17.361703Z",
    "report_issues": "https://github.com/gatoniel/napari-timeseries-opener-plugin/issues",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "qtpy",
      "magicgui",
      "tifffile",
      "stardist",
      "tensorflow"
    ],
    "summary": "Simple plugin that opens separate .tif files as a 3-dimensional layer.",
    "support": "https://github.com/gatoniel/napari-timeseries-opener-plugin/issues",
    "twitter": "",
    "version": "0.1.8",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Nicolas Pielawski" }],
    "code_repository": null,
    "conda": [{ "channel": "conda-forge", "package": "napari-tissuumaps" }],
    "description": "# 🏝 napari-tissuumaps 🧫  [![License MIT](https://img.shields.io/pypi/l/napari-tissuumaps.svg?color=green)](https://github.com/npielawski/napari-tissuumaps/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-tissuumaps.svg?color=green)](https://pypi.org/project/napari-tissuumaps) [![Python Version](https://img.shields.io/pypi/pyversions/napari-tissuumaps.svg?color=green)](https://python.org) [![tests](https://github.com/npielawski/napari-tissuumaps/workflows/tests/badge.svg)](https://github.com/npielawski/napari-tissuumaps/actions) [![codecov](https://codecov.io/gh/npielawski/napari-tissuumaps/branch/main/graph/badge.svg)](https://codecov.io/gh/npielawski/napari-tissuumaps) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-tissuumaps)](https://napari-hub.org/plugins/napari-tissuumaps)  A plugin to export Napari projects to [TissUUmaps](https://tissuumaps.research.it.uu.se/).  ----------------------------------  This plugins adds a new writer to [Napari] to export projects to [TissUUmaps](https://github.com/TissUUmaps/TissUUmaps). Exported projects can then be open on the browser or on a standalone GUI with [TissUUmaps](https://github.com/TissUUmaps/TissUUmaps). More information and demonstrations are available on the [TissUUmaps webpage](https://tissuumaps.research.it.uu.se/).  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/stable/plugins/index.html -->  ## 🚀 Features  <p align=\"center\">   <img src=\"images/screenshot.jpg\" alt=\"Demonstration of a project exported from Napari to TissUUmaps.\" width=\"500\" /> </p>  The plugin now supports:  * Exporting images * Exporting labels * Exporting points * Exporting shapes, including:     * Polygons     * Rectangles     * Lines     * Paths     * Ellipses  The plugin exports the right color for the points, shapes and labels and also saves the visibility/opacity of each layers. The shapes are exported in the GeoJSON format, the points in CSV files, and images as TIFFs.  ## 📺 Installation  You can install `napari-tissuumaps` via [pip]:      pip install napari-tissuumaps  You can also install `napari-tissumaps` via [napari]:  In Napari, access the menubar, Plugins > Install/Uninstall Plugins. Search for napari-tissuumaps in the list and choose install, or type `napari-tissuumaps` in the \"install by name/url, or drop file...\" text area and choose install.  ## ⛏ Usage  To export a project for TissUUmaps, access the menubar, File > Save All Layers... and type in a filename. Choose the `.tmap` extension in the dropdown and click on the Save button, It will create a folder containing all the necessary files for TissUUmaps.  ## 👩‍💻 Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## ⚖️ License  Distributed under the terms of the [MIT] license, \"napari-tissuumaps\" is free and open source software  ## 🚒 Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "🏝 napari-tissuumaps 🧫       A plugin to export Napari projects to TissUUmaps.  This plugins adds a new writer to Napari to export projects to TissUUmaps. Exported projects can then be open on the browser or on a standalone GUI with TissUUmaps. More information and demonstrations are available on the TissUUmaps webpage.  🚀 Features    The plugin now supports:  Exporting images Exporting labels Exporting points Exporting shapes, including: Polygons Rectangles Lines Paths Ellipses    The plugin exports the right color for the points, shapes and labels and also saves the visibility/opacity of each layers. The shapes are exported in the GeoJSON format, the points in CSV files, and images as TIFFs. 📺 Installation You can install napari-tissuumaps via pip: pip install napari-tissuumaps  You can also install napari-tissumaps via napari: In Napari, access the menubar, Plugins > Install/Uninstall Plugins. Search for napari-tissuumaps in the list and choose install, or type napari-tissuumaps in the \"install by name/url, or drop file...\" text area and choose install. ⛏ Usage To export a project for TissUUmaps, access the menubar, File > Save All Layers... and type in a filename. Choose the .tmap extension in the dropdown and click on the Save button, It will create a folder containing all the necessary files for TissUUmaps. 👩‍💻 Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. ⚖️ License Distributed under the terms of the MIT license, \"napari-tissuumaps\" is free and open source software 🚒 Issues If you encounter any problems, please [file an issue] along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "Napari TissUUmaps",
    "documentation": "",
    "first_released": "2021-09-02T12:25:02.194075Z",
    "license": "MIT",
    "name": "napari-tissuumaps",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["writer"],
    "project_site": "",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-08-05T09:38:47.546257Z",
    "report_issues": "",
    "requirements": [
      "numpy",
      "napari ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "tox ; extra == 'testing'"
    ],
    "summary": "A plugin to export Napari projects to TissUUmaps.",
    "support": "",
    "twitter": "",
    "version": "1.1.2",
    "writer_file_extensions": [".tmap"],
    "writer_save_layers": ["shapes*", "labels*", "points*", "image*"]
  },
  {
    "authors": [
      { "name": "Rocco D'Antuono", "orcid": "0000-0003-0180-6500" },
      { "name": "Giuseppina Pisignano", "orcid": "0000-0001-7476-3447" }
    ],
    "category": {},
    "category_hierarchy": {},
    "code_repository": "https://github.com/RoccoDAnt/napari-zelda",
    "description": "# napari-zelda  [![License](https://img.shields.io/pypi/l/napari-zelda.svg?color=green)](https://github.com/RoccoDAnt/napari-zelda/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-zelda.svg?color=green)](https://pypi.org/project/napari-zelda) [![Python Version](https://img.shields.io/pypi/pyversions/napari-zelda.svg?color=green)](https://python.org) [![tests](https://github.com/RoccoDAnt/napari-zelda/workflows/tests/badge.svg)](https://github.com/RoccoDAnt/napari-zelda/actions) [![codecov](https://codecov.io/gh/RoccoDAnt/napari-zelda/branch/master/graph/badge.svg)](https://codecov.io/gh/RoccoDAnt/napari-zelda)  ## ZELDA: a 3D Image Segmentation and Parent-Child relation plugin for microscopy image analysis in napari ##### Authors: Rocco D'Antuono, Giuseppina Pisignano ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/docs/plugins/index.html -->  ## What you can do with ZELDA plugin for napari 1. Segment objects such as cells and organelles in 2D/3D.  2. Segment two populations in 2D/3D (e.g. cells and organelles, nuclei and nuclear spots, tissue structures and cells) establishing the \"Parent-Child\" relation: count how many mitochondria are contained in each cell, how many spots localize in every nucleus, how many cells are within a tissue compartment.    Example: cell cytoplasms (parent objects) and mitochondria (child objects)   ![](https://raw.githubusercontent.com/RoccoDAnt/napari-zelda/main/docs/2D-AF488.png) <br> **Actin** | ![](https://raw.githubusercontent.com/RoccoDAnt/napari-zelda/main/docs/2D-MT.png) <br> **Mitochondria**| ![](https://raw.githubusercontent.com/RoccoDAnt/napari-zelda/main/docs/2D-AF488_MT.png) <br> **Merge**   ------ | ------| -----   ![](https://raw.githubusercontent.com/RoccoDAnt/napari-zelda/main/docs/2D-AF488_parents.png) <br> **Parent cell cytoplasms** | ![](https://raw.githubusercontent.com/RoccoDAnt/napari-zelda/main/docs/2D-MT_children.png) <br> **Children mitochondria**| ![](https://raw.githubusercontent.com/RoccoDAnt/napari-zelda/main/docs/2D-MT_childrenbyParent.png) <br> **Children labelled by Parents**   3. Plot results within napari interface.      ![](https://raw.githubusercontent.com/RoccoDAnt/napari-zelda/main/docs/Plot_hist_Area.png) <br> **Histogram** | ![](https://raw.githubusercontent.com/RoccoDAnt/napari-zelda/main/docs/Plot_scatter_Area-EqDiam.png) <br> **Scatterplot**|     ------ | ------|  4. Customize an image analysis workflow in graphical mode (no scripting knowledge required).      | ![](https://raw.githubusercontent.com/RoccoDAnt/napari-zelda/main/docs/CustomProtocol.png) <br> **Custom image analysis workflow** |     ------ |  5. Import and Export Protocols (image analysis workflows) in graphical mode (share with the community!).      | ![](https://raw.githubusercontent.com/RoccoDAnt/napari-zelda/main/docs/Clipboard_Import_and_Export_Protocols.png) <br> **Import and Export of ZELDA Protocols** |     ------ |  ## Installation  **Option A.** You can install `napari-zelda` via [pip]. For the best experience, create a conda environment and use napari!=0.4.11, using the following instructions:      conda create -y -n napari-env python==3.8       conda activate napari-env       pip install \"napari[all]\"       pip install napari-zelda     **Option B.** Alternatively, clone the repository and install locally via [pip]:      pip install -e .  **Option C.** Another option is to use the napari interface to install it (make sure napari!=0.4.11): 1. Plugins / Install/Uninstall Package(s) ![](https://raw.githubusercontent.com/RoccoDAnt/napari-zelda/main/docs/Clipboard_ZELDA_Plugin_install_in_napari.png)  2. Choose ZELDA ![](https://raw.githubusercontent.com/RoccoDAnt/napari-zelda/main/docs/Clipboard_ZELDA_Plugin_install_ZELDA_in_napari_Arrow.png)  3. ZELDA is installed ![](https://raw.githubusercontent.com/RoccoDAnt/napari-zelda/main/docs/Clipboard_ZELDA_Plugin_installed_ZELDA_in_napari_Arrow.png)  4. Launch ZELDA ![](https://raw.githubusercontent.com/RoccoDAnt/napari-zelda/main/docs/Clipboard_ZELDA_Launch_ZELDA.png)   ## Contributing  Contributions are welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-zelda\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-zelda      ZELDA: a 3D Image Segmentation and Parent-Child relation plugin for microscopy image analysis in napari Authors: Rocco D'Antuono, Giuseppina Pisignano  This napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.  What you can do with ZELDA plugin for napari   Segment objects such as cells and organelles in 2D/3D.   Segment two populations in 2D/3D (e.g. cells and organelles, nuclei and nuclear spots, tissue structures and cells) establishing the \"Parent-Child\" relation: count how many mitochondria are contained in each cell, how many spots localize in every nucleus, how many cells are within a tissue compartment.   Example: cell cytoplasms (parent objects) and mitochondria (child objects)     Actin |   Mitochondria|   Merge   ------ | ------| -----     Parent cell cytoplasms |   Children mitochondria|   Children labelled by Parents   Plot results within napari interface.   Histogram |   Scatterplot| ------ | ------|   Customize an image analysis workflow in graphical mode (no scripting knowledge required). |   Custom image analysis workflow | ------ |   Import and Export Protocols (image analysis workflows) in graphical mode (share with the community!). |   Import and Export of ZELDA Protocols | ------ |   Installation Option A. You can install napari-zelda via pip. For the best experience, create a conda environment and use napari!=0.4.11, using the following instructions: conda create -y -n napari-env python==3.8   conda activate napari-env   pip install \"napari[all]\"   pip install napari-zelda  Option B. Alternatively, clone the repository and install locally via pip: pip install -e .  Option C. Another option is to use the napari interface to install it (make sure napari!=0.4.11): 1. Plugins / Install/Uninstall Package(s)    Choose ZELDA    ZELDA is installed    Launch ZELDA    Contributing Contributions are welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-zelda\" is free and open source software Issues If you encounter any problems, please [file an issue] along with a detailed description.",
    "development_status": ["Development Status :: 3 - Alpha"],
    "display_name": "napari-zelda",
    "documentation": "",
    "first_released": "2021-10-17T18:55:43.014476Z",
    "license": "BSD-3-Clause",
    "name": "napari-zelda",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/RoccoDAnt/napari-zelda",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-01-26T15:44:51.608705Z",
    "report_issues": "",
    "requirements": null,
    "summary": "ZELDA: a 3D Image Segmentation and Parent-Child relation plugin for microscopy image analysis in napari",
    "support": "",
    "twitter": "",
    "version": "0.1.10",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      {
        "email": "ddoncilapop@contractor.chanzuckerberg.com",
        "name": "Draga Doncila Pop"
      }
    ],
    "citations": {
      "APA": "Doncila Pop D. (2021). napari Accelerator Grant Demo Plugin (version 2.0.4). DOI: 10.5281/zenodo.FAKE ",
      "BibTex": "@misc{YourReferenceHere, author = {Doncila Pop, Draga}, doi = {10.5281/zenodo.FAKE}, month = {12}, title = {napari Accelerator Grant Demo Plugin}, year = {2021} } ",
      "RIS": "TY  - GEN AU  - Doncila Pop, Draga DA  - 2021-12-15 DO  - 10.5281/zenodo.FAKE PY  - 2021 TI  - napari Accelerator Grant Demo Plugin ER ",
      "citation": " cff-version: 1.2.0 message: \"Thank you for citing our plugin!\" authors:   - family-names: Doncila Pop     given-names: Draga title: \"napari Accelerator Grant Demo Plugin\" version: 2.0.4 doi: 10.5281/zenodo.FAKE date-released: 2021-12-15 "
    },
    "code_repository": "https://github.com/DragaDoncila/workshop-demo",
    "description": "## What is this?  This plugin was created to serve as a semi-meaningful example of a plugin using the new napari [npe2](https://pypi.org/project/npe2/) architecture.  It provides a reader, a writer and two dock widgets to support opening, processing and writing out [cell tracking challenge](https://celltrackingchallenge.net/) data.  We've provided comments and example tests that can be used as a reference when building your own plugin.  ## Using this plugin  ### Sample Data You can download sample data for this plugin from the tracking challenge website. Any 2D+T sequence should work, but this plugin has been tested only with the  [Human hepatocarcinoma-derived cells expressing the fusion protein YFP-TIA-1](http://data.celltrackingchallenge.net/training-datasets/Fluo-C2DL-Huh7.zip)  dataset. ### Reading Data This plugin's reader is designed for tracking challenge segmentation gold standard ground truth data conforming to the file format described in the [data specification](https://public.celltrackingchallenge.net/documents/Naming%20and%20file%20content%20conventions.pdf).  Ground truth data is only provided for a subset of the frames of the entire sequence. This reader will attempt to find the number of frames of the associated sequence in a sister directory of the ground truth data directory and open a labels layer with the same number of frames, thus ensuring the labelled data is correctly overlaid onto the original sequence.  https://user-images.githubusercontent.com/17995243/146114062-36124c05-f44a-488e-8991-f39a702c917f.mov  ### Segmenting Data One of the dock widgets provided by this plugin is \"Segment by Threshold\". The widget allows you to select a 2D+T image layer in the viewer (e.g. any of the sequences in the Human  hepatocarcinoma dataset above) and segment it using a selection of scikit-image thresholding functions.  The segmentation is then returned as a `Labels` layer into the viewer.  https://user-images.githubusercontent.com/17995243/146114088-f6fb645e-8d78-4880-827b-2f0334dad859.mov  ### Highlighting Segmentation Differences The second dock widget provided by this plugin allows you to visually compare your segmentation against the ground truth data by computing the difference between the two and adding this as a layer in the napari viewer.  To use this widget, open it from the Plugins menu and select the two layers you wish to compare.  https://user-images.githubusercontent.com/17995243/146114112-c891723f-8640-4708-8014-c78731fb3396.mov  ### Writing to Zip Finally, you can save your segmentation to a zip file whose internal directory structure will closely mimic that of the tracking challenge datasets, so that it may be opened  again in the viewer.  To save your layer, choose File -> Save selected layer(s) with *one* labels layer selected, then select label zipper from the dropdown choices.  https://user-images.githubusercontent.com/17995243/146114163-ee886990-979c-4756-97c5-aaf2c39dccde.mov ",
    "description_content_type": "text/markdown",
    "description_text": "What is this? This plugin was created to serve as a semi-meaningful example of a plugin using the new napari npe2 architecture. It provides a reader, a writer and two dock widgets to support opening, processing and writing out cell tracking challenge data. We've provided comments and example tests that can be used as a reference when building your own plugin. Using this plugin Sample Data You can download sample data for this plugin from the tracking challenge website. Any 2D+T sequence should work, but this plugin has been tested only with the  Human hepatocarcinoma-derived cells expressing the fusion protein YFP-TIA-1  dataset. Reading Data This plugin's reader is designed for tracking challenge segmentation gold standard ground truth data conforming to the file format described in the data specification. Ground truth data is only provided for a subset of the frames of the entire sequence. This reader will attempt to find the number of frames of the associated sequence in a sister directory of the ground truth data directory and open a labels layer with the same number of frames, thus ensuring the labelled data is correctly overlaid onto the original sequence. https://user-images.githubusercontent.com/17995243/146114062-36124c05-f44a-488e-8991-f39a702c917f.mov Segmenting Data One of the dock widgets provided by this plugin is \"Segment by Threshold\". The widget allows you to select a 2D+T image layer in the viewer (e.g. any of the sequences in the Human  hepatocarcinoma dataset above) and segment it using a selection of scikit-image thresholding functions. The segmentation is then returned as a Labels layer into the viewer. https://user-images.githubusercontent.com/17995243/146114088-f6fb645e-8d78-4880-827b-2f0334dad859.mov Highlighting Segmentation Differences The second dock widget provided by this plugin allows you to visually compare your segmentation against the ground truth data by computing the difference between the two and adding this as a layer in the napari viewer. To use this widget, open it from the Plugins menu and select the two layers you wish to compare. https://user-images.githubusercontent.com/17995243/146114112-c891723f-8640-4708-8014-c78731fb3396.mov Writing to Zip Finally, you can save your segmentation to a zip file whose internal directory structure will closely mimic that of the tracking challenge datasets, so that it may be opened  again in the viewer. To save your layer, choose File -> Save selected layer(s) with one labels layer selected, then select label zipper from the dropdown choices. https://user-images.githubusercontent.com/17995243/146114163-ee886990-979c-4756-97c5-aaf2c39dccde.mov",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "workshop demo",
    "documentation": "https://github.com/DragaDoncila/workshop-demo#README.md",
    "first_released": "2021-12-15T04:03:25.373260Z",
    "license": "BSD-3-Clause",
    "name": "workshop-demo",
    "npe2": true,
    "operating_system": [
      "Operating System :: MacOS :: MacOS X",
      "Operating System :: POSIX :: Linux"
    ],
    "plugin_types": ["reader", "writer", "widget"],
    "project_site": "https://github.com/DragaDoncila/workshop-demo",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-03-10T23:52:12.666134Z",
    "report_issues": "https://github.com/DragaDoncila/workshop-demo/issues",
    "requirements": [
      "dask[array]",
      "imagecodecs",
      "napari",
      "napari-plugin-engine (>=0.1.4)",
      "npe2",
      "numpy",
      "scikit-image",
      "tifffile"
    ],
    "summary": "A demo napari plugin incorporating reader, writer and dock widget contributions using the new npe2 plugin architecture.",
    "support": "https://github.com/DragaDoncila/workshop-demo/issues",
    "twitter": "",
    "version": "0.0.2",
    "visibility": "public",
    "writer_file_extensions": [".zip"],
    "writer_save_layers": ["labels*"]
  },
  {
    "authors": [{ "name": "Jan Clemens" }],
    "category": { "Supported data": ["Time series"] },
    "category_hierarchy": { "Supported data": [["Time series"]] },
    "code_repository": "https://github.com/janclemenslab/napari-video",
    "conda": [],
    "description": "# napari-video [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari_video)](https://napari-hub.org/plugins/napari_video)  Napari plugin for working with videos.  Relies on [pyvideoreader](https://pypi.org/project/pyvideoreader/) as a backend which itself uses [opencv](https://opencv.org) for reading videos.  ## Installation ```shell pip install napari[all] napari_video ```  ## Usage From a terminal: ```shell napari video.avi ```  Or from within python: ```shell import napari from napari_video.napari_video import VideoReaderNP  path='video.mp4' vr = VideoReaderNP(path) with napari.gui_qt():     viewer = napari.view_image(vr, name=path) ```  ## Internals `napari_video.napari_video.VideoReaderNP` exposes a video with a numpy-like interface, using opencv as a backend.  For instance, open a video: ```python vr = VideoReaderNP('video.avi') print(vr) ``` ``` video.avi with 60932 frames of size (920, 912, 3) at 100.00 fps ``` Then  - `vr[100]` will return the 100th frame as a numpy array with shape `(902, 912, 3)`. - `vr[100:200:10]` will return 10 frames evenly spaced between frame number 100 and 200 (shape `(10, 902, 912, 3)`). - Note that by default, single-frame and slice indexing return 3D and 4D arrays, respectively. To consistently return 4D arrays, open the video with `remove_leading_singleton=False`. `vr[100]` will then return a `(1, 902, 912, 3)` array. - We can also request specific ROIs and channels. For instance, `vr[100:200:10,100:400,800:850,1]` will return an array with shape `(10, 300, 50, 1)`.  ",
    "description_content_type": "text/markdown",
    "description_text": "napari-video  Napari plugin for working with videos. Relies on pyvideoreader as a backend which itself uses opencv for reading videos. Installation shell pip install napari[all] napari_video Usage From a terminal: shell napari video.avi Or from within python: ```shell import napari from napari_video.napari_video import VideoReaderNP path='video.mp4' vr = VideoReaderNP(path) with napari.gui_qt():     viewer = napari.view_image(vr, name=path) ``` Internals napari_video.napari_video.VideoReaderNP exposes a video with a numpy-like interface, using opencv as a backend. For instance, open a video: python vr = VideoReaderNP('video.avi') print(vr) video.avi with 60932 frames of size (920, 912, 3) at 100.00 fps Then  vr[100] will return the 100th frame as a numpy array with shape (902, 912, 3). vr[100:200:10] will return 10 frames evenly spaced between frame number 100 and 200 (shape (10, 902, 912, 3)). Note that by default, single-frame and slice indexing return 3D and 4D arrays, respectively. To consistently return 4D arrays, open the video with remove_leading_singleton=False. vr[100] will then return a (1, 902, 912, 3) array. We can also request specific ROIs and channels. For instance, vr[100:200:10,100:400,800:850,1] will return an array with shape (10, 300, 50, 1). ",
    "development_status": [],
    "display_name": "napari_video",
    "documentation": "https://github.com/janclemenslab/napari-video/blob/main/README.md",
    "first_released": "2021-02-27T15:40:14.736684Z",
    "license": "MIT",
    "name": "napari_video",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader"],
    "project_site": "https://github.com/janclemenslab/napari-video",
    "python_version": ">=3.6",
    "reader_file_extensions": ["*"],
    "release_date": "2022-08-23T12:13:46.443797Z",
    "report_issues": "https://github.com/janclemenslab/napari-video/issues",
    "requirements": ["numpy", "pyvideoreader"],
    "summary": "napari plugin for reading videos.",
    "support": "https://github.com/janclemenslab/napari-video/issues",
    "twitter": "",
    "version": "0.2.9",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Richard De Mets" }],
    "code_repository": "https://github.com/rdemets/napari-yolov5",
    "conda": [{ "channel": "conda-forge", "package": "napari-yolov5" }],
    "description": "# napari-yolov5\\r \\r [![License](https://img.shields.io/pypi/l/napari-yolov5.svg?color=green)](https://github.com/rdemets/napari-yolov5/raw/main/LICENSE)\\r [![PyPI](https://img.shields.io/pypi/v/napari-yolov5.svg?color=green)](https://pypi.org/project/napari-yolov5)\\r [![Python Version](https://img.shields.io/pypi/pyversions/napari-yolov5.svg?color=green)](https://python.org)\\r [![tests](https://github.com/rdemets/napari-yolov5/workflows/tests/badge.svg)](https://github.com/rdemets/napari-yolov5/actions)\\r [![codecov](https://codecov.io/gh/rdemets/napari-yolov5/branch/main/graph/badge.svg)](https://codecov.io/gh/rdemets/napari-yolov5)\\r [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-yolov5)](https://napari-hub.org/plugins/napari-yolov5)\\r \\r Plugin adapted from Ultralytics to bring YOLOv5 into Napari. \\r \\r Training and detection can be done using the GUI. Training dataset must be prepared prior to using this plugin. Further development will allow users to use Napari to prepare the dataset. Follow instructions stated on [Ultralytics Github](https://github.com/ultralytics/yolov5) to prepare the dataset.\\r \\r The plugin includes 3 pre-trained networks that are able to identify mitosis stages or apoptosis on soSPIM images. More details can be found on the [pre-print](https://www.biorxiv.org/content/10.1101/2021.03.26.437121v1.full).\\r \\r ----------------------------------\\r \\r This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\r \\r <!--\\r Don't miss the full getting started guide to set up your new package:\\r https://github.com/napari/cookiecutter-napari-plugin#getting-started\\r \\r and review the napari docs for plugin developers:\\r https://napari.org/plugins/stable/index.html\\r -->\\r \\r ## Installation\\r \\r First install conda and create an environment for the plugin\\r ```\\r conda create --prefix env-napari-yolov5 python=3.9\\r conda activate env-napari-yolov5\\r ```\\r You can install `napari-yolov5` and `napari` via [pip]:\\r \\r     pip install napari-yolov5\\r     pip install napari[all]\\r \\r For GPU support :\\r ```\\r pip uninstall torch\\r pip install torchvision==0.10.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html\\r ```\\r \\r ## Usage\\r \\r First select if you would like to train a new network or detect objects.\\r \\r ![alt text](https://github.com/rdemets/napari-yolov5/blob/main/src/napari_yolov5/resources/Readme/1.jpg?raw=true)\\r \\r \\r ***For `Training` :***\\r \\r Data preparation should be done following [Ultralytics'](https://github.com/ultralytics/yolov5) instructions.\\r \\r Select the size of the network, the number of epochs, the number of images per batch to load on the GPU, the size of the images (must be a stride of 32), and the name of the network.\\r \\r ![alt text](https://github.com/rdemets/napari-yolov5/blob/main/src/napari_yolov5/resources/Readme/2.jpg?raw=true)\\r \\r An example of the YAML config file is provided in `src/napari_yolov5/resources` folder.\\r \\r ![alt text](https://github.com/rdemets/napari-yolov5/blob/main/src/napari_yolov5/resources/Readme/3.jpg?raw=true)\\r \\r \\r Progress can be seen on the Terminal or on the right-hand side of the viewer.\\r \\r ![alt text](https://github.com/rdemets/napari-yolov5/blob/main/src/napari_yolov5/resources/Readme/4.jpg?raw=true)\\r \\r \\r ***For `Detection` :***\\r \\r It is possible to perform the detection on a single layer chosen in the list, all the layers opened, or by giving a folder path. For folder detection, all the images will be loaded as a single stack.\\r \\r ![alt text](https://github.com/rdemets/napari-yolov5/blob/main/src/napari_yolov5/resources/Readme/5.jpg?raw=true)\\r \\r Nucleus size of the prediction layer has te be filled to resize the image to the training dataset. Nucleus size of the training dataset will be asked in case of a custom network.\\r \\r Confidence threshold defines the minimum value for a detected object to be considered positive. \\r iou nms threshold (intersection-over-union non-max-suppression) defines the overlapping area of two boxes as a single object. Only the box with the maximum confidence is kept.\\r Progress can be seen on the Terminal.\\r \\r ![alt text](https://github.com/rdemets/napari-yolov5/blob/main/src/napari_yolov5/resources/Readme/6.jpg?raw=true)\\r \\r Few options allow for modification on how the boxes are being displayed (default : box + class + confidence score ; box + class ; box only) and if the box coordinates and the image overlay will be exported.\\r Post-processing option will perform a simple 3D assignment based on 3D connected component analysis. A median filter (1x1x3 XYZ) is applied prior to the assignment. \\r The centroid of each object is then saved into a new point layer as a 3D point with a random color for each class. \\r \\r ![alt text](https://github.com/rdemets/napari-yolov5/blob/main/src/napari_yolov5/resources/Readme/7.jpg?raw=true)\\r \\r The localisation of each centroid is saved and the path is shown in the Terminal at the end of the detection. It is also possible now to define the export folder.\\r \\r ![alt text](https://github.com/rdemets/napari-yolov5/blob/main/src/napari_yolov5/resources/Readme/8.jpg?raw=true)\\r \\r \\r ## Contributing\\r \\r Contributions are very welcome. Tests can be run with [tox], please ensure\\r the coverage at least stays the same before you submit a pull request.\\r \\r ## License\\r \\r Distributed under the terms of the [GNU GPL v3.0] license,\\r \"napari-yolov5\" is free and open source software\\r \\r ## Issues\\r \\r If you encounter any problems, please [file an issue] along with a detailed description.\\r \\r [napari]: https://github.com/napari/napari\\r [Cookiecutter]: https://github.com/audreyr/cookiecutter\\r [@napari]: https://github.com/napari\\r [MIT]: http://opensource.org/licenses/MIT\\r [BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\r [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\r [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\r [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\r [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\r [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\r \\r [napari]: https://github.com/napari/napari\\r [tox]: https://tox.readthedocs.io/en/latest/\\r [pip]: https://pypi.org/project/pip/\\r [PyPI]: https://pypi.org/\\r ",
    "description_content_type": "text/markdown",
    "description_text": "napari-yolov5       Plugin adapted from Ultralytics to bring YOLOv5 into Napari.  Training and detection can be done using the GUI. Training dataset must be prepared prior to using this plugin. Further development will allow users to use Napari to prepare the dataset. Follow instructions stated on Ultralytics Github to prepare the dataset. The plugin includes 3 pre-trained networks that are able to identify mitosis stages or apoptosis on soSPIM images. More details can be found on the pre-print.  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation First install conda and create an environment for the plugin conda create --prefix env-napari-yolov5 python=3.9 conda activate env-napari-yolov5 You can install napari-yolov5 and napari via pip: pip install napari-yolov5 pip install napari[all]  For GPU support : pip uninstall torch pip install torchvision==0.10.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html Usage First select if you would like to train a new network or detect objects.  For Training : Data preparation should be done following Ultralytics' instructions. Select the size of the network, the number of epochs, the number of images per batch to load on the GPU, the size of the images (must be a stride of 32), and the name of the network.  An example of the YAML config file is provided in src/napari_yolov5/resources folder.  Progress can be seen on the Terminal or on the right-hand side of the viewer.  For Detection : It is possible to perform the detection on a single layer chosen in the list, all the layers opened, or by giving a folder path. For folder detection, all the images will be loaded as a single stack.  Nucleus size of the prediction layer has te be filled to resize the image to the training dataset. Nucleus size of the training dataset will be asked in case of a custom network. Confidence threshold defines the minimum value for a detected object to be considered positive.  iou nms threshold (intersection-over-union non-max-suppression) defines the overlapping area of two boxes as a single object. Only the box with the maximum confidence is kept. Progress can be seen on the Terminal.  Few options allow for modification on how the boxes are being displayed (default : box + class + confidence score ; box + class ; box only) and if the box coordinates and the image overlay will be exported. Post-processing option will perform a simple 3D assignment based on 3D connected component analysis. A median filter (1x1x3 XYZ) is applied prior to the assignment.  The centroid of each object is then saved into a new point layer as a 3D point with a random color for each class.   The localisation of each centroid is saved and the path is shown in the Terminal at the end of the detection. It is also possible now to define the export folder.  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the GNU GPL v3.0 license, \"napari-yolov5\" is free and open source software Issues If you encounter any problems, please [file an issue] along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari-yolov5",
    "documentation": "",
    "first_released": "2021-12-29T06:54:55.003736Z",
    "license": "GPL-3.0-only",
    "name": "napari-yolov5",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/rdemets/napari-yolov5",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-10-26T11:56:51.398591Z",
    "report_issues": "",
    "requirements": null,
    "summary": "Plugin adapted from Ultralytics to bring YOLOv5 into Napari",
    "support": "",
    "twitter": "",
    "version": "0.2.14",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "email": "tdmorello@gmail.com", "name": "Tim Morello" }],
    "code_repository": "https://github.com/tdmorello/napari-tiler",
    "description": "# napari-tiler  [![License](https://img.shields.io/pypi/l/napari-tiler.svg?color=green)](https://github.com/tdmorello/napari-tiler/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-tiler.svg?color=green)](https://pypi.org/project/napari-tiler) [![Python Version](https://img.shields.io/pypi/pyversions/napari-tiler.svg?color=green)](https://python.org) [![tests](https://github.com/tdmorello/napari-tiler/workflows/tests/badge.svg)](https://github.com/tdmorello/napari-tiler/actions) [![codecov](https://codecov.io/gh/tdmorello/napari-tiler/branch/main/graph/badge.svg)](https://codecov.io/gh/tdmorello/napari-tiler) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-tiler)](https://napari-hub.org/plugins/napari-tiler) [![PyPI - Downloads](https://img.shields.io/pypi/dm/napari-tiler.svg)](https://pypistats.org/packages/napari-tiler) [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/python/black) [![Development Status](https://img.shields.io/pypi/status/napari-tiler.svg)](https://github.com/tdmorello/napari-tiler)  N-dimensional tiling and merging support for napari  This plugin allows the user to split an image into a stack of tiles and subsequently merge the tiles to reconstruct the orignal image. See [Tiler](https://pypi.org/project/tiler/) by [@thelay](https://github.com/the-lay) for more details.  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/plugins/stable/index.html -->  ## Installation  ### Option 1 (recommended)  You can install `napari-tiler` from the napari plugin manager. Go to `Plugins -> Install/Uninstall Package(s)`, then search for `napari-tiler`. Click `Install`.  ### Option 2  You can also install `napari-tiler` via [pip]:      pip install napari-tiler  To install latest development version:      pip install git+https://github.com/tdmorello/napari-tiler.git  ## Quick Start  1. Open a file in napari. The file may have any number of dimensions (e.g. z-stack, time series, ...) 2. Start the plugin ( `Plugins -> napari-tiler: make_tiles` ) 3. Select the input layer from the dropdown box 4. Select parameters for tiling 5. Click `Run`  ## Contributing  This project uses [Poetry](https://github.com/python-poetry/poetry) for dependency management. To set up the development environment, it is recommended to use:      conda env create -f environment.yaml  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-tiler\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/tdmorello/napari-tiler/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-tiler          N-dimensional tiling and merging support for napari This plugin allows the user to split an image into a stack of tiles and subsequently merge the tiles to reconstruct the orignal image. See Tiler by @thelay for more details.  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation Option 1 (recommended) You can install napari-tiler from the napari plugin manager. Go to Plugins -> Install/Uninstall Package(s), then search for napari-tiler. Click Install. Option 2 You can also install napari-tiler via pip: pip install napari-tiler  To install latest development version: pip install git+https://github.com/tdmorello/napari-tiler.git  Quick Start  Open a file in napari. The file may have any number of dimensions (e.g. z-stack, time series, ...) Start the plugin ( Plugins -> napari-tiler: make_tiles ) Select the input layer from the dropdown box Select parameters for tiling Click Run  Contributing This project uses Poetry for dependency management. To set up the development environment, it is recommended to use: conda env create -f environment.yaml  Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-tiler\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari-tiler",
    "documentation": "https://github.com/tdmorello/napari-tiler#README.md",
    "first_released": "2021-12-13T18:14:50.932526Z",
    "license": "BSD-3-Clause",
    "name": "napari-tiler",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "",
    "python_version": ">=3.7,<3.11",
    "reader_file_extensions": [],
    "release_date": "2021-12-29T17:10:15.480731Z",
    "report_issues": "https://github.com/tdmorello/napari-tiler/issues",
    "requirements": [
      "importlib-metadata (<4.3); python_version < \"3.8\"",
      "napari-plugin-engine (>=0.2.0,<0.3.0)",
      "numpy (>=1.21.4,<2.0.0)",
      "tiler (>=0.4.1,<0.5.0)",
      "napari-tools-menu (>=0.1.7,<0.2.0)"
    ],
    "summary": "N-dimensional tiling and merging support for napari",
    "support": "https://github.com/tdmorello/napari-tiler/issues",
    "twitter": "",
    "version": "0.0.9",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Varun Kapoor" }],
    "code_repository": "https://github.com/Kapoorlabs-CAPED/vollseg-napari-mtrack",
    "description": "# vollseg-napari-mtrack  [![License BSD-3](https://img.shields.io/pypi/l/vollseg-napari-mtrack.svg?color=green)](https://github.com/Kapoorlabs-CAPED/vollseg-napari-mtrack/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/vollseg-napari-mtrack.svg?color=green)](https://pypi.org/project/vollseg-napari-mtrack) [![Python Version](https://img.shields.io/pypi/pyversions/vollseg-napari-mtrack.svg?color=green)](https://python.org) [![tests](https://github.com/Kapoorlabs-CAPED/vollseg-napari-mtrack/workflows/tests/badge.svg)](https://github.com/Kapoorlabs-CAPED/vollseg-napari-mtrack/actions) [![codecov](https://codecov.io/gh/Kapoorlabs-CAPED/vollseg-napari-mtrack/branch/main/graph/badge.svg)](https://codecov.io/gh/Kapoorlabs-CAPED/vollseg-napari-mtrack) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/vollseg-napari-mtrack)](https://napari-hub.org/plugins/vollseg-napari-mtrack)  Segment kymographs of microtubules, actin filaments and perform Ransac based fits to compute dynamic instability parameters for individual kymographs and also in batch  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/stable/plugins/index.html -->  ## Installation  You can install `vollseg-napari-mtrack` via [pip]:      pip install vollseg-napari-mtrack    To install latest development version :      pip install git+https://github.com/Kapoorlabs-CAPED/vollseg-napari-mtrack.git   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"vollseg-napari-mtrack\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/Kapoorlabs-CAPED/vollseg-napari-mtrack/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "vollseg-napari-mtrack       Segment kymographs of microtubules, actin filaments and perform Ransac based fits to compute dynamic instability parameters for individual kymographs and also in batch  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install vollseg-napari-mtrack via pip: pip install vollseg-napari-mtrack  To install latest development version : pip install git+https://github.com/Kapoorlabs-CAPED/vollseg-napari-mtrack.git  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"vollseg-napari-mtrack\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "",
    "documentation": "https://github.com/Kapoorlabs-CAPED/vollseg-napari-mtrack#README.md",
    "first_released": "2022-12-20T18:07:40.732817Z",
    "license": "BSD-3-Clause",
    "name": "vollseg-napari-mtrack",
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": [],
    "project_site": "https://github.com/Kapoorlabs-CAPED/vollseg-napari-mtrack",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-12-25T14:01:43.687963Z",
    "report_issues": "https://github.com/Kapoorlabs-CAPED/vollseg-napari-mtrack/issues",
    "requirements": [
      "numpy",
      "magicgui",
      "qtpy",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "summary": "Segment kymographs of microtubules, actin filaments and perform Ransac based fits to compute dynamic instability parameters for individual kymographs and also in batch",
    "support": "https://github.com/Kapoorlabs-CAPED/vollseg-napari-mtrack/issues",
    "twitter": "",
    "version": "1.1.0",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Charlotte Godard" }],
    "code_repository": "https://github.com/chgodard/hesperos",
    "description": "<div align=\"justify\">      # HESPEROS PLUGIN FOR NAPARI  [![License](https://img.shields.io/pypi/l/hesperos.svg?color=green)](https://github.com/DBC/hesperos/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/hesperos.svg?color=green)](https://pypi.org/project/hesperos) [![Python Version](https://img.shields.io/pypi/pyversions/hesperos.svg?color=green)](https://python.org) [![tests](https://github.com/DBC/hesperos/workflows/tests/badge.svg)](https://github.com/DBC/hesperos/actions) [![codecov](https://codecov.io/gh/DBC/hesperos/branch/main/graph/badge.svg)](https://codecov.io/gh/DBC/hesperos) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/hesperos)](https://napari-hub.org/plugins/hesperos)  A Napari plugin for pre-defined manual segmentation or semi-automatic segmentation with a one-shot learning procedure. The objective was to simplify the interface as much as possible so that the user can concentrate on annotation tasks using a pen on a tablet, or a mouse on a computer.       This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.       # Table of Contents - [Installation and Usage](#installation-and-usage)     * [Automatic installation](#automatic-installation)     * [Manual installation](#manual-installation)     * [Upgrade Hesperos version](#upgrade-hesperos-version) - [Hesperos: *Manual Segmentation and Correction* mode](#hesperos-manual-segmentation-and-correction-mode)     * [Import and adjust your image](#import-and-adjust-your-image-use-panel-1)     * [Layer controls](#layer-controls)     * [Annotate your image](#annotate-your-image-use-panel-2)     * [Select slices of interest](#select-slices-of-interest-use-panel-3----only-displayed-for-the-shoulder-bones-category)     * [Export annotations](#export-annotations-use-panel-3----or-4-if-the-shoulder-bones-category-is-selected) - [Hesperos: *OneShot Segmentation* mode](#hesperos-oneshot-segmentation-mode)     * [Import and adjust your image](#import-and-adjust-your-image-use-panel-1)     * [Annotate your image](#annotate-your-image-use-panel-2)     * [Run automatic segmentation](#run-automatic-segmentation-use-panel-3)     * [Export annotations](#export-annotations-use-panel-4)           # Installation and Usage The Hesperos plugin is designed to run on Windows (11 or less) and MacOS with Python 3.8 / 3.9 / 3.10.            ## Automatic installation 1. Install [Anaconda] and unselect *Add to PATH*. Keep in mind the path where you choose to install anaconda. 2. Only download the *script_files* folder for [Windows](/script_files/for_Windows/) or [Macos](/script_files/for_Windows/).  3. Add your Anaconda path in these script files:     1. <ins>For Windows</ins>:      Right click on the .bat files (for [installation](/script_files/for_Windows/install_hesperos_env.bat) and [running](/script_files/for_Windows/run_hesperos.bat)) and select *Modify*. Change *PATH_TO_ADD* with your Anaconda path. Then save the changes.         > for exemple:         ```         anaconda_dir=C:\\\\Users\\\\chgodard\\\\anaconda3         ```     2. <ins>For Macos</ins>:         1. Right click on the .command files (for [installation](/script_files/for_Macos/install_hesperos_env.command) and [running](/script_files/for_Macos/run_hesperos.command)) and select *Open with TextEdit*. Change *PATH_TO_ADD* with your Anaconda path. Then save the changes.             > for exemple:             ```             source ~/opt/anaconda3/etc/profile.d/conda.sh             ```         2. In your terminal, change the permissions to allow the following .command files to be run (change *PATH* with the path of your .command files):              ```              chmod u+x PATH/install_hesperos_env.command              chmod u+x PATH/run_hesperos.command              ``` 4. Double click on the **install_hesperos_env file** to create a virtual environment in Anaconda with python 3.9 and Napari 0.4.14.      > /!\\\\ The Hesperos plugin is not yet compatible with Napari versions superior to 0.4.14. 5. Double click on the **run_hesperos file** to run Napari from your virtual environment. 6. In Napari:      1. Go to *Plugins/Install Plugins...*     2. Search for \"hesperos\" (it can take a while to load).     3. Install the **hesperos** plugin.     4. When the installation is done, close Napari. A restart of Napari is required to finish the plugin installation. 7. Double click on the **run_hesperos file** to run Napari. 8. In Napari, use the Hesperos plugin with *Plugins/hesperos*.       ## Manual installation 1. Install [Anaconda] and unselect *Add to PATH*. 2. Open your Anaconda prompt command. 3. Create a virtual environment with Python 3.8 / 3.9 / 3.10:     ```     conda create -n hesperos_env python=3.9     ``` 4. Install the required Python packages in your virtual environment:     ```     conda activate hesperos_env     conda install -c conda-forge napari=0.4.14      conda install -c anaconda pyqt     pip install hesperos     ```     > /!\\\\ Hesperos plugin is not yet compatible with napari version superior to 0.4.14. 5. Launch Napari:     ```     napari     ```      ## Upgrade Hesperos version 1. Double click on the **run_hesperos file** to run Napari.  2. In Napari:      1. Go to *Plugins/Install Plugins...*     2. Search for \"hesperos\" (it can take a while to load).     3. Click on *Update* if a new version of Hesperos has been found. You can check the latest version of Hesperos in the [Napari Hub](https://www.napari-hub.org/plugins/hesperos).     4. When the installation is done, close Napari. A restart of Napari is required to finish the plugin installation.          # Hesperos: *Manual Segmentation and Correction* mode       The ***Manual Segmentation and Correction*** mode of the Hesperos plugin is a simplified and optimized interface to do basic 2D manual segmentation of several structures in a 3D image using a mouse or a stylet with a tablet.        <img src=\"https://user-images.githubusercontent.com/49953723/193262711-710673f2-5b53-4eb6-a7c7-6dada9d28d92.PNG\" width=\"1000px\"/>      ## Import and adjust your image *(use Panel 1)* The Hesperos plugin can be used with Digital Imaging and COmmunications in Medicine (DICOM), Neuroimaging Informatics Technology Initiative (NIfTI) or Tagged Image File Format (TIFF) images. To improve performances, use images that are located on your own disk.  1. To import data:     - use the <img src=\"https://user-images.githubusercontent.com/49953723/193262334-3c28e733-36ab-4504-9a6d-acd298c15994.PNG\" width=\"100px\"/> button for *(.tiff, .tif, .nii or .nii.gz)* image files.     - use the <img src=\"https://user-images.githubusercontent.com/49953723/193262624-149a4461-fbac-4498-a2b8-33bdd88e3a9f.PNG\" width=\"100px\"/> button for a DICOM serie. /!\\\\ Folder with multiple DICOM series is not supported.   2. After the image has loaded, a slider appears that allows to zoom in/out: <img src=\"https://user-images.githubusercontent.com/49953723/193262738-7e6e68a9-0890-4e18-92a9-dbf2168a6bb5.PNG\" width=\"100px\"/>. Zooming is also possible with the <img src=\"https://user-images.githubusercontent.com/49953723/193262725-7d4f7b09-d119-45cf-a9d4-c42c5f848c1a.PNG\" width=\"25px\"/> button in the layer controls panel.  3. If your data is a DICOM serie, you have the possibility to directly change the contrast of the image (according to the Hounsfield Unit):     - by choosing one of the two predefined contrasts: *CT bone* or *CT Soft* in <img src=\"https://user-images.githubusercontent.com/49953723/193262708-17e1d301-0a9a-497f-9feb-613e69893c06.PNG\" width=\"150px\"/>.     - by creating a custom default contrast with the <img src=\"https://user-images.githubusercontent.com/49953723/193262707-466917b4-b885-429b-9924-6481fa6410bb.PNG\" width=\"30px\"/> button and selecting *Custom Contrast*. Settings can be exported as a .json file with the <img src=\"https://user-images.githubusercontent.com/49953723/193262709-e1ad5321-1f60-4b60-a715-7c494670e1cd.PNG\" width=\"30px\"/> button.     - by loading a saved default contrast with the <img src=\"https://user-images.githubusercontent.com/49953723/193262710-c9f66354-f896-4e59-8718-70e5509875af.PNG\" width=\"30px\"/> button and selecting *Custom Contrast*. 4. In the bottom left corner of the application you also have the possibility to:      - <img src=\"https://user-images.githubusercontent.com/49953723/193262716-d9947eb9-d87f-4251-af76-2d906cd36018.PNG\" width=\"25px\"/>: change the order of the visible axis (for example go to sagittal, axial or coronal planes).     - <img src=\"https://user-images.githubusercontent.com/49953723/193262717-12afbfb1-49ae-4a77-a83e-5bc99850734a.PNG\" width=\"25px\"/>: transpose the 3D image on the current axis being displayed.   ## Layer controls  When data is loading, two layers are created: the *`image`* layer and the *`annotations`* layer. Order in the layer list correspond to the overlayed order. By clicking on these layers you will have acces to different layer controls (at the top left corner of the application). All actions can be undone/redone with the Ctrl-Z/Shift-Ctrl-Z keyboard shortcuts. You can also hide a layer by clicking on its eye icon on the layer list.           <ins>For the *image* layer:</ins> - *`opacity`*: a slider to control the global opacity of the layer. - *`contrast limits`*: a double slider to manually control the contrast of the image (same as the <img src=\"https://user-images.githubusercontent.com/49953723/193262708-17e1d301-0a9a-497f-9feb-613e69893c06.PNG\" width=\"150px\"/> option for DICOM data).       <ins>For the *annotations* layer:</ins> - <img src=\"https://user-images.githubusercontent.com/49953723/193262718-30882770-59eb-4d2b-9cfe-8b88537560c4.PNG\" width=\"25px\"/>: erase brush to erase all labels at once (if *`preserve labels`* is not selected) or only erase the selected label (if *`preserve labels`* is selected). - <img src=\"https://user-images.githubusercontent.com/49953723/193262722-6bb6e6a4-ae7a-4ad1-b7f8-898e54ad62c3.PNG\" width=\"25px\"/>: paint brush with the same color than the *`label`* rectangle. - <img src=\"https://user-images.githubusercontent.com/49953723/193262719-f816b21e-78fd-4ba7-b415-30a461cbd652.PNG\" width=\"25px\"/>: fill bucket with the same color than the *`label`* rectangle. - <img src=\"https://user-images.githubusercontent.com/49953723/193262725-7d4f7b09-d119-45cf-a9d4-c42c5f848c1a.PNG\" width=\"25px\"/>: select to zoom in and out with the mouse wheel (same as the zoom slider at the top right corner in Panel 1). - *`label`*: a colored rectangle to represent the selected label.   - *`opacity`*: a slider to control the global opacity of the layer.   - *`brush size limits`*: a slider to control size of the paint/erase brush.     - *`preserve labels`*: if selected, all actions are applied only on the selected label (see the *`label`* rectangle); if not selected, actions are applied on all labels. - *`show selected`*: if selected, only the selected label will be display on the layer; if not selected, all labels are displayed.          >*Remark*: a second option for filling has been added >1. Drawn the egde of a closed shape with the paint brush mode.   >2. Double click to activate the fill bucket.   >3. Click inside the closed area to fill it.   >4. Double click on the filled area to deactivate the fill bucket and reactivate the paint brush mode.       ## Annotate your image *(use Panel 2)*      Manual annotation and correction on the segmented file is done using the layer controls of the *`annotations`* layer. Click on the layer to display them. /!\\\\ You have to choose a structure to start annotating *(see 2.)*. 1. To modify an existing segmentation, you can directy open the segmented file with the <img src=\"https://user-images.githubusercontent.com/49953723/193262702-df3b4fb8-63d0-4a1b-b1c9-8391cf8c3f22.PNG\" width=\"130px\"/> button. The file needs to have the same dimensions as the original image.      > /!\\\\ Only .tiff, .tif, .nii and .nii.gz files are supported as segmented files.        2. Choose a structure to annotate in the drop-down menu     - *`Fetus`*: to annotate pregnancy image.     - *`Shoulder`*: to annotate bones and muscles for shoulder surgery.     - *`Shoulder Bones`*: to annotate only few bones for shoulder surgery.     - *`Feta Challenge`*: to annotate fetal brain MRI with the same label than the FeTA Challenge (see ADD LIEN WEB).      > When selecting a structure, a new panel appears with a list of elements to annotate. Each element has its own label and color. Select one element in the list to automatically activate the paint brush mode with the corresponding color (color is updated in the *`label`* rectangle in the layer controls panel).      3. All actions can be undone with the <img src=\"https://user-images.githubusercontent.com/49953723/193265848-8c458035-609a-433e-aa82-5d9588971425.PNG\" width=\"30px\"/> button or Ctrl-Z.      4. If you need to work on a specific slice of your 3D image, but also have to explore the volume to understand some complex structures, you can use the locking option to facilitate the annotation task.     - <ins>To activate the functionality</ins>:          1. Go to the slice of interest.         2. Click on the <img src=\"https://user-images.githubusercontent.com/49953723/193262706-40f3dbca-5589-406d-81e8-e150ae8bfab6.PNG\" width=\"30px\"/> button => will change the button to <img src=\"https://user-images.githubusercontent.com/49953723/193262703-2b2ea2dc-24fa-438b-a75c-3aa42b210f53.PNG\" width=\"30px\"/> and save the layer index.         3. Scroll in the z-axis to explore the data (with the mouse wheel or the slider under the image).         4. To go back to your slice of interest, click on the <img src=\"https://user-images.githubusercontent.com/49953723/193262703-2b2ea2dc-24fa-438b-a75c-3aa42b210f53.PNG\" width=\"30px\"/> button.     - <ins>To deactivate the functionality</ins> (or change the locked slice index):          1. Go to the locked slice.         2. Click on the <img src=\"https://user-images.githubusercontent.com/49953723/193262703-2b2ea2dc-24fa-438b-a75c-3aa42b210f53.PNG\" width=\"30px\"/> button  => change the button to <img src=\"https://user-images.githubusercontent.com/49953723/193262706-40f3dbca-5589-406d-81e8-e150ae8bfab6.PNG\" width=\"30px\"/> and \"unlock\" the slice.   ## Select slices of interest *(use Panel 3 -- only displayed for the Shoulder Bones category)*  This panel will only be displayed if the *`Shoulder Bones`* category is selected. A maxiumum of 10 slices can be selected in a 3D image and the corresponding z-indexes will be integrated in the metadata during the exportation of the segmentation file.        > /!\\\\ Metadata integration is available only for exported .tiff and .tif files and with the *`Unique`* save option.   - <img src=\"https://user-images.githubusercontent.com/49953723/201736039-4ed10553-4a4b-4d5e-9d61-826dc139e437.png\" width=\"25px\"/> : to add the currently displayed z-index in the drop-down menu. - <img src=\"https://user-images.githubusercontent.com/49953723/201736105-a9c45264-412a-453b-8475-5a9ab856b07d.png\" width=\"25px\"/> : to remove the currently displayed z-index from the drop-down menu. - <img src=\"https://user-images.githubusercontent.com/49953723/201736152-319d8559-dbfc-4e52-aeb3-e8e34445f67a.png\" width=\"25px\"/> : to go to the z-index selected in the drop-down menu. The icon will be checked when the currently displayed z-index matches the selected z-index in the drop-down menu. - <img src=\"https://user-images.githubusercontent.com/49953723/201733835-7bee453a-bc07-416f-8b95-aaf803683cac.png\" width=\"100px\"/> : a drop-down menu containing the list of selected z-indexes. Select a z-index from the list to work with it more easily.   ## Export annotations *(use Panel 3 -- or 4 if the Shoulder Bones category is selected)*      1. Annotations can be exported as .tif, .tiff, .nii or .nii.gz file with the <img src=\"https://user-images.githubusercontent.com/49953723/201735102-113f64b7-4da4-40ee-b058-9900268d270d.png\" width=\"95px\"/> button in one of the two following saving mode:     - *`Unique`*: segmented data is exported as a unique 3D image with corresponding label ids (1-2-3-...). This file can be re-opened in the application.     - *`Several`*: segmented data is exported as several binary 3D images (0 or 255), one for each label id. 2. <img src=\"https://user-images.githubusercontent.com/49953723/193262699-95758bdb-ac40-439b-8959-d924781a2368.PNG\" width=\"100px\"/>: delete annotation data. 3. *`Automatic segmentation backup`*: if selected, the segmentation data will be automatically exported as a unique 3D image when the image slice is changed.     > /!\\\\ This process can slow down the display if the image is large.  # Hesperos: *OneShot Segmentation* mode       The ***OneShot Segmentation*** mode of the Hesperos plugin is a 2D version of the VoxelLearning method implemented in DIVA (see [our Github](https://github.com/DecBayComp/VoxelLearning) and the latest article [Guérinot, C., Marcon, V., Godard, C., et al. (2022). New Approach to Accelerated Image Annotation by Leveraging Virtual Reality and Cloud Computing. _Frontiers in Bioinformatics_. doi:10.3389/fbinf.2021.777101](https://www.frontiersin.org/articles/10.3389/fbinf.2021.777101/full)).       The principle is to accelerate the segmentation without prior information. The procedure consists of: 1. A **rapid tagging** of few pixels in the image with two labels: one for the structure of interest (named positive tags), and one for the other structures (named negative tags). 2. A **training** of a simple random forest classifier with these tagged pixels and their features (mean, gaussian, ...). 3. An **inference** of all the pixels of the image to automatically segment the structure of interest. The output is a probability image (0-255) of belonging to a specific class. 4. Iterative corrections if needed.      <img src=\"https://user-images.githubusercontent.com/49953723/193262714-8699cd59-3825-4d71-b27a-bbcad1e36d55.PNG\" width=\"1000px\"/>       ## Import and adjust your image *(use Panel 1)*      Same panel as the *Manual Segmentation and Correction* mode *(see [panel 1 description](#import-and-adjust-your-image-use-panel-1))*.          ## Annotate your image *(use Panel 2)*      Annotations and corrections on the segmented file is done using the layer controls of the *`annotations`* layer. Click on the layer to display them. Only two labels are available: *`Structure of interest`* and *`Other`*.   The rapid manual tagging step of the one-shot learning method aims to learn and attribute different features to each label. <img align=\"right\" src=\"https://user-images.githubusercontent.com/49953723/193262735-5dce56fb-8a2c-4aeb-9ee7-9727122d8089.PNG\" width=\"220px\"/>  To achieve that, the user has to: - with the label *`Structure of interest`*, tag few pixels of the structure of interest. - with the label *`Other`*, tag the greatest diversity of uninteresting structures in the 3D image (avoid tagging too much pixels).  > see the exemple image with *`Structure of interest`* label in red and *`Other`* label in cyan.      1. To modify an existing segmentation, you can directy open the segmented file with the <img src=\"https://user-images.githubusercontent.com/49953723/193266118-dfd241f6-8f0b-4cb9-94e7-5e74a3ce8b6e.PNG\" width=\"130px\"/> button. The file needs to have the same dimensions as the original image.      > /!\\\\ Only .tiff, .tif, .nii and .nii.gz files are supported as segmented files.  2. All actions can be undone with the <img src=\"https://user-images.githubusercontent.com/49953723/193265848-8c458035-609a-433e-aa82-5d9588971425.PNG\" width=\"30px\"/> button or Ctrl-Z.       ## Run automatic segmentation *(use Panel 3)*  From the previously tagged pixels, features are extracted and used to train a basic classifier : the Random Forest Classifier (RFC). When the training of the pixel classifier is done, it is applied to each pixel of the complete volume and outputs a probability to belong to the structure of interest.  To run training and inference, click on the <img src=\"https://user-images.githubusercontent.com/49953723/193262731-719c226a-f7c5-4252-b2bb-fade4ab7f5b3.PNG\" width=\"115px\"/> button: 1. You will be asked to save a .pckl file which corresponds to the model. 2. A new status will appears under the *Panel 4* : *`Computing...`*. You must wait for the message to change to: *`Ready`* before doing anything in the application (otherwise the application may freeze or crash). 3. When the processing is done, two new layers will appear:     - the *`probabilities`* layer which corresponds to the direct probability (between 0 and 1) of a pixel to belong to the structure of interest. This layer is disabled by default, to enable it click on its eye icon in the layer list.     - the *`segmented probabilities`* layer which corresponds to a binary image obtained from the probability image normed and thresholded according to a value manually defined with the *`Probability threshold`* slider: <img src=\"https://user-images.githubusercontent.com/49953723/193262730-6998c8a5-92f1-4ff1-bbf5-6972a373afd2.PNG\" width=\"80px\"/>.  >Remark: If the output is not perfect, you have two possibilities to improve the result: >1. Add some tags with the paint brush to take in consideration unintersting structures or add information in critical areas of your structure of interest (such as in thin sections). Then, run the training and inference process again. /!\\\\ This will overwrite all previous segmentation data. >2. Export your segmentation data and re-open it with the *Manual Annotation and Correction* mode of Hesperos to manually erase or add annotations.           ## Export annotations *(use Panel 4)*      1. Segmented probabilites can be exported as .tif, .tiff, .nii or .nii.gz file with the <img src=\"https://user-images.githubusercontent.com/49953723/193262734-57159a97-2f46-4aba-b3bf-b55a35dfacbd.PNG\" width=\"105px\"/> button. The image is exported as a unique 3D binary image (value 0 and 255). This file can be re-opened in the application for correction. 2. Probabilities can be exported as .tif, .tiff, .nii or .nii.gz file with the <img src=\"https://user-images.githubusercontent.com/49953723/193262733-26e37392-55b2-4c36-9287-b2f5d8d30e03.PNG\" width=\"105px\"/> button as a unique 3D image. The probabilities image is normed between 0 and 255. 3. <img src=\"https://user-images.githubusercontent.com/49953723/193266056-9514b648-b3e0-43f5-901a-a45fa1390f00.PNG\" width=\"100px\"/>: delete annotation data.   # License  Distributed under the terms of the [BSD-3] license, **Hesperos** is a free and open source software.       [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ [Anaconda]: https://www.anaconda.com/products/distribution#Downloads [VoxelLearning]: https://github.com/DecBayComp/VoxelLearning ",
    "description_content_type": "text/markdown",
    "description_text": "  # HESPEROS PLUGIN FOR NAPARI  [![License](https://img.shields.io/pypi/l/hesperos.svg?color=green)](https://github.com/DBC/hesperos/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/hesperos.svg?color=green)](https://pypi.org/project/hesperos) [![Python Version](https://img.shields.io/pypi/pyversions/hesperos.svg?color=green)](https://python.org) [![tests](https://github.com/DBC/hesperos/workflows/tests/badge.svg)](https://github.com/DBC/hesperos/actions) [![codecov](https://codecov.io/gh/DBC/hesperos/branch/main/graph/badge.svg)](https://codecov.io/gh/DBC/hesperos) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/hesperos)](https://napari-hub.org/plugins/hesperos)  A Napari plugin for pre-defined manual segmentation or semi-automatic segmentation with a one-shot learning procedure. The objective was to simplify the interface as much as possible so that the user can concentrate on annotation tasks using a pen on a tablet, or a mouse on a computer.   This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.   # Table of Contents - [Installation and Usage](#installation-and-usage)     * [Automatic installation](#automatic-installation)     * [Manual installation](#manual-installation)     * [Upgrade Hesperos version](#upgrade-hesperos-version) - [Hesperos: *Manual Segmentation and Correction* mode](#hesperos-manual-segmentation-and-correction-mode)     * [Import and adjust your image](#import-and-adjust-your-image-use-panel-1)     * [Layer controls](#layer-controls)     * [Annotate your image](#annotate-your-image-use-panel-2)     * [Select slices of interest](#select-slices-of-interest-use-panel-3----only-displayed-for-the-shoulder-bones-category)     * [Export annotations](#export-annotations-use-panel-3----or-4-if-the-shoulder-bones-category-is-selected) - [Hesperos: *OneShot Segmentation* mode](#hesperos-oneshot-segmentation-mode)     * [Import and adjust your image](#import-and-adjust-your-image-use-panel-1)     * [Annotate your image](#annotate-your-image-use-panel-2)     * [Run automatic segmentation](#run-automatic-segmentation-use-panel-3)     * [Export annotations](#export-annotations-use-panel-4)   # Installation and Usage The Hesperos plugin is designed to run on Windows (11 or less) and MacOS with Python 3.8 / 3.9 / 3.10.   ## Automatic installation 1. Install [Anaconda] and unselect *Add to PATH*. Keep in mind the path where you choose to install anaconda. 2. Only download the *script_files* folder for [Windows](/script_files/for_Windows/) or [Macos](/script_files/for_Windows/).  3. Add your Anaconda path in these script files:     1. For Windows:      Right click on the .bat files (for [installation](/script_files/for_Windows/install_hesperos_env.bat) and [running](/script_files/for_Windows/run_hesperos.bat)) and select *Modify*. Change *PATH_TO_ADD* with your Anaconda path. Then save the changes.         > for exemple:         ```         anaconda_dir=C:\\\\Users\\\\chgodard\\\\anaconda3         ```     2. For Macos:         1. Right click on the .command files (for [installation](/script_files/for_Macos/install_hesperos_env.command) and [running](/script_files/for_Macos/run_hesperos.command)) and select *Open with TextEdit*. Change *PATH_TO_ADD* with your Anaconda path. Then save the changes.             > for exemple:             ```             source ~/opt/anaconda3/etc/profile.d/conda.sh             ```         2. In your terminal, change the permissions to allow the following .command files to be run (change *PATH* with the path of your .command files):              ```              chmod u+x PATH/install_hesperos_env.command              chmod u+x PATH/run_hesperos.command              ``` 4. Double click on the **install_hesperos_env file** to create a virtual environment in Anaconda with python 3.9 and Napari 0.4.14.      > /!\\\\ The Hesperos plugin is not yet compatible with Napari versions superior to 0.4.14. 5. Double click on the **run_hesperos file** to run Napari from your virtual environment. 6. In Napari:      1. Go to *Plugins/Install Plugins...*     2. Search for \"hesperos\" (it can take a while to load).     3. Install the **hesperos** plugin.     4. When the installation is done, close Napari. A restart of Napari is required to finish the plugin installation. 7. Double click on the **run_hesperos file** to run Napari. 8. In Napari, use the Hesperos plugin with *Plugins/hesperos*.   ## Manual installation 1. Install [Anaconda] and unselect *Add to PATH*. 2. Open your Anaconda prompt command. 3. Create a virtual environment with Python 3.8 / 3.9 / 3.10:     ```     conda create -n hesperos_env python=3.9     ``` 4. Install the required Python packages in your virtual environment:     ```     conda activate hesperos_env     conda install -c conda-forge napari=0.4.14      conda install -c anaconda pyqt     pip install hesperos     ```     > /!\\\\ Hesperos plugin is not yet compatible with napari version superior to 0.4.14. 5. Launch Napari:     ```     napari     ```  ## Upgrade Hesperos version 1. Double click on the **run_hesperos file** to run Napari.  2. In Napari:      1. Go to *Plugins/Install Plugins...*     2. Search for \"hesperos\" (it can take a while to load).     3. Click on *Update* if a new version of Hesperos has been found. You can check the latest version of Hesperos in the [Napari Hub](https://www.napari-hub.org/plugins/hesperos).     4. When the installation is done, close Napari. A restart of Napari is required to finish the plugin installation.   # Hesperos: *Manual Segmentation and Correction* mode   The ***Manual Segmentation and Correction*** mode of the Hesperos plugin is a simplified and optimized interface to do basic 2D manual segmentation of several structures in a 3D image using a mouse or a stylet with a tablet.      ## Import and adjust your image *(use Panel 1)* The Hesperos plugin can be used with Digital Imaging and COmmunications in Medicine (DICOM), Neuroimaging Informatics Technology Initiative (NIfTI) or Tagged Image File Format (TIFF) images. To improve performances, use images that are located on your own disk.  1. To import data:     - use the  button for *(.tiff, .tif, .nii or .nii.gz)* image files.     - use the  button for a DICOM serie. /!\\\\ Folder with multiple DICOM series is not supported.   2. After the image has loaded, a slider appears that allows to zoom in/out: . Zooming is also possible with the  button in the layer controls panel.  3. If your data is a DICOM serie, you have the possibility to directly change the contrast of the image (according to the Hounsfield Unit):     - by choosing one of the two predefined contrasts: *CT bone* or *CT Soft* in .     - by creating a custom default contrast with the  button and selecting *Custom Contrast*. Settings can be exported as a .json file with the  button.     - by loading a saved default contrast with the  button and selecting *Custom Contrast*. 4. In the bottom left corner of the application you also have the possibility to:      - : change the order of the visible axis (for example go to sagittal, axial or coronal planes).     - : transpose the 3D image on the current axis being displayed.   ## Layer controls  When data is loading, two layers are created: the *`image`* layer and the *`annotations`* layer. Order in the layer list correspond to the overlayed order. By clicking on these layers you will have acces to different layer controls (at the top left corner of the application). All actions can be undone/redone with the Ctrl-Z/Shift-Ctrl-Z keyboard shortcuts. You can also hide a layer by clicking on its eye icon on the layer list.   For the *image* layer: - *`opacity`*: a slider to control the global opacity of the layer. - *`contrast limits`*: a double slider to manually control the contrast of the image (same as the  option for DICOM data).   For the *annotations* layer: - : erase brush to erase all labels at once (if *`preserve labels`* is not selected) or only erase the selected label (if *`preserve labels`* is selected). - : paint brush with the same color than the *`label`* rectangle. - : fill bucket with the same color than the *`label`* rectangle. - : select to zoom in and out with the mouse wheel (same as the zoom slider at the top right corner in Panel 1). - *`label`*: a colored rectangle to represent the selected label.   - *`opacity`*: a slider to control the global opacity of the layer.   - *`brush size limits`*: a slider to control size of the paint/erase brush.     - *`preserve labels`*: if selected, all actions are applied only on the selected label (see the *`label`* rectangle); if not selected, actions are applied on all labels. - *`show selected`*: if selected, only the selected label will be display on the layer; if not selected, all labels are displayed.   >*Remark*: a second option for filling has been added >1. Drawn the egde of a closed shape with the paint brush mode.   >2. Double click to activate the fill bucket.   >3. Click inside the closed area to fill it.   >4. Double click on the filled area to deactivate the fill bucket and reactivate the paint brush mode.   ## Annotate your image *(use Panel 2)*  Manual annotation and correction on the segmented file is done using the layer controls of the *`annotations`* layer. Click on the layer to display them. /!\\\\ You have to choose a structure to start annotating *(see 2.)*. 1. To modify an existing segmentation, you can directy open the segmented file with the  button. The file needs to have the same dimensions as the original image.      > /!\\\\ Only .tiff, .tif, .nii and .nii.gz files are supported as segmented files.    2. Choose a structure to annotate in the drop-down menu     - *`Fetus`*: to annotate pregnancy image.     - *`Shoulder`*: to annotate bones and muscles for shoulder surgery.     - *`Shoulder Bones`*: to annotate only few bones for shoulder surgery.     - *`Feta Challenge`*: to annotate fetal brain MRI with the same label than the FeTA Challenge (see ADD LIEN WEB).  > When selecting a structure, a new panel appears with a list of elements to annotate. Each element has its own label and color. Select one element in the list to automatically activate the paint brush mode with the corresponding color (color is updated in the *`label`* rectangle in the layer controls panel).  3. All actions can be undone with the  button or Ctrl-Z.  4. If you need to work on a specific slice of your 3D image, but also have to explore the volume to understand some complex structures, you can use the locking option to facilitate the annotation task.     - To activate the functionality:          1. Go to the slice of interest.         2. Click on the  button => will change the button to  and save the layer index.         3. Scroll in the z-axis to explore the data (with the mouse wheel or the slider under the image).         4. To go back to your slice of interest, click on the  button.     - To deactivate the functionality (or change the locked slice index):          1. Go to the locked slice.         2. Click on the  button  => change the button to  and \"unlock\" the slice.   ## Select slices of interest *(use Panel 3 -- only displayed for the Shoulder Bones category)*  This panel will only be displayed if the *`Shoulder Bones`* category is selected. A maxiumum of 10 slices can be selected in a 3D image and the corresponding z-indexes will be integrated in the metadata during the exportation of the segmentation file.     > /!\\\\ Metadata integration is available only for exported .tiff and .tif files and with the *`Unique`* save option.   -  : to add the currently displayed z-index in the drop-down menu. -  : to remove the currently displayed z-index from the drop-down menu. -  : to go to the z-index selected in the drop-down menu. The icon will be checked when the currently displayed z-index matches the selected z-index in the drop-down menu. -  : a drop-down menu containing the list of selected z-indexes. Select a z-index from the list to work with it more easily.   ## Export annotations *(use Panel 3 -- or 4 if the Shoulder Bones category is selected)*  1. Annotations can be exported as .tif, .tiff, .nii or .nii.gz file with the  button in one of the two following saving mode:     - *`Unique`*: segmented data is exported as a unique 3D image with corresponding label ids (1-2-3-...). This file can be re-opened in the application.     - *`Several`*: segmented data is exported as several binary 3D images (0 or 255), one for each label id. 2. : delete annotation data. 3. *`Automatic segmentation backup`*: if selected, the segmentation data will be automatically exported as a unique 3D image when the image slice is changed.     > /!\\\\ This process can slow down the display if the image is large.  # Hesperos: *OneShot Segmentation* mode   The ***OneShot Segmentation*** mode of the Hesperos plugin is a 2D version of the VoxelLearning method implemented in DIVA (see [our Github](https://github.com/DecBayComp/VoxelLearning) and the latest article [Guérinot, C., Marcon, V., Godard, C., et al. (2022). New Approach to Accelerated Image Annotation by Leveraging Virtual Reality and Cloud Computing. _Frontiers in Bioinformatics_. doi:10.3389/fbinf.2021.777101](https://www.frontiersin.org/articles/10.3389/fbinf.2021.777101/full)).   The principle is to accelerate the segmentation without prior information. The procedure consists of: 1. A **rapid tagging** of few pixels in the image with two labels: one for the structure of interest (named positive tags), and one for the other structures (named negative tags). 2. A **training** of a simple random forest classifier with these tagged pixels and their features (mean, gaussian, ...). 3. An **inference** of all the pixels of the image to automatically segment the structure of interest. The output is a probability image (0-255) of belonging to a specific class. 4. Iterative corrections if needed.     ## Import and adjust your image *(use Panel 1)*  Same panel as the *Manual Segmentation and Correction* mode *(see [panel 1 description](#import-and-adjust-your-image-use-panel-1))*.   ## Annotate your image *(use Panel 2)*  Annotations and corrections on the segmented file is done using the layer controls of the *`annotations`* layer. Click on the layer to display them. Only two labels are available: *`Structure of interest`* and *`Other`*.   The rapid manual tagging step of the one-shot learning method aims to learn and attribute different features to each label.   To achieve that, the user has to: - with the label *`Structure of interest`*, tag few pixels of the structure of interest. - with the label *`Other`*, tag the greatest diversity of uninteresting structures in the 3D image (avoid tagging too much pixels).  > see the exemple image with *`Structure of interest`* label in red and *`Other`* label in cyan.  1. To modify an existing segmentation, you can directy open the segmented file with the  button. The file needs to have the same dimensions as the original image.      > /!\\\\ Only .tiff, .tif, .nii and .nii.gz files are supported as segmented files.  2. All actions can be undone with the  button or Ctrl-Z.   ## Run automatic segmentation *(use Panel 3)*  From the previously tagged pixels, features are extracted and used to train a basic classifier : the Random Forest Classifier (RFC). When the training of the pixel classifier is done, it is applied to each pixel of the complete volume and outputs a probability to belong to the structure of interest.  To run training and inference, click on the  button: 1. You will be asked to save a .pckl file which corresponds to the model. 2. A new status will appears under the *Panel 4* : *`Computing...`*. You must wait for the message to change to: *`Ready`* before doing anything in the application (otherwise the application may freeze or crash). 3. When the processing is done, two new layers will appear:     - the *`probabilities`* layer which corresponds to the direct probability (between 0 and 1) of a pixel to belong to the structure of interest. This layer is disabled by default, to enable it click on its eye icon in the layer list.     - the *`segmented probabilities`* layer which corresponds to a binary image obtained from the probability image normed and thresholded according to a value manually defined with the *`Probability threshold`* slider: .  >Remark: If the output is not perfect, you have two possibilities to improve the result: >1. Add some tags with the paint brush to take in consideration unintersting structures or add information in critical areas of your structure of interest (such as in thin sections). Then, run the training and inference process again. /!\\\\ This will overwrite all previous segmentation data. >2. Export your segmentation data and re-open it with the *Manual Annotation and Correction* mode of Hesperos to manually erase or add annotations.   ## Export annotations *(use Panel 4)*  1. Segmented probabilites can be exported as .tif, .tiff, .nii or .nii.gz file with the  button. The image is exported as a unique 3D binary image (value 0 and 255). This file can be re-opened in the application for correction. 2. Probabilities can be exported as .tif, .tiff, .nii or .nii.gz file with the  button as a unique 3D image. The probabilities image is normed between 0 and 255. 3. : delete annotation data.   # License  Distributed under the terms of the [BSD-3] license, **Hesperos** is a free and open source software.   [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ [Anaconda]: https://www.anaconda.com/products/distribution#Downloads [VoxelLearning]: https://github.com/DecBayComp/VoxelLearning",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "Hesperos application",
    "documentation": "https://github.com/chgodard/hesperos/blob/main/README.md",
    "first_released": "2022-05-30T20:12:28.151101Z",
    "license": "BSD-3-Clause",
    "name": "hesperos",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/chgodard/hesperos",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-11-30T14:31:14.617847Z",
    "report_issues": "",
    "requirements": [
      "numpy",
      "qtpy",
      "tifffile",
      "scikit-image",
      "scikit-learn",
      "SimpleITK",
      "pandas",
      "napari (<0.4.15)",
      "napari-plugin-engine",
      "imageio-ffmpeg",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "summary": "A plugin to manually or semi-automatically segment medical data and correct previous segmentation data.",
    "support": "",
    "twitter": "",
    "version": "0.1.36",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "email": "andrea1.bassi@polimi.it", "name": "Andrea Bassi" }],
    "category": { "Image modality": ["Fluorescence microscopy"] },
    "category_hierarchy": {
      "Image modality": [["Fluorescence microscopy", "Fluorescence"]]
    },
    "code_repository": "https://github.com/andreabassi78/napari-psf-simulator",
    "conda": [],
    "description": "# napari-psf-simulator  [![License](https://img.shields.io/pypi/l/napari-psf-simulator.svg?color=green)](https://github.com/andreabassi78/napari-psf-simulator/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-psf-simulator.svg?color=green)](https://pypi.org/project/napari-psf-simulator) [![Python Version](https://img.shields.io/pypi/pyversions/napari-psf-simulator.svg?color=green)](https://python.org) [![tests](https://github.com/andreabassi78/napari-psf-simulator/workflows/tests/badge.svg)](https://github.com/andreabassi78/napari-psf-simulator/actions) [![codecov](https://codecov.io/gh/andreabassi78/napari-psf-simulator/branch/main/graph/badge.svg)](https://codecov.io/gh/andreabassi78/napari-psf-simulator) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-psf-simulator)](https://napari-hub.org/plugins/napari-psf-simulator)  A plugin for the simulation of the 3D Point Spread Function of an optical systen, particularly a microscope objective.   Calculates the PSF as the squared Fourier Transform of the pupil and uses the angular spectrum to propagate the PSF in 3D.   The following aberrations are included: - phase aberration described by a Zernike polynomials with n-m coefficients - aberration induced by a slab, with a refractive index different from the one at the object (only scalar approximation is used, polarization not considered yet).    ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/plugins/stable/index.html -->  ## Installation  You can install `napari-psf-simulator` via [pip]:      pip install napari-psf-simulator   To install latest development version :      pip install git+https://github.com/andreabassi78/napari-psf-simulator.git   ## Usage  1) Lauch the plugin and select the parameters of the microscope: `NA` (numerical aperture), `wavelenght`, `n` (refractive index at the object),    `Nxy` (number of pixels), `Nz` (number of slices), `dxy` (pixel size, transverse sampling), `dz` (voxel depth, axial sampling)  2) Select an aberration type (if needed) and press `Calculate PSF` to run the simulator. This will create a new image layer with the 3D PSF.      The option `Show Airy disk` creates 3 ellipses in a Shapes layer, showing the boundaries of the diffraction limited blob.  ![raw](https://github.com/andreabassi78/napari-psf-simulator/raw/main/images/figure.png) **Napari viewer with the psf-simulator widget showing the in-focus plane of an aberrated PSF**  ![raw](https://github.com/andreabassi78/napari-psf-simulator/raw/main/images/animation.gif) **Slicing through a PSF aberrated with Zernike polynomials of order N=3, M=1 (coma)**  3) Click on the `Plot PSF Profile in Console` checkbox to see the x and z profiles of the PSF.    They will show up in  the viewer console when `Calculate PSF` is executed.  ![raw](https://github.com/andreabassi78/napari-psf-simulator/raw/main/images/Plot.png) **Plot profile of the PSF, shown in the Console**   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  The plugin has been concived to be modular allowing the insertion of new aberations and pupils. Please contact the developers on github for adding new propagations and aberrations types.  Any suggestions or contributions are welcome.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-psf-simulator\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/andreabassi78/napari-psf-simulator/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-psf-simulator       A plugin for the simulation of the 3D Point Spread Function of an optical systen, particularly a microscope objective. Calculates the PSF as the squared Fourier Transform of the pupil and uses the angular spectrum to propagate the PSF in 3D. The following aberrations are included: - phase aberration described by a Zernike polynomials with n-m coefficients - aberration induced by a slab, with a refractive index different from the one at the object (only scalar approximation is used, polarization not considered yet).    This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install napari-psf-simulator via pip: pip install napari-psf-simulator  To install latest development version : pip install git+https://github.com/andreabassi78/napari-psf-simulator.git  Usage 1) Lauch the plugin and select the parameters of the microscope: NA (numerical aperture), wavelenght, n (refractive index at the object),    Nxy (number of pixels), Nz (number of slices), dxy (pixel size, transverse sampling), dz (voxel depth, axial sampling) 2) Select an aberration type (if needed) and press Calculate PSF to run the simulator. This will create a new image layer with the 3D PSF. The option Show Airy disk creates 3 ellipses in a Shapes layer, showing the boundaries of the diffraction limited blob.  Napari viewer with the psf-simulator widget showing the in-focus plane of an aberrated PSF  Slicing through a PSF aberrated with Zernike polynomials of order N=3, M=1 (coma) 3) Click on the Plot PSF Profile in Console checkbox to see the x and z profiles of the PSF.    They will show up in  the viewer console when Calculate PSF is executed.  Plot profile of the PSF, shown in the Console Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request.  The plugin has been concived to be modular allowing the insertion of new aberations and pupils. Please contact the developers on github for adding new propagations and aberrations types.  Any suggestions or contributions are welcome. License Distributed under the terms of the BSD-3 license, \"napari-psf-simulator\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "PSF simulator",
    "documentation": "https://github.com/andreabassi78/napari-psf-simulator#README.md",
    "first_released": "2022-04-14T20:30:28.449098Z",
    "license": "BSD-3-Clause",
    "name": "napari-psf-simulator",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/andreabassi78/napari-psf-simulator",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-06-08T21:52:42.447846Z",
    "report_issues": "https://github.com/andreabassi78/napari-psf-simulator/issues",
    "requirements": [
      "numpy",
      "magicgui",
      "qtpy",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "summary": "A plugin for simulations of the Point Spread Function, with aberrations",
    "support": "https://github.com/andreabassi78/napari-psf-simulator/issues",
    "twitter": "",
    "version": "0.1.5",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "email": "tristan.cotte@sgs.com", "name": "Tristan Cotte" }],
    "code_repository": "https://github.com/tcotte/napari-IDS",
    "description": "# napari-IDS\\r \\r [![License](https://img.shields.io/pypi/l/napari-IDS.svg?color=green)](https://github.com/githubuser/napari-IDS/raw/main/LICENSE)\\r [![PyPI](https://img.shields.io/pypi/v/napari-IDS.svg?color=green)](https://pypi.org/project/napari-IDS)\\r [![Python Version](https://img.shields.io/pypi/pyversions/napari-IDS.svg?color=green)](https://python.org)\\r [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-IDS)](https://napari-hub.org/plugins/napari-IDS)\\r \\r A simple plugin to use with napari\\r \\r ----------------------------------\\r \\r This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\r \\r <!--\\r Don't miss the full getting started guide to set up your new package:\\r https://github.com/napari/cookiecutter-napari-plugin#getting-started\\r \\r and review the napari docs for plugin developers:\\r https://napari.org/plugins/stable/index.html\\r -->\\r \\r ## Installation\\r \\r You can install `napari-IDS` via [pip]:\\r \\r     pip install napari-IDS\\r \\r \\r \\r To install latest development version :\\r \\r     pip install git+https://github.com/githubuser/napari-IDS.git\\r \\r \\r ## First utilisation\\r \\r Suggested environment : \\r - Python 3.8\\r - IDS 1.2.0.5 version installed\\r \\r To use this package for the first time :\\r 1. Install Napari `pip install \"napari[all]\"`\\r 2. Install napari-IDS package\\r 3. Install IDS Python api thanks to the command `ids_packages`\\r \\r If your environment is not the suggested environment, you have to install IDS packages manually. \\r \\r \\r ## Contributing\\r \\r Contributions are very welcome. Tests can be run with [tox], please ensure\\r the coverage at least stays the same before you submit a pull request.\\r \\r ## License\\r \\r Distributed under the terms of the [BSD-3] license,\\r \"napari-IDS\" is free and open source software\\r \\r ## Issues\\r \\r If you encounter any problems, please [file an issue] along with a detailed description.\\r \\r [napari]: https://github.com/napari/napari\\r [Cookiecutter]: https://github.com/audreyr/cookiecutter\\r [@napari]: https://github.com/napari\\r [MIT]: http://opensource.org/licenses/MIT\\r [BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\r [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\r [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\r [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\r [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\r [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\r \\r [file an issue]: https://github.com/githubuser/napari-IDS/issues\\r \\r [napari]: https://github.com/napari/napari\\r [tox]: https://tox.readthedocs.io/en/latest/\\r [pip]: https://pypi.org/project/pip/\\r [PyPI]: https://pypi.org/\\r \\r \\r ",
    "description_content_type": "text/markdown",
    "description_text": "napari-IDS     A simple plugin to use with napari  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install napari-IDS via pip: pip install napari-IDS  To install latest development version : pip install git+https://github.com/githubuser/napari-IDS.git  First utilisation Suggested environment :  - Python 3.8 - IDS 1.2.0.5 version installed To use this package for the first time : 1. Install Napari pip install \"napari[all]\" 2. Install napari-IDS package 3. Install IDS Python api thanks to the command ids_packages If your environment is not the suggested environment, you have to install IDS packages manually.  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-IDS\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "napari IDS",
    "documentation": "https://github.com/tcotte/napari-IDS#README.md",
    "first_released": "2022-02-17T16:27:12.336842Z",
    "license": "BSD-3-Clause",
    "name": "napari-IDS",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/tcotte/napari-IDS",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-02-23T15:00:23.108346Z",
    "report_issues": "https://github.com/tcotte/napari-IDS/issues",
    "requirements": null,
    "summary": "Plug in which enables to take photo with IDS uEye camera",
    "support": "https://github.com/tcotte/napari-IDS/issues",
    "twitter": "",
    "version": "0.0.8",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Bastian Eichenberger" }],
    "code_repository": "https://github.com/bbquercus/koopa",
    "description": "[![License MIT](https://img.shields.io/pypi/l/koopa-viz.svg?color=green)](https://github.com/bbquercus/koopa/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/koopa-viz.svg?color=green)](https://pypi.org/project/koopa-viz) [![Python Version](https://img.shields.io/pypi/pyversions/koopa-viz.svg?color=green)](https://python.org) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/koopa-viz)](https://napari-hub.org/plugins/koopa-viz)  # koopa-viz  Vizualization plugin for koopa image analysis  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  More information can be found on the official [GitHub repo].  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [GitHub repo]: https://github.com/bbquercus/koopa [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [file an issue]: https://github.com/bbquercus/koopa/issues ",
    "description_content_type": "text/markdown",
    "description_text": "    koopa-viz Vizualization plugin for koopa image analysis This napari plugin was generated with Cookiecutter using @napari's [cookiecutter-napari-plugin] template. More information can be found on the official GitHub repo. Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "Koopa",
    "documentation": "https://github.com/bbquercus/koopa#README.md",
    "first_released": "2022-09-29T14:00:59.874419Z",
    "license": "MIT",
    "name": "koopa-viz",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/bbquercus/koopa",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-12-06T13:46:14.447526Z",
    "report_issues": "https://github.com/bbquercus/koopa/issues",
    "requirements": null,
    "summary": "Vizualization plugin for koopa image analysis",
    "support": "https://github.com/bbquercus/koopa/issues",
    "twitter": "",
    "version": "0.0.2",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "email": "jhnnsrs@gmail.com", "name": "jhnnsrs" }],
    "code_repository": null,
    "description": "",
    "description_content_type": "None",
    "description_text": "",
    "development_status": [],
    "display_name": "faser",
    "documentation": "",
    "first_released": "2022-05-23T13:06:03.257630Z",
    "license": "",
    "name": "faser",
    "npe2": true,
    "operating_system": [],
    "plugin_types": ["widget"],
    "project_site": "",
    "python_version": ">=3.8,<3.11",
    "reader_file_extensions": [],
    "release_date": "2022-06-01T13:42:15.985064Z",
    "report_issues": "",
    "requirements": [
      "magicgui (>=0.4.0,<0.5.0)",
      "napari-plugin_engine (>=0.1.4,<0.2.0)",
      "numpy (>=1.22.4,<2.0.0)",
      "pydantic (>=1.9.1,<2.0.0)"
    ],
    "summary": "",
    "support": "",
    "twitter": "",
    "version": "0.2.9",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Thorsten Beier" }],
    "code_repository": "https://github.com/uhlmanngroup/napari-splineit",
    "conda": [{ "channel": "conda-forge", "package": "napari-splineit" }],
    "description": "# napari-splineit  [![License](https://img.shields.io/pypi/l/napari-splineit.svg?color=green)](https://github.com/uhlmanngroup/napari-splineit/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-splineit.svg?color=green)](https://pypi.org/project/napari-splineit) [![Python Version](https://img.shields.io/pypi/pyversions/napari-splineit.svg?color=green)](https://python.org) [![tests](https://github.com/uhlmanngroup/napari-splineit/workflows/tests/badge.svg)](https://github.com/uhlmanngroup/napari-splineit/actions) [![codecov](https://codecov.io/gh/uhlmanngroup/napari-splineit/branch/main/graph/badge.svg)](https://codecov.io/gh/uhlmanngroup/napari-splineit) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-splineit)](https://napari-hub.org/plugins/napari-splineit)  A napari plugin for the interactive manipulation of spline-interpolation based geometrical models  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/plugins/stable/index.html -->  ## Installation  You can install `napari-splineit` via [pip]:      pip install napari-splineit    To install latest development version :      pip install git+https://github.com/uhlmanngroup/napari-splineit.git   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-splineit\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/uhlmanngroup/napari-splineit/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-splineit       A napari plugin for the interactive manipulation of spline-interpolation based geometrical models  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install napari-splineit via pip: pip install napari-splineit  To install latest development version : pip install git+https://github.com/uhlmanngroup/napari-splineit.git  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-splineit\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "Napari SplineIt2",
    "documentation": "https://github.com/uhlmanngroup/napari-splineit#README.md",
    "first_released": "2022-07-05T10:07:35.650619Z",
    "license": "BSD-3-Clause",
    "name": "napari-splineit",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader", "writer", "widget", "sample_data"],
    "project_site": "https://github.com/uhlmanngroup/napari-splineit",
    "python_version": ">=3.7",
    "reader_file_extensions": ["*.splineit"],
    "release_date": "2022-10-24T13:39:06.290642Z",
    "report_issues": "https://github.com/uhlmanngroup/napari-splineit/issues",
    "requirements": [
      "numpy",
      "qtpy",
      "scikit-image",
      "scipy",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "tox ; extra == 'testing'"
    ],
    "summary": "A napari plugin for spline manipulation",
    "support": "https://github.com/uhlmanngroup/napari-splineit/issues",
    "twitter": "",
    "version": "0.3.0",
    "visibility": "public",
    "writer_file_extensions": [".splineit"],
    "writer_save_layers": ["shapes"]
  },
  {
    "authors": [
      { "email": "cqzhang@g.ecc.u-tokyo.ac.jp", "name": "Chenqi Zhang" }
    ],
    "code_repository": "https://github.com/zcqwh/iacs_ipac_reader",
    "description": "# iacs_ipac_reader  [![License](https://img.shields.io/pypi/l/iacs_ipac_reader.svg?color=green)](https://github.com/zcqwh/iacs_ipac_reader/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/iacs_ipac_reader.svg?color=green)](https://pypi.org/project/iacs_ipac_reader) [![Python Version](https://img.shields.io/pypi/pyversions/iacs_ipac_reader.svg?color=green)](https://python.org) [![tests](https://github.com/zcqwh/iacs_ipac_reader/workflows/tests/badge.svg)](https://github.com/zcqwh/iacs_ipac_reader/actions) [![codecov](https://codecov.io/gh/zcqwh/iacs_ipac_reader/branch/main/graph/badge.svg)](https://codecov.io/gh/zcqwh/iacs_ipac_reader) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/iacs_ipac_reader)](https://napari-hub.org/plugins/iacs_ipac_reader)  A plugin used a convolutional neural network (CNN) to distinguish single platelets, platelet clusters, and white blood cells and performed classical image analysis for each subpopulation individually. Based on the derived single-cell features for each population, a Random Forest (RF) model was trained and used to classify COVID-19 associated thrombosis and non-COVID-19 associated thrombosis.  More information about IACS/iPAC.   __IACS__: DOI: [10.1016/j.cell.2018.08.028](https://www.sciencedirect.com/science/article/pii/S0092867418310444)    __iPAC__: DOI: [10.7554/eLife.52938](https://elifesciences.org/articles/52938)  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/plugins/stable/index.html -->  ## Installation  You can install `iacs_ipac_reader` via [pip]:      pip install iacs_ipac_reader    To install latest development version :      pip install git+https://github.com/zcqwh/iacs_ipac_reader.git   ## Introduction  The iacs-ipac-reader plugin mainly include 3 functional tabs:  * iPAC * IACS * AID classif.    ### iPAC image contour tracker <center>Interface of iPAC contour tracker</center>      ![ipac.](https://github.com/zcqwh/iacs_ipac_reader/blob/main/Tutorial/pictures/ipac.png?raw=true \"iPAC\")  ### IACS image contour tracker <center>Interface of IACS contour tracker</center>      ![iacs.](https://github.com/zcqwh/iacs_ipac_reader/blob/main/Tutorial/pictures/iacs.png?raw=true \"IACS\")  ### AID classif. <center>Interface of AID classif.</center>        ![AID_classif.](https://github.com/zcqwh/iacs_ipac_reader/blob/main/Tutorial/pictures/classifier.jpg?raw=true \"AID classif\")    ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"iacs_ipac_reader\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/zcqwh/iacs_ipac_reader/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/    ",
    "description_content_type": "text/markdown",
    "description_text": "iacs_ipac_reader       A plugin used a convolutional neural network (CNN) to distinguish single platelets, platelet clusters, and white blood cells and performed classical image analysis for each subpopulation individually. Based on the derived single-cell features for each population, a Random Forest (RF) model was trained and used to classify COVID-19 associated thrombosis and non-COVID-19 associated thrombosis. More information about IACS/iPAC. IACS: DOI: 10.1016/j.cell.2018.08.028  iPAC: DOI: 10.7554/eLife.52938  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install iacs_ipac_reader via pip: pip install iacs_ipac_reader  To install latest development version : pip install git+https://github.com/zcqwh/iacs_ipac_reader.git  Introduction The iacs-ipac-reader plugin mainly include 3 functional tabs:  iPAC IACS AID classif.  iPAC image contour tracker Interface of iPAC contour tracker   IACS image contour tracker Interface of IACS contour tracker   AID classif. Interface of AID classif.   Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"iacs_ipac_reader\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "iacs-ipac-reader",
    "documentation": "https://github.com/zcqwh/iacs_ipac_reader#README.md",
    "first_released": "2022-01-21T08:07:34.145777Z",
    "license": "BSD-3-Clause",
    "name": "iacs-ipac-reader",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/zcqwh/iacs_ipac_reader",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-04-12T06:17:48.661473Z",
    "report_issues": "https://github.com/zcqwh/iacs_ipac_reader/issues",
    "requirements": [
      "h5py (>=3.5.0)",
      "napari (>=0.4.12)",
      "napari-plugin-engine (>=0.2.0)",
      "numpy (>=1.21.4)",
      "opencv-contrib-python-headless (>=4.4.0.46)",
      "openpyxl (>=3.0.9)",
      "sklearn (>=0.0)",
      "PyQt5 (==5.12.3)",
      "pandas (>=1.4.0)"
    ],
    "summary": "A reader plugin for read iacs/ipac images and export .rtdc files.",
    "support": "https://github.com/zcqwh/iacs_ipac_reader/issues",
    "twitter": "",
    "version": "0.0.13",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      {
        "email": "emmanuel.bouilhol@u-bordeaux.fr",
        "name": "Emmanuel Bouilhol"
      }
    ],
    "code_repository": "https://github.com/ebouilhol/napari-DeepSpot",
    "description": "# napari-DeepSpot  [![License](https://img.shields.io/pypi/l/napari-DeepSpot.svg?color=green)](https://github.com/ebouilhol/napari-DeepSpot/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-DeepSpot.svg?color=green)](https://pypi.org/project/napari-DeepSpot) [![Python Version](https://img.shields.io/pypi/pyversions/napari-DeepSpot.svg?color=green)](https://python.org) [![tests](https://github.com/ebouilhol/napari-DeepSpot/workflows/tests/badge.svg)](https://github.com/ebouilhol/napari-DeepSpot/actions) [![codecov](https://codecov.io/gh/ebouilhol/napari-DeepSpot/branch/main/graph/badge.svg)](https://codecov.io/gh/ebouilhol/napari-DeepSpot) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-DeepSpot)](https://napari-hub.org/plugins/napari-DeepSpot)  RNA spot enhancement for fluorescent microscopy images.  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/docs/plugins/index.html -->  ## Installation  You can install `napari-DeepSpot` via [pip]:      pip install napari-DeepSpot  ## Build from source  This plugin is using Tensorflow, make sure your Python environment has Tensorflow, on create a new environment using the following commands: * Conda:   `conda env create -f environment.yml`   `conda activate deepspot-napari` * Or pip:    `pip install -r requirements.txt`  ## Usage  Open one or multiple images using Napari GUI :  File > Open > Select your image  The images are then displayed on Napari  Load the Plugin: Plugins > Napari-DeepSpot:Enhance Spot  ![Usage](./image/napari.png)  Click on the right panel Button \"Enhance\"  Wait a few seconds for the magic to happen :  ![Usage](./image/napari_enhance.png)  You can see the original images and the enhanced version in the left panel in the layer section.  To save the images : File > Save all layers or File > Save selected layers.   ![Usage](./image/napari_video.gif)    ## Citation If you use this plugin please cite the [paper](https://www.biorxiv.org/content/10.1101/2021.11.25.469984v1):  >@article {Bouilhol2021DeepSpot,   >\\t author = {Bouilhol, Emmanuel and Lefevre, Edgar and Dartigues, Benjamin and Brackin, Robyn and Savulescu, Anca Flavia and Nikolski, Macha},   >\\t title = {DeepSpot: a deep neural network for RNA spot enhancement in smFISH microscopy images},   >\\t elocation-id = {2021.11.25.469984},   >\\t year = {2021},   >\\t doi = {10.1101/2021.11.25.469984},   >\\t publisher = {Cold Spring Harbor Laboratory},   >\\t URL = {https://www.biorxiv.org/content/early/2021/11/25/2021.11.25.469984},   >\\t eprint = {https://www.biorxiv.org/content/early/2021/11/25/2021.11.25.469984.full.pdf},   >\\t journal = {bioRxiv}   >}    ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [MIT] license, \"napari-DeepSpot\" is free and open source software  ## Known Issues  If you have troubles with the Python packages `typing extensions`, use the command :   `pip install typing-extensions --upgrade`    When using \"Enhance\" on multiple images, Napari may freeze. Just wait until it comes to life again, the images will still be enhanced. This is due to Napari memory usage and will be fix one day.   ## Other Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/ebouilhol/napari-DeepSpot/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-DeepSpot       RNA spot enhancement for fluorescent microscopy images.  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install napari-DeepSpot via pip: pip install napari-DeepSpot  Build from source This plugin is using Tensorflow, make sure your Python environment has Tensorflow, on create a new environment using the following commands: * Conda: conda env create -f environment.yml conda activate deepspot-napari * Or pip:  pip install -r requirements.txt Usage Open one or multiple images using Napari GUI :  File > Open > Select your image The images are then displayed on Napari Load the Plugin: Plugins > Napari-DeepSpot:Enhance Spot  Click on the right panel Button \"Enhance\" Wait a few seconds for the magic to happen :  You can see the original images and the enhanced version in the left panel in the layer section. To save the images : File > Save all layers or File > Save selected layers.  Citation If you use this plugin please cite the paper:  @article {Bouilhol2021DeepSpot,    author = {Bouilhol, Emmanuel and Lefevre, Edgar and Dartigues, Benjamin and Brackin, Robyn and Savulescu, Anca Flavia and Nikolski, Macha},    title = {DeepSpot: a deep neural network for RNA spot enhancement in smFISH microscopy images},    elocation-id = {2021.11.25.469984},    year = {2021},    doi = {10.1101/2021.11.25.469984},    publisher = {Cold Spring Harbor Laboratory},    URL = {https://www.biorxiv.org/content/early/2021/11/25/2021.11.25.469984},    eprint = {https://www.biorxiv.org/content/early/2021/11/25/2021.11.25.469984.full.pdf},    journal = {bioRxiv} }    Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the MIT license, \"napari-DeepSpot\" is free and open source software Known Issues If you have troubles with the Python packages typing extensions, use the command : pip install typing-extensions --upgrade  When using \"Enhance\" on multiple images, Napari may freeze. Just wait until it comes to life again, the images will still be enhanced. This is due to Napari memory usage and will be fix one day. Other Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari-DeepSpot",
    "documentation": "https://github.com/ebouilhol/napari-DeepSpot#README.md",
    "first_released": "2021-11-29T15:55:18.777627Z",
    "license": "MIT",
    "name": "napari-DeepSpot",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/ebouilhol/napari-DeepSpot",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2021-11-30T17:41:05.396583Z",
    "report_issues": "https://github.com/ebouilhol/napari-DeepSpot/issues",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "pytest",
      "pytest-cov",
      "pytest-xvfb",
      "pytest-qt",
      "napari",
      "qtpy (==1.9.0)",
      "pyqt5",
      "tensorflow",
      "scikit-image",
      "opencv-python"
    ],
    "summary": "RNA spot enhancement for fluorescent microscopy images",
    "support": "https://github.com/ebouilhol/napari-DeepSpot/issues",
    "twitter": "",
    "version": "0.0.7",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Mara Lampert" }],
    "code_repository": "https://github.com/biapol/guanine-crystal-analysis",
    "conda": [],
    "description": "# guanine-crystal-analysis  [![License BSD-3](https://img.shields.io/pypi/l/guanine-crystal-analysis.svg?color=green)](https://github.com/biopo/guanine-crystal-analysis/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/guanine-crystal-analysis.svg?color=green)](https://pypi.org/project/guanine-crystal-analysis) [![Python Version](https://img.shields.io/pypi/pyversions/guanine-crystal-analysis.svg?color=green)](https://python.org) [![tests](https://github.com/biopo/guanine-crystal-analysis/workflows/tests/badge.svg)](https://github.com/biopo/guanine-crystal-analysis/actions) [![codecov](https://codecov.io/gh/biopo/guanine-crystal-analysis/branch/main/graph/badge.svg)](https://codecov.io/gh/biopo/guanine-crystal-analysis) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/guanine-crystal-analysis)](https://napari-hub.org/plugins/guanine-crystal-analysis)  A plugin for the guanine crystal segmentation, classification and characterization in the zebrafish eye  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/plugins/index.html -->  ## Installation  You can install `guanine-crystal-analysis` via [pip]:      pip install guanine-crystal-analysis    To install latest development version :      pip install git+https://github.com/biopo/guanine-crystal-analysis.git   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"guanine-crystal-analysis\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/biopo/guanine-crystal-analysis/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "guanine-crystal-analysis       A plugin for the guanine crystal segmentation, classification and characterization in the zebrafish eye  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install guanine-crystal-analysis via pip: pip install guanine-crystal-analysis  To install latest development version : pip install git+https://github.com/biopo/guanine-crystal-analysis.git  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"guanine-crystal-analysis\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "Guanine Crystal Analysis",
    "documentation": "https://github.com/biapol/guanine-crystal-analysis#README.md",
    "first_released": "2022-07-26T15:15:12.639399Z",
    "license": "BSD-3-Clause",
    "name": "guanine-crystal-analysis",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget", "sample_data"],
    "project_site": "https://github.com/biapol/guanine-crystal-analysis",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-07-26T15:15:12.639399Z",
    "report_issues": "https://github.com/biapol/guanine-crystal-analysis/issues",
    "requirements": [
      "numpy",
      "magicgui",
      "qtpy",
      "apoc",
      "scikit-image",
      "pandas",
      "napari-simpleitk-image-processing",
      "napari-skimage-regionprops",
      "pyclesperanto-prototype",
      "scikit-learn",
      "napari-workflows",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "summary": "A plugin for the guanine crystal segmentation, classification and characterization in the zebrafish eye",
    "support": "https://github.com/biapol/guanine-crystal-analysis/issues",
    "twitter": "",
    "version": "0.0.1",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "email": "niknett@gmail.com", "name": "Niklas Netter" }],
    "code_repository": "https://github.com/gatoniel/napari-3d-ortho-viewer",
    "description": "# napari-3d-ortho-viewer  [![License](https://img.shields.io/pypi/l/napari-3d-ortho-viewer.svg?color=green)](https://github.com/gatoniel/napari-3d-ortho-viewer/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-3d-ortho-viewer.svg?color=green)](https://pypi.org/project/napari-3d-ortho-viewer) [![Python Version](https://img.shields.io/pypi/pyversions/napari-3d-ortho-viewer.svg?color=green)](https://python.org) [![tests](https://github.com/gatoniel/napari-3d-ortho-viewer/workflows/tests/badge.svg)](https://github.com/gatoniel/napari-3d-ortho-viewer/actions) [![codecov](https://codecov.io/gh/gatoniel/napari-3d-ortho-viewer/branch/main/graph/badge.svg)](https://codecov.io/gh/gatoniel/napari-3d-ortho-viewer) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-3d-ortho-viewer)](https://napari-hub.org/plugins/napari-3d-ortho-viewer)  Napari 3D Ortho Viewer - an ortho viewer for napari for 3D images  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/docs/plugins/index.html -->  ## Installation  You can install `napari-3d-ortho-viewer` via [pip]:      pip install napari-3d-ortho-viewer    To install latest development version :      pip install git+https://github.com/gatoniel/napari-3d-ortho-viewer.git   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [MIT] license, \"napari-3d-ortho-viewer\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/gatoniel/napari-3d-ortho-viewer/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-3d-ortho-viewer       Napari 3D Ortho Viewer - an ortho viewer for napari for 3D images  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install napari-3d-ortho-viewer via pip: pip install napari-3d-ortho-viewer  To install latest development version : pip install git+https://github.com/gatoniel/napari-3d-ortho-viewer.git  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the MIT license, \"napari-3d-ortho-viewer\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari-3d-ortho-viewer",
    "documentation": "https://github.com/gatoniel/napari-3d-ortho-viewer#README.md",
    "first_released": "2021-12-03T01:07:40.403247Z",
    "license": "MIT",
    "name": "napari-3d-ortho-viewer",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/gatoniel/napari-3d-ortho-viewer",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2021-12-03T01:07:40.403247Z",
    "report_issues": "https://github.com/gatoniel/napari-3d-ortho-viewer/issues",
    "requirements": ["napari-plugin-engine (>=0.1.4)", "numpy"],
    "summary": "Napari 3D Ortho Viewer - an ortho viewer for napari for 3D images",
    "support": "https://github.com/gatoniel/napari-3d-ortho-viewer/issues",
    "twitter": "",
    "version": "0.0.1",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "email": "kpandit@nygenome.org", "name": "Kunal Pandit" }],
    "code_repository": "https://github.com/nygctech/PICASSO",
    "conda": [],
    "description": "# napari-PICASSO  [![License](https://img.shields.io/pypi/l/napari-curtain.svg?color=green)](https://github.com/nygctech/PICASSO/blob/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-PICASSO.svg?color=green)](https://pypi.org/project/napari-PICASSO) [![Python Version](https://img.shields.io/pypi/pyversions/napari-PICASSO.svg?color=green)](https://python.org) [![tests](https://github.com/nygctech/PICASSO/actions/workflows/test_and_deploy.yml/badge.svg?event=push)](https://github.com/nygctech/PICASSO/actions/workflows/test_and_deploy.yml) [![codecov](https://codecov.io/gh/nygctech/napari-PICASSO/branch/main/graph/badge.svg)](https://codecov.io/gh/nygctech/napari-PICASSO) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-PICASSO)](https://napari-hub.org/plugins/napari-PICASSO)  Unmix spectral spillover  ![](https://user-images.githubusercontent.com/72306584/176486552-50e1bca9-65fd-4466-8c92-a114e48d2278.gif)  ## Automatic Usage  You can find the `PICASSO` plugin in the menu `Plugins > napari-PICASSO: PICASSO`. Select sink images that have spectral spillover from corresponding source images, then click run to optimise the mixing parameters with PICASSO.   ## Manual Usage  ![](https://user-images.githubusercontent.com/72306584/176505151-572bd762-abe6-47b1-9821-4f3aaa4704c9.gif)  Select the manual button in options pop up window. Then select sink images that have spectral spillover from corresponding source images. In the source images window, sliders for each $source$ control the mixing spillover, $m$ (top), and background, $b$ (bottom, optional).  ## Mixing model  $$ sink = \\\\sum_{i} m_i(source - b_i) $$  ## Installation  You can install `napari-PICASSO` via [pip]:      pip install napari-PICASSO  ## Details  napari-PICASSO is a napari widget to blindly unmix fluorescence images of known members using PICASSO<sup>1</sup>.   For example, if 2 fluorophores with overlapping spectra are imaged, spillover fluorescesce from a channel into an adjacent channel could be removed if you know which channel is the source of the spillover fluorescence and which channel is the sink of the spillover fluorescence.   PICASSO is an algorithm to remove spillover fluorescence by minimizing the mutual information between sink and source images. The original algorithm described by Seo et al, minimized the mutual information between pairs of sink and source images using a Nelson-Mead simplex algorithm and computing the mutual information outright with custom written MATLAB code<sup>1</sup>. The napari plugin uses a neural net to estimate and minimize the mutual information (MINE<sup>2</sup>) between pairs of sink and source images using stochastic gradient descent with GPU acceleration.  ## References  1. Seo, J. et al. PICASSO allows ultra-multiplexed fluorescence imaging of spatially overlapping proteins without reference spectra measurements. Nat Commun 13, 2475 (2022). 2. Belghazi, M. I. et al. MINE: Mutual Information Neural Estimation. arXiv:1801.04062 [cs, stat] (2018).   [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-PICASSO       Unmix spectral spillover  Automatic Usage You can find the PICASSO plugin in the menu Plugins > napari-PICASSO: PICASSO. Select sink images that have spectral spillover from corresponding source images, then click run to optimise the mixing parameters with PICASSO.  Manual Usage  Select the manual button in options pop up window. Then select sink images that have spectral spillover from corresponding source images. In the source images window, sliders for each $source$ control the mixing spillover, $m$ (top), and background, $b$ (bottom, optional). Mixing model $$ sink = \\\\sum_{i} m_i(source - b_i) $$ Installation You can install napari-PICASSO via pip: pip install napari-PICASSO  Details napari-PICASSO is a napari widget to blindly unmix fluorescence images of known members using PICASSO1.  For example, if 2 fluorophores with overlapping spectra are imaged, spillover fluorescesce from a channel into an adjacent channel could be removed if you know which channel is the source of the spillover fluorescence and which channel is the sink of the spillover fluorescence.  PICASSO is an algorithm to remove spillover fluorescence by minimizing the mutual information between sink and source images. The original algorithm described by Seo et al, minimized the mutual information between pairs of sink and source images using a Nelson-Mead simplex algorithm and computing the mutual information outright with custom written MATLAB code1. The napari plugin uses a neural net to estimate and minimize the mutual information (MINE2) between pairs of sink and source images using stochastic gradient descent with GPU acceleration. References  Seo, J. et al. PICASSO allows ultra-multiplexed fluorescence imaging of spatially overlapping proteins without reference spectra measurements. Nat Commun 13, 2475 (2022). Belghazi, M. I. et al. MINE: Mutual Information Neural Estimation. arXiv:1801.04062 [cs, stat] (2018). ",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "napari-PICASSO",
    "documentation": "",
    "first_released": "2022-06-01T01:48:01.350779Z",
    "license": "GPL-3.0",
    "name": "napari-PICASSO",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/nygctech/PICASSO",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-06-29T18:36:43.599731Z",
    "report_issues": "https://github.com/nygctech/PICASSO/issues",
    "requirements": [
      "numpy",
      "magicgui",
      "qtpy",
      "dask",
      "psutil",
      "tox ; extra == 'testing'",
      "napari[all] ; extra == 'testing'",
      "torch ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "xarray ; extra == 'testing'"
    ],
    "summary": "Blind fluorescence unmixing",
    "support": "",
    "twitter": "",
    "version": "0.3.0",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Volker Baecker" }],
    "code_repository": "https://github.com/MontpellierRessourcesImagerie/napari-J",
    "conda": [{ "channel": "conda-forge", "package": "napari-j" }],
    "description": "# napari-J  [![License](https://img.shields.io/pypi/l/napari-J.svg?color=green)](https://github.com/MontpellierRessourcesImagerie/napari-J/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-J.svg?color=green)](https://pypi.org/project/napari-J) [![Python Version](https://img.shields.io/pypi/pyversions/napari-J.svg?color=green)](https://python.org) [![tests](https://github.com/MontpellierRessourcesImagerie/napari-J/workflows/tests/badge.svg)](https://github.com/MontpellierRessourcesImagerie/napari-J/actions) [![codecov](https://codecov.io/gh/MontpellierRessourcesImagerie/napari-J/branch/master/graph/badge.svg)](https://codecov.io/gh/MontpellierRessourcesImagerie/napari-J)  A plugin to exchange data with FIJI and to use FIJI image analysis from napari. Current features are:   * get the active image from FIJI  * send a screenshot to FIJI  * get a set of points from the FIJI results table  * filter the points in napari  * send the filtered points back to FIJI   Known problems:  * Crashes on linux  when the file-dialog is opened. Workaround: Set the option ``Use JFileChooser to open/save`` from the ``Edit>Options>Input/Output`` menu. * 03.05.2022 - For now please use it with napari 0.4.12, there is a vispy bug in 0.4.15 concerning labelled masks * 03.05.2022 - Currently you need to have the range of the quality values for point between 0 and 255, in the new version they can have any range, but we are waiting for the bug in napari 0.4.15 to be fixed to release this.   ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/docs/plugins/index.html -->  ## Installation  You can install `napari-J` via [pip]:      pip install napari-J  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [MIT] license, \"napari-J\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue](https://github.com/MontpellierRessourcesImagerie/napari-J/issues) along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin [file an issue]: https://github.com/MontpellierRessourcesImagerie/napari-J/issues [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-J      A plugin to exchange data with FIJI and to use FIJI image analysis from napari. Current features are:  get the active image from FIJI send a screenshot to FIJI get a set of points from the FIJI results table filter the points in napari send the filtered points back to FIJI  Known problems:  Crashes on linux  when the file-dialog is opened. Workaround: Set the option Use JFileChooser to open/save from the Edit>Options>Input/Output menu. 03.05.2022 - For now please use it with napari 0.4.12, there is a vispy bug in 0.4.15 concerning labelled masks 03.05.2022 - Currently you need to have the range of the quality values for point between 0 and 255, in the new version they can have any range, but we are waiting for the bug in napari 0.4.15 to be fixed to release this.    This napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.  Installation You can install napari-J via pip: pip install napari-J  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the MIT license, \"napari-J\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "napari-J",
    "documentation": "https://github.com/MontpellierRessourcesImagerie/napari-J#README.md",
    "first_released": "2022-01-07T13:39:21.434595Z",
    "license": "MIT",
    "name": "napari-J",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/MontpellierRessourcesImagerie/napari-J",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-10-16T21:53:27.380502Z",
    "report_issues": "https://github.com/MontpellierRessourcesImagerie/napari-J/issues",
    "requirements": [
      "JPype1 (>=1.2.1)",
      "matplotlib",
      "imageio-ffmpeg",
      "matplotlib ; extra == 'testing'",
      "imageio-ffmpeg ; extra == 'testing'",
      "python-matplotlib-qt5 ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'"
    ],
    "summary": "A plugin to exchange data with FIJI and to use FIJI image analysis from napari",
    "support": "https://github.com/MontpellierRessourcesImagerie/napari-J/issues",
    "twitter": "",
    "version": "0.2.4",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      { "email": "cqzhang@g.ecc.u-tokyo.ac.jp", "name": "Chenqi Zhang" }
    ],
    "code_repository": "https://github.com/zcqwh/napari-aideveloper",
    "conda": [],
    "description": "# napari-aideveloper  [![License](https://img.shields.io/pypi/l/napari-aideveloper.svg?color=green)](https://github.com/zcqwh/napari-aideveloper/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-aideveloper.svg?color=green)](https://pypi.org/project/napari-aideveloper) [![Python Version](https://img.shields.io/pypi/pyversions/napari-aideveloper.svg?color=green)](https://python.org) [![tests](https://github.com/zcqwh/napari-aideveloper/workflows/tests/badge.svg)](https://github.com/zcqwh/napari-aideveloper/actions) [![codecov](https://codecov.io/gh/zcqwh/napari-aideveloper/branch/main/graph/badge.svg)](https://codecov.io/gh/zcqwh/napari-aideveloper) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-aideveloper)](https://napari-hub.org/plugins/napari-aideveloper)  [napari_aideveloper](https://www.napari-hub.org/plugins/napari-aideveloper) is a napari-plugin derived from [AIDeveloper](https://github.com/maikherbig/AIDeveloper) that allows you to train, evaluate, and apply deep neural nets for image classification within a graphical user-interface (GUI).   <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/plugins/index.html -->  ## Installation  You can install `napari-aideveloper` via [pip]:      pip install napari-aideveloper  ## Introduction ### Main functions * [Build](#build) * [History](#history)  ****  ### Build  #### 1. Load data Drag and drop your data in .rtdc (HDF5) format into the file table and set the class and training/validation. ![alt Load_data](https://github.com/zcqwh/napari-aideveloper/blob/main/Tutorial/00_Load_data.gif?raw=true)  #### 2. Choose Neural Networks ![alt Load_data](https://github.com/zcqwh/napari-aideveloper/blob/main/Tutorial/01_choose%20NN.gif?raw=true)  #### 3. Set model storage path ![alt Load_data](https://github.com/zcqwh/napari-aideveloper/blob/main/Tutorial/02_save_model.gif?raw=true)  #### 4. Start fitting ![alt Load_data](https://github.com/zcqwh/napari-aideveloper/blob/main/Tutorial/03_start_fitting.gif?raw=true) ![alt Load_data](https://github.com/zcqwh/napari-aideveloper/blob/main/Tutorial/04_fitting.gif?raw=true)  #### Preview image ![alt Load_data](https://github.com/zcqwh/napari-aideveloper/blob/main/Tutorial/05_preview.gif?raw=true)  #### Image augmentation ![alt Load_data](https://github.com/zcqwh/napari-aideveloper/blob/main/Tutorial/06_augmentation.gif?raw=true)  ****  ### History #### 1. Load meta data ![alt Load_data](https://github.com/zcqwh/napari-aideveloper/blob/development/Tutorial/GIF/History/01_Load_metadata.gif?raw=true)  #### 2. Check model details ![alt Load_data](https://github.com/zcqwh/napari-aideveloper/blob/development/Tutorial/GIF/History/02_model_detail.gif?raw=true)  #### 3. Rolling median & Linear fit ![alt Load_data](https://github.com/zcqwh/napari-aideveloper/blob/development/Tutorial/GIF/History/03_rolling_linear.gif?raw=true)       ## Contributing  Contributions are very welcome. You can submit your pull request on [GitHub](https://github.com/zcqwh/napari-aideveloper/pulls). Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-aideveloper\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue](https://github.com/zcqwh/napari-aideveloper/issues) along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template. ",
    "description_content_type": "text/markdown",
    "description_text": "napari-aideveloper       napari_aideveloper is a napari-plugin derived from AIDeveloper that allows you to train, evaluate, and apply deep neural nets for image classification within a graphical user-interface (GUI).  Installation You can install napari-aideveloper via pip: pip install napari-aideveloper  Introduction Main functions  Build History   Build 1. Load data Drag and drop your data in .rtdc (HDF5) format into the file table and set the class and training/validation.  2. Choose Neural Networks  3. Set model storage path  4. Start fitting   Preview image  Image augmentation   History 1. Load meta data  2. Check model details  3. Rolling median & Linear fit  Contributing Contributions are very welcome. You can submit your pull request on GitHub. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-aideveloper\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari AIDeveloper",
    "documentation": "https://github.com/zcqwh/napari-aideveloper#README.md",
    "first_released": "2022-05-20T04:19:53.303321Z",
    "license": "BSD-3-Clause",
    "name": "napari-aideveloper",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/zcqwh/napari-aideveloper",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-06-17T05:41:19.373520Z",
    "report_issues": "https://github.com/zcqwh/napari-aideveloper/issues",
    "requirements": [
      "numpy",
      "magicgui",
      "qtpy",
      "dclab (>=0.39.9)",
      "h5py (>=3.6.0)",
      "Keras-Applications (>=1.0.8)",
      "keras (>=2.8.0)",
      "keras-metrics (>=1.1.0)",
      "napari-plugin-engine (>=0.2.0)",
      "napari (>=0.4.14)",
      "onnx (>=1.11.0)",
      "opencv-contrib-python-headless (>=4.5.5.62)",
      "openpyxl (>=3.0.9)",
      "pandas (>=1.4.1)",
      "Pillow (>=9.1.1)",
      "psutil (>=5.9.0)",
      "pyqtgraph (>=0.12.3)",
      "pytest (>=7.1.2)",
      "scikit-learn (>=1.1.1)",
      "scipy (>=1.8.0)",
      "setuptools (>=58.0.4)",
      "six (>=1.16.0)",
      "tensorboard (>=2.8.0)",
      "tensorflow (>=2.8.0)",
      "tf2onnx (>=1.9.3)",
      "xlrd (>=2.0.1)",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "summary": "napari_aideveloper is a napari-plugin deived from AIDeveloper that allows you to train, evaluate and apply deep neural nets for image classification within a graphical user-interface (GUI).",
    "support": "https://github.com/zcqwh/napari-aideveloper/issues",
    "twitter": "",
    "version": "0.0.4",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Kevin Yamauchi" }],
    "code_repository": "https://github.com/morphometrics/morphospace",
    "description": "# morphospace  [![License BSD-3](https://img.shields.io/pypi/l/morphospace.svg?color=green)](https://github.com/morphometrics/morphospace/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/morphospace.svg?color=green)](https://pypi.org/project/morphospace) [![Python Version](https://img.shields.io/pypi/pyversions/morphospace.svg?color=green)](https://python.org) [![tests](https://github.com/morphometrics/morphospace/workflows/tests/badge.svg)](https://github.com/morphometrics/morphospace/actions) [![codecov](https://codecov.io/gh/morphometrics/morphospace/branch/main/graph/badge.svg)](https://codecov.io/gh/morphometrics/morphospace) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/morphospace)](https://napari-hub.org/plugins/morphospace)  a library for creating  and exploring morphospaces.  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/plugins/index.html -->  ## Installation  You can install `morphospace` via [pip]:      pip install morphospace    To install latest development version :      pip install git+https://github.com/morphometrics/morphospace.git   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"morphospace\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/morphometrics/morphospace/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "morphospace       a library for creating  and exploring morphospaces.  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install morphospace via pip: pip install morphospace  To install latest development version : pip install git+https://github.com/morphometrics/morphospace.git  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"morphospace\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "morphospace",
    "documentation": "https://github.com/morphometrics/morphospace#README.md",
    "first_released": "2022-12-10T13:52:40.629551Z",
    "license": "BSD-3-Clause",
    "name": "morphospaces",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget", "sample_data"],
    "project_site": "https://github.com/morphometrics/morphospaces",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-12-10T13:52:40.629551Z",
    "report_issues": "https://github.com/morphometrics/morphospace/issues",
    "requirements": [
      "h5py",
      "magicgui",
      "numpy",
      "pytorch-lightning",
      "qtpy",
      "scikit-image",
      "scipy",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "tox ; extra == 'testing'"
    ],
    "summary": "a library for creating  and exploring morphospaces.",
    "support": "https://github.com/morphometrics/morphospace/issues",
    "twitter": "",
    "version": "0.0.3",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "William Patton" }],
    "code_repository": "https://github.com/pattonw/napari-affinities",
    "description": "# napari-affinities  [![License](https://img.shields.io/pypi/l/napari-affinities.svg?color=green)](https://github.com/pattonw/napari-affinities/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-affinities.svg?color=green)](https://pypi.org/project/napari-affinities) [![Python Version](https://img.shields.io/pypi/pyversions/napari-affinities.svg?color=green)](https://python.org) [![tests](https://github.com/pattonw/napari-affinities/workflows/tests/badge.svg)](https://github.com/pattonw/napari-affinities/actions) [![codecov](https://codecov.io/gh/pattonw/napari-affinities/branch/main/graph/badge.svg)](https://codecov.io/gh/pattonw/napari-affinities) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-affinities)](https://napari-hub.org/plugins/napari-affinities)  A plugin for creating, visualizing, and processing affinities  ---  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/plugins/stable/index.html -->  ## Installation  You will need a conda environment for everything to run smoothly. Supported python versions are 3.7, 3.8, 3.9.  ### pip You can install `napari-affinities` via [pip]:      `pip install napari-affinities`  To install latest development version :      `pip install git+https://github.com/pattonw/napari-affinities.git`  Install torch according to your system [(follow the instructions here)](https://pytorch.org/get-started/locally/). For example with cuda 10.2 available, run:      conda install pytorch torchvision cudatoolkit=10.2 -c pytorch  Install conda requirements:      conda install -c conda-forge affogato  ### conda  If you install via conda, there are fewer steps since affogato and pytorch will be installed for you.  You can install `napari-affinities` via [conda]:      `conda install -c conda-forge napari-affinities`  ### Download example model:  #### 2D:  [epithelial example model](https://oc.embl.de/index.php/s/zfWMKu7HoQnSJLs) Place the model zip file wherever you want. You can open it in the plugin with the \"load from file\" button.  #### 3D  [lightsheet example model](https://owncloud.gwdg.de/index.php/s/LsShICsOcilqPRs) Unpack the tar file into test data (`lightsheet_nuclei_test_data` (an hdf5 file)) and model (`LightsheetNucleusSegmentation.zip` (a bioimageio model)). Move the data into sample_data which will enable you to load the \"Lightsheet Sample\" data in napari. Place the model zip file anywhere you want. You can open it in the plugin with the \"load from file\" button.  ##### Workarounds to be fixed:  1. you need to update the `rdf.yaml` in the `LightsheetNucleusSegmentation.zip` with the following:    - \"shape\" for \"input0\" should be updated with a larger minimum input size and \"output0\" should be updated with a larger halo. If not fixed, there will be significant tiling artifacts.    - (Optional) \"output0\" should be renamed to affinities. The plugin supports multiple outputs and relies on names for figuring out which one is which. If unrecognized names are provided we assume the outputs are ordered (affinities, fgbg, lsds) but this is less reliable than explicit names. 2. This model also generates foreground in the same array as affinities, i.e. a 10 channel output `(fgbg, [-1, 0, 0], [0, -1, 0], [0, 0, -1], [-2, 0, 0], ...)`. Although predictions will work, post processing such as mutex watershed will break unless you manually separate the first channel.  ## Use  Requirements for the model:  1. Bioimageio packaged pytorch model 2. Outputs with names \"affinities\", \"fgbg\"(optional) or \"lsds\"(optional)    - if these names are not used, it will be assumed that the outputs are affinities, fgbg, then lsds in that order  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [MIT] license, \"napari-affinities\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [mit]: http://opensource.org/licenses/MIT [bsd-3]: http://opensource.org/licenses/BSD-3-Clause [gnu gpl v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [gnu lgpl v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [apache software license 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [mozilla public license 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin [file an issue]: https://github.com/pattonw/napari-affinities/issues [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [pypi]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-affinities       A plugin for creating, visualizing, and processing affinities  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You will need a conda environment for everything to run smoothly. Supported python versions are 3.7, 3.8, 3.9. pip You can install napari-affinities via pip: `pip install napari-affinities`  To install latest development version : `pip install git+https://github.com/pattonw/napari-affinities.git`  Install torch according to your system (follow the instructions here). For example with cuda 10.2 available, run: conda install pytorch torchvision cudatoolkit=10.2 -c pytorch  Install conda requirements: conda install -c conda-forge affogato  conda If you install via conda, there are fewer steps since affogato and pytorch will be installed for you. You can install napari-affinities via [conda]: `conda install -c conda-forge napari-affinities`  Download example model: 2D: epithelial example model Place the model zip file wherever you want. You can open it in the plugin with the \"load from file\" button. 3D lightsheet example model Unpack the tar file into test data (lightsheet_nuclei_test_data (an hdf5 file)) and model (LightsheetNucleusSegmentation.zip (a bioimageio model)). Move the data into sample_data which will enable you to load the \"Lightsheet Sample\" data in napari. Place the model zip file anywhere you want. You can open it in the plugin with the \"load from file\" button. Workarounds to be fixed:  you need to update the rdf.yaml in the LightsheetNucleusSegmentation.zip with the following: \"shape\" for \"input0\" should be updated with a larger minimum input size and \"output0\" should be updated with a larger halo. If not fixed, there will be significant tiling artifacts. (Optional) \"output0\" should be renamed to affinities. The plugin supports multiple outputs and relies on names for figuring out which one is which. If unrecognized names are provided we assume the outputs are ordered (affinities, fgbg, lsds) but this is less reliable than explicit names. This model also generates foreground in the same array as affinities, i.e. a 10 channel output (fgbg, [-1, 0, 0], [0, -1, 0], [0, 0, -1], [-2, 0, 0], ...). Although predictions will work, post processing such as mutex watershed will break unless you manually separate the first channel.  Use Requirements for the model:  Bioimageio packaged pytorch model Outputs with names \"affinities\", \"fgbg\"(optional) or \"lsds\"(optional) if these names are not used, it will be assumed that the outputs are affinities, fgbg, then lsds in that order  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the MIT license, \"napari-affinities\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari Affinities",
    "documentation": "https://github.com/pattonw/napari-affinities#README.md",
    "first_released": "2022-06-12T17:19:57.548964Z",
    "license": "MIT",
    "name": "napari-affinities",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget", "sample_data"],
    "project_site": "https://github.com/pattonw/napari-affinities",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-11-27T01:13:39.002044Z",
    "report_issues": "https://github.com/pattonw/napari-affinities/issues",
    "requirements": [
      "numpy",
      "zarr",
      "magicgui",
      "bioimageio.core",
      "gunpowder",
      "matplotlib",
      "torch",
      "lsds"
    ],
    "summary": "A plugin for creating, visualizing, and processing affinities",
    "support": "https://github.com/pattonw/napari-affinities/issues",
    "twitter": "",
    "version": "0.1.3",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Jules Scholler" }],
    "code_repository": null,
    "conda": [],
    "description": "Plugin for annotating TissueScope data. ",
    "description_content_type": "text/markdown",
    "description_text": "Plugin for annotating TissueScope data.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari-annotate",
    "documentation": "",
    "first_released": "2022-11-03T14:24:57.004714Z",
    "license": "MPL-2.0",
    "name": "napari-annotate",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader", "widget"],
    "project_site": "https://github.com/WyssCenter",
    "python_version": ">=3.8",
    "reader_file_extensions": ["*_pbm"],
    "release_date": "2022-11-03T14:24:57.004714Z",
    "report_issues": "",
    "requirements": [
      "numpy",
      "napari[all]",
      "napari-tools-menu",
      "magic-class",
      "napari-plugin-engine (>=0.1.4)"
    ],
    "summary": "Annotate large 2D slides",
    "support": "",
    "twitter": "",
    "version": "0.0.1",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [],
    "code_repository": "https://github.com/AllenCellModeling/napari-aicsimageio",
    "conda": [{ "channel": "conda-forge", "package": "napari-aicsimageio" }],
    "description": "## Features  -   Supports reading metadata and imaging data for:     -   `OME-TIFF`     -   `TIFF`     -   `CZI` (Zeiss)     -   `LIF` (Leica)     -   `ND2` (Nikon)     -   `DV` (DeltaVision)     -   Any formats supported by [aicsimageio](https://github.com/AllenCellModeling/aicsimageio)     -   Any formats supported by [bioformats](https://github.com/tlambert03/bioformats_jar)         -   `SLD` (Slidebook)         -   `SVS` (Aperio)         -   [Full List](https://docs.openmicroscopy.org/bio-formats/6.5.1/supported-formats.html)     -   Any additional format supported by [imageio](https://github.com/imageio/imageio)         -   `PNG`         -   `JPG`         -   `GIF`         -   `AVI`         -   [Full List](https://imageio.readthedocs.io/en/v2.4.1/formats.html)  _While upstream `aicsimageio` is released under BSD-3 license, this plugin is released under GPLv3 license because it installs all format reader dependencies._  ### Reading Mode Threshold  This image reading plugin will load the provided image directly into memory if it meets the following two conditions:  1. The filesize is less than 4GB. 2. The filesize is less than 30% of machine memory available.  If either of these conditions isn't met, the image is loaded in chunks only as needed.  ## Examples of Features  #### General Image Reading  All image file formats supported by [aicsimageio](https://github.com/AllenCellModeling/aicsimageio) will be read and all raw data will be available in the napari viewer.  In addition, when reading an OME-TIFF, you can view all OME metadata directly in the napari viewer thanks to `ome-types`.  ![screenshot of an OME-TIFF image view, multi-channel, z-stack, with metadata viewer](https://raw.githubusercontent.com/AllenCellModeling/napari-aicsimageio/main/images/ome-tiff-with-metadata-viewer.png)  #### Multi-Scene Selection  When reading a multi-scene file, a widget will be added to the napari viewer to manage scene selection (clearing the viewer each time you change scene or adding the scene content to the viewer) and a list of all scenes in the file.  ![gif of drag and drop file to scene selection and management](https://raw.githubusercontent.com/AllenCellModeling/napari-aicsimageio/main/images/scene-selection.gif)  #### Access to the AICSImage Object and Metadata  ![napari viewer with console open showing `viewer.layers[0].metadata`](https://raw.githubusercontent.com/AllenCellModeling/napari-aicsimageio/main/images/console-access.png)  You can access the `AICSImage` object used to load the image pixel data and image metadata using the built-in napari console:  ```python img = viewer.layers[0].metadata[\"aicsimage\"] img.dims.order  # TCZYX img.channel_names  # [\"Bright\", \"Struct\", \"Nuc\", \"Memb\"] img.get_image_dask_data(\"ZYX\")  # dask.array.Array ```  The napari layer metadata dictionary also stores a shorthand for the raw image metadata:  ```python viewer.layers[0].metadata[\"raw_image_metadata\"] ```  The metadata is returned in whichever format is used by the underlying file format reader, i.e. for CZI the raw metadata is returned as an `xml.etree.ElementTree.Element`, for OME-TIFF the raw metadata is returned as an `OME` object from `ome-types`.  Lastly, if the underlying file format reader has an OME metadata conversion function, you may additionally see a key in the napari layer metadata dictionary called `\"ome_types\"`. For example, because the AICSImageIO `CZIReader` and `BioformatsReader` both support converting raw image metadata to OME metadata, you will see an `\"ome_types\"` key that stores the metadata transformed into the OME metadata model.  ```python viewer.layers[0].metadata[\"ome_types\"]  # OME object from ome-types ```  #### Mosaic Reading  When reading CZI or LIF images, if the image is a mosaic tiled image, `napari-aicsimageio` will return the reconstructed image:  ![screenshot of a reconstructed / restitched mosaic tile LIF](https://raw.githubusercontent.com/AllenCellModeling/napari-aicsimageio/main/images/tiled-lif.png)  ## Citation  If you find `aicsimageio` _(or `napari-aicsimageio`)_ useful, please cite as:  > AICSImageIO Contributors (2021). AICSImageIO: Image Reading, Metadata Conversion, and Image Writing for Microscopy Images in Pure Python [Computer software]. GitHub. https://github.com/AllenCellModeling/aicsimageio  _Free software: GPLv3_ ",
    "description_content_type": "text/markdown",
    "description_text": "Features  Supports reading metadata and imaging data for: OME-TIFF TIFF CZI (Zeiss) LIF (Leica) ND2 (Nikon) DV (DeltaVision) Any formats supported by aicsimageio Any formats supported by bioformats SLD (Slidebook) SVS (Aperio) Full List   Any additional format supported by imageio PNG JPG GIF AVI Full List      While upstream aicsimageio is released under BSD-3 license, this plugin is released under GPLv3 license because it installs all format reader dependencies. Reading Mode Threshold This image reading plugin will load the provided image directly into memory if it meets the following two conditions:  The filesize is less than 4GB. The filesize is less than 30% of machine memory available.  If either of these conditions isn't met, the image is loaded in chunks only as needed. Examples of Features General Image Reading All image file formats supported by aicsimageio will be read and all raw data will be available in the napari viewer. In addition, when reading an OME-TIFF, you can view all OME metadata directly in the napari viewer thanks to ome-types.  Multi-Scene Selection When reading a multi-scene file, a widget will be added to the napari viewer to manage scene selection (clearing the viewer each time you change scene or adding the scene content to the viewer) and a list of all scenes in the file.  Access to the AICSImage Object and Metadata  You can access the AICSImage object used to load the image pixel data and image metadata using the built-in napari console: python img = viewer.layers[0].metadata[\"aicsimage\"] img.dims.order  # TCZYX img.channel_names  # [\"Bright\", \"Struct\", \"Nuc\", \"Memb\"] img.get_image_dask_data(\"ZYX\")  # dask.array.Array The napari layer metadata dictionary also stores a shorthand for the raw image metadata: python viewer.layers[0].metadata[\"raw_image_metadata\"] The metadata is returned in whichever format is used by the underlying file format reader, i.e. for CZI the raw metadata is returned as an xml.etree.ElementTree.Element, for OME-TIFF the raw metadata is returned as an OME object from ome-types. Lastly, if the underlying file format reader has an OME metadata conversion function, you may additionally see a key in the napari layer metadata dictionary called \"ome_types\". For example, because the AICSImageIO CZIReader and BioformatsReader both support converting raw image metadata to OME metadata, you will see an \"ome_types\" key that stores the metadata transformed into the OME metadata model. python viewer.layers[0].metadata[\"ome_types\"]  # OME object from ome-types Mosaic Reading When reading CZI or LIF images, if the image is a mosaic tiled image, napari-aicsimageio will return the reconstructed image:  Citation If you find aicsimageio (or napari-aicsimageio) useful, please cite as:  AICSImageIO Contributors (2021). AICSImageIO: Image Reading, Metadata Conversion, and Image Writing for Microscopy Images in Pure Python [Computer software]. GitHub. https://github.com/AllenCellModeling/aicsimageio  Free software: GPLv3",
    "development_status": ["Development Status :: 5 - Production/Stable"],
    "display_name": "napari-aicsimageio",
    "documentation": "https://github.com/AllenCellModeling/napari-aicsimageio#README.md",
    "first_released": "2020-03-26T20:10:54.889461Z",
    "license": "GPL-3.0",
    "name": "napari-aicsimageio",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader"],
    "project_site": "",
    "python_version": ">=3.8",
    "reader_file_extensions": [
      "*.afi",
      "*.flex",
      "*.vtk",
      "*.fpx",
      "*.wpi",
      "*.spc",
      "*.ffr",
      "*.mod",
      "*.l2d",
      "*.htd",
      "*.spe",
      "*.npz",
      "*.mpo",
      "*.stp",
      "*.tiff",
      "*.3fr",
      "*.lif",
      "*.gif",
      "*.img",
      "*.csv",
      "*.bw",
      "*.cur",
      "*.tim",
      "*.xdce",
      "*.gdcm",
      "*.fit",
      "*.mic",
      "*.sti",
      "*.mri",
      "*.tfr",
      "*.cut",
      "*.nii",
      "*.crw",
      "*.jp2",
      "*.obf",
      "*.czi",
      "*.grey",
      "*.mvd2",
      "*.targa",
      "*.ras",
      "*.cine",
      "*.zip",
      "*.ano",
      "*.bsdf",
      "*.zfr",
      "*.grib",
      "*.cap",
      "*.db",
      "*.mha",
      "*.par",
      "*.pef",
      "*.flc",
      "*.j2c",
      "*.raw",
      "*.array-like",
      "*.cr2",
      "*.lbm",
      "*.mkv",
      "*.psd",
      "*.al3d",
      "*.drf",
      "*.xpm",
      "*.bmp",
      "*.rwz",
      "*.ftc",
      "*.dsc",
      "*.sm3",
      "*.pict",
      "*.tnb",
      "*.wmf",
      "*.ome",
      "*.dti",
      "*.swf",
      "*.his",
      "*.koa",
      "*.nia",
      "*.lei",
      "*.im",
      "*.srf",
      "*.bufr",
      "*.ch5",
      "*.kdc",
      "*.mdb",
      "*.lms",
      "*.nhdr",
      "*.pgm",
      "*.fid",
      "*.rec",
      "*.rgba",
      "*.webp",
      "*.zfp",
      "*.zpo",
      "*.liff",
      "*.gel",
      "*.mos",
      "*.nrw",
      "*.exr",
      "*.mhd",
      "*.mnc2",
      "*.pxn",
      "*.ndpis",
      "*.inr",
      "*.cxd",
      "*.sif",
      "*.wap",
      "*.labels",
      "*.rdc",
      "*.cat",
      "*.naf",
      "*.oir",
      "*.pbm",
      "*.svs",
      "*.imggz",
      "*.ipl",
      "*.png",
      "*.mef",
      "*.dng",
      "*.jng",
      "*.zvi",
      "*.wbmp",
      "*.mnc",
      "*.ftu",
      "*.ico",
      "*.dat",
      "*.lfp",
      "*.hdf",
      "*.mgh",
      "*.orf",
      "*.iff",
      "*.amiramesh",
      "*.xml",
      "*.nii.gz",
      "*.ecw",
      "*.arw",
      "*.bin",
      "*.dcx",
      "*.fli",
      "*.mp4",
      "*.exp",
      "*.oib",
      "*.cfg",
      "*.sr2",
      "*.top",
      "*.1sc",
      "*.jxr",
      "*.jfif",
      "*.fits",
      "*.mrc",
      "*.emf",
      "*.jpc",
      "*.arf",
      "*.ics",
      "*.bif",
      "*.dcr",
      "*.nrrd",
      "*.sdt",
      "*.k25",
      "*.dv",
      "*.iim",
      "*.mdc",
      "*.ome.tif",
      "*.jpg",
      "*.bip",
      "*.rw2",
      "*.pxr",
      "*.j2k",
      "*.spi",
      "*.klb",
      "*.vsi",
      "*.qptiff",
      "*.jpe",
      "*.xbm",
      "*.afm",
      "*.pic",
      "*.sld",
      "*.dm2",
      "*.wat",
      "*.ome.tiff",
      "*.rcpnl",
      "*.ipw",
      "*.mng",
      "*.pnl",
      "*.vws",
      "*.wav",
      "*.jpf",
      "*.lsm",
      "*.wmv",
      "*.srw",
      "*.jpk",
      "*.ia",
      "*.pcx",
      "*.c01",
      "*.html",
      "*.nef",
      "*.dicom",
      "*.htm",
      "*.gipl",
      "*.qtk",
      "*.cif",
      "*.h5",
      "*.avi",
      "*.fdf",
      "*.im3",
      "*.mrw",
      "*.thm",
      "*.msr",
      "*.epsi",
      "*.inf",
      "*.ptx",
      "*.wdp",
      "*.xys",
      "*.i2i",
      "*.kc2",
      "*.am",
      "*.bay",
      "*.icns",
      "*.df3",
      "*.ali",
      "*.mov",
      "*.scn",
      "*.dc2",
      "*.fff",
      "*.jpx",
      "*.pct",
      "*.hx",
      "*.stk",
      "*.lfr",
      "*.frm",
      "*.hed",
      "*.jpeg",
      "*.ndpi",
      "*.st",
      "*.ps",
      "*.dds",
      "*.ims",
      "*.tga",
      "*.ct",
      "*.dcm",
      "*.ct.img",
      "*.dm3",
      "*.nd",
      "*.seq",
      "*.hdr",
      "*.raf",
      "*.vms",
      "*.xqf",
      "*.pr3",
      "*.jif",
      "*.rwl",
      "*.apl",
      "*.mpg",
      "*.hdp",
      "*.tif",
      "*.xv",
      "*.pcd",
      "*.lim",
      "*.eps",
      "*.hdf5",
      "*.bmq",
      "*.fake",
      "*.vff",
      "*.wbm",
      "*.oif",
      "*.pcoraw",
      "*.nd2",
      "*.pfm",
      "*.v",
      "*.fz",
      "*.mpeg",
      "*.niigz",
      "*.fts",
      "*.mtb",
      "*.aim",
      "*.scan",
      "*.gbr",
      "*.msp",
      "*.acqp",
      "*.txt",
      "*.ids",
      "*.xqd",
      "*.ipm",
      "*.rgb",
      "*.cs1",
      "*.sm2",
      "*.sxm",
      "*.wlz",
      "*.2fl",
      "*.ppm",
      "*.acff",
      "*.g3",
      "*.iiq",
      "*.r3d",
      "*.erf"
    ],
    "release_date": "2022-08-22T15:31:20.051131Z",
    "report_issues": "https://github.com/AllenCellModeling/napari-aicsimageio/issues",
    "requirements": [
      "aicsimageio[all] (>=4.6.3)",
      "fsspec[http] (>=2022.7.1)",
      "napari (>=0.4.11)",
      "psutil (>=5.7.0)",
      "aicspylibczi (>=3.0.5)",
      "bioformats-jar",
      "readlif (>=0.6.4)",
      "black (>=19.10b0) ; extra == 'dev'",
      "coverage (>=5.1) ; extra == 'dev'",
      "docutils (<0.16,>=0.10) ; extra == 'dev'",
      "flake8-debugger (>=3.2.1) ; extra == 'dev'",
      "flake8-pyprojecttoml ; extra == 'dev'",
      "flake8 (>=3.8.3) ; extra == 'dev'",
      "ipython (>=7.15.0) ; extra == 'dev'",
      "isort (>=5.7.0) ; extra == 'dev'",
      "mypy (>=0.800) ; extra == 'dev'",
      "pytest-runner (>=5.2) ; extra == 'dev'",
      "twine (>=3.1.1) ; extra == 'dev'",
      "wheel (>=0.34.2) ; extra == 'dev'",
      "PyQt5 ; extra == 'test'",
      "pytest (>=5.4.3) ; extra == 'test'",
      "pytest-qt (~=4.0) ; extra == 'test'",
      "pytest-cov (>=2.9.0) ; extra == 'test'",
      "pytest-raises (>=0.11) ; extra == 'test'",
      "pytest-xvfb (~=2.0) ; extra == 'test'",
      "quilt3 (~=3.4.0) ; extra == 'test'"
    ],
    "summary": "AICSImageIO bindings for napari",
    "support": "https://github.com/AllenCellModeling/napari-aicsimageio/issues",
    "twitter": "",
    "version": "0.7.2",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Alessandro Zunino" }],
    "code_repository": "https://github.com/VicidominiLab/napari-ISM",
    "conda": [{ "channel": "conda-forge", "package": "napari-ism" }],
    "description": "# napari-ISM  [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-ISM)](https://napari-hub.org/plugins/napari-ISM) [![License](https://img.shields.io/pypi/l/napari-ISM.svg?color=green)](https://github.com/VicidominiLab/napari-ISM/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-ISM.svg?color=green)](https://pypi.org/project/napari-ISM) [![Python Version](https://img.shields.io/pypi/pyversions/napari-ISM.svg?color=green)](https://python.org) <!-- [![tests](https://github.com/VicidominiLab/napari-ISM/workflows/tests/badge.svg)](https://github.com/VicidominiLab/napari-ISM/actions) [![codecov](https://codecov.io/gh/VicidominiLab/napari-ISM/branch/main/graph/badge.svg)](https://codecov.io/gh/VicidominiLab/napari-ISM) -->   It performs Adaptive Pixel Reassignment via a phase-correlation algorithm. Once installed, you can upload any ISM-dataset in .h5 or .npy format. The plugin expects a numpy array of the format _rzxytc_ (r: repetition, z: axial dimension, xy: lateral dimensions, t: time dimension, c: detector element). If the _rzt_ dimensions are not present, add manually fake dimensions (e.g. using the _numpy.expand_dims_ function). You can also generate a synthetic ISM-dataset from the File menu. Once a dataset is uploaded on an image layer, you can use the Napari plugin menu to perform either a sum on the _c_ dimension or perform Adaptive Pixel Reassignment on the _c_ dimension.  ----------------------------------  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/plugins/index.html -->  ## Installation  You can install `napari-ISM` via [pip]:      pip install napari-ISM  It requires the following Python packages      numpy \\tscipy \\tscikit-image     h5py \\tnapari  <!--  To install latest development version :      pip install git+https://github.com/VicidominiLab/napari-ISM.git -->  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [GNU LGPL v3.0] license, \"napari-ISM\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/VicidominiLab/napari-ISM/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-ISM      It performs Adaptive Pixel Reassignment via a phase-correlation algorithm. Once installed, you can upload any ISM-dataset in .h5 or .npy format. The plugin expects a numpy array of the format rzxytc (r: repetition, z: axial dimension, xy: lateral dimensions, t: time dimension, c: detector element). If the rzt dimensions are not present, add manually fake dimensions (e.g. using the numpy.expand_dims function). You can also generate a synthetic ISM-dataset from the File menu. Once a dataset is uploaded on an image layer, you can use the Napari plugin menu to perform either a sum on the c dimension or perform Adaptive Pixel Reassignment on the c dimension.   Installation You can install napari-ISM via pip: pip install napari-ISM  It requires the following Python packages numpy scipy scikit-image h5py napari   Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the GNU LGPL v3.0 license, \"napari-ISM\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "Adaptive Pixel Reassignment",
    "documentation": "https://github.com/VicidominiLab/napari-ISM#README.md",
    "first_released": "2022-05-26T13:44:23.753094Z",
    "license": "LGPL-3.0",
    "name": "napari-ISM",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader", "writer", "widget", "sample_data"],
    "project_site": "https://github.com/VicidominiLab/napari-ISM",
    "python_version": ">=3.8",
    "reader_file_extensions": ["*.h5", "*.npy"],
    "release_date": "2022-10-19T10:35:45.400630Z",
    "report_issues": "https://github.com/VicidominiLab/napari-ISM/issues",
    "requirements": [
      "numpy",
      "scipy",
      "scikit-image",
      "h5py",
      "napari",
      "magicgui",
      "qtpy",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "summary": "It performs Adaptive Pixel Reassignment.",
    "support": "https://github.com/VicidominiLab/napari-ISM/issues",
    "twitter": "",
    "version": "0.1.1",
    "visibility": "public",
    "writer_file_extensions": [".h5", ".npy"],
    "writer_save_layers": ["image", "labels*", "image*"]
  },
  {
    "authors": [{ "name": "Alexander Marx" }],
    "code_repository": "https://github.com/marx-alex/napari-bleach-correct",
    "conda": [],
    "description": "## Bleach correction for napari  This plugin is a python implementation of three different algorithms for bleach correction and can be used  to correct time-lapse images that lose intensity due to photobleaching. The implementation is based on the ImageJ  plugin Bleach Corrector by Miura et al. All methods work with 2D and 3D time series.  Napari Bleach correction is easy to use:  ![Demo](../data/demo.gif)  ### Ratio Method  This is the simplest method. Every pixel in a frame is multiplied by the ratio from the mean intensity of the  first frame to that of the *i-th* frame.  Assumptions: * the mean intensity is constant through the time-lapse * the background fluorescence is the same for every pixel and frame  Parameters: * Background Intensity: Must be estimated  ### Exponential Curve Fitting  Drift estimation of fluorescence signal by fitting the mean intensity to an exponential curve. The image is corrected by the decay in the normalized exponential function.  Assumptions: * time intervals between frames are equal  Parameters: * Exponential Curve: Bleaching can be modelled as a mono- or bi-exponential curve  ### Histogram Matching  Bleaching correction by matching histograms to a reference image. The correct pixel values can be calculated by the cumulative distribution function of a frame and its reference frame. This method introduced by Miura et al.  Parameters: * Reference Frame: Match the frame's histogram with the first our neighbor frame   The Histogram Matching method using the neighbor frame as reference is a good start to correct bleaching. All methods are described in detail in Miura et al.  ## References  * Miura K. [Bleach correction ImageJ plugin for compensating the photobleaching of time-lapse sequences.](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7871415/) F1000Res. 2020 Dec 21;9:1494. doi: 10.12688/f1000research.27171.1 * [Documentation of the ImageJ plugin](https://wiki.cmci.info/downloads/bleach_corrector)  <!-- This file is designed to provide you with a starting template for documenting the functionality of your plugin. Its content will be rendered on your plugin's napari hub page.  The sections below are given as a guide for the flow of information only, and are in no way prescriptive. You should feel free to merge, remove, add and  rename sections at will to make this document work best for your plugin.   ## Description  This should be a detailed description of the context of your plugin and its  intended purpose.  If you have videos or screenshots of your plugin in action, you should include them here as well, to make them front and center for new users.   You should use absolute links to these assets, so that we can easily display them  on the hub. The easiest way to include a video is to use a GIF, for example hosted on imgur. You can then reference this GIF as an image.  ![Example GIF hosted on Imgur](https://i.imgur.com/A5phCX4.gif)  Note that GIFs larger than 5MB won't be rendered by GitHub - we will however, render them on the napari hub.  The other alternative, if you prefer to keep a video, is to use GitHub's video embedding feature.  1. Push your `DESCRIPTION.md` to GitHub on your repository (this can also be done as part of a Pull Request) 2. Edit `.napari/DESCRIPTION.md` **on GitHub**. 3. Drag and drop your video into its desired location. It will be uploaded and hosted on GitHub for you, but will not be placed in your repository. 4. We will take the resolved link to the video and render it on the hub.  Here is an example of an mp4 video embedded this way.  https://user-images.githubusercontent.com/17995243/120088305-6c093380-c132-11eb-822d-620e81eb5f0e.mp4  ## Intended Audience & Supported Data  This section should describe the target audience for this plugin (any knowledge, skills and experience required), as well as a description of the types of data supported by this plugin.  Try to make the data description as explicit as possible, so that users know the format your plugin expects. This applies both to reader plugins reading file formats and to function/dock widget plugins accepting layers and/or layer data. For example, if you know your plugin only works with 3D integer data in \"tyx\" order, make sure to mention this.  If you know of researchers, groups or labs using your plugin, or if it has been cited anywhere, feel free to also include this information here.  ## Quickstart  This section should go through step-by-step examples of how your plugin should be used. Where your plugin provides multiple dock widgets or functions, you should split these out into separate subsections for easy browsing. Include screenshots and videos wherever possible to elucidate your descriptions.   Ideally, this section should start with minimal examples for those who just want a quick overview of the plugin's functionality, but you should definitely link out to more complex and in-depth tutorials highlighting any intricacies of your plugin, and more detailed documentation if you have it.  ## Additional Install Steps (uncommon) We will be providing installation instructions on the hub, which will be sufficient for the majority of plugins. They will include instructions to pip install, and to install via napari itself.  Most plugins can be installed out-of-the-box by just specifying the package requirements over in `setup.cfg`. However, if your plugin has any more complex dependencies, or  requires any additional preparation before (or after) installation, you should add  this information here.  ## Getting Help  This section should point users to your preferred support tools, whether this be raising an issue on GitHub, asking a question on image.sc, or using some other method of contact. If you distinguish between usage support and bug/feature support, you should state that here.  ## How to Cite  Many plugins may be used in the course of published (or publishable) research, as well as during conference talks and other public facing events. If you'd like to be cited in a particular format, or have a DOI you'd like used, you should provide that information here. -->  ",
    "description_content_type": "text/markdown",
    "description_text": "Bleach correction for napari This plugin is a python implementation of three different algorithms for bleach correction and can be used  to correct time-lapse images that lose intensity due to photobleaching. The implementation is based on the ImageJ  plugin Bleach Corrector by Miura et al. All methods work with 2D and 3D time series. Napari Bleach correction is easy to use:  Ratio Method This is the simplest method. Every pixel in a frame is multiplied by the ratio from the mean intensity of the  first frame to that of the i-th frame. Assumptions: * the mean intensity is constant through the time-lapse * the background fluorescence is the same for every pixel and frame Parameters: * Background Intensity: Must be estimated Exponential Curve Fitting Drift estimation of fluorescence signal by fitting the mean intensity to an exponential curve. The image is corrected by the decay in the normalized exponential function. Assumptions: * time intervals between frames are equal Parameters: * Exponential Curve: Bleaching can be modelled as a mono- or bi-exponential curve Histogram Matching Bleaching correction by matching histograms to a reference image. The correct pixel values can be calculated by the cumulative distribution function of a frame and its reference frame. This method introduced by Miura et al. Parameters: * Reference Frame: Match the frame's histogram with the first our neighbor frame  The Histogram Matching method using the neighbor frame as reference is a good start to correct bleaching. All methods are described in detail in Miura et al. References  Miura K. Bleach correction ImageJ plugin for compensating the photobleaching of time-lapse sequences. F1000Res. 2020 Dec 21;9:1494. doi: 10.12688/f1000research.27171.1 Documentation of the ImageJ plugin  ",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "Bleaching Correction",
    "documentation": "https://github.com/marx-alex/napari-bleach-correct#README.md",
    "first_released": "2022-09-22T19:22:06.030744Z",
    "license": "MIT",
    "name": "napari-bleach-correct",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget", "sample_data"],
    "project_site": "https://github.com/marx-alex/napari-bleach-correct",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-09-22T19:22:06.030744Z",
    "report_issues": "https://github.com/marx-alex/napari-bleach-correct/issues",
    "requirements": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "scipy",
      "pyqtgraph",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "summary": "A napari plugin to correct time-lapse images for photobleaching.",
    "support": "https://github.com/marx-alex/napari-bleach-correct/issues",
    "twitter": "",
    "version": "0.0.1",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      { "name": "Grzegorz Bokota" },
      { "name": "Jacek Sroka" },
      { "name": "Subhadip Basu" },
      { "name": "Nirmal Das" },
      { "name": "Paweł Trzaskoma" },
      { "name": "Yana Yushkevich" },
      { "name": "Agnieszka Grabowska" },
      { "name": "Adriana Magalska" },
      { "name": "Dariusz Plewczyński" }
    ],
    "category": {
      "Supported data": ["3D"],
      "Workflow step": [
        "Image Segmentation",
        "Image reconstruction",
        "Image feature detection",
        "Object feature extraction"
      ]
    },
    "category_hierarchy": {
      "Supported data": [["3D"]],
      "Workflow step": [
        ["Image Segmentation", "Cell segmentation"],
        ["Image Segmentation", "Connected-component analysis"],
        ["Image reconstruction"],
        ["Image feature detection"],
        ["Object feature extraction"]
      ]
    },
    "citations": {
      "APA": "Bokota G., Sroka J., Basu S., Das N., Trzaskoma P., Yushkevich Y., Grabowska A., Magalska A., Plewczyński D. PartSeg: a tool for quantitative feature extraction from 3D microscopy images for dummies DOI: doi.org/10.1186/s12859-021-03984-1 ",
      "BibTex": "@misc{YourReferenceHere, author = {Bokota, Grzegorz and Sroka, Jacek and Basu, Subhadip and Das, Nirmal and Trzaskoma, Paweł and Yushkevich, Yana and Grabowska, Agnieszka and Magalska, Adriana and Plewczyński, Dariusz}, doi = {doi.org/10.1186/s12859-021-03984-1}, title = {PartSeg: a tool for quantitative feature extraction from 3D microscopy images for dummies} } ",
      "RIS": "TY  - GEN AU  - Bokota, Grzegorz AU  - Sroka, Jacek AU  - Basu, Subhadip AU  - Das, Nirmal AU  - Trzaskoma, Paweł AU  - Yushkevich, Yana AU  - Grabowska, Agnieszka AU  - Magalska, Adriana AU  - Plewczyński, Dariusz DO  - doi.org/10.1186/s12859-021-03984-1 TI  - PartSeg: a tool for quantitative feature extraction from 3D microscopy images for dummies ER ",
      "citation": "# This CITATION.cff file was generated with cffinit. # Visit https://bit.ly/cffinit to generate yours today!  cff-version: 1.2.0 title: >-   PartSeg: a tool for quantitative feature extraction   from 3D microscopy images for dummies message: Cite as type: software authors:   - given-names: Grzegorz     family-names: Bokota     email: g.bokota@mimuw.edu.pl     orcid: 'https://orcid.org/0000-0002-5470-1676'   - given-names: Jacek     family-names: Sroka     orcid: 'https://orcid.org/0000-0002-1714-9667'   - orcid: 'https://orcid.org/0000-0003-1780-0461'     given-names: Subhadip     family-names: Basu   - given-names: Nirmal     family-names: Das   - given-names: Paweł     family-names: Trzaskoma     orcid: 'https://orcid.org/0000-0002-5823-6572'   - given-names: Yana     family-names: Yushkevich     orcid: 'https://orcid.org/0000-0002-5475-8613'   - given-names: Agnieszka     family-names: Grabowska     orcid: 'https://orcid.org/0000-0002-8804-6817'   - given-names: Adriana     family-names: Magalska     orcid: 'https://orcid.org/0000-0002-0517-7134'   - given-names: Dariusz     family-names: Plewczyński     orcid: 'https://orcid.org/0000-0002-3840-7610' doi: doi.org/10.1186/s12859-021-03984-1 "
    },
    "code_repository": "https://github.com/4DNucleome/PartSeg",
    "description": "# Description  PartSeg is a specialized GUI for feature extraction from multichannel light microscopy images, but also most of its features are available as napari Widgets. Information about PartSeg as standalone program or library are [here](https://github.com/4DNucleome/PartSeg)  ## Who is This For?  This plugin is for 2D and 3D segmentation of distinct objects from images and measuring various parameters of these objects.  This plugin process everything in memory, so it may not work with images stored in dask arrays. Currently, this plugin does not support the processing of Time data.  ## How-to Guide  The example data is stored [here](https://4dnucleome.cent.uw.edu.pl/PartSeg/Downloads/test_data.tbz2).  https://user-images.githubusercontent.com/3826210/149146100-973498c7-7d2e-4298-a3c8-3051912e3183.mp4  The above video presents simple segmentation and measurement of various parameters of this segmentation (ROI).  As bellow described, algorithms are the result of porting PartSeg utilities to napari then detailed description could be found in PartSeg documentation/  ### ROI Extraction (Segmentation, pixel labeling)  The PartSeg is focused on the reproducible ROI Extraction process and offers two groups of algorithms:  *   __ROI Mask Extraction__ set of algorithms (from PartSeg ROI Analysis) to work on a whole stack and mainly used for extracting nucleus or cell from a stack. *   __ROI Analysis Extraction__ set of algorithms (from PartSeg ROI Mask) for detailed segmentation on the level of a single nucleus.     If possible, they use an inner caching mechanism to improve performance while adjusting parameters.  Algorithms from both groups should support masking. (perform ROI extraction only on the mask layer's area has non-zero values).  Parameters of ROI Extraction could be saved for later reuse (in the program) or exported to JSON and imported in another instance. With an accuracy of up to channel selection, they are identical to PartSeg, so importing should work both ways, but the channel selection step needs to be repeated.  The list of available algorithms could be extensible using the PartSeg plugin mechanism.  ### Measurement widgets  PartSeg offers two measurement widgets:  #### Measurement  Interface to PartSeg measurement engine. In this widget, there are two tabs. **Measurement settings* that allow to define, delete, import, and export set of measurements  ![Measurement Settings](https://i.imgur.com/cfuXRRD.png)  and **Measurement** for performing measures using an already defined set.  ![Measurement](https://i.imgur.com/4LzvqRp.png)  The list of available measurements could be extensible using the PartSeg plugin mechanism.  #### Simple Measurement  ![Simple Measurement](https://i.imgur.com/Rnq6lF5.png)  Set of measurements that could be calculated per component respecting data voxel size. In comparison to  *Measurement* list of available measures is limited to ones that do not need *Mask* information and could be calculated per component.  This widget is equivalent to the PartSeg ROI Mask Simple Measurement window.  ### Search label  Widget to find the layer with the given number By highlighting it or zooming on it. The highlight widget uses white color, so the highlight may not be visible if the label has a bright color.  https://user-images.githubusercontent.com/3826210/154669409-cdac9be8-3dbf-4a0e-a66f-af8a44aed0fb.mp4  ### Mask create  Transform labels layer into another labels layer with the possibility to dilate, and filling small holes  ![Mask create widget](https://i.imgur.com/FIJGLjb.png)  ## Reader plugins  In this plugin, there are also all PartSeg readers and writers. The most important readers are this, which allows loading PartSeg projects to napari. The one which could impact a user workflow is tiff reader. In comparison to the napari default one, there are two essential differences. Napari's built-in plugin loads data as they are in a file. PartSeg plugin read file metadata and return data in TZYX order. PartSeg reader returns each channel as a separate layer. PartSeg reader also tries to parse voxel size metadata and set scale parameters to nanometers' size. ",
    "description_content_type": "text/markdown",
    "description_text": "Description PartSeg is a specialized GUI for feature extraction from multichannel light microscopy images, but also most of its features are available as napari Widgets. Information about PartSeg as standalone program or library are here Who is This For? This plugin is for 2D and 3D segmentation of distinct objects from images and measuring various parameters of these objects. This plugin process everything in memory, so it may not work with images stored in dask arrays. Currently, this plugin does not support the processing of Time data. How-to Guide The example data is stored here. https://user-images.githubusercontent.com/3826210/149146100-973498c7-7d2e-4298-a3c8-3051912e3183.mp4 The above video presents simple segmentation and measurement of various parameters of this segmentation (ROI). As bellow described, algorithms are the result of porting PartSeg utilities to napari then detailed description could be found in PartSeg documentation/ ROI Extraction (Segmentation, pixel labeling) The PartSeg is focused on the reproducible ROI Extraction process and offers two groups of algorithms:  ROI Mask Extraction set of algorithms (from PartSeg ROI Analysis) to work on a whole stack and mainly used for extracting nucleus or cell from a stack. ROI Analysis Extraction set of algorithms (from PartSeg ROI Mask) for detailed segmentation on the level of a single nucleus.     If possible, they use an inner caching mechanism to improve performance while adjusting parameters.  Algorithms from both groups should support masking. (perform ROI extraction only on the mask layer's area has non-zero values). Parameters of ROI Extraction could be saved for later reuse (in the program) or exported to JSON and imported in another instance. With an accuracy of up to channel selection, they are identical to PartSeg, so importing should work both ways, but the channel selection step needs to be repeated. The list of available algorithms could be extensible using the PartSeg plugin mechanism. Measurement widgets PartSeg offers two measurement widgets: Measurement Interface to PartSeg measurement engine. In this widget, there are two tabs. *Measurement settings that allow to define, delete, import, and export set of measurements  and Measurement for performing measures using an already defined set.  The list of available measurements could be extensible using the PartSeg plugin mechanism. Simple Measurement  Set of measurements that could be calculated per component respecting data voxel size. In comparison to  Measurement list of available measures is limited to ones that do not need Mask information and could be calculated per component. This widget is equivalent to the PartSeg ROI Mask Simple Measurement window. Search label Widget to find the layer with the given number By highlighting it or zooming on it. The highlight widget uses white color, so the highlight may not be visible if the label has a bright color. https://user-images.githubusercontent.com/3826210/154669409-cdac9be8-3dbf-4a0e-a66f-af8a44aed0fb.mp4 Mask create Transform labels layer into another labels layer with the possibility to dilate, and filling small holes  Reader plugins In this plugin, there are also all PartSeg readers and writers. The most important readers are this, which allows loading PartSeg projects to napari. The one which could impact a user workflow is tiff reader. In comparison to the napari default one, there are two essential differences. Napari's built-in plugin loads data as they are in a file. PartSeg plugin read file metadata and return data in TZYX order. PartSeg reader returns each channel as a separate layer. PartSeg reader also tries to parse voxel size metadata and set scale parameters to nanometers' size.",
    "development_status": ["Development Status :: 3 - Alpha"],
    "display_name": "PartSeg",
    "documentation": "https://partseg.readthedocs.io/en/stable/",
    "first_released": "2019-01-14T23:29:35.670697Z",
    "license": "BSD-3-Clause",
    "name": "PartSeg",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader", "writer", "widget"],
    "project_site": "https://4dnucleome.cent.uw.edu.pl/PartSeg/",
    "python_version": ">=3.7",
    "reader_file_extensions": ["*"],
    "release_date": "2022-11-13T16:23:50.506449Z",
    "report_issues": "https://github.com/4DNucleome/PartSeg/issues",
    "requirements": [
      "IPython (>=7.7.0)",
      "PartSegCore-compiled-backend (>=0.13.11)",
      "PartSegData (==0.10.0)",
      "QtAwesome (!=1.2.0,>=1.0.3)",
      "QtPy (>=1.7.0)",
      "SimpleITK (>=1.1.0)",
      "appdirs (>=1.4.4)",
      "czifile (>=2019.5.22)",
      "defusedxml (>=0.6.0)",
      "h5py (>=2.8.0)",
      "imagecodecs (>=2020.5.30)",
      "imageio (>=2.5.0)",
      "ipykernel (>=5.2.0)",
      "magicgui (!=0.5.0,>=0.4.0)",
      "mahotas (>=1.4.9)",
      "napari (>=0.4.12)",
      "nme (>=0.1.6)",
      "numpy (>=1.18.5)",
      "oiffile (>=2020.1.18)",
      "openpyxl (>=2.4.9)",
      "packaging (>=20.0)",
      "pandas (>=0.24.0)",
      "psygnal (>=0.3.1)",
      "pydantic (>=1.8.1)",
      "pygments (>=2.4.0)",
      "qtconsole (>=4.7.7)",
      "requests (>=2.18.0)",
      "scipy (>=1.2.0)",
      "sentry-sdk (>=0.14.3)",
      "six (>=1.11.0)",
      "superqt (>=0.2.4)",
      "sympy (>=1.1.1)",
      "tifffile (>=2020.9.30)",
      "traceback-with-variables (>=2.0.4)",
      "vispy (>=0.6.4)",
      "xlrd (>=1.1.0)",
      "xlsxwriter",
      "importlib-metadata (<4.12.0) ; python_version < \"3.8\"",
      "typing-extensions (>=3.7.4.3) ; python_version < \"3.8\"",
      "vispy (<0.10.0) ; python_version < \"3.8\"",
      "PyOpenGL-accelerate (>=3.1.5) ; extra == 'accelerate'",
      "PyOpenGL-accelerate (>=3.1.5) ; extra == 'all'",
      "PyQt5 (!=5.15.0,>=5.12.3) ; extra == 'all'",
      "autodoc-pydantic (==1.7.2) ; extra == 'docs'",
      "sphinx (!=3.0.0,!=3.5.0) ; extra == 'docs'",
      "sphinx-autodoc-typehints (==1.18.3) ; extra == 'docs'",
      "sphinx-qt-documentation (==0.4) ; extra == 'docs'",
      "PyQt5 (!=5.15.0,>=5.12.3) ; extra == 'pyqt'",
      "PyQt5 (!=5.15.0,>=5.12.3) ; extra == 'pyqt5'",
      "PySide2 (!=5.15.0,>=5.12.3) ; extra == 'pyside'",
      "PySide2 (!=5.15.0,>=5.12.3) ; extra == 'pyside2'",
      "codecov ; extra == 'test'",
      "lxml ; extra == 'test'",
      "pytest (>=7.0.0) ; extra == 'test'",
      "pytest-cov ; extra == 'test'",
      "pytest-qt ; extra == 'test'",
      "pytest-timeout ; extra == 'test'",
      "scikit-image ; extra == 'test'",
      "tox ; extra == 'test'"
    ],
    "summary": "PartSeg is python GUI and set of napari plugins for bio imaging analysis especially nucleus analysis,",
    "support": "https://github.com/4DNucleome/PartSeg/issues",
    "twitter": "",
    "version": "0.14.6",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": ["image", "labels"]
  },
  {
    "authors": [{ "name": "Luis Perdigao" }],
    "code_repository": "https://github.com/rosalindfranklininstitute/RedLionfish",
    "conda": [{ "channel": "conda-forge", "package": "redlionfish" }],
    "description": "# Red lionfish (RL) deconvolution  *Richardson-Lucy deconvolution for fishes, scientists and engineers.*   This software is for filtering 3D data using the Richardson-Lucy deconvolution algorithm.  Richardson-Lucy is an iterative deconvolution algorithm that is used to remove point spread function (PSF) or optical transfer function (OTF) artifacts from experimental images.  The method was originally developed for astronomy to remove optical effects and simultaneously reduce poisson noise in 2D images.  [Lucy, L. B. An iterative technique for the rectification of observed distributions. The Astronomical Journal 79, 745 (1974). DOI: 10.1086/111605](https://ui.adsabs.harvard.edu/abs/1974AJ.....79..745L/abstract)  The method can also be applied to 3D data. Nowadays this filtering technique is also widely used by microscopists.  The Richardson-Lucy deconvolution algorigthm is iterative. Each iteration involves the calculation of 2 convolutions, one element-wise multiplication and one element-wise division.  When dealing with 3D data, the Richardson-Lucy algorithm is quite computional intensive primarly due to the calculation of the convolution, and can take a while to complete depending on the resources available. Convolution is significantly sped up using FFT compared to raw convolution.  This software was developed with the aim to make the R-L computation faster by exploiting GPU resources, and with the use of FFT convolution.  To make RedLionfish easily accessible, it is available through PyPi and anaconda (conda-forge channel). A useful plugin for Napari is also available.  Please note that this software only works with 3D data. For 2D data there are many alternatives such as the DeconvolutionLab2 in Fiji (ImageJ) and sckikit-image.  ## Napari plugin  You can now use the Napari's plugin installation in *Menu -> Plugins -> Install/Uninstall Plugins...*. However, if you chose to use this method, GPU acceleration may not be available and it will use the CPU backend. Better check.  ![](resources\\\\imag1.jpg)  Alternatively, if you follow the installation instructions below, and install the napari in the same python environment then the plugin should be immediately available in the *Menu -> Plugins -> RedLionfish*.   ## Installation  Previously there was a problem in installing using `pip`, because no PyOpenCL wheels for windows were avaiable. It is now avaialble. This package can be installed using pip or conda.  ### Conda install  This package is available in conda-forge channel. It contains the precompiled libraries and it will install all the requirments for GPU-accelerated RL calculations.  `conda install redlionfish -c conda-forge`  ### Install from PyPi  ``` pip install redlionfish ```  In Linux , the package `ocl-icd-system` may also be useful.  ``` conda install reikna pyopencl ocl-icd-system -c conda-forge ```   #### Manual installation using the conda package file.  Download the appropriate conda package .bz2 at [https://github.com/rosalindfranklininstitute/RedLionfish/releases](https://github.com/rosalindfranklininstitute/RedLionfish/releases)  In the command line, successively run: ``` conda install <filename.bz2> conda update --all -c conda-forge ``` The second line is needed because you are installing from a local file, conda installer will not install dependencies. Right after this you should run the update command given.   ### Manual installation (advanced and for developers)  Please note that in order to use OpenCL GPU accelerations, PyOpenCL must be installed. The best way to get it working is to install it under a conda environment.  The installation is similar to the previously described for PyPi.  `conda install reikna pyopencl`  or `conda install reikna pyopencl ocl-icd-system -c conda-forge` (Linux)  Clone/download from source [https://github.com/rosalindfranklininstitute/RedLionfish/](https://github.com/rosalindfranklininstitute/RedLionfish/)  and run  `python setup.py install`   ### Debug installation If you want to test and modify the code then you should probably install in debug mode using:  `python setup.py develop`  or  `pip install -e .`   ## More information  The software has algorithms for Richardson-Lucy deconvolution that use either CPU and GPU.  The CPU version is very similar to the [skimage.restoration.richardson_lucy](https://scikit-image.org/docs/dev/api/skimage.restoration.html#skimage.restoration.richardson_lucy) code, with improvments in speed. major differences are:  - the convolution steps use FFT only. - PSF and PSF-flipped FFTs are precalculated before starting iterations.  The GPU version, was written in to use Reikna package, which does FFT using OpenCL, via PyOpenCL.  Unfortunately, a major limitation in RAM usage exists with PyOpenCL. Large 3D data volumes with cause out-of-memory error when trying to upload data to the GPU for FFT calculations. As such, to overcome this problem, a block algorithm is used, which splits data into blocks with padded data. The results are then combined together to give the final result. This affects the perfomance of the calculation rather significantly, but with the advantage of being possible to handle large data volumes.  If Richardson-Lucy deconvolution using the GPU method fails, RedLionfish will falls back to CPU calculation. Check console output for messages.  If you are using the RedLionfish in your code, note that, by default, `def doRLDeconvolutionFromNpArrays()` method it uses the GPU OpenCL version.  ## Testing  Many examples can be found in `/test' folder.  A quick and benchmarking installation can be run from the proect root using the command:  'python test\\\\test_and_benchm.py'  or (*nix)  'python test/test_and_benchm.py'  This will print out information about your GPU device (if available) and run some deconvolutions. It initially creates some data programatically, convolutes with a gaussian PSF, and add Poisson noise. Then it executes executes the Richardson-Lucy deconvolution calculation using CPU and GPU methods, for 10 iterations. During the calculation it will print some information to the console/terminal, including the time it takes to run the calculation.   Computer generated data and an experimental PSF can be found in `test\\\\testdata`  ### Testing Redlionfish in napari  Here is an example testing the Redlionfish plugin in napari:  1. load data `test\\\\testdata\\\\gendata_psfconv_poiss_large.tif` (can use draga and drop) 2. load psf data `test\\\\testdata\\\\PSF_RFI_8bit.tif` 3. In the RedLionfish side window ensure that 'gendata_psfconv_poiss_large' is selected in data dropdown widget, and `PSF_RFI_8bit` is selected in psfdata widget. 4. Choose number of iterations (default=10) 5. Click 'Go' button and wait until result shows as a new data layer. 6. Use controls of the left panel to compare before and after RL deconvolution: select 'RL-deconvolution' layer and set colormap to red. Hide PSF_RFI_8bit. Make sure that both 'RL-deconvolution' and 'gendata-psfconv' are visible. Now, hide/unhide RL-deconvolution layer to see before and after deconvolution. Adjust contrast limits of each layer as desired.   ## GPU vs CPU  You may notice that choosing GPU does not make RL-calculation much faster compared with CPU, and sometimes is slower.  Which method runs the R-L deconvolution faster. That depends on the computer configuration/architecture.  GPU calculations will be generally faster than CPU with bigger data volumes.  GPU calculation will be significantly faster if using a dedicated GPU card.  Please see benchmark values that highlights significant variability in calculation speeds.   ## Coding  Please feel free to browse /test folder for examples.  In your code, add the import.  `import RedLionfishDeconv`  in order to use the functions.  The most useful function is perhaps the following.  `def doRLDeconvolutionFromNpArrays(data_np , psf_np ,*, niter=10, method='gpu', useBlockAlgorithm=False, callbkTickFunc=None, resAsUint8 = False) `  This will do the Richardson-Lucy deconvolution on the data_np (numpy, 3 dimensional data volume) using the provided PSF data volume, for 10 iterations. GPU method is generally faster but it may fail. If it does fail, the program will automatically use the CPU version that uses the scipy fft package.    ## Manually building the conda package  For this installation, ensure that the conda-build package is installed  `conda install conda-build`  In windows, simply execute  `conda-create-package.bat`   Or, execute the following command-line to create the installation package.  `conda-build --output-folder ./conda-built-packages -c conda-forge conda-recipe`  and the conda package will be created in folder *conda-built-packages*.  Otherwise, navigate to `conda-recipe`, and execute on the command-line `conda build .`  It will take a while to complete.  ## Contact  Report issues and questions in project's github page, please. Please don't try to send emails as they may be igored or spam-filtered.  ",
    "description_content_type": "text/markdown",
    "description_text": "Red lionfish (RL) deconvolution Richardson-Lucy deconvolution for fishes, scientists and engineers. This software is for filtering 3D data using the Richardson-Lucy deconvolution algorithm. Richardson-Lucy is an iterative deconvolution algorithm that is used to remove point spread function (PSF) or optical transfer function (OTF) artifacts from experimental images. The method was originally developed for astronomy to remove optical effects and simultaneously reduce poisson noise in 2D images. Lucy, L. B. An iterative technique for the rectification of observed distributions. The Astronomical Journal 79, 745 (1974). DOI: 10.1086/111605 The method can also be applied to 3D data. Nowadays this filtering technique is also widely used by microscopists. The Richardson-Lucy deconvolution algorigthm is iterative. Each iteration involves the calculation of 2 convolutions, one element-wise multiplication and one element-wise division. When dealing with 3D data, the Richardson-Lucy algorithm is quite computional intensive primarly due to the calculation of the convolution, and can take a while to complete depending on the resources available. Convolution is significantly sped up using FFT compared to raw convolution. This software was developed with the aim to make the R-L computation faster by exploiting GPU resources, and with the use of FFT convolution. To make RedLionfish easily accessible, it is available through PyPi and anaconda (conda-forge channel). A useful plugin for Napari is also available. Please note that this software only works with 3D data. For 2D data there are many alternatives such as the DeconvolutionLab2 in Fiji (ImageJ) and sckikit-image. Napari plugin You can now use the Napari's plugin installation in Menu -> Plugins -> Install/Uninstall Plugins.... However, if you chose to use this method, GPU acceleration may not be available and it will use the CPU backend. Better check.  Alternatively, if you follow the installation instructions below, and install the napari in the same python environment then the plugin should be immediately available in the Menu -> Plugins -> RedLionfish. Installation Previously there was a problem in installing using pip, because no PyOpenCL wheels for windows were avaiable. It is now avaialble. This package can be installed using pip or conda. Conda install This package is available in conda-forge channel. It contains the precompiled libraries and it will install all the requirments for GPU-accelerated RL calculations. conda install redlionfish -c conda-forge Install from PyPi pip install redlionfish In Linux , the package ocl-icd-system may also be useful. conda install reikna pyopencl ocl-icd-system -c conda-forge Manual installation using the conda package file. Download the appropriate conda package .bz2 at https://github.com/rosalindfranklininstitute/RedLionfish/releases In the command line, successively run: conda install <filename.bz2> conda update --all -c conda-forge The second line is needed because you are installing from a local file, conda installer will not install dependencies. Right after this you should run the update command given. Manual installation (advanced and for developers) Please note that in order to use OpenCL GPU accelerations, PyOpenCL must be installed. The best way to get it working is to install it under a conda environment. The installation is similar to the previously described for PyPi. conda install reikna pyopencl or conda install reikna pyopencl ocl-icd-system -c conda-forge (Linux) Clone/download from source https://github.com/rosalindfranklininstitute/RedLionfish/ and run python setup.py install Debug installation If you want to test and modify the code then you should probably install in debug mode using: python setup.py develop or pip install -e . More information The software has algorithms for Richardson-Lucy deconvolution that use either CPU and GPU. The CPU version is very similar to the skimage.restoration.richardson_lucy code, with improvments in speed. major differences are:  the convolution steps use FFT only. PSF and PSF-flipped FFTs are precalculated before starting iterations.  The GPU version, was written in to use Reikna package, which does FFT using OpenCL, via PyOpenCL. Unfortunately, a major limitation in RAM usage exists with PyOpenCL. Large 3D data volumes with cause out-of-memory error when trying to upload data to the GPU for FFT calculations. As such, to overcome this problem, a block algorithm is used, which splits data into blocks with padded data. The results are then combined together to give the final result. This affects the perfomance of the calculation rather significantly, but with the advantage of being possible to handle large data volumes. If Richardson-Lucy deconvolution using the GPU method fails, RedLionfish will falls back to CPU calculation. Check console output for messages. If you are using the RedLionfish in your code, note that, by default, def doRLDeconvolutionFromNpArrays() method it uses the GPU OpenCL version. Testing Many examples can be found in `/test' folder. A quick and benchmarking installation can be run from the proect root using the command: 'python test\\\\test_and_benchm.py' or (*nix) 'python test/test_and_benchm.py' This will print out information about your GPU device (if available) and run some deconvolutions. It initially creates some data programatically, convolutes with a gaussian PSF, and add Poisson noise. Then it executes executes the Richardson-Lucy deconvolution calculation using CPU and GPU methods, for 10 iterations. During the calculation it will print some information to the console/terminal, including the time it takes to run the calculation. Computer generated data and an experimental PSF can be found in test\\\\testdata Testing Redlionfish in napari Here is an example testing the Redlionfish plugin in napari:  load data test\\\\testdata\\\\gendata_psfconv_poiss_large.tif (can use draga and drop) load psf data test\\\\testdata\\\\PSF_RFI_8bit.tif In the RedLionfish side window ensure that 'gendata_psfconv_poiss_large' is selected in data dropdown widget, and PSF_RFI_8bit is selected in psfdata widget. Choose number of iterations (default=10) Click 'Go' button and wait until result shows as a new data layer. Use controls of the left panel to compare before and after RL deconvolution: select 'RL-deconvolution' layer and set colormap to red. Hide PSF_RFI_8bit. Make sure that both 'RL-deconvolution' and 'gendata-psfconv' are visible. Now, hide/unhide RL-deconvolution layer to see before and after deconvolution. Adjust contrast limits of each layer as desired.  GPU vs CPU You may notice that choosing GPU does not make RL-calculation much faster compared with CPU, and sometimes is slower. Which method runs the R-L deconvolution faster. That depends on the computer configuration/architecture. GPU calculations will be generally faster than CPU with bigger data volumes. GPU calculation will be significantly faster if using a dedicated GPU card. Please see benchmark values that highlights significant variability in calculation speeds. Coding Please feel free to browse /test folder for examples. In your code, add the import. import RedLionfishDeconv in order to use the functions. The most useful function is perhaps the following. def doRLDeconvolutionFromNpArrays(data_np , psf_np ,*, niter=10, method='gpu', useBlockAlgorithm=False, callbkTickFunc=None, resAsUint8 = False) This will do the Richardson-Lucy deconvolution on the data_np (numpy, 3 dimensional data volume) using the provided PSF data volume, for 10 iterations. GPU method is generally faster but it may fail. If it does fail, the program will automatically use the CPU version that uses the scipy fft package. Manually building the conda package For this installation, ensure that the conda-build package is installed conda install conda-build In windows, simply execute conda-create-package.bat Or, execute the following command-line to create the installation package. conda-build --output-folder ./conda-built-packages -c conda-forge conda-recipe and the conda package will be created in folder conda-built-packages. Otherwise, navigate to conda-recipe, and execute on the command-line conda build . It will take a while to complete. Contact Report issues and questions in project's github page, please. Please don't try to send emails as they may be igored or spam-filtered.",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "RedLionfish",
    "documentation": "",
    "first_released": "2021-11-19T14:55:32.357000Z",
    "license": "Apache-2.0",
    "name": "RedLionfish",
    "npe2": false,
    "operating_system": [
      "Operating System :: MacOS",
      "Operating System :: Microsoft :: Windows",
      "Operating System :: POSIX :: Linux"
    ],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/rosalindfranklininstitute/RedLionfish",
    "python_version": "",
    "reader_file_extensions": [],
    "release_date": "2022-09-30T20:40:24.914946Z",
    "report_issues": "",
    "requirements": ["numpy", "scipy", "pyopencl", "reikna"],
    "summary": "Fast Richardson-Lucy deconvolution of 3D volume data using GPU or CPU with napari plugin.",
    "support": "",
    "twitter": "",
    "version": "0.8",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "email": "benkantor@gmail.com", "name": "Ben Kantor" }],
    "code_repository": "https://github.com/bkntr/napari-bigwarp",
    "description": "# napari-bigwarp  [![License](https://img.shields.io/pypi/l/napari-bigwarp.svg?color=green)](https://github.com/bkntr/napari-bigwarp/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-bigwarp.svg?color=green)](https://pypi.org/project/napari-bigwarp) [![Python Version](https://img.shields.io/pypi/pyversions/napari-bigwarp.svg?color=green)](https://python.org) [![tests](https://github.com/bkntr/napari-bigwarp/workflows/tests/badge.svg)](https://github.com/bkntr/napari-bigwarp/actions) [![codecov](https://codecov.io/gh/bkntr/napari-bigwarp/branch/main/graph/badge.svg)](https://codecov.io/gh/bkntr/napari-bigwarp) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-bigwarp)](https://napari-hub.org/plugins/napari-bigwarp)  BigWarp-like interface for napari  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/plugins/stable/index.html -->  ## Installation  You can install `napari-bigwarp` via [pip]:      pip install napari-bigwarp    To install latest development version :      pip install git+https://github.com/bkntr/napari-bigwarp.git   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-bigwarp\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/bkntr/napari-bigwarp/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-bigwarp       BigWarp-like interface for napari  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install napari-bigwarp via pip: pip install napari-bigwarp  To install latest development version : pip install git+https://github.com/bkntr/napari-bigwarp.git  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-bigwarp\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari-bigwarp",
    "documentation": "https://github.com/bkntr/napari-bigwarp#README.md",
    "first_released": "2022-01-26T09:04:00.005701Z",
    "license": "BSD-3-Clause",
    "name": "napari-bigwarp",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/bkntr/napari-bigwarp",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-01-26T09:04:00.005701Z",
    "report_issues": "https://github.com/bkntr/napari-bigwarp/issues",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "opencv-contrib-python",
      "opencv-python"
    ],
    "summary": "BigWarp-like interface for napari",
    "support": "https://github.com/bkntr/napari-bigwarp/issues",
    "twitter": "",
    "version": "0.0.1",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "email": "jru@stowers.org", "name": "Jay Unruh" }],
    "code_repository": "https://github.com/jayunruh/napari-IP-workflow",
    "description": "# napari-IP-workflow  [![License](https://img.shields.io/pypi/l/napari-IP-workflow.svg?color=green)](https://github.com/jayunruh/napari-IP-workflow/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-IP-workflow.svg?color=green)](https://pypi.org/project/napari-IP-workflow) [![Python Version](https://img.shields.io/pypi/pyversions/napari-IP-workflow.svg?color=green)](https://python.org) [![tests](https://github.com/jayunruh/napari-IP-workflow/workflows/tests/badge.svg)](https://github.com/jayunruh/napari-IP-workflow/actions) [![codecov](https://codecov.io/gh/jayunruh/napari-IP-workflow/branch/main/graph/badge.svg)](https://codecov.io/gh/jayunruh/napari-IP-workflow) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-IP-workflow)](https://napari-hub.org/plugins/napari-IP-workflow)  A plugin to do image preprocessing, segmentation, and measurements on other images.  The typical workflow is background subtraction followed by smoothing, thresholding, and size filtering.  This is typically done on nuclear stained images.  Segmentation can optionally be followed by circular label expansion to find cytoplasmic signals. The labeled signals are then measured on background subtracted images.  ##General organization  The code is separated into non-interactive processing functions (ipfunctions module) and an interactive widget (segwidget module).  Please look at the code on github for examples: [Github](https://github.com/jayunruh/napari-ip-workflow). The expected workflow is from jupyter notebooks with an interactive workflow shown in src/napari-ip-workflow/_tests/standard_segementation_widget.ipynb and a non-interactive workflow shown in src/napari-ip-workflow/_tests/standard_segmentation.ipynb.  The expectation is to find the best parameters in an interactive way (ideally testing on several images) and then use the non-interactive workflow to batch through more data sets.  All image processing algorithms are in the ipfunctions module and the segwidget module has the Napari widget code.  Below I describe the strategies that are utilized in the workflow.  ## Background subtraction strategy  Automated background subtraction (e.g. as in Fiji) is often accomplished with a low pass filter-style approach like rolling ball background subtraction.  This approach fails as feature sizes grow larger or measurements approach the background.  Manual selection of the background is more robust but introduces human variability and isn't compatible with high throughput analyses.  Our approach is to attempt to automate regional selection of background as follows.  First the image is smoothed with a Gaussian filter to eliminate background noise.  Next, minimum values are subtracted from each channel and the resulting images are summed.   Next, a uniform 2D boxcar smoothing is applied to the image--background level regions in the resulting image are at least the boxcar size distance away from foreground objects.  The minimum pixel in that resulting image is a good approximation for the background region of the image.  A thick border is specified to avoid lower intensity regions at the border of the image.  This algorithm is implemented in the ipfunctions module as findBackground.  Once the background region is found, it can be measured with measureCirc.  ## Segmentation and thresholding strategy  There are many automated thresholding algorithms available via python and, by extension, Napari.  This program uses a very simplistic but powerful method.  Most segmentable images consist of foreground and background components.  In imaging, the foreground is more noisy than the background.  Ideally a smoothed background subtracted image will have a maximum intensity that represents the foreground well and a background intensity of 0.  In that case, the threshold level can be easily defined as a fraction of that smoothed maximum intensity.  A threshold fraction of 0.25 tends to work well but lower values may be more robust if background is fairly smooth and the foreground is noisier.  In some cases the foreground has anomalous high values that will skew the estimation.  In that case it may be better to estimate the foreground as e.g. the 99th percentile of the intensity.  In some cases it may be useful to use the average intensity as a reference point instead or use the raw intensity value (statistic is Identity).  Those last options tend to be less robust and it may be desired in those cases to use some of the more complex autothresholding methods.  After thresholding, objects on the image edge are eliminated and objects are filtered according to size.  The minimum area can easy remove small debris that can contaminate a measurement.  The maximum area can be used for large contaminants or poorly segmented clusters of cells that might not be desired in the analysis.  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/plugins/index.html -->  ## Installation  You can install `napari-IP-workflow` via [pip]:      pip install napari-IP-workflow    To install latest development version :      pip install git+https://github.com/jayunruh/napari-IP-workflow.git   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [GNU GPL v3.0] license, \"napari-IP-workflow\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/jayunruh/napari-IP-workflow/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-IP-workflow       A plugin to do image preprocessing, segmentation, and measurements on other images.  The typical workflow is background subtraction followed by smoothing, thresholding, and size filtering.  This is typically done on nuclear stained images.  Segmentation can optionally be followed by circular label expansion to find cytoplasmic signals. The labeled signals are then measured on background subtracted images. General organization The code is separated into non-interactive processing functions (ipfunctions module) and an interactive widget (segwidget module).  Please look at the code on github for examples: Github. The expected workflow is from jupyter notebooks with an interactive workflow shown in src/napari-ip-workflow/_tests/standard_segementation_widget.ipynb and a non-interactive workflow shown in src/napari-ip-workflow/_tests/standard_segmentation.ipynb.  The expectation is to find the best parameters in an interactive way (ideally testing on several images) and then use the non-interactive workflow to batch through more data sets.  All image processing algorithms are in the ipfunctions module and the segwidget module has the Napari widget code.  Below I describe the strategies that are utilized in the workflow. Background subtraction strategy Automated background subtraction (e.g. as in Fiji) is often accomplished with a low pass filter-style approach like rolling ball background subtraction.  This approach fails as feature sizes grow larger or measurements approach the background.  Manual selection of the background is more robust but introduces human variability and isn't compatible with high throughput analyses.  Our approach is to attempt to automate regional selection of background as follows.  First the image is smoothed with a Gaussian filter to eliminate background noise.  Next, minimum values are subtracted from each channel and the resulting images are summed.   Next, a uniform 2D boxcar smoothing is applied to the image--background level regions in the resulting image are at least the boxcar size distance away from foreground objects.  The minimum pixel in that resulting image is a good approximation for the background region of the image.  A thick border is specified to avoid lower intensity regions at the border of the image.  This algorithm is implemented in the ipfunctions module as findBackground.  Once the background region is found, it can be measured with measureCirc. Segmentation and thresholding strategy There are many automated thresholding algorithms available via python and, by extension, Napari.  This program uses a very simplistic but powerful method.  Most segmentable images consist of foreground and background components.  In imaging, the foreground is more noisy than the background.  Ideally a smoothed background subtracted image will have a maximum intensity that represents the foreground well and a background intensity of 0.  In that case, the threshold level can be easily defined as a fraction of that smoothed maximum intensity.  A threshold fraction of 0.25 tends to work well but lower values may be more robust if background is fairly smooth and the foreground is noisier.  In some cases the foreground has anomalous high values that will skew the estimation.  In that case it may be better to estimate the foreground as e.g. the 99th percentile of the intensity.  In some cases it may be useful to use the average intensity as a reference point instead or use the raw intensity value (statistic is Identity).  Those last options tend to be less robust and it may be desired in those cases to use some of the more complex autothresholding methods.  After thresholding, objects on the image edge are eliminated and objects are filtered according to size.  The minimum area can easy remove small debris that can contaminate a measurement.  The maximum area can be used for large contaminants or poorly segmented clusters of cells that might not be desired in the analysis.  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install napari-IP-workflow via pip: pip install napari-IP-workflow  To install latest development version : pip install git+https://github.com/jayunruh/napari-IP-workflow.git  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the GNU GPL v3.0 license, \"napari-IP-workflow\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "Image Processing Workflow",
    "documentation": "https://github.com/jayunruh/napari-IP-workflow#README.md",
    "first_released": "2022-05-17T20:25:56.447707Z",
    "license": "GPL-3.0",
    "name": "napari-IP-workflow",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/jayunruh/napari-IP-workflow",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-05-18T16:41:45.901404Z",
    "report_issues": "https://github.com/jayunruh/napari-IP-workflow/issues",
    "requirements": [
      "numpy",
      "magicgui",
      "qtpy",
      "pandas",
      "skimage",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "summary": "A plugin to do image preprocessing, segmentation, and measurements on other images.",
    "support": "https://github.com/jayunruh/napari-IP-workflow/issues",
    "twitter": "",
    "version": "0.0.3",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Aurelien Maillot" }],
    "code_repository": "https://github.com/AurelienMaillot/napari-bud-cell-segmenter",
    "description": "# napari-bud-cell-segmenter  [![License BSD-3](https://img.shields.io/pypi/l/napari-bud-cell-segmenter.svg?color=green)](https://github.com/AurelienMaillot/napari-bud-cell-segmenter/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-bud-cell-segmenter.svg?color=green)](https://pypi.org/project/napari-bud-cell-segmenter) [![Python Version](https://img.shields.io/pypi/pyversions/napari-bud-cell-segmenter.svg?color=green)](https://python.org) [![tests](https://github.com/AurelienMaillot/napari-bud-cell-segmenter/workflows/tests/badge.svg)](https://github.com/AurelienMaillot/napari-bud-cell-segmenter/actions) [![codecov](https://codecov.io/gh/AurelienMaillot/napari-bud-cell-segmenter/branch/main/graph/badge.svg)](https://codecov.io/gh/AurelienMaillot/napari-bud-cell-segmenter) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-bud-cell-segmenter)](https://napari-hub.org/plugins/napari-bud-cell-segmenter)  A plugin to segment embryonic mammary bud cells and detect 2 RNA probes  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/stable/plugins/index.html -->  ## Installation  You can install `napari-bud-cell-segmenter` via [pip]:      pip install napari-bud-cell-segmenter    To install latest development version :      pip install git+https://github.com/AurelienMaillot/napari-bud-cell-segmenter.git   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-bud-cell-segmenter\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/AurelienMaillot/napari-bud-cell-segmenter/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-bud-cell-segmenter       A plugin to segment embryonic mammary bud cells and detect 2 RNA probes  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install napari-bud-cell-segmenter via pip: pip install napari-bud-cell-segmenter  To install latest development version : pip install git+https://github.com/AurelienMaillot/napari-bud-cell-segmenter.git  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-bud-cell-segmenter\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "",
    "documentation": "https://github.com/AurelienMaillot/napari-bud-cell-segmenter#README.md",
    "first_released": "2022-11-14T14:07:34.318509Z",
    "license": "BSD-3-Clause",
    "name": "napari-bud-cell-segmenter",
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": [],
    "project_site": "https://github.com/AurelienMaillot/napari-bud-cell-segmenter",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-12-16T14:31:02.717645Z",
    "report_issues": "https://github.com/AurelienMaillot/napari-bud-cell-segmenter/issues",
    "requirements": [
      "numpy",
      "magicgui",
      "qtpy",
      "pandas",
      "scikit-image",
      "napari",
      "tifffile",
      "matplotlib",
      "scipy",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "summary": "A plugin to segment embryonic mammary bud cells and detect 2 RNA probes",
    "support": "https://github.com/AurelienMaillot/napari-bud-cell-segmenter/issues",
    "twitter": "",
    "version": "0.1.4",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Markus Stabrin" }],
    "code_repository": null,
    "description": "# napari-boxmanager  [![License Mozilla Public License 2.0](https://img.shields.io/pypi/l/napari-boxmanager.svg?color=green)](https://github.com/mstabrin/napari-boxmanager/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-boxmanager.svg?color=green)](https://pypi.org/project/napari-boxmanager) [![Python Version](https://img.shields.io/pypi/pyversions/napari-boxmanager.svg?color=green)](https://python.org) [![tests](https://github.com/mstabrin/napari-boxmanager/workflows/tests/badge.svg)](https://github.com/mstabrin/napari-boxmanager/actions) [![codecov](https://codecov.io/gh/mstabrin/napari-boxmanager/branch/main/graph/badge.svg)](https://codecov.io/gh/mstabrin/napari-boxmanager) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-boxmanager)](https://napari-hub.org/plugins/napari-boxmanager)  Particle selection tool for cryo-em  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/plugins/index.html -->  ## Installation  **!!!** The installation of the napari-boxmanager requires at least napari version `0.4.16.rc8`! **!!!** If the version is not yet available use:      conda create -y -n napari-env -c conda-forge python=3.10     conda activate napari-env     pip install 'napari[all]'     pip uninstall napari     pip install git+https://github.com/napari/napari  You can install `napari-boxmanager` via [pip]:      pip install napari-boxmanager     ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [Mozilla Public License 2.0] license, \"napari-boxmanager\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-boxmanager       Particle selection tool for cryo-em  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation !!! The installation of the napari-boxmanager requires at least napari version 0.4.16.rc8! !!! If the version is not yet available use: conda create -y -n napari-env -c conda-forge python=3.10 conda activate napari-env pip install 'napari[all]' pip uninstall napari pip install git+https://github.com/napari/napari  You can install napari-boxmanager via pip: pip install napari-boxmanager  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the Mozilla Public License 2.0 license, \"napari-boxmanager\" is free and open source software Issues If you encounter any problems, please [file an issue] along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "Box Manager",
    "documentation": "",
    "first_released": "2022-09-26T11:24:43.663677Z",
    "license": "MPL-2.0",
    "name": "napari-boxmanager",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader", "writer", "widget"],
    "project_site": "",
    "python_version": ">=3.10",
    "reader_file_extensions": [
      "*.mrcs",
      "*.tmap",
      "*.tloc",
      "*.box",
      "*.star",
      "*.st",
      "*.rec",
      "*.cbox",
      "*.mrc",
      "*.temb",
      "*.coords"
    ],
    "release_date": "2022-11-09T07:29:47.055950Z",
    "report_issues": "",
    "requirements": [
      "matplotlib",
      "mrcfile",
      "numpy",
      "pandas",
      "pystardb",
      "mrcfile ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "tox ; extra == 'testing'"
    ],
    "summary": "Particle selection tool for cryo-em",
    "support": "",
    "twitter": "",
    "version": "0.2.10",
    "writer_file_extensions": [
      ".rec",
      ".tmap",
      ".mrcs",
      ".star",
      ".cbox",
      ".box",
      ".mrc",
      ".temb",
      ".coords",
      ".st",
      ".tloc"
    ],
    "writer_save_layers": ["points*", "image*"]
  },
  {
    "authors": [{ "name": "Herearii Metuarea" }],
    "code_repository": "https://github.com/hereariim/napari-blossom",
    "description": "# napari-blossom\\r \\r [![License BSD-3](https://img.shields.io/pypi/l/napari-blossom.svg?color=green)](https://github.com/hereariim/napari-blossom/raw/main/LICENSE)\\r [![PyPI](https://img.shields.io/pypi/v/napari-blossom.svg?color=green)](https://pypi.org/project/napari-blossom)\\r [![Python Version](https://img.shields.io/pypi/pyversions/napari-blossom.svg?color=green)](https://python.org)\\r [![tests](https://github.com/hereariim/napari-blossom/workflows/tests/badge.svg)](https://github.com/hereariim/napari-blossom/actions)\\r [![codecov](https://codecov.io/gh/hereariim/napari-blossom/branch/main/graph/badge.svg)](https://codecov.io/gh/hereariim/napari-blossom)\\r [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-blossom)](https://napari-hub.org/plugins/napari-blossom)\\r \\r Segmentation of blossom apple tree images\\r \\r ----------------------------------\\r \\r This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\r \\r <!--\\r Don't miss the full getting started guide to set up your new package:\\r https://github.com/napari/cookiecutter-napari-plugin#getting-started\\r \\r and review the napari docs for plugin developers:\\r https://napari.org/plugins/index.html\\r -->\\r \\r ## Installation\\r \\r You can install `napari-blossom` via [pip]:\\r \\r     pip install napari-blossom\\r \\r \\r \\r To install latest development version :\\r \\r     pip install git+https://github.com/hereariim/napari-blossom.git\\r \\r ## How does it works\\r \\r This module offers a plugin that allows you to segment the images of the apple tree flowers. As input, you can enter a **single image** with the image selection widget. Once the image is entered in the napari window, you can segment the apple blossoms with the image segmentation widget by running the run button. The segmented image will appear in the napari window.\\r \\r ## Contributing\\r \\r Contributions are very welcome. Tests can be run with [tox], please ensure\\r the coverage at least stays the same before you submit a pull request.\\r \\r ## License\\r \\r Distributed under the terms of the [BSD-3] license,\\r \"napari-blossom\" is free and open source software\\r \\r ## Issues\\r \\r If you encounter any problems, please [file an issue] along with a detailed description.\\r \\r [napari]: https://github.com/napari/napari\\r [Cookiecutter]: https://github.com/audreyr/cookiecutter\\r [@napari]: https://github.com/napari\\r [MIT]: http://opensource.org/licenses/MIT\\r [BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\r [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\r [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\r [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\r [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\r [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\r \\r [file an issue]: https://github.com/hereariim/napari-blossom/issues\\r \\r [napari]: https://github.com/napari/napari\\r [tox]: https://tox.readthedocs.io/en/latest/\\r [pip]: https://pypi.org/project/pip/\\r [PyPI]: https://pypi.org/\\r ",
    "description_content_type": "text/markdown",
    "description_text": "napari-blossom       Segmentation of blossom apple tree images  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install napari-blossom via pip: pip install napari-blossom  To install latest development version : pip install git+https://github.com/hereariim/napari-blossom.git  How does it works This module offers a plugin that allows you to segment the images of the apple tree flowers. As input, you can enter a single image with the image selection widget. Once the image is entered in the napari window, you can segment the apple blossoms with the image segmentation widget by running the run button. The segmented image will appear in the napari window. Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-blossom\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "Blossom",
    "documentation": "https://github.com/hereariim/napari-blossom#README.md",
    "first_released": "2022-06-20T20:23:41.594296Z",
    "license": "BSD-3-Clause",
    "name": "napari-blossom",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader", "writer", "widget", "sample_data"],
    "project_site": "https://github.com/hereariim/napari-blossom",
    "python_version": ">=3.8",
    "reader_file_extensions": ["*.npy"],
    "release_date": "2022-12-08T12:44:23.508285Z",
    "report_issues": "https://github.com/hereariim/napari-blossom/issues",
    "requirements": [
      "numpy",
      "magicgui",
      "qtpy",
      "opencv-python-headless",
      "tensorflow",
      "scikit-image",
      "napari",
      "focal-loss",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "summary": "Segmentation of blossom apple tree images",
    "support": "https://github.com/hereariim/napari-blossom/issues",
    "twitter": "",
    "version": "0.0.3",
    "visibility": "public",
    "writer_file_extensions": [".npy"],
    "writer_save_layers": ["image*", "labels*", "image"]
  },
  {
    "authors": [{ "name": "Andy Sweet" }, { "name": "Chi-Li Chiu" }],
    "code_repository": "https://github.com/andy-sweet/napari-blob-detection",
    "conda": [{ "channel": "conda-forge", "package": "napari-blob-detection" }],
    "description": "# napari-blob-detection  [![License](https://img.shields.io/pypi/l/napari-blob-detection.svg?color=green)](https://github.com/andy-sweet/napari-blob-detection/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-blob-detection.svg?color=green)](https://pypi.org/project/napari-blob-detection) [![Python Version](https://img.shields.io/pypi/pyversions/napari-blob-detection.svg?color=green)](https://python.org) [![tests](https://github.com/andy-sweet/napari-blob-detection/workflows/tests/badge.svg)](https://github.com/andy-sweet/napari-blob-detection/actions) [![codecov](https://codecov.io/gh/andy-sweet/napari-blob-detection/branch/main/graph/badge.svg)](https://codecov.io/gh/andy-sweet/napari-blob-detection) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-blob-detection)](https://napari-hub.org/plugins/napari-blob-detection)  Detects blobs in images  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/plugins/stable/index.html -->  This plugin consists of two widgets:  1. Detects blobs on images 2. Convert points layer to labels layer  ----------------------------------  ### Detects blobs on images  This widget uses [scikit-image's blob detection algorithms](https://scikit-image.org/docs/stable/auto_examples/features_detection/plot_blob.html) to detect bright blobs on dark backgrounds.  Parameters  - method: Laplacian of Gaussian (most accurate) or Difference of Gaussian (faster approximation)  - image: Image layer for blob detection. Can be a 2D, 3D, or higher dimensionality image. - dimensionality: users can specify if the image is 2D(+t) or 3D(+t). - min sigma: the smallest blob size to detect - max sigma: the largest blob size to detect - threshold: the lower the threshold, the more low intensity blobs are detected.   Output  Blobs are represented by the Points layer. The size of each blob is proportional to `Points.feature['sigma']`, which signifies the scale at which the feature point was found.  ### Convert points layer to labels layer  This widget takes a points layer and converts it into a labels layer, with the image dimension matching the selected image layer. By converting points to labels, users can leverage feature extraction functions that are available to labels to the detected points.  ----------------------------------  ## Installation  You can install `napari-blob-detection` via [pip]:      pip install napari-blob-detection    To install latest development version :      pip install git+https://github.com/andy-sweet/napari-blob-detection.git   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-blob-detection\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/andy-sweet/napari-blob-detection/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-blob-detection       Detects blobs in images  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  This plugin consists of two widgets:  Detects blobs on images Convert points layer to labels layer   Detects blobs on images This widget uses scikit-image's blob detection algorithms to detect bright blobs on dark backgrounds. Parameters  method: Laplacian of Gaussian (most accurate) or Difference of Gaussian (faster approximation)  image: Image layer for blob detection. Can be a 2D, 3D, or higher dimensionality image. dimensionality: users can specify if the image is 2D(+t) or 3D(+t). min sigma: the smallest blob size to detect max sigma: the largest blob size to detect threshold: the lower the threshold, the more low intensity blobs are detected.   Output Blobs are represented by the Points layer. The size of each blob is proportional to Points.feature['sigma'], which signifies the scale at which the feature point was found. Convert points layer to labels layer This widget takes a points layer and converts it into a labels layer, with the image dimension matching the selected image layer. By converting points to labels, users can leverage feature extraction functions that are available to labels to the detected points.  Installation You can install napari-blob-detection via pip: pip install napari-blob-detection  To install latest development version : pip install git+https://github.com/andy-sweet/napari-blob-detection.git  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-blob-detection\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari blob detection",
    "documentation": "https://github.com/andy-sweet/napari-blob-detection#README.md",
    "first_released": "2022-04-22T20:36:05.579776Z",
    "license": "BSD-3-Clause",
    "name": "napari-blob-detection",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/andy-sweet/napari-blob-detection",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-04-22T21:55:45.936883Z",
    "report_issues": "https://github.com/andy-sweet/napari-blob-detection/issues",
    "requirements": [
      "napari (>=0.4.13)",
      "numpy",
      "scikit-image",
      "magicgui",
      "pytest ; extra == 'test'"
    ],
    "summary": "Detects blobs in images",
    "support": "https://github.com/andy-sweet/napari-blob-detection/issues",
    "twitter": "",
    "version": "0.0.2",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      { "email": "robert.haase@tu-dresden.de", "name": "Robert Haase" }
    ],
    "code_repository": "https://github.com/haesleinhuepf/napari-curtain",
    "description": "# napari-curtain  [![License](https://img.shields.io/pypi/l/napari-curtain.svg?color=green)](https://github.com/haesleinhuepf/napari-curtain/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-curtain.svg?color=green)](https://pypi.org/project/napari-curtain) [![Python Version](https://img.shields.io/pypi/pyversions/napari-curtain.svg?color=green)](https://python.org) [![tests](https://github.com/haesleinhuepf/napari-curtain/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-curtain/actions) [![codecov](https://codecov.io/gh/haesleinhuepf/napari-curtain/branch/main/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-curtain) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-curtain)](https://napari-hub.org/plugins/napari-curtain)  View one image over another as curtain  ![](https://github.com/haesleinhuepf/napari-curtain/raw/main/docs/curtain_screencast.gif)  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  ## Usage  You find the `Curtain` plugin in the menu `Tools > Visualization > Curtain`. Move the position of the slider left/right  as shown in the video above. In case one image is much bright than the other, you can modify the `factors` above the  slider until visualization pleases.  ## Installation  You can install `napari-curtain` via [pip]:      pip install napari-curtain   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-curtain\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/haesleinhuepf/napari-curtain/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-curtain       View one image over another as curtain  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template. Usage You find the Curtain plugin in the menu Tools > Visualization > Curtain. Move the position of the slider left/right  as shown in the video above. In case one image is much bright than the other, you can modify the factors above the  slider until visualization pleases. Installation You can install napari-curtain via pip: pip install napari-curtain  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-curtain\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 3 - Alpha"],
    "display_name": "napari-curtain",
    "documentation": "https://github.com/haesleinhuepf/napari-curtain#README.md",
    "first_released": "2021-11-07T16:51:53.631261Z",
    "license": "BSD-3-Clause",
    "name": "napari-curtain",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/haesleinhuepf/napari-curtain",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-01-01T16:24:39.640036Z",
    "report_issues": "https://github.com/haesleinhuepf/napari-curtain/issues",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "napari-tools-menu"
    ],
    "summary": "View one image over another as curtain",
    "support": "https://github.com/haesleinhuepf/napari-curtain/issues",
    "twitter": "",
    "version": "0.1.1",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "email": "", "name": "" }],
    "code_repository": null,
    "conda": [],
    "description": "# napari-bioimageio  napari plugin for managing AI models in the [BioImage Model Zoo](https://bioimage.io).  > **WARNING**: This is an alpha release. The API may change in future versions, and please feel free to create issues to report bugs or provide feedbacks.  ![](assets/screenshot-model-manager-1.png)  ## Installation  ``` pip install napari-bioimageio ```  (If you don't have napari installed, run `pip install napari[pyqt5]`)  ## Usage  This library is meant for helping developers to ease the handling of models in napari.  We provide a set of API functions for managing and selecting models. ### `show_model_manager()` Show the model manager with a model list pulled from the BioImage Model Zoo, the user can explore all the available models, download or remove models.  ### `show_model_selector(filter=None)` Display a dialog for selecting models from the BioImage Model Zoo, the user can either select an existing model or download from the BioImage Model Zoo.  The selecte model information (a dictionary) will be returned if the user selected a model, otherwise it returns `None`.  Once the user selected the model, you can access the name, and also the file path to the model resource description file (via the `rdf_source` key). With the `bioimageio.core` library (installed via `pip install bioimageio.core` or `conda install -c conda-forge bioimageio.core`), you can run inference directly, the following examples shows how to implement it:  ```python # Popup a model selection dialog for choosing the model model_info = show_model_selector(filter=nuclear_segmentation_model_filter)  if model_info:   self.nucseg_model_source = model_info[\"rdf_source\"]   # Load model    model_description = bioimageio.core.load_resource_description(model_info[\"rdf_source\"])   input_image = imageio.imread(\"./my-image.tif\")    with bioimageio.core.create_prediction_pipeline(       bioimageio_model=model_description   ) as pipeline:     output_image = bioimageio.core.prediction.predict_with_padding(         pipeline, input_image, padding=padding     ) ``` Note: To run the models, you need to setup the conda environment properly according to the [installation guide of bioimageio.core](https://github.com/bioimage-io/core-bioimage-io-python#installation).  For more examples, see [this example notebook](https://github.com/bioimage-io/core-bioimage-io-python/blob/main/example/bioimageio-core-usage.ipynb) for `bioimageio.core`.  You can also access the weight files directly by searching the model folder (e.g. extract the model folder path via `os.path.dirname(model_description[\"rdf_source\"])`), this will be useful if you prefer to use your own model inference logic. ### `show_model_uploader()` Display a dialog to instruct the user to upload a model package to the BioImage Model Zoo. Currently, it only shows a message, in the future, we will try to support direct uploading with user's credentials obtained from Zenodo (a public data repository used by the BioImage Model Zoo to store models).  To create a BioImageIO-compatible model package, you can use the `build_model` function as demonstrated in [this notebook]((https://github.com/bioimage-io/core-bioimage-io-python/blob/main/example/bioimageio-core-usage.ipynb)).  ## Development  - Install and set up development environment.    ```sh   pip install -r requirements_dev.txt   ```    This will install all requirements. It will also install this package in development mode, so that code changes are applied immediately without reinstall necessary.  - Here's a list of development tools we use.   - [black](https://pypi.org/project/black/)   - [flake8](https://pypi.org/project/flake8/)   - [mypy](https://pypi.org/project/mypy/)   - [pydocstyle](https://pypi.org/project/pydocstyle/)   - [pylint](https://pypi.org/project/pylint/)   - [pytest](https://pypi.org/project/pytest/)   - [tox](https://pypi.org/project/tox/) - It's recommended to use the corresponding code formatter and linters also in your code editor to get instant feedback. A popular editor that can do this is [`vscode`](https://code.visualstudio.com/). - Run all tests, check formatting and linting.    ```sh   tox   ```  - Run a single tox environment.    ```sh   tox -e lint   ```  - Reinstall all tox environments.    ```sh   tox -r   ```  - Run pytest and all tests.    ```sh   pytest   ```  - Run pytest and calculate coverage for the package.    ```sh   pytest --cov-report term-missing --cov=napari-bioimageio   ```  - Continuous integration is by default supported via [GitHub actions](https://help.github.com/en/actions). GitHub actions is free for public repositories and comes with 2000 free Ubuntu build minutes per month for private repositories. ",
    "description_content_type": "text/markdown; charset=UTF-8; variant=GFM",
    "description_text": "napari-bioimageio napari plugin for managing AI models in the BioImage Model Zoo.  WARNING: This is an alpha release. The API may change in future versions, and please feel free to create issues to report bugs or provide feedbacks.   Installation pip install napari-bioimageio (If you don't have napari installed, run pip install napari[pyqt5]) Usage This library is meant for helping developers to ease the handling of models in napari. We provide a set of API functions for managing and selecting models. show_model_manager() Show the model manager with a model list pulled from the BioImage Model Zoo, the user can explore all the available models, download or remove models. show_model_selector(filter=None) Display a dialog for selecting models from the BioImage Model Zoo, the user can either select an existing model or download from the BioImage Model Zoo. The selecte model information (a dictionary) will be returned if the user selected a model, otherwise it returns None. Once the user selected the model, you can access the name, and also the file path to the model resource description file (via the rdf_source key). With the bioimageio.core library (installed via pip install bioimageio.core or conda install -c conda-forge bioimageio.core), you can run inference directly, the following examples shows how to implement it: ```python Popup a model selection dialog for choosing the model model_info = show_model_selector(filter=nuclear_segmentation_model_filter) if model_info:   self.nucseg_model_source = model_info[\"rdf_source\"]   # Load model    model_description = bioimageio.core.load_resource_description(model_info[\"rdf_source\"])   input_image = imageio.imread(\"./my-image.tif\") with bioimageio.core.create_prediction_pipeline(       bioimageio_model=model_description   ) as pipeline:     output_image = bioimageio.core.prediction.predict_with_padding(         pipeline, input_image, padding=padding     ) ``` Note: To run the models, you need to setup the conda environment properly according to the installation guide of bioimageio.core. For more examples, see this example notebook for bioimageio.core. You can also access the weight files directly by searching the model folder (e.g. extract the model folder path via os.path.dirname(model_description[\"rdf_source\"])), this will be useful if you prefer to use your own model inference logic. show_model_uploader() Display a dialog to instruct the user to upload a model package to the BioImage Model Zoo. Currently, it only shows a message, in the future, we will try to support direct uploading with user's credentials obtained from Zenodo (a public data repository used by the BioImage Model Zoo to store models). To create a BioImageIO-compatible model package, you can use the build_model function as demonstrated in this notebook. Development  Install and set up development environment.  sh   pip install -r requirements_dev.txt This will install all requirements. It will also install this package in development mode, so that code changes are applied immediately without reinstall necessary.  Here's a list of development tools we use. black flake8 mypy pydocstyle pylint pytest tox It's recommended to use the corresponding code formatter and linters also in your code editor to get instant feedback. A popular editor that can do this is vscode. Run all tests, check formatting and linting.  sh   tox  Run a single tox environment.  sh   tox -e lint  Reinstall all tox environments.  sh   tox -r  Run pytest and all tests.  sh   pytest  Run pytest and calculate coverage for the package.  sh   pytest --cov-report term-missing --cov=napari-bioimageio  Continuous integration is by default supported via GitHub actions. GitHub actions is free for public repositories and comes with 2000 free Ubuntu build minutes per month for private repositories. ",
    "development_status": [],
    "display_name": "BioImage.IO Model Manager",
    "documentation": "",
    "first_released": "2022-07-05T18:33:59.492886Z",
    "license": "",
    "name": "napari-bioimageio",
    "npe2": true,
    "operating_system": [],
    "plugin_types": ["widget"],
    "project_site": "",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-07-07T06:49:26.410343Z",
    "report_issues": "",
    "requirements": ["napari", "bioimageio.core (>=0.5.1)", "PyYAML (>=6.0)"],
    "summary": "",
    "support": "",
    "twitter": "",
    "version": "0.1.3",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      {
        "email": "p.schoennenbeck@fz-juelich.de",
        "name": "Philipp Schoennenbeck"
      }
    ],
    "code_repository": "https://github.com/Croxa/napari-brushsettings",
    "description": "# napari-brushsettings  [![License](https://img.shields.io/pypi/l/napari-brushsettings.svg?color=green)](https://github.com/Croxa/napari-brushsettings/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-brushsettings.svg?color=green)](https://pypi.org/project/napari-brushsettings) [![Python Version](https://img.shields.io/pypi/pyversions/napari-brushsettings.svg?color=green)](https://python.org) [![tests](https://github.com/Croxa/napari-brushsettings/workflows/tests/badge.svg)](https://github.com/Croxa/napari-brushsettings/actions) [![codecov](https://codecov.io/gh/Croxa/napari-brushsettings/branch/master/graph/badge.svg)](https://codecov.io/gh/Croxa/napari-brushsettings)  A simple plugin to set the brush settings for segmentation in napari  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/docs/plugins/index.html -->  ## Installation  You can install `napari-brushsettings` via [pip]:      pip install napari-brushsettings  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-brushsettings\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/Croxa/napari-brushsettings/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-brushsettings      A simple plugin to set the brush settings for segmentation in napari  This napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.  Installation You can install napari-brushsettings via pip: pip install napari-brushsettings  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-brushsettings\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari-brushsettings",
    "documentation": "https://github.com/Croxa/napari-brushsettings#README.md",
    "first_released": "2021-08-30T14:03:28.700519Z",
    "license": "BSD-3-Clause",
    "name": "napari-brushsettings",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/Croxa/napari-brushsettings",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2021-08-30T14:36:18.687181Z",
    "report_issues": "https://github.com/Croxa/napari-brushsettings/issues",
    "requirements": ["napari-plugin-engine (>=0.1.4)", "numpy"],
    "summary": "A simple plugin to set the brush settings for segmentation in napari",
    "support": "https://github.com/Croxa/napari-brushsettings/issues",
    "twitter": "",
    "version": "0.0.2",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      { "email": "robert.haase@tu-dresden.de", "name": "Robert Haase" }
    ],
    "code_repository": "https://github.com/haesleinhuepf/napari-animated-gif-io",
    "description": "# napari-animated-gif-io  [![License](https://img.shields.io/pypi/l/napari-animated-gif-io.svg?color=green)](https://github.com/haesleinhuepf/napari-animated-gif-io/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-animated-gif-io.svg?color=green)](https://pypi.org/project/napari-animated-gif-io) [![Python Version](https://img.shields.io/pypi/pyversions/napari-animated-gif-io.svg?color=green)](https://python.org) [![tests](https://github.com/haesleinhuepf/napari-animated-gif-io/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-animated-gif-io/actions) [![codecov](https://codecov.io/gh/haesleinhuepf/napari-animated-gif-io/branch/main/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-animated-gif-io) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-animated-gif-io)](https://napari-hub.org/plugins/napari-animated-gif-io)  Open and save 3D image stacks as animated gifs  You find the menus for opening and saving animated gifs in the `Tools > File Import/Export` menu:  ![img.png](https://github.com/haesleinhuepf/napari-animated-gif-io/raw/main/docs/screenshot.png)  Furthermore, if in 3D view, you can save the current view with a little tilt animation as animated gif. Under the hood this uses the [microfilm](https://github.com/guiwitz/microfilm) library.  ![img.png](https://github.com/haesleinhuepf/napari-animated-gif-io/raw/main/docs/video.gif)  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  ## Installation  You can install `napari-animated-gif-io` via [pip]:      pip install napari-animated-gif-io    To install latest development version :      pip install git+https://github.com/haesleinhuepf/napari-animated-gif-io.git   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-animated-gif-io\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/haesleinhuepf/napari-animated-gif-io/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-animated-gif-io       Open and save 3D image stacks as animated gifs You find the menus for opening and saving animated gifs in the Tools > File Import/Export menu:  Furthermore, if in 3D view, you can save the current view with a little tilt animation as animated gif. Under the hood this uses the microfilm library.   This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template. Installation You can install napari-animated-gif-io via pip: pip install napari-animated-gif-io  To install latest development version : pip install git+https://github.com/haesleinhuepf/napari-animated-gif-io.git  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-animated-gif-io\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari-animated-gif-io",
    "documentation": "https://github.com/haesleinhuepf/napari-animated-gif-io#README.md",
    "first_released": "2021-12-03T21:52:38.121004Z",
    "license": "BSD-3-Clause",
    "name": "napari-animated-gif-io",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["writer", "widget"],
    "project_site": "https://github.com/haesleinhuepf/napari-animated-gif-io",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-04-15T12:26:28.329312Z",
    "report_issues": "https://github.com/haesleinhuepf/napari-animated-gif-io/issues",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "imageio",
      "napari-tools-menu",
      "napari",
      "microfilm"
    ],
    "summary": "Save 3D image stacks as animated gifs",
    "support": "https://github.com/haesleinhuepf/napari-animated-gif-io/issues",
    "twitter": "",
    "version": "0.1.2",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": ["image"]
  },
  {
    "authors": [{ "name": "Robert Haase" }, { "name": "Ryan Savill" }],
    "code_repository": "https://github.com/haesleinhuepf/napari-assistant",
    "description": "# napari-assistant\\r [![License](https://img.shields.io/pypi/l/napari-assistant.svg?color=green)](https://github.com/haesleinhuepf/napari-assistant/raw/master/LICENSE)\\r [![PyPI](https://img.shields.io/pypi/v/napari-assistant.svg?color=green)](https://pypi.org/project/napari-assistant)\\r [![Python Version](https://img.shields.io/pypi/pyversions/napari-assistant.svg?color=green)](https://python.org)\\r [![tests](https://github.com/haesleinhuepf/napari-assistant/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-assistant/actions)\\r [![codecov](https://codecov.io/gh/haesleinhuepf/napari-assistant/branch/master/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-assistant)\\r [![Development Status](https://img.shields.io/pypi/status/napari-assistant.svg)](https://en.wikipedia.org/wiki/Software_release_life_cycle#Alpha)\\r [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-assistant)](https://napari-hub.org/plugins/napari-assistant)\\r [![DOI](https://zenodo.org/badge/463875112.svg)](https://zenodo.org/badge/latestdoi/463875112)\\r \\r \\r The napari-assistant is a [napari](https://github.com/napari/napari) meta-plugin for building image processing workflows. \\r \\r ## Usage\\r \\r After installing one or more napari plugins that use the napari-assistant as user interface, you can start it from the \\r menu `Tools > Utilities > Assistant (na)` or run `naparia` from the command line. \\r \\r By clicking on the buttons in the assistant, you can setup a workflow for processing the images.\\r \\r ![img.png](https://github.com/haesleinhuepf/napari-assistant/raw/main/docs/napari-assistant-screenshot.png)\\r \\r While setting up your workflow, you can at any point select a layer from the layer list (1) and change the parameters of\\r the corresponding operation (2). The layer will update when you change parameters and also all subsequent operations. \\r You can also vary which operation is applied to the image (3). Also make sure the right input image layer is selected (4).\\r \\r ![img.png](https://github.com/haesleinhuepf/napari-assistant/raw/main/docs/design_workflows.png)\\r \\r ### Saving and loading workflows\\r \\r You can also save and load workflows to disk. \\r \\r ![img.png](https://github.com/haesleinhuepf/napari-assistant/raw/main/docs/save_and_load.png)\\r \\r After loading a workflow, make sure that the right input images are selected.\\r \\r ### Code generation\\r \\r The napari-assistant allows exporting the given workflow as Python script and Jupyter Notebook. \\r \\r ![img.png](https://github.com/haesleinhuepf/napari-assistant/raw/main/docs/code_generator.png)\\r \\r Furthermore, if you have the [napari-script-editor](https://www.napari-hub.org/plugins/napari-script-editor) installed,\\r you can also send the current workflow as code to the script editor from the same menu.\\r \\r ![img.png](https://github.com/haesleinhuepf/napari-assistant/raw/main/docs/napari_script_editor.png)\\r \\r ### Plugin generation\\r \\r There is also a Napari plugin generator available. Check out [its documentation](https://github.com/haesleinhuepf/napari-assistant-plugin-generator) to learn how napari-assistant compatible plugins can be generated directly from within the assistant.\\r \\r ## Installation\\r \\r It is recommended to install the napari-assistant via one of the plugins that use it as graphical user interface.\\r You find a complete list of plugins that use the assistant [on the napari-hub](https://www.napari-hub.org/?search=napari-assistant&sort=relevance).\\r Multiple of these plugins come bundled when installing [devbio-napari](https://www.napari-hub.org/plugins/devbio-napari).\\r \\r ## For developers\\r \\r If you want to make your napari-plugin accessible from the napari-assistant, consider programming functions with a simple \\r interface that consume images, labels, integers, floats and strings. Annotate input and return types, e.g. like this:\\r ```python\\r def example_function_widget(image: \"napari.types.ImageData\") -> \"napari.types.LabelsData\":\\r     from skimage.filters import threshold_otsu\\r     binary_image = image > threshold_otsu(image)\\r \\r     from skimage.measure import label\\r     return label(binary_image)\\r ```\\r \\r Furthermore, please add your function to the napari.yaml which uses [npe2](https://github.com/napari/npe2):\\r ```\\r name: napari-npe2-test\\r display_name: napari-npe2-test\\r contributions:\\r   commands: \\r     - id: napari-npe2-test.make_magic_widget\\r       python_name: napari_npe2_test._widget:example_magic_widget\\r       title: Make example magic widget\\r   widgets:\\r     - command: napari-npe2-test.make_magic_widget\\r       display_name: Segmentation / labeling > Otsu Labeling (nnpe2t)\\r ```\\r \\r To put it in the right button within the napari-assistant, please use one of the following prefixes for the `display_name`:\\r * `Filtering / noise removal > `\\r * `Filtering / background removal > `\\r * `Filtering > `\\r * `Image math > `\\r * `Transform > `\\r * `Projection > `\\r * `Segmentation / binarization > `\\r * `Segmentation / labeling > `\\r * `Segmentation post-processing > `\\r * `Measurement > `\\r * `Label neighbor filters > `\\r * `Label filters > `\\r * `Visualization > `\\r \\r You find a fully functional example [here](https://github.com/haesleinhuepf/napari-npe2-test).\\r \\r Last but not least, to make your napari-plugin is listed in the napari-hub when searching for \"napari-assistant\", make sure\\r you mention it in your `readme`.\\r \\r ## Feedback welcome!\\r \\r The napari-assistant is developed in the open because we believe in the open source community. Feel free to drop feedback as [github issue](https://github.com/haesleinhuepf/napari-assistant/issues) or via [image.sc](https://image.sc)\\r \\r ## Contributing\\r \\r Contributions are very welcome. Please ensure\\r the test coverage at least stays the same before you submit a pull request.\\r \\r ## License\\r \\r Distributed under the terms of the [BSD-3] license,\\r \"napari-assistant\" is free and open source software\\r \\r ## Acknowledgements\\r This project was supported by the Deutsche Forschungsgemeinschaft under Germany’s Excellence Strategy – EXC2068 - Cluster of Excellence \"Physics of Life\" of TU Dresden. \\r This project has been made possible in part by grant number [2021-240341 (Napari plugin accelerator grant)](https://chanzuckerberg.com/science/programs-resources/imaging/napari/improving-image-processing/) from the Chan Zuckerberg Initiative DAF, an advised fund of the Silicon Valley Community Foundation.\\r \\r [BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\r \\r ",
    "description_content_type": "text/markdown",
    "description_text": "napari-assistant         The napari-assistant is a napari meta-plugin for building image processing workflows.  Usage After installing one or more napari plugins that use the napari-assistant as user interface, you can start it from the  menu Tools > Utilities > Assistant (na) or run naparia from the command line.  By clicking on the buttons in the assistant, you can setup a workflow for processing the images.  While setting up your workflow, you can at any point select a layer from the layer list (1) and change the parameters of the corresponding operation (2). The layer will update when you change parameters and also all subsequent operations.  You can also vary which operation is applied to the image (3). Also make sure the right input image layer is selected (4).  Saving and loading workflows You can also save and load workflows to disk.   After loading a workflow, make sure that the right input images are selected. Code generation The napari-assistant allows exporting the given workflow as Python script and Jupyter Notebook.   Furthermore, if you have the napari-script-editor installed, you can also send the current workflow as code to the script editor from the same menu.  Plugin generation There is also a Napari plugin generator available. Check out its documentation to learn how napari-assistant compatible plugins can be generated directly from within the assistant. Installation It is recommended to install the napari-assistant via one of the plugins that use it as graphical user interface. You find a complete list of plugins that use the assistant on the napari-hub. Multiple of these plugins come bundled when installing devbio-napari. For developers If you want to make your napari-plugin accessible from the napari-assistant, consider programming functions with a simple  interface that consume images, labels, integers, floats and strings. Annotate input and return types, e.g. like this: ```python def example_function_widget(image: \"napari.types.ImageData\") -> \"napari.types.LabelsData\":     from skimage.filters import threshold_otsu     binary_image = image > threshold_otsu(image) from skimage.measure import label return label(binary_image)  ``` Furthermore, please add your function to the napari.yaml which uses npe2: name: napari-npe2-test display_name: napari-npe2-test contributions:   commands:      - id: napari-npe2-test.make_magic_widget       python_name: napari_npe2_test._widget:example_magic_widget       title: Make example magic widget   widgets:     - command: napari-npe2-test.make_magic_widget       display_name: Segmentation / labeling > Otsu Labeling (nnpe2t) To put it in the right button within the napari-assistant, please use one of the following prefixes for the display_name: * Filtering / noise removal > * Filtering / background removal > * Filtering > * Image math > * Transform > * Projection > * Segmentation / binarization > * Segmentation / labeling > * Segmentation post-processing > * Measurement > * Label neighbor filters > * Label filters > * Visualization > You find a fully functional example here. Last but not least, to make your napari-plugin is listed in the napari-hub when searching for \"napari-assistant\", make sure you mention it in your readme. Feedback welcome! The napari-assistant is developed in the open because we believe in the open source community. Feel free to drop feedback as github issue or via image.sc Contributing Contributions are very welcome. Please ensure the test coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-assistant\" is free and open source software Acknowledgements This project was supported by the Deutsche Forschungsgemeinschaft under Germany’s Excellence Strategy – EXC2068 - Cluster of Excellence \"Physics of Life\" of TU Dresden.  This project has been made possible in part by grant number 2021-240341 (Napari plugin accelerator grant) from the Chan Zuckerberg Initiative DAF, an advised fund of the Silicon Valley Community Foundation.",
    "development_status": [],
    "display_name": "napari-assistant",
    "documentation": "https://github.com/haesleinhuepf/napari-assistant/",
    "first_released": "2022-03-05T08:51:21.134774Z",
    "license": "BSD-3-Clause",
    "name": "napari-assistant",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/haesleinhuepf/napari-assistant/",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-12-24T21:08:37.108920Z",
    "report_issues": "https://github.com/haesleinhuepf/napari-assistant/issues",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "toolz",
      "napari (>=0.4.14)",
      "magicgui",
      "numpy (!=1.19.4)",
      "pyperclip",
      "loguru",
      "jupytext",
      "jupyter",
      "pandas",
      "napari-time-slicer (>=0.4.8)",
      "napari-workflows (>=0.2.4)"
    ],
    "summary": "A pocket calculator like interface to image processing in napari",
    "support": "https://forum.image.sc/",
    "twitter": "",
    "version": "0.4.4",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "email": "lalit@mpi-cbg.de", "name": "Manan Lalit" }],
    "category": {
      "Image modality": ["Fluorescence microscopy", "Confocal microscopy"],
      "Supported data": ["3D"],
      "Workflow step": ["Image registration", "Object feature extraction"]
    },
    "category_hierarchy": {
      "Image modality": [
        ["Fluorescence microscopy", "Light-sheet microscopy"],
        ["Confocal microscopy"]
      ],
      "Supported data": [["3D"]],
      "Workflow step": [
        ["Image registration"],
        ["Object feature extraction", "Shape features extraction"]
      ]
    },
    "code_repository": "https://github.com/juglab/PlatyMatch",
    "description": "[![DOI:10.1007/978-3-030-66415-2_30](https://zenodo.org/badge/DOI/10.1007/978-3-030-66415-2_30.svg)](https://link.springer.com/chapter/10.1007/978-3-030-66415-2_30) [![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT) [![PyPI](https://img.shields.io/pypi/v/PlatyMatch.svg?color=green)](https://pypi.org/project/PlatyMatch) [![Python Version](https://img.shields.io/pypi/pyversions/PlatyMatch.svg?color=green)](https://python.org) [![tests](https://github.com/juglab/PlatyMatch/workflows/tests/badge.svg)](https://github.com/juglab/PlatyMatch/actions) [![codecov](https://codecov.io/gh/juglab/PlatyMatch/branch/master/graph/badge.svg)](https://codecov.io/gh/juglab/PlatyMatch)   <p align=\"center\">   <img src=\"https://user-images.githubusercontent.com/34229641/117537510-b26ee500-b001-11eb-9642-3baa461bfc94.png\" width=400 /> </p> <h2 align=\"center\">Registration of Multi-modal Volumetric Images by Establishing Cell Correspondence</h2>  ## Table of Contents  - **[Introduction](#introduction)** - **[Dependencies](#dependencies)** - **[Getting Started](#getting-started)** - **[Datasets](#datasets)** - **[Registering your data](#registering-your-data)** - **[Contributing](#contributing)** - **[Issues](#issues)** - **[Citation](#citation)**  ### Introduction This repository hosts the version of the code used for the **[publication](https://link.springer.com/chapter/10.1007/978-3-030-66415-2_30)** **Registration of Multi-modal Volumetric Images by Establishing Cell Correspondence**.   We refer to the techniques elaborated in the publication, here as **PlatyMatch**. `PlatyMatch` performs a linear registration of volumetric, microscopy images of embryos by establishing correspondences between cells.   `PlatyMatch` first detects nuclei in the two images being considered, next calculates unique `shape context` features for each nucleus detection which encapsulates the neighborhood as seen by that nucleus, and finally identifies pairs of matching nuclei through maximum bipartite matching applied to the pairwise distance matrix generated from these features.   ### Dependencies   You can install `PlatyMatch` via **[pip]**:  ``` conda create -y -n PlatyMatchEnv python==3.8 conda activate PlatyMatchEnv python3 -m pip install PlatyMatch ```  ### Getting Started  Type in the following commands in a new terminal window.  ``` conda activate PlatyMatchEnv napari ```  Next, select `PlatyMatch` from `Plugins> Add Dock Widget`.  ### Datasets  Datasets are available in **`bic_eccv_data.zip`** as release assets **[here](https://github.com/juglab/PlatyMatch/releases/tag/v0.0.1)**. These comprise of images, nuclei detections and keypoint locations for confocal images of 12 individual specimens under the `01-insitus` directory and static snapshots of a live embryo imaged through Light Sheet Microscopy under the `02-live` directory.  Folders with the same name in these two directories correspond in their developmental age, for example, `01-insitus/02` corresponds to `02-live/02`, `01-insitus/03` corresponds to `02-live/03` and so on.      ### Registering your data  - **Detect Nuclei**  \\t- Drag and drop your images in the viewer  \\t- Click on `Sync with Viewer` button to refresh the drop-down menus  \\t- Select the appropriate image in the drop down menu (for which nuclei detections are desired) \\t- Select **`Detect Nuclei`** from the drop-down menu \\t- Specify the anisotropy factor (`Anisotropy (Z)`) (i.e. the ratio of the size of the z pixel with respect to the x or y pixel. This factor is typically more than 1.0 because the z dimension is often undersampled) \\t- Ideally min scales and max scales should be estimated from your data (`min_scale` should be set as `min_radius/sqrt(3)` and `max_scale` should be set as `max_radius/sqrt(3)`. The default values of `min_scale=5` and `max_scale=9` generally works well).   \\t- Click `Run Scale Space Log` button. Please note that this step takes a few minutes. \\t- Wait until a confirmation message suggesting that nuclei detection is over shows up on the terminal \\t- Export the nuclei locations (`Export detections to csv`) to a csv file \\t- Repeat this step for all images which need to be matched     https://user-images.githubusercontent.com/34229641/120660618-cd5d3980-c487-11eb-8996-326264a4df87.mp4   - **Estimate Transform** \\t- In case, nuclei were exported to a csv in the `Detect Nuclei` panel, tick `csv` checkbox \\t- If the nuclei detected were specified in the order id, z, y and x in the csv file, then tick `IZYXR` checkbox \\t- Additionally if there is a header in the csv file, tick `Header` checkbox \\t- Load the detections for the `Moving Image`, which is defined as the image which will be transformed to later match another `fixed` image \\t- Load the detections for the `Fixed Image` \\t- Click on `Run` pushbutton. Once the calculation is complete, a confirmation message shows up in the terminal. Export the transform matrix to a csv (Note that this step can take a few minutes) \\t- It is also possible to estimate the transform in a `supervised` fashion. For this, upload the locations of a few matching keypoints in both images. These locations serve to provide a good starting point for the transform calculation. Once the keypoint files have been uploaded for both the images, then click `Run` and then export the transform matrix to a csv file    https://user-images.githubusercontent.com/34229641/120685628-53857a00-c4a0-11eb-8f92-7ffac730e28a.mp4    - **Evaluate Metrics** \\t- Drag images which need to be transformed, in the viewer \\t- Click on `Sync with Viewer` button to refresh the drop-down menus \\t- Specify the anisotropy factor (`Moving Image Anisotropy (Z)` and `Fixed Image Anisotropy (Z)`) (i.e. the ratio of the size of the z pixel with respect to the x or y pixel. This factor is typically more than 1.0 because the z dimension is often undersampled) \\t- Load the transform which was calculated in the previous steps \\t- If you simply wish to export a transformed version of the moving image, click on `Export Transformed Image` \\t- Additionally, one could quantify metrics such as average registration error evaluated on a few keypoints. To do so, tick the `csv` checkbox, if keypoints and detections are available as a csv file. Then load the keypoints for the moving image (`Moving Kepoints`) and the fixed image (`Fixed Keypoints`) \\t- Also, upload the detections calculated in the previous steps (`Detect Nuclei`)  by uploading the `Moving Detections` and the `Fixed Detections` \\t- Click on the `Run` push button \\t- The text fields such as `Matching Accuracy`(0 to 1, with 1 being the best) and `Average Registration Error` (the lower the better) should become populated once the results are available    https://user-images.githubusercontent.com/34229641/120685654-5b451e80-c4a0-11eb-8d7d-de58b8b8304d.mp4   ### Contributing  Contributions are very welcome. Tests can be run with **[tox]**.  ### Issues  If you encounter any problems, please **[file an issue]** along with a detailed description.  [file an issue]: https://github.com/juglab/PlatyMatch/issues [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/EmbedSeg/   ### Citation If you find our work useful in your research, please consider citing:  ```bibtex @InProceedings{10.1007/978-3-030-66415-2_30, author=\"Lalit, Manan and Handberg-Thorsager, Mette and Hsieh, Yu-Wen and Jug, Florian and Tomancak, Pavel\", editor=\"Bartoli, Adrien and Fusiello, Andrea\", title=\"Registration of Multi-modal Volumetric Images by Establishing Cell Correspondence\", booktitle=\"Computer Vision -- ECCV 2020 Workshops\", year=\"2020\", publisher=\"Springer International Publishing\", address=\"Cham\", pages=\"458--473\", isbn=\"978-3-030-66415-2\" } ```  `PlatyMatch` plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin [file an issue]: https://github.com/juglab/PlatyMatch/issues [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "         Registration of Multi-modal Volumetric Images by Establishing Cell Correspondence Table of Contents  Introduction Dependencies Getting Started Datasets Registering your data Contributing Issues Citation  Introduction This repository hosts the version of the code used for the publication Registration of Multi-modal Volumetric Images by Establishing Cell Correspondence.  We refer to the techniques elaborated in the publication, here as PlatyMatch. PlatyMatch performs a linear registration of volumetric, microscopy images of embryos by establishing correspondences between cells.  PlatyMatch first detects nuclei in the two images being considered, next calculates unique shape context features for each nucleus detection which encapsulates the neighborhood as seen by that nucleus, and finally identifies pairs of matching nuclei through maximum bipartite matching applied to the pairwise distance matrix generated from these features.  Dependencies You can install PlatyMatch via pip: conda create -y -n PlatyMatchEnv python==3.8 conda activate PlatyMatchEnv python3 -m pip install PlatyMatch Getting Started Type in the following commands in a new terminal window. conda activate PlatyMatchEnv napari Next, select PlatyMatch from Plugins> Add Dock Widget. Datasets Datasets are available in bic_eccv_data.zip as release assets here. These comprise of images, nuclei detections and keypoint locations for confocal images of 12 individual specimens under the 01-insitus directory and static snapshots of a live embryo imaged through Light Sheet Microscopy under the 02-live directory.  Folders with the same name in these two directories correspond in their developmental age, for example, 01-insitus/02 corresponds to 02-live/02, 01-insitus/03 corresponds to 02-live/03 and so on.    Registering your data  Detect Nuclei  Drag and drop your images in the viewer  Click on Sync with Viewer button to refresh the drop-down menus  Select the appropriate image in the drop down menu (for which nuclei detections are desired) Select Detect Nuclei from the drop-down menu Specify the anisotropy factor (Anisotropy (Z)) (i.e. the ratio of the size of the z pixel with respect to the x or y pixel. This factor is typically more than 1.0 because the z dimension is often undersampled) Ideally min scales and max scales should be estimated from your data (min_scale should be set as min_radius/sqrt(3) and max_scale should be set as max_radius/sqrt(3). The default values of min_scale=5 and max_scale=9 generally works well).   Click Run Scale Space Log button. Please note that this step takes a few minutes. Wait until a confirmation message suggesting that nuclei detection is over shows up on the terminal Export the nuclei locations (Export detections to csv) to a csv file Repeat this step for all images which need to be matched    https://user-images.githubusercontent.com/34229641/120660618-cd5d3980-c487-11eb-8996-326264a4df87.mp4  Estimate Transform In case, nuclei were exported to a csv in the Detect Nuclei panel, tick csv checkbox If the nuclei detected were specified in the order id, z, y and x in the csv file, then tick IZYXR checkbox Additionally if there is a header in the csv file, tick Header checkbox Load the detections for the Moving Image, which is defined as the image which will be transformed to later match another fixed image Load the detections for the Fixed Image Click on Run pushbutton. Once the calculation is complete, a confirmation message shows up in the terminal. Export the transform matrix to a csv (Note that this step can take a few minutes) It is also possible to estimate the transform in a supervised fashion. For this, upload the locations of a few matching keypoints in both images. These locations serve to provide a good starting point for the transform calculation. Once the keypoint files have been uploaded for both the images, then click Run and then export the transform matrix to a csv file     https://user-images.githubusercontent.com/34229641/120685628-53857a00-c4a0-11eb-8f92-7ffac730e28a.mp4  Evaluate Metrics Drag images which need to be transformed, in the viewer Click on Sync with Viewer button to refresh the drop-down menus Specify the anisotropy factor (Moving Image Anisotropy (Z) and Fixed Image Anisotropy (Z)) (i.e. the ratio of the size of the z pixel with respect to the x or y pixel. This factor is typically more than 1.0 because the z dimension is often undersampled) Load the transform which was calculated in the previous steps If you simply wish to export a transformed version of the moving image, click on Export Transformed Image Additionally, one could quantify metrics such as average registration error evaluated on a few keypoints. To do so, tick the csv checkbox, if keypoints and detections are available as a csv file. Then load the keypoints for the moving image (Moving Kepoints) and the fixed image (Fixed Keypoints) Also, upload the detections calculated in the previous steps (Detect Nuclei)  by uploading the Moving Detections and the Fixed Detections Click on the Run push button The text fields such as Matching Accuracy(0 to 1, with 1 being the best) and Average Registration Error (the lower the better) should become populated once the results are available    https://user-images.githubusercontent.com/34229641/120685654-5b451e80-c4a0-11eb-8d7d-de58b8b8304d.mp4 Contributing Contributions are very welcome. Tests can be run with tox. Issues If you encounter any problems, please file an issue along with a detailed description. Citation If you find our work useful in your research, please consider citing: bibtex @InProceedings{10.1007/978-3-030-66415-2_30, author=\"Lalit, Manan and Handberg-Thorsager, Mette and Hsieh, Yu-Wen and Jug, Florian and Tomancak, Pavel\", editor=\"Bartoli, Adrien and Fusiello, Andrea\", title=\"Registration of Multi-modal Volumetric Images by Establishing Cell Correspondence\", booktitle=\"Computer Vision -- ECCV 2020 Workshops\", year=\"2020\", publisher=\"Springer International Publishing\", address=\"Cham\", pages=\"458--473\", isbn=\"978-3-030-66415-2\" } PlatyMatch plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "PlatyMatch",
    "documentation": "",
    "first_released": "2021-05-28T20:21:08.816113Z",
    "license": "BSD-3",
    "name": "PlatyMatch",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/juglab/PlatyMatch",
    "python_version": ">=3.6",
    "reader_file_extensions": [],
    "release_date": "2021-06-08T12:34:42.356971Z",
    "report_issues": "",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "scikit-image",
      "scikit-learn",
      "tqdm",
      "simpleitk",
      "napari[all]",
      "pandas",
      "pytest"
    ],
    "summary": "PlatyMatch allows registration of volumetric images of embryos by establishing correspondences between cells",
    "support": "",
    "twitter": "",
    "version": "0.0.3",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Herearii Metuarea" }],
    "code_repository": "https://github.com/hereariim/napari-conidie",
    "description": "# napari-conidie\\r \\r [![License BSD-3](https://img.shields.io/pypi/l/napari-conidie.svg?color=green)](https://github.com/hereariim/napari-conidie/raw/main/LICENSE)\\r [![PyPI](https://img.shields.io/pypi/v/napari-conidie.svg?color=green)](https://pypi.org/project/napari-conidie)\\r [![Python Version](https://img.shields.io/pypi/pyversions/napari-conidie.svg?color=green)](https://python.org)\\r [![tests](https://github.com/hereariim/napari-conidie/workflows/tests/badge.svg)](https://github.com/hereariim/napari-conidie/actions)\\r [![codecov](https://codecov.io/gh/hereariim/napari-conidie/branch/main/graph/badge.svg)](https://codecov.io/gh/hereariim/napari-conidie)\\r [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-conidie)](https://napari-hub.org/plugins/napari-conidie)\\r \\r A segmentation tool to get conidie and hyphe\\r \\r ----------------------------------\\r \\r This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\r \\r <!--\\r Don't miss the full getting started guide to set up your new package:\\r https://github.com/napari/cookiecutter-napari-plugin#getting-started\\r \\r and review the napari docs for plugin developers:\\r https://napari.org/stable/plugins/index.html\\r -->\\r \\r This plugin is a use case for obtaining conidia and hyphae surface from images. This plugin is a private tool dedicated exclusively to the work of the QUASAV team.\\r \\r ## Installation\\r \\r This private tool cannot be found in the built-in napari. The installation therefore follows two steps:\\r \\r 1 - Install latest development version :\\r \\r     git clone https://github.com/hereariim/napari-conidie.git\\r \\r Or:\\r \\r     Download napari-conidie zip file\\r \\r 2 - Drag and drop napari-conidie file into the built-in\\r \\r ## Contributing\\r \\r Contributions are very welcome. Tests can be run with [tox], please ensure\\r the coverage at least stays the same before you submit a pull request.\\r \\r ## License\\r \\r Distributed under the terms of the [BSD-3] license,\\r \"napari-conidie\" is free and open source software\\r \\r ## Issues\\r \\r If you encounter any problems, please [file an issue] along with a detailed description.\\r \\r [napari]: https://github.com/napari/napari\\r [Cookiecutter]: https://github.com/audreyr/cookiecutter\\r [@napari]: https://github.com/napari\\r [MIT]: http://opensource.org/licenses/MIT\\r [BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\r [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\r [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\r [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\r [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\r [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\r \\r [file an issue]: https://github.com/hereariim/napari-conidie/issues\\r \\r [napari]: https://github.com/napari/napari\\r [tox]: https://tox.readthedocs.io/en/latest/\\r [pip]: https://pypi.org/project/pip/\\r [PyPI]: https://pypi.org/\\r ",
    "description_content_type": "text/markdown",
    "description_text": "napari-conidie       A segmentation tool to get conidie and hyphe  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  This plugin is a use case for obtaining conidia and hyphae surface from images. This plugin is a private tool dedicated exclusively to the work of the QUASAV team. Installation This private tool cannot be found in the built-in napari. The installation therefore follows two steps: 1 - Install latest development version : git clone https://github.com/hereariim/napari-conidie.git  Or: Download napari-conidie zip file  2 - Drag and drop napari-conidie file into the built-in Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-conidie\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "Conidie",
    "documentation": "https://github.com/hereariim/napari-conidie#README.md",
    "first_released": "2022-12-15T13:02:30.925644Z",
    "license": "BSD-3-Clause",
    "name": "napari-conidie",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/hereariim/napari-conidie",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-12-15T13:02:30.925644Z",
    "report_issues": "https://github.com/hereariim/napari-conidie/issues",
    "requirements": [
      "numpy",
      "magicgui",
      "qtpy",
      "pandas",
      "h5py",
      "scikit-image",
      "napari",
      "matplotlib",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "summary": "A segmentation tool to get conidie and hyphe",
    "support": "https://github.com/hereariim/napari-conidie/issues",
    "twitter": "",
    "version": "0.0.1",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      { "name": "Nicholas Sofroniew" },
      { "name": "Alister Burt" },
      { "name": "Guillaume Witz" },
      { "name": "Faris Abouakil" },
      { "name": "Talley Lambert" }
    ],
    "code_repository": "https://github.com/napari/napari-animation",
    "description": "# napari-animation  [![License](https://img.shields.io/pypi/l/napari-animation.svg?color=green)](https://github.com/napari/napari-animation/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-animation.svg?color=green)](https://pypi.org/project/napari-animation) [![Python Version](https://img.shields.io/pypi/pyversions/napari-animation.svg?color=green)](https://python.org) [![tests](https://github.com/napari/napari-animation/actions/workflows/test_and_deploy.yml/badge.svg)](https://github.com/napari/napari-animation/actions) [![codecov](https://codecov.io/gh/napari/napari-animation/branch/main/graph/badge.svg)](https://codecov.io/gh/napari/napari-animation)  **napari-animation** is a plugin for making animations in [napari].  <p align=\"center\">   <img width=\"500\" src=\"https://user-images.githubusercontent.com/7307488/196110138-6c4663b1-67b2-4c79-97b7-57b706d1d49c.gif\"> </p>  ----------------------------------  This plugin is built on [naparimovie](https://github.com/guiwitz/naparimovie) from @guiwitz. naparimovie was submitted to napari in [PR#851](https://github.com/napari/napari/pull/780) before napari plugin infrastructure existed.  ---------------------------------- ## Overview  **napari-animation** provides a framework for the creation of animations in napari, the plugin contains: - an easy to use GUI for creating animations interactively - a Python package for the programmatic creation of animations  This plugin remains under development and contributions are very welcome, please open an issue to discuss potential improvements.  ## Installation  ### PyPI `napari-animation` is available through the Python package index and can be installed using `pip`.  ```sh pip install napari-animation ```  ### Local You can clone this repository and install locally with      pip install -e .  ### Interactive use **napari-animation** can be used interactively.  An animation is created by capturing [keyframes](https://en.wikipedia.org/wiki/Key_frame) containing the current viewer state.  <p align=\"center\">   <img width=\"600\" src=\"https://user-images.githubusercontent.com/7307488/196113682-96ce0da3-fa5c-411e-8fb1-52dc3a8f96b6.png\"> </p>  To activate the GUI, select **napari-animation: wizard** from the *plugins menu*  <p align=\"center\">   <img width=\"200\" src=\"https://user-images.githubusercontent.com/7307488/196114466-56cb5985-0d79-4cfa-96f1-38cf3ccfbc48.png\"> </p>  ### Headless **napari-animation** can also be run headless, allowing for reproducible, scripted creation of animations.  ```python from napari_animation import Animation  animation = Animation(viewer)  viewer.dims.ndisplay = 3 viewer.camera.angles = (0.0, 0.0, 90.0) animation.capture_keyframe() viewer.camera.zoom = 2.4 animation.capture_keyframe() viewer.camera.angles = (-7.0, 15.7, 62.4) animation.capture_keyframe(steps=60) viewer.camera.angles = (2.0, -24.4, -36.7) animation.capture_keyframe(steps=60) viewer.reset_view() viewer.camera.angles = (0.0, 0.0, 90.0) animation.capture_keyframe() animation.animate('demo.mov', canvas_only=False) ```  ## Examples Examples can be found in our [examples](examples) folder. Simple examples for both interactive and headless  use of the plugin follow.  ## Contributing  Contributions are very welcome and a detailed contributing guide is coming soon.   Tests are run with `pytest`.  We use [`pre-commit`](https://pre-commit.com) to sort imports with [`isort`](https://github.com/timothycrosley/isort), format code with [`black`](https://github.com/psf/black), and lint with [`flake8`](https://github.com/PyCQA/flake8) automatically prior to each commit. To minmize test errors when submitting pull requests, please install `pre-commit` in your environment as follows:  ```sh pre-commit install ```  ## License  Distributed under the terms of the [BSD-3] license, \"napari-animation\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin [file an issue]: https://github.com/sofroniewn/napari-animation/issues [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-animation      napari-animation is a plugin for making animations in napari.     This plugin is built on naparimovie from @guiwitz. naparimovie was submitted to napari in PR#851 before napari plugin infrastructure existed.  Overview napari-animation provides a framework for the creation of animations in napari, the plugin contains: - an easy to use GUI for creating animations interactively - a Python package for the programmatic creation of animations This plugin remains under development and contributions are very welcome, please open an issue to discuss potential improvements. Installation PyPI napari-animation is available through the Python package index and can be installed using pip. sh pip install napari-animation Local You can clone this repository and install locally with pip install -e .  Interactive use napari-animation can be used interactively. An animation is created by capturing keyframes containing the current viewer state.    To activate the GUI, select napari-animation: wizard from the plugins menu    Headless napari-animation can also be run headless, allowing for reproducible, scripted creation of animations. ```python from napari_animation import Animation animation = Animation(viewer) viewer.dims.ndisplay = 3 viewer.camera.angles = (0.0, 0.0, 90.0) animation.capture_keyframe() viewer.camera.zoom = 2.4 animation.capture_keyframe() viewer.camera.angles = (-7.0, 15.7, 62.4) animation.capture_keyframe(steps=60) viewer.camera.angles = (2.0, -24.4, -36.7) animation.capture_keyframe(steps=60) viewer.reset_view() viewer.camera.angles = (0.0, 0.0, 90.0) animation.capture_keyframe() animation.animate('demo.mov', canvas_only=False) ``` Examples Examples can be found in our examples folder. Simple examples for both interactive and headless  use of the plugin follow. Contributing Contributions are very welcome and a detailed contributing guide is coming soon.  Tests are run with pytest. We use pre-commit to sort imports with isort, format code with black, and lint with flake8 automatically prior to each commit. To minmize test errors when submitting pull requests, please install pre-commit in your environment as follows: sh pre-commit install License Distributed under the terms of the BSD-3 license, \"napari-animation\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 3 - Alpha"],
    "display_name": "napari-animation",
    "documentation": "",
    "first_released": "2021-04-23T16:11:20.051462Z",
    "license": "BSD 3-Clause",
    "name": "napari-animation",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/napari/napari-animation",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-11-12T17:21:07.189390Z",
    "report_issues": "",
    "requirements": [
      "imageio",
      "imageio-ffmpeg",
      "napari",
      "npe2",
      "numpy",
      "qtpy",
      "scipy",
      "tqdm (>=4.56.0)",
      "superqt",
      "pre-commit ; extra == 'dev'",
      "black ; extra == 'dev'",
      "flake8 ; extra == 'dev'",
      "check-manifest ; extra == 'dev'",
      "PyQt5 (!=5.15.0,>=5.12.3) ; extra == 'dev'",
      "pytest ; extra == 'dev'",
      "pytest ; extra == 'testing'"
    ],
    "summary": "A plugin for making animations in napari",
    "support": "",
    "twitter": "",
    "version": "0.0.5",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "email": "alan.watson@pitt.edu", "name": "Alan M Watson" }],
    "code_repository": "https://github.com/brain-image-library/napari-bil-data-viewer",
    "description": "<p href=\"https://www.brainimagelibrary.org/\">     <align=\"center\" width=\"100%\">     <img width=\"100%\" src=\"https://i.imgur.com/ljZKq8h.png\"> </p>   # Description  View datasets archived at the **[Brain Image Library](https://www.brainimagelibrary.org/)**.  **NOTE: This plugin is under early development.  Currently, only a subset of single-channel, fMOST datasets which include projections are available to view.  An example can be found [here]( https://download.brainimagelibrary.org/2b/da/2bdaf9e66a246844/mouseID_405429-182725/).    ![Plugin Demo GIF](https://imgur.com/gkDCsMd.gif \"Plugin Demo GIF\")    ### Features  * Multiscale Rendering   * In datasets that include multiple resolution representations of the data, each resolution can be combined to improve the speed of browsing and user experience.  An example of a dataset with multiple resolution projections can be found [here](https://download.brainimagelibrary.org/2b/da/2bdaf9e66a246844/mouseID_405429-182725/).   * All datasets included in the current release of napari-bil-data-viewer use multi-resolution datasets. * 3D rendering of whole datasets.  The lowest resolution is used for rendering.  Currently, this is a limitation imposed by napari. * The plugin does NOT require a BIL account as datasets are already accessible via https.  ### Known Issues / limitations * Currently the only datasets that are available are those which have been manually selected by the developers.  If you would like a specific dataset to be included please consider adding the dataset(s) to the [dataset_info.py](https://github.com/brain-image-library/napari-bil-data-viewer/blob/main/napari_bil_data_viewer/dataset_info.py) file and submitting a pull request. * To inquire about this plugin please contact Brain Image Library support:  bil-support@psc.edu * The plugin is still under development.  We appreciate all [reports of issues / errors](https://github.com/brain-image-library/napari-bil-data-viewer/issues) which occur during use.   ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/docs/plugins/index.html -->  ## Installation  Option #1: Install plugin via the napari plugin menu  1. Menu: Plugins >> Install/Uninstall Plugins 2. Search: napari-bil-data-viewer 3. Select install  Option #2:  Install a fresh python virtual environment  ```bash # Example of venv creation using conda conda create -y -n bil-viewer python=3.8 conda activate bil-viewer  # Install napari-bil-data-viewer pip install napari-bil-data-viewer  # Run Napari napari ```  ## Contributing  Please consider contributing to this project!  Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-bil-data-viewer\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/brain-image-library/napari-bil-data-viewer/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/  ## Change Log:  ##### <u>v0.1.0:</u>  Initial release.  <u>**v0.1.1 & v0.1.2:**</u>  Changes to documentation",
    "description_content_type": "text/markdown",
    "description_text": "    Description View datasets archived at the Brain Image Library. **NOTE: This plugin is under early development.  Currently, only a subset of single-channel, fMOST datasets which include projections are available to view.  An example can be found here.  Features  Multiscale Rendering In datasets that include multiple resolution representations of the data, each resolution can be combined to improve the speed of browsing and user experience.  An example of a dataset with multiple resolution projections can be found here. All datasets included in the current release of napari-bil-data-viewer use multi-resolution datasets. 3D rendering of whole datasets.  The lowest resolution is used for rendering.  Currently, this is a limitation imposed by napari. The plugin does NOT require a BIL account as datasets are already accessible via https.  Known Issues / limitations  Currently the only datasets that are available are those which have been manually selected by the developers.  If you would like a specific dataset to be included please consider adding the dataset(s) to the dataset_info.py file and submitting a pull request. To inquire about this plugin please contact Brain Image Library support:  bil-support@psc.edu The plugin is still under development.  We appreciate all reports of issues / errors which occur during use.   This napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.  Installation Option #1: Install plugin via the napari plugin menu  Menu: Plugins >> Install/Uninstall Plugins Search: napari-bil-data-viewer Select install  Option #2:  Install a fresh python virtual environment ```bash Example of venv creation using conda conda create -y -n bil-viewer python=3.8 conda activate bil-viewer Install napari-bil-data-viewer pip install napari-bil-data-viewer Run Napari napari ``` Contributing Please consider contributing to this project!  Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-bil-data-viewer\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description. Change Log: v0.1.0: Initial release. v0.1.1 & v0.1.2: Changes to documentation",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari-bil-data-viewer",
    "documentation": "https://github.com/brain-image-library/napari-bil-data-viewer#README.md",
    "first_released": "2022-01-24T23:17:23.459374Z",
    "license": "BSD-3-Clause",
    "name": "napari-bil-data-viewer",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/brain-image-library/napari-bil-data-viewer",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-01-25T16:30:08.480658Z",
    "report_issues": "https://github.com/brain-image-library/napari-bil-data-viewer/issues",
    "requirements": [
      "napari[all]",
      "napari-plugin-engine (>=0.1.4)",
      "scikit-image",
      "fsspec",
      "requests",
      "aiohttp",
      "imagecodecs",
      "beautifulsoup4",
      "dask"
    ],
    "summary": "Napari plugin for viewing Brain Image Library datasets",
    "support": "https://github.com/brain-image-library/napari-bil-data-viewer",
    "twitter": "",
    "version": "0.1.2",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      { "name": "Geneva Schlafly" },
      { "name": "Amitabh Verma" },
      { "name": "Rudolf Oldenbourg" }
    ],
    "code_repository": "https://github.com/PolarizedLightFieldMicroscopy/napari-LF",
    "conda": [],
    "description": "  <!-- This file is designed to provide you with a starting template for documenting the functionality of your plugin. Its content will be rendered on your plugin's napari hub page.  The sections below are given as a guide for the flow of information only, and are in no way prescriptive. You should feel free to merge, remove, add and  rename sections at will to make this document work best for your plugin.   # Description  This should be a detailed description of the context of your plugin and its  intended purpose.  If you have videos or screenshots of your plugin in action, you should include them here as well, to make them front and center for new users.   You should use absolute links to these assets, so that we can easily display them  on the hub. The easiest way to include a video is to use a GIF, for example hosted on imgur. You can then reference this GIF as an image.  ![Example GIF hosted on Imgur](https://i.imgur.com/A5phCX4.gif)  Note that GIFs larger than 5MB won't be rendered by GitHub - we will however, render them on the napari hub.  The other alternative, if you prefer to keep a video, is to use GitHub's video embedding feature.  1. Push your `DESCRIPTION.md` to GitHub on your repository (this can also be done as part of a Pull Request) 2. Edit `.napari/DESCRIPTION.md` **on GitHub**. 3. Drag and drop your video into its desired location. It will be uploaded and hosted on GitHub for you, but will not be placed in your repository. 4. We will take the resolved link to the video and render it on the hub.  Here is an example of an mp4 video embedded this way.  https://user-images.githubusercontent.com/17995243/120088305-6c093380-c132-11eb-822d-620e81eb5f0e.mp4  # Intended Audience & Supported Data  This section should describe the target audience for this plugin (any knowledge, skills and experience required), as well as a description of the types of data supported by this plugin.  Try to make the data description as explicit as possible, so that users know the format your plugin expects. This applies both to reader plugins reading file formats and to function/dock widget plugins accepting layers and/or layer data. For example, if you know your plugin only works with 3D integer data in \"tyx\" order, make sure to mention this.  If you know of researchers, groups or labs using your plugin, or if it has been cited anywhere, feel free to also include this information here.  # Quickstart  This section should go through step-by-step examples of how your plugin should be used. Where your plugin provides multiple dock widgets or functions, you should split these out into separate subsections for easy browsing. Include screenshots and videos wherever possible to elucidate your descriptions.   Ideally, this section should start with minimal examples for those who just want a quick overview of the plugin's functionality, but you should definitely link out to more complex and in-depth tutorials highlighting any intricacies of your plugin, and more detailed documentation if you have it.  # Additional Install Steps (uncommon) We will be providing installation instructions on the hub, which will be sufficient for the majority of plugins. They will include instructions to pip install, and to install via napari itself.  Most plugins can be installed out-of-the-box by just specifying the package requirements over in `setup.cfg`. However, if your plugin has any more complex dependencies, or  requires any additional preparation before (or after) installation, you should add  this information here.  # Getting Help  This section should point users to your preferred support tools, whether this be raising an issue on GitHub, asking a question on image.sc, or using some other method of contact. If you distinguish between usage support and bug/feature support, you should state that here.  # How to Cite  Many plugins may be used in the course of published (or publishable) research, as well as during conference talks and other public facing events. If you'd like to be cited in a particular format, or have a DOI you'd like used, you should provide that information here. -->   Deconvolves a 4D light field image into a full 3D focal stack reconstruction  https://user-images.githubusercontent.com/23206511/180571940-9500dd19-119b-4d0d-8b33-5ab1705e9b6f.mov  napari-LF provides three basic processes to Calibrate, Rectify, and Deconvolve light field images:  The **Calibrate** process generates a calibration file that represents the optical setup that was used to record the light field images. The same calibration file can be used to rectify and deconvolve all light field images that were recorded with the same optical setup, usually the same microscope and light field camera. The Calibrate process requires as input the radiometry frame, dark frame, optical parameters, and volume parameters to generate the calibration file, which is subsequently used to rectify and deconvolve related light field images. The calibration file includes a point spread function (PSF) derived from the optical and volume parameters and is stored in HDF5 file format.  The **Rectify** process uses the calibration file for an affine transformation to scale and rotate experimental light field images that were recorded with a light field camera whose microlens array was (slightly) rotated with respect to the pixel array of the area detector and whose pixel pitch is not commensurate with the microlens pitch. After rectification, the rectified light field has the same integer number of pixels behind each microlens. When the Deconvolve process is called for an experimental light field image, rectifying the light field image is automatically applied before the iterative deconvolution does begin. However, the rectified light field image is not saved and is not available for viewing. Therefore, by pushing the Rectify button in the middle of the napari-LF widget, only the rectification step is invoked and the rectified light field image is saved to the project directory.  The **Deconvolve** process uses the PSF and a wave optics model to iteratively deconvolve a light field image into a stack of optical sections.  The **Parameter** panels, located in the lower half of the napari-LF widget, allows the user to specify settings for the reconstruction process. Once the appropriate parameters are selected, the Calibrate button followed by the Deconvolve button can be pushed to complete the reconstruction.  ## Quickstart 1. Install the napari-LF plugin into your napari environment, as described below under **Installation**. 1. From the napari Plugins menu, select the napari-LF plugin to install its widget into the napari viewer 1. Near the top of the widget, select your project folder containing the following images: light field, radiometry, and dark frame. 1. Write the name of the metadata file you want for recording your reconstruction settings, e.g. metadata.txt. This file will be updated each time a calibration process is started. 1. Calibration     - In the parameters panel, navigate to **Calibrate, Required** (top tab **Calibrate**, bottom tab **Required**), which is the default selection.     - Select **radiometry** and **dark frame** images from pull down menus.     - Write the name of the **calibration file** you would like to produce, e.g. calibration.lfc.     - Enter the appropriate **optical parameters** according to your microscope and sample material.     - Enter the **volume parameters** you would like for your 3D reconstuction.     - Push the `Calibrate` button. 1. Deconvolution     - In the parameters panel, navigate to **Deconvolve, Required**.     - Select **light field** image and **calibration file** from pull down menus.     - Write the name of the **output image stack** you would like to produce, e.g. output_stack.tif.     - Push the `Deconvolve` button. 3D focal stack reconstruction will display in the napari viewer and be saved in your original project folder.  ## Getting Help For details about each parameter, hover over each parameter textbox to read the tooltip description. For additional information about the reconstruction process, see our [User Guide](https://github.com/PolarizedLightFieldMicroscopy/napari-LF/blob/description/docs/napari-LF_UserGuide_5July2022.docx) along with our general documentation on [GitHub](https://github.com/PolarizedLightFieldMicroscopy/napari-LF).  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-LF\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.   [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [file an issue]: https://github.com/PolarizedLightFieldMicroscopy/napari-LF/issues [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ ",
    "description_content_type": "text/markdown",
    "description_text": " Deconvolves a 4D light field image into a full 3D focal stack reconstruction https://user-images.githubusercontent.com/23206511/180571940-9500dd19-119b-4d0d-8b33-5ab1705e9b6f.mov napari-LF provides three basic processes to Calibrate, Rectify, and Deconvolve light field images: The Calibrate process generates a calibration file that represents the optical setup that was used to record the light field images. The same calibration file can be used to rectify and deconvolve all light field images that were recorded with the same optical setup, usually the same microscope and light field camera. The Calibrate process requires as input the radiometry frame, dark frame, optical parameters, and volume parameters to generate the calibration file, which is subsequently used to rectify and deconvolve related light field images. The calibration file includes a point spread function (PSF) derived from the optical and volume parameters and is stored in HDF5 file format. The Rectify process uses the calibration file for an affine transformation to scale and rotate experimental light field images that were recorded with a light field camera whose microlens array was (slightly) rotated with respect to the pixel array of the area detector and whose pixel pitch is not commensurate with the microlens pitch. After rectification, the rectified light field has the same integer number of pixels behind each microlens. When the Deconvolve process is called for an experimental light field image, rectifying the light field image is automatically applied before the iterative deconvolution does begin. However, the rectified light field image is not saved and is not available for viewing. Therefore, by pushing the Rectify button in the middle of the napari-LF widget, only the rectification step is invoked and the rectified light field image is saved to the project directory. The Deconvolve process uses the PSF and a wave optics model to iteratively deconvolve a light field image into a stack of optical sections. The Parameter panels, located in the lower half of the napari-LF widget, allows the user to specify settings for the reconstruction process. Once the appropriate parameters are selected, the Calibrate button followed by the Deconvolve button can be pushed to complete the reconstruction. Quickstart  Install the napari-LF plugin into your napari environment, as described below under Installation. From the napari Plugins menu, select the napari-LF plugin to install its widget into the napari viewer Near the top of the widget, select your project folder containing the following images: light field, radiometry, and dark frame. Write the name of the metadata file you want for recording your reconstruction settings, e.g. metadata.txt. This file will be updated each time a calibration process is started. Calibration In the parameters panel, navigate to Calibrate, Required (top tab Calibrate, bottom tab Required), which is the default selection. Select radiometry and dark frame images from pull down menus. Write the name of the calibration file you would like to produce, e.g. calibration.lfc. Enter the appropriate optical parameters according to your microscope and sample material. Enter the volume parameters you would like for your 3D reconstuction. Push the Calibrate button.   Deconvolution In the parameters panel, navigate to Deconvolve, Required. Select light field image and calibration file from pull down menus. Write the name of the output image stack you would like to produce, e.g. output_stack.tif. Push the Deconvolve button. 3D focal stack reconstruction will display in the napari viewer and be saved in your original project folder.    Getting Help For details about each parameter, hover over each parameter textbox to read the tooltip description. For additional information about the reconstruction process, see our User Guide along with our general documentation on GitHub. Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-LF\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari LF",
    "documentation": "https://github.com/PolarizedLightFieldMicroscopy/napari-LF#README.md",
    "first_released": "2022-07-21T17:24:54.916244Z",
    "license": "BSD-3-Clause",
    "name": "napari-LF",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader", "writer", "widget", "sample_data"],
    "project_site": "https://github.com/PolarizedLightFieldMicroscopy/napari-LF",
    "python_version": ">=3.7",
    "reader_file_extensions": ["*.npy"],
    "release_date": "2022-07-28T20:10:48.107469Z",
    "report_issues": "https://github.com/PolarizedLightFieldMicroscopy/napari-LF/issues",
    "requirements": [
      "numpy",
      "h5py",
      "pyopencl",
      "napari",
      "opencv-contrib-python"
    ],
    "summary": "Light field imaging plugin for napari",
    "support": "https://github.com/PolarizedLightFieldMicroscopy/napari-LF/issues",
    "twitter": "",
    "version": "0.0.4",
    "visibility": "public",
    "writer_file_extensions": [".npy"],
    "writer_save_layers": ["labels*", "image", "image*"]
  },
  {
    "authors": [
      { "email": "v.o.van_der_valk@lumc.nl", "name": "Viktor van der Valk" }
    ],
    "code_repository": "https://github.com/ViktorvdValk/napari-checkerboard",
    "description": "# napari-checkerboard  [![License](https://img.shields.io/pypi/l/napari-checkerboard.svg?color=green)](https://github.com/ViktorvdValk/napari-checkerboard/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-checkerboard.svg?color=green)](https://pypi.org/project/napari-checkerboard) [![Python Version](https://img.shields.io/pypi/pyversions/napari-checkerboard.svg?color=green)](https://python.org) [![tests](https://github.com/ViktorvdValk/napari-checkerboard/workflows/tests/badge.svg)](https://github.com/ViktorvdValk/napari-checkerboard/actions) [![codecov](https://codecov.io/gh/ViktorvdValk/napari-checkerboard/branch/master/graph/badge.svg)](https://codecov.io/gh/ViktorvdValk/napari-checkerboard)  Compare two images with the itk checkerboard filter   <img width=\"1430\" alt=\"Screenshot 2021-05-12 at 15 03 17\" src=\"https://user-images.githubusercontent.com/33719474/117979519-48bd4680-b333-11eb-874c-d9ec09681d93.png\">   ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/docs/plugins/index.html -->  ## Installation  You can install `napari-checkerboard` via [pip]:      pip install napari-checkerboard  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [Apache Software License 2.0] license, \"napari-checkerboard\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin [file an issue]: https://github.com/ViktorvdValk/napari-checkerboard/issues [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-checkerboard      Compare two images with the itk checkerboard filter   This napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.  Installation You can install napari-checkerboard via pip: pip install napari-checkerboard  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the Apache Software License 2.0 license, \"napari-checkerboard\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "napari-checkerboard",
    "documentation": "",
    "first_released": "2021-05-11T14:38:31.343521Z",
    "license": "Apache-2.0",
    "name": "napari-checkerboard",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/ViktorvdValk/napari-checkerboard",
    "python_version": ">=3.6",
    "reader_file_extensions": [],
    "release_date": "2021-05-31T15:45:25.395347Z",
    "report_issues": "",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy (>=1.19.0)",
      "napari (>=0.4.6)",
      "magicgui (>=0.2.6)",
      "itk-elastix (>=0.11.1)",
      "itk-napari-conversion (>=0.3.1)",
      "napari-itk-io (>=0.1.0)"
    ],
    "summary": "Compare two images with the itk checkerboard filter",
    "support": "",
    "twitter": "",
    "version": "0.0.3",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Robert Haase" }],
    "code_repository": "https://github.com/haesleinhuepf/devbio-napari",
    "description": "# devbio-napari\\r \\r [![License](https://img.shields.io/pypi/l/devbio-napari.svg?color=green)](https://github.com/haesleinhuepf/devbio-napari/raw/master/LICENSE)\\r [![PyPI](https://img.shields.io/pypi/v/devbio-napari.svg?color=green)](https://pypi.org/project/devbio-napari)\\r [![Python Version](https://img.shields.io/pypi/pyversions/devbio-napari.svg?color=green)](https://python.org)\\r [![tests](https://github.com/haesleinhuepf/devbio-napari/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-plot-profile/actions)\\r [![codecov](https://codecov.io/gh/haesleinhuepf/devbio-napari/branch/master/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/devbio-napari)\\r [![Development Status](https://img.shields.io/pypi/status/devbio-napari.svg)](https://en.wikipedia.org/wiki/Software_release_life_cycle#Alpha)\\r [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/devbio-napari)](https://napari-hub.org/plugins/devbio-napari)\\r \\r \\r A bundle of napari plugins useful for 3D+t image processing and analysis for studying developmental biology.\\r \\r * [accelerated-pixel-and-object-classification](https://www.napari-hub.org/plugins/napari-accelerated-pixel-and-object-classification)\\r   * Instance segmentation\\r   * Semantic segmentation\\r   * Object classification\\r   * Random Forest Classifier training\\r * [animation](https://www.napari-hub.org/plugins/napari-animation) \\r   * Visualization\\r * [blob-detection](https://www.napari-hub.org/plugins/napari-blob-detection)\\r   * Detection\\r * [brightness-contrast](https://www.napari-hub.org/plugins/napari-brightness-contrast)\\r   * Visualization\\r * [clusters-plotter](https://www.napari-hub.org/plugins/napari-clusters-plotter)\\r   * Visualization\\r   * Plotting\\r   * Semantic object segmentation\\r   * Dimensionality reduction\\r   * Unsupervised machine learning\\r * [crop](https://www.napari-hub.org/plugins/napari-crop)\\r   * Transformation\\r * [curtain](https://www.napari-hub.org/plugins/napari-curtain)\\r   * Visualization \\r * [czifile2](https://www.napari-hub.org/plugins/napari-czifile2)\\r   * File input/output\\r * [folder-browser](https://www.napari-hub.org/plugins/napari-folder-browser)\\r   * File input/output\\r * [layer-details-display](https://www.napari-hub.org/plugins/napari-layer-details-display)\\r   * Visualization\\r * [mouse-controls](https://www.napari-hub.org/plugins/napari-mouse-controls)\\r   * Interaction\\r * [PlatyMatch](https://www.napari-hub.org/plugins/PlatyMatch)\\r   * Image registration\\r * [plot-profile](https://www.napari-hub.org/plugins/napari-plot-profile)\\r   * Visualization\\r   * Quantification\\r * [plugin-search](https://www.napari-hub.org/plugins/napari-plugin-search)\\r   * Interaction\\r * [pyclesperanto-assistant](https://www.napari-hub.org/plugins/napari-pyclesperanto-assistant)\\r   * Filtering\\r   * Instance segmentation\\r   * Semantic segmentation\\r   * Quantification\\r * [pystackreg](https://www.napari-hub.org/plugins/napari-pystackreg)\\r   * Image registration\\r   * Motion correction\\r * [RedLionfish](https://www.napari-hub.org/plugins/RedLionfish)\\r   * Deconvolution\\r   * Processing\\r * [roi](https://www.napari-hub.org/plugins/napari-roi)\\r   * Manual segmentation\\r * [segment-blobs-and-things-with-membranes](https://www.napari-hub.org/plugins/napari-segment-blobs-and-things-with-membranes)\\r   * Filtering\\r   * Instance segmentation\\r   * Semantic segmentation\\r * [simpleitk-image-processing](https://www.napari-hub.org/plugins/napari-simpleitk-image-processing)\\r   * Filtering\\r   * Instance segmentation\\r   * Semantic segmentation\\r   * Quantification\\r * [skimage-regionprops](https://www.napari-hub.org/plugins/napari-skimage-regionprops)\\r   * Quantification\\r * [tabu](https://www.napari-hub.org/plugins/napari-tabu)\\r   * Interaction\\r * [the-segmentation-game](https://www.napari-hub.org/plugins/the-segmentation-game)\\r   * Quantification\\r   * Segmentation quality assurance\\r * [workflow-inspector](https://www.napari-hub.org/plugins/napari-workflow-inspector)\\r   * Visualization\\r * [workflow-optimizer](https://www.napari-hub.org/plugins/napari-workflow-optimizer)\\r   * Interaction\\r   * Optimization\\r \\r ----------------------------------\\r \\r This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\\r \\r ## Installation\\r \\r You can install `devbio-napari` via conda/mamba. If you have never used conda before, please [read this guide first](https://biapol.github.io/blog/johannes_mueller/anaconda_getting_started/).  \\r Start by installing mamba in your base environment:\\r \\r ```\\r conda install mamba -c conda-forge\\r ```\\r \\r Afterwards, create an environment using mamba.\\r \\r ```\\r mamba create --name devbio-napari-env python=3.9 devbio-napari -c conda-forge\\r ```\\r \\r Afterwards, activate the environment like this:\\r     \\r     conda activate devbio-napari-env\\r \\r Afterwards, run this command from the command line\\r \\r ```\\r naparia\\r ```\\r \\r This window should open. It shows the [Assistant](https://www.napari-hub.org/plugins/napari-assistant) graphical user interface. \\r Read more about how to use it in its [documentation](https://www.napari-hub.org/plugins/napari-assistant).\\r \\r ![img.png](https://github.com/haesleinhuepf/devbio-napari/raw/master/docs/screenshot.png)\\r \\r ## Troubleshooting: Graphics cards drivers\\r \\r In case error messages contains \"ImportError: DLL load failed while importing cl: The specified procedure could not be found\" [see also](https://github.com/clEsperanto/pyclesperanto_prototype/issues/55) or \"\"clGetPlatformIDs failed: PLATFORM_NOT_FOUND_KHR\", please install recent drivers for your graphics card and/or OpenCL device. Select the right driver source depending on your hardware from this list:\\r \\r * [AMD drivers](https://www.amd.com/en/support)\\r * [NVidia drivers](https://www.nvidia.com/download/index.aspx)\\r * [Intel CPU OpenCL drivers](https://www.intel.com/content/www/us/en/developer/articles/tool/opencl-drivers.html#latest_CPU_runtime)\\r * [Microsoft Windows OpenCL support](https://www.microsoft.com/en-us/p/opencl-and-opengl-compatibility-pack/9nqpsl29bfff)\\r \\r Sometimes, mac-users need to install this:\\r \\r     conda install -c conda-forge ocl_icd_wrapper_apple\\r \\r Sometimes, linux users need to install this:\\r \\r     conda install -c conda-forge ocl-icd-system\\r \\r \\r In case installation didn't work in the first attempt, you may have to call this command line to reset the napari configuration:\\r \\r ```\\r napari --reset\\r ```\\r \\r ## Contributing\\r \\r Contributions are very welcome. \\r If you want to [suggest a new napari plugin](https://github.com/haesleinhuepf/devbio-napari/pulls) to become part of this distribution, please make sure it interoperates nicely with the other plugins. \\r For example, if the plugin you suggest provided cell segmentation algorithms, please check if the resulting segmented cells can be analysed using napari-skimage-regionprops.\\r Furthermore, please make sure the README of the plugin you are proposing comes with user documentation, e.g. a step-by-step guide with screenshots explaining what users can do with the plugin and how to use it. \\r It is recommended to provide example data as well so that end-users can try out the plugin under conditions it was developed for.\\r \\r ## License\\r \\r Distributed under the terms of the [BSD-3] license,\\r \"devbio-napari\" is free and open source software\\r \\r ## Issues\\r \\r If you encounter any problems, please [file an issue] along with a detailed description.\\r \\r [napari]: https://github.com/napari/napari\\r [Cookiecutter]: https://github.com/audreyr/cookiecutter\\r [@napari]: https://github.com/napari\\r [MIT]: http://opensource.org/licenses/MIT\\r [BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\r [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\r [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\r [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\r [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\r [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\r [file an issue]: https://github.com/haesleinhuepf/devbio/issues\\r [napari]: https://github.com/napari/napari\\r [tox]: https://tox.readthedocs.io/en/latest/\\r [pip]: https://pypi.org/project/pip/\\r [PyPI]: https://pypi.org/\\r ",
    "description_content_type": "text/markdown",
    "description_text": "devbio-napari        A bundle of napari plugins useful for 3D+t image processing and analysis for studying developmental biology.  accelerated-pixel-and-object-classification Instance segmentation Semantic segmentation Object classification Random Forest Classifier training animation  Visualization blob-detection Detection brightness-contrast Visualization clusters-plotter Visualization Plotting Semantic object segmentation Dimensionality reduction Unsupervised machine learning crop Transformation curtain Visualization  czifile2 File input/output folder-browser File input/output layer-details-display Visualization mouse-controls Interaction PlatyMatch Image registration plot-profile Visualization Quantification plugin-search Interaction pyclesperanto-assistant Filtering Instance segmentation Semantic segmentation Quantification pystackreg Image registration Motion correction RedLionfish Deconvolution Processing roi Manual segmentation segment-blobs-and-things-with-membranes Filtering Instance segmentation Semantic segmentation simpleitk-image-processing Filtering Instance segmentation Semantic segmentation Quantification skimage-regionprops Quantification tabu Interaction the-segmentation-game Quantification Segmentation quality assurance workflow-inspector Visualization workflow-optimizer Interaction Optimization   This napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template. Installation You can install devbio-napari via conda/mamba. If you have never used conda before, please read this guide first. Start by installing mamba in your base environment: conda install mamba -c conda-forge Afterwards, create an environment using mamba. mamba create --name devbio-napari-env python=3.9 devbio-napari -c conda-forge Afterwards, activate the environment like this: conda activate devbio-napari-env  Afterwards, run this command from the command line naparia This window should open. It shows the Assistant graphical user interface.  Read more about how to use it in its documentation.  Troubleshooting: Graphics cards drivers In case error messages contains \"ImportError: DLL load failed while importing cl: The specified procedure could not be found\" see also or \"\"clGetPlatformIDs failed: PLATFORM_NOT_FOUND_KHR\", please install recent drivers for your graphics card and/or OpenCL device. Select the right driver source depending on your hardware from this list:  AMD drivers NVidia drivers Intel CPU OpenCL drivers Microsoft Windows OpenCL support  Sometimes, mac-users need to install this: conda install -c conda-forge ocl_icd_wrapper_apple  Sometimes, linux users need to install this: conda install -c conda-forge ocl-icd-system  In case installation didn't work in the first attempt, you may have to call this command line to reset the napari configuration: napari --reset Contributing Contributions are very welcome.  If you want to suggest a new napari plugin to become part of this distribution, please make sure it interoperates nicely with the other plugins.  For example, if the plugin you suggest provided cell segmentation algorithms, please check if the resulting segmented cells can be analysed using napari-skimage-regionprops. Furthermore, please make sure the README of the plugin you are proposing comes with user documentation, e.g. a step-by-step guide with screenshots explaining what users can do with the plugin and how to use it.  It is recommended to provide example data as well so that end-users can try out the plugin under conditions it was developed for. License Distributed under the terms of the BSD-3 license, \"devbio-napari\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 3 - Alpha"],
    "display_name": "devbio-napari",
    "documentation": "https://github.com/haesleinhuepf/devbio-napari#README.md",
    "first_released": "2021-06-08T07:39:43.455373Z",
    "license": "BSD-3-Clause",
    "name": "devbio-napari",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": [],
    "project_site": "https://github.com/haesleinhuepf/devbio-napari",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-12-11T11:01:07.549032Z",
    "report_issues": "https://github.com/haesleinhuepf/devbio-napari/issues",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy (>=1.21.4)",
      "napari-pyclesperanto-assistant",
      "napari-skimage-regionprops",
      "napari-animation",
      "PlatyMatch",
      "napari-plot-profile",
      "napari-accelerated-pixel-and-object-classification",
      "napari-brightness-contrast",
      "napari-plugin-search",
      "napari-segment-blobs-and-things-with-membranes",
      "napari-simpleitk-image-processing",
      "napari-folder-browser",
      "napari-crop",
      "napari-clusters-plotter",
      "napari-tabu",
      "napari-workflow-optimizer",
      "napari-workflow-inspector",
      "napari-curtain",
      "napari-layer-details-display",
      "napari",
      "vispy",
      "napari-mouse-controls",
      "the-segmentation-game",
      "napari-blob-detection",
      "jupyterlab",
      "napari-czifile2",
      "napari-roi",
      "pydantic (!=1.10.0)",
      "napari-pystackreg",
      "imageio (!=2.22.1)",
      "redlionfish",
      "jupyter-server (<2.0.0)"
    ],
    "summary": "A bundle of napari plugins useful for 3D+t image processing and analysis for studying developmental biology.",
    "support": "https://github.com/haesleinhuepf/devbio-napari/issues",
    "twitter": "",
    "version": "0.8.1",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Robert Haase" }],
    "code_repository": "https://github.com/haesleinhuepf/napari-brightness-contrast",
    "conda": [
      { "channel": "conda-forge", "package": "napari-brightness-contrast" }
    ],
    "description": "# napari-brightness-contrast  [![License](https://img.shields.io/pypi/l/napari-brightness-contrast.svg?color=green)](https://github.com/haesleinhuepf/napari-brightness-contrast/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-brightness-contrast.svg?color=green)](https://pypi.org/project/napari-brightness-contrast) [![Python Version](https://img.shields.io/pypi/pyversions/napari-brightness-contrast.svg?color=green)](https://python.org) [![tests](https://github.com/haesleinhuepf/napari-brightness-contrast/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-brightness-contrast/actions) [![codecov](https://codecov.io/gh/haesleinhuepf/napari-brightness-contrast/branch/master/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-brightness-contrast) [![Development Status](https://img.shields.io/pypi/status/napari-brightness-contrast.svg)](https://en.wikipedia.org/wiki/Software_release_life_cycle#Alpha) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-brightness-contrast)](https://napari-hub.org/plugins/napari-brightness-contrast)  Advanced layer histogram visualization options, e.g. for brightness / contrast ![](https://github.com/haesleinhuepf/napari-brightness-contrast/blob/main/docs/images/napari-brightness-contrast3.gif?raw=true)  Note: This will not work for big image data at the moment.  If the user interface feels slow, consider installing [pyclesperanto](https://github.com/clEsperanto/pyclesperanto_prototype) to speed it up.  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/docs/plugins/index.html -->  ## Installation  You can install `napari-brightness-contrast` via [pip]:      pip install napari-brightness-contrast  ## Contributing  Contributions are very welcome.   After cloning the repo, install using `pip install -e .[tests]` to enable testing via `pytest`.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-brightness-contrast\" is free and open source software  ## Issues  If you encounter any problems, please [open a thread on image.sc](https://image.sc) along with a detailed description and tag [@haesleinhuepf](https://github.com/haesleinhuepf).  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin [file an issue]: https://github.com/haesleinhuepf/napari-brightness-contrast/issues [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-brightness-contrast        Advanced layer histogram visualization options, e.g. for brightness / contrast  Note: This will not work for big image data at the moment.  If the user interface feels slow, consider installing pyclesperanto to speed it up.  This napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.  Installation You can install napari-brightness-contrast via pip: pip install napari-brightness-contrast  Contributing Contributions are very welcome. After cloning the repo, install using pip install -e .[tests] to enable testing via pytest. License Distributed under the terms of the BSD-3 license, \"napari-brightness-contrast\" is free and open source software Issues If you encounter any problems, please open a thread on image.sc along with a detailed description and tag @haesleinhuepf.",
    "development_status": ["Development Status :: 3 - Alpha"],
    "display_name": "napari-brightness-contrast",
    "documentation": "https://github.com/haesleinhuepf/napari-brightness-contrast#README.md",
    "first_released": "2021-08-07T07:55:39.881737Z",
    "license": "BSD-3-Clause",
    "name": "napari-brightness-contrast",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/haesleinhuepf/napari-brightness-contrast",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-09-25T14:18:56.879055Z",
    "report_issues": "https://github.com/haesleinhuepf/napari-brightness-contrast/issues",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "napari",
      "numpy",
      "pyqtgraph",
      "superqt",
      "napari-tools-menu",
      "pytest ; extra == 'tests'",
      "pytest-qt ; extra == 'tests'"
    ],
    "summary": "Advanced layer visualization options",
    "support": "https://github.com/haesleinhuepf/napari-brightness-contrast/issues",
    "twitter": "",
    "version": "0.1.8",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Herearii Metuarea" }],
    "code_repository": "https://github.com/hereariim/napari-apple",
    "description": "# napari-apple\\r \\r [![License BSD-3](https://img.shields.io/pypi/l/napari-apple.svg?color=green)](https://github.com/hereariim/napari-apple/raw/main/LICENSE)\\r [![PyPI](https://img.shields.io/pypi/v/napari-apple.svg?color=green)](https://pypi.org/project/napari-apple)\\r [![Python Version](https://img.shields.io/pypi/pyversions/napari-apple.svg?color=green)](https://python.org)\\r [![tests](https://github.com/hereariim/napari-apple/workflows/tests/badge.svg)](https://github.com/hereariim/napari-apple/actions)\\r [![codecov](https://codecov.io/gh/hereariim/napari-apple/branch/main/graph/badge.svg)](https://codecov.io/gh/hereariim/napari-apple)\\r [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-apple)](https://napari-hub.org/plugins/napari-apple)\\r \\r Detection of apple based on YOLOv4 model\\r \\r ----------------------------------\\r \\r This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\r \\r <!--\\r Don't miss the full getting started guide to set up your new package:\\r https://github.com/napari/cookiecutter-napari-plugin#getting-started\\r \\r and review the napari docs for plugin developers:\\r https://napari.org/plugins/index.html\\r -->\\r \\r ## Installation\\r \\r First, please note that this module **only works** on a Linux Ubuntu system. Indeed, the launch of the YOLO module is a command that is executed on a Linux Ubuntu system.\\r \\r Before you can operate the module, you must install the `napari-apple` module and Darknet on your machine.\\r \\r ### Instruction for napari-module\\r \\r You can install `napari-apple` via [pip]:\\r \\r     pip install napari-apple\\r \\r To install latest development version :\\r \\r     pip install git+https://github.com/hereariim/napari-apple.git\\r \\r ### Instruction Darknet\\r \\r Darknet is the module where the pre-trained YOLO model is located. You can install Darknet by running this command:\\r \\r     git clone https://github.com/pjreddie/darknet\\r     cd darknet\\r     make\\r     \\r When Darknet is installed, you have to put the weights of the apple detection model in the cfg subfolder. You find the weights in the weight-darknet folder. \\r \\r ## Contributing\\r \\r Contributions are very welcome. Tests can be run with [tox], please ensure\\r the coverage at least stays the same before you submit a pull request.\\r \\r ## License\\r \\r Distributed under the terms of the [BSD-3] license,\\r \"napari-apple\" is free and open source software\\r \\r ## Issues\\r \\r If you encounter any problems, please [file an issue] along with a detailed description.\\r \\r [napari]: https://github.com/napari/napari\\r [Cookiecutter]: https://github.com/audreyr/cookiecutter\\r [@napari]: https://github.com/napari\\r [MIT]: http://opensource.org/licenses/MIT\\r [BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\r [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\r [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\r [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\r [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\r [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\r \\r [file an issue]: https://github.com/hereariim/napari-apple/issues\\r \\r [napari]: https://github.com/napari/napari\\r [tox]: https://tox.readthedocs.io/en/latest/\\r [pip]: https://pypi.org/project/pip/\\r [PyPI]: https://pypi.org/\\r ",
    "description_content_type": "text/markdown",
    "description_text": "napari-apple       Detection of apple based on YOLOv4 model  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation First, please note that this module only works on a Linux Ubuntu system. Indeed, the launch of the YOLO module is a command that is executed on a Linux Ubuntu system. Before you can operate the module, you must install the napari-apple module and Darknet on your machine. Instruction for napari-module You can install napari-apple via pip: pip install napari-apple  To install latest development version : pip install git+https://github.com/hereariim/napari-apple.git  Instruction Darknet Darknet is the module where the pre-trained YOLO model is located. You can install Darknet by running this command: git clone https://github.com/pjreddie/darknet cd darknet make  When Darknet is installed, you have to put the weights of the apple detection model in the cfg subfolder. You find the weights in the weight-darknet folder.  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-apple\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "Apple",
    "documentation": "https://github.com/hereariim/napari-apple#README.md",
    "first_released": "2022-06-23T21:15:48.770489Z",
    "license": "BSD-3-Clause",
    "name": "napari-apple",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader", "writer", "widget", "sample_data"],
    "project_site": "https://github.com/hereariim/napari-apple",
    "python_version": ">=3.8",
    "reader_file_extensions": ["*.npy"],
    "release_date": "2022-12-08T16:36:22.328263Z",
    "report_issues": "https://github.com/hereariim/napari-apple/issues",
    "requirements": [
      "numpy",
      "magicgui",
      "qtpy",
      "opencv-python-headless",
      "scikit-image",
      "napari",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "summary": "Detection of apple based on YOLOv4 model",
    "support": "https://github.com/hereariim/napari-apple/issues",
    "twitter": "",
    "version": "0.0.2",
    "visibility": "public",
    "writer_file_extensions": [".npy"],
    "writer_save_layers": ["labels*", "image*", "image"]
  },
  {
    "authors": [
      { "email": "talley.lambert@gmail.com", "name": "Talley Lambert" }
    ],
    "code_repository": "https://github.com/tlambert03/napari-bioformats",
    "description": "# napari-bioformats  [![License](https://img.shields.io/pypi/l/napari-bioformats.svg?color=green)](https://github.com/napari/napari-bioformats/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-bioformats.svg?color=green)](https://pypi.org/project/napari-bioformats) [![Conda](https://img.shields.io/conda/v/conda-forge/napari-bioformats)](https://anaconda.org/conda-forge/napari-bioformats) [![Python Version](https://img.shields.io/pypi/pyversions/napari-bioformats.svg?color=green)](https://python.org) [![tests](https://github.com/tlambert03/napari-bioformats/workflows/tests/badge.svg)](https://github.com/tlambert03/napari-bioformats/actions) [![codecov](https://codecov.io/gh/tlambert03/napari-bioformats/branch/master/graph/badge.svg)](https://codecov.io/gh/tlambert03/napari-bioformats)  Bioformats plugin for napari using [pims-bioformats](http://soft-matter.github.io/pims/v0.5/bioformats.html)  ----------------------------------  ## Use this plugin as a fallback!  Anyone coming to napari from the Fiji/ImageJ world will likely be aware of the _incredible_ [Bio-Formats](https://docs.openmicroscopy.org/bio-formats/6.6.1/index.html) library.  A heroic effort, built over years, to read [more than a 100 file formats](https://docs.openmicroscopy.org/bio-formats/6.6.1/supported-formats.html).  Naturally, we want some of that goodness for `napari` ... hence this plugin.  **However:** it's important to note that this plugin _still_ requires having a java runtime engine installed.  This is easy enough to do (the plugin will ask to install it for you if you're in a `conda` environment), but it definitely makes for a more complicated environment setup, it's not very \"pythonic\", and the performance will likely not feel as snappy as a native \"pure\" python module.  So, before you reflexively install this plugin to fill that bio-formats sized hole in your python heart, consider trying some of the other pure-python plugins designed to read your format of interest:  - **Zeiss (.czi)**: [napari-aicsimageio](https://github.com/AllenCellModeling/napari-aicsimageio), [napari-czifile2](https://github.com/BodenmillerGroup/napari-czifile2) - **Nikon (.nd2)**: [napari-nikon-nd2](https://github.com/cwood1967/napari-nikon-nd2), [nd2-dask](https://github.com/DragaDoncila/nd2-dask) - **Leica (.lif)**: [napari-aicsimageio](https://github.com/AllenCellModeling/napari-aicsimageio) - **Olympus (.oif)**: no plugin?  (but see [oiffile](https://pypi.org/project/oiffile/) ) - **DeltaVision (.dv, .mrc)**: [napari-dv](https://github.com/tlambert03/napari-dv)  > *if you have a pure-python reader for a bio-formats-supported file format that you'd like to see added to this list, please open an issue*  ## Installation  The easiest way to install `napari-bioformats` is via [conda], from the [conda-forge] channel:      conda install -c conda-forge napari-bioformats  It is also possible to install via [pip], but you will need to have a working JVM installed, and may need to set the `JAVA_HOME` environment variable      pip install napari-bioformats  ### First Usage  The first time you attempt to open a file with napari-bioformats, you will likely notice a long delay as pims downloads the `loci_tools.jar` (speed will depend on your internet connection). Subsequent files should open more quickly.  ## License  Distributed under the terms of the [GPLv3] license, \"napari-bioformats\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  _This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template._  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [GPLv3]: https://opensource.org/licenses/GPL-3.0 [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin [file an issue]: https://github.com/tlambert03/napari-bioformats/issues [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [conda]: https://docs.conda.io/en/latest/ [conda-forge]: https://conda-forge.org [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-bioformats       Bioformats plugin for napari using pims-bioformats  Use this plugin as a fallback! Anyone coming to napari from the Fiji/ImageJ world will likely be aware of the incredible Bio-Formats library.  A heroic effort, built over years, to read more than a 100 file formats.  Naturally, we want some of that goodness for napari ... hence this plugin. However: it's important to note that this plugin still requires having a java runtime engine installed.  This is easy enough to do (the plugin will ask to install it for you if you're in a conda environment), but it definitely makes for a more complicated environment setup, it's not very \"pythonic\", and the performance will likely not feel as snappy as a native \"pure\" python module. So, before you reflexively install this plugin to fill that bio-formats sized hole in your python heart, consider trying some of the other pure-python plugins designed to read your format of interest:  Zeiss (.czi): napari-aicsimageio, napari-czifile2 Nikon (.nd2): napari-nikon-nd2, nd2-dask Leica (.lif): napari-aicsimageio Olympus (.oif): no plugin?  (but see oiffile ) DeltaVision (.dv, .mrc): napari-dv   if you have a pure-python reader for a bio-formats-supported file format that you'd like to see added to this list, please open an issue  Installation The easiest way to install napari-bioformats is via conda, from the conda-forge channel: conda install -c conda-forge napari-bioformats  It is also possible to install via pip, but you will need to have a working JVM installed, and may need to set the JAVA_HOME environment variable pip install napari-bioformats  First Usage The first time you attempt to open a file with napari-bioformats, you will likely notice a long delay as pims downloads the loci_tools.jar (speed will depend on your internet connection). Subsequent files should open more quickly. License Distributed under the terms of the GPLv3 license, \"napari-bioformats\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description. This napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "napari-bioformats",
    "documentation": "https://github.com/tlambert03/napari-bioformats#README.md",
    "first_released": "2021-06-25T21:06:09.152167Z",
    "license": "GPL-3.0",
    "name": "napari-bioformats",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader"],
    "project_site": "https://github.com/tlambert03/napari-bioformats",
    "python_version": ">=3.7",
    "reader_file_extensions": ["*"],
    "release_date": "2021-08-11T03:46:24.676720Z",
    "report_issues": "https://github.com/tlambert03/napari-bioformats/issues",
    "requirements": [
      "jpype1",
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "ome-types",
      "pims",
      "requests",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'"
    ],
    "summary": "Bioformats for napari, using pims",
    "support": "https://github.com/tlambert03/napari-bioformats/issues",
    "twitter": "",
    "version": "0.2.1",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "email": "dkrentzel@pm.me", "name": "Daniel Krentzel" }],
    "category": {
      "Image modality": ["Electron microscopy", "Multimodal imaging"],
      "Workflow step": ["Image registration"]
    },
    "category_hierarchy": {
      "Image modality": [
        ["Electron microscopy", "Correlative light and electron microscopy"],
        ["Multimodal imaging", "Correlative light and electron microscopy"]
      ],
      "Workflow step": [["Image registration"]]
    },
    "code_repository": "https://github.com/krentzd/napari-clemreg",
    "description": "# napari-clemreg  <!-- [![License](https://img.shields.io/pypi/l/napari-clemreg.svg?color=green)](https://github.com/krentzd/napari-clemreg/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-clemreg.svg?color=green)](https://pypi.org/project/napari-clemreg) [![Python Version](https://img.shields.io/pypi/pyversions/napari-clemreg.svg?color=green)](https://python.org) [![tests](https://github.com/krentzd/napari-clemreg/workflows/tests/badge.svg)](https://github.com/krentzd/napari-clemreg/actions) [![codecov](https://codecov.io/gh/krentzd/napari-clemreg/branch/master/graph/badge.svg)](https://codecov.io/gh/krentzd/napari-clemreg) -->  An automated point-set based registration algorithm for correlative light and electron microscopy (CLEM)  ---------------------------------- ## Installation  To install `napari-clemreg` it is recommended to create a fresh [conda] enviornment with Python 3.8:  ``` conda create -n clemreg_env python=3.8 ``` Next, install `napari` with the following command via [pip]:   ``` pip install \"napari[all]\" ```  Finally, `napari-clemreg` can be installed with: ``` pip install napari-clemreg ```   When installing `napari-clemreg` on a Windows machine, the following error might appear: ``` error Microsoft Visual C++ 14.0 is required ``` Ensure that [Visual Studios C++ 14.00](https://visualstudio.microsoft.com/thank-you-downloading-visual-studio/?sku=BuildTools&rel=16) is installed  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [MIT] license, \"napari-clemreg\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin [file an issue]: https://github.com/krentzd/napari-clemreg/issues [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ [conda]: https://docs.conda.io/en/latest/  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-clemreg  An automated point-set based registration algorithm for correlative light and electron microscopy (CLEM) Installation To install napari-clemreg it is recommended to create a fresh conda enviornment with Python 3.8: conda create -n clemreg_env python=3.8 Next, install napari with the following command via pip:  pip install \"napari[all]\" Finally, napari-clemreg can be installed with: pip install napari-clemreg When installing napari-clemreg on a Windows machine, the following error might appear: error Microsoft Visual C++ 14.0 is required Ensure that Visual Studios C++ 14.00 is installed Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the MIT license, \"napari-clemreg\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description. This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari-clemreg",
    "documentation": "https://github.com/krentzd/napari-clemreg#README.md",
    "first_released": "2021-06-01T13:21:09.681333Z",
    "license": "MIT",
    "name": "napari-clemreg",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader", "widget"],
    "project_site": "https://github.com/krentzd/napari-clemreg",
    "python_version": ">=3.7",
    "reader_file_extensions": ["*"],
    "release_date": "2022-02-06T23:32:42.086726Z",
    "report_issues": "https://github.com/krentzd/napari-clemreg/issues",
    "requirements": null,
    "summary": "A plugin for registering multimodal image volumes based on common segmented structures of interest with point-clouds.",
    "support": "https://github.com/krentzd/napari-clemreg/issues",
    "twitter": "",
    "version": "0.0.1a4",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "email": "code@arlowe.co.uk", "name": "Alan R. Lowe" }],
    "category": {
      "Supported data": ["Time series"],
      "Workflow step": ["Object tracking"]
    },
    "category_hierarchy": {
      "Supported data": [["Time series"]],
      "Workflow step": [["Object tracking"]]
    },
    "code_repository": "https://github.com/quantumjot/napari-btrack-reader",
    "description": "# napari-btrack-reader  [![License](https://img.shields.io/pypi/l/napari-btrack-reader.svg?color=green)](https://github.com/napari/napari-btrack-reader/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-btrack-reader.svg?color=green)](https://pypi.org/project/napari-btrack-reader) [![Python Version](https://img.shields.io/pypi/pyversions/napari-btrack-reader.svg?color=green)](https://python.org) [![tests](https://github.com/quantumjot/napari-btrack-reader/workflows/tests/badge.svg)](https://github.com/quantumjot/napari-btrack-reader/actions) [![codecov](https://codecov.io/gh/quantumjot/napari-btrack-reader/branch/master/graph/badge.svg)](https://codecov.io/gh/quantumjot/napari-btrack-reader)  A plugin to load btrack files  ----------------------------------  This plugin reads tracking data generated by BayesianTracker (btrack).  This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/docs/plugins/index.html -->  ## Installation  You can install `napari-btrack-reader` via [pip]:      pip install napari-btrack-reader  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-btrack-reader\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin [file an issue]: https://github.com/quantumjot/napari-btrack-reader/issues [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-btrack-reader      A plugin to load btrack files  This plugin reads tracking data generated by BayesianTracker (btrack). This napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.  Installation You can install napari-btrack-reader via pip: pip install napari-btrack-reader  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-btrack-reader\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "napari-btrack-reader",
    "documentation": "",
    "first_released": "2020-11-05T10:23:48.064223Z",
    "license": "BSD-3-Clause",
    "name": "napari-btrack-reader",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader"],
    "project_site": "https://github.com/quantumjot/napari-btrack-reader",
    "python_version": ">=3.6",
    "reader_file_extensions": ["*"],
    "release_date": "2020-11-05T10:23:48.064223Z",
    "report_issues": "",
    "requirements": ["napari-plugin-engine (>=0.1.4)", "btrack (>=0.4.0)"],
    "summary": "A plugin to load btrack files",
    "support": "",
    "twitter": "",
    "version": "0.1.0",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "email": "ddoncila@gmail.com", "name": "Draga Doncila" }],
    "code_repository": "https://github.com/DragaDoncila/napari-compressed-labels-io",
    "description": "# napari-compressed-labels-io  [![License](https://img.shields.io/pypi/l/napari-compressed-labels-io.svg?color=green)](https://github.com/DragaDoncila/napari-compressed-labels-io/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-compressed-labels-io.svg?color=green)](https://pypi.org/project/napari-compressed-labels-io) [![Python Version](https://img.shields.io/pypi/pyversions/napari-compressed-labels-io.svg?color=green)](https://python.org) [![tests](https://github.com/DragaDoncila/napari-compressed-labels-io/workflows/tests/badge.svg)](https://github.com/DragaDoncila/napari-compressed-labels-io/actions) [![codecov](https://codecov.io/gh/DragaDoncila/napari-compressed-labels-io/branch/master/graph/badge.svg)](https://codecov.io/gh/DragaDoncila/napari-compressed-labels-io)   ## Description  This napari plugin provides readers and writers for labels and their corresponding image layers into zarr format for compression and portability. Each reader/writer pair supports a round trip of saving and loading image and labels layers.  ## Writers Two writers are provided by this plugin, each with its own reader.  ### `labels_to_zarr` This writer is an alternative to napari's default label writer and will write an entire labels layer, regardless of its dimensions, into a single zarr file. This writer provides the best compression option and its associated reader `get_zarr_labels` will read the layer back into napari.  This writer will be called when the user tries to save a selected labels layer into a path ending with .zarr  ### `label_image_pairs_to_zarr` This writer will save 3-dimensional labels and image layers from the viewer into individual zarrs for portability and convenience. For example, given one labels and one image layer of the shape (10, 200, 200) saved to my_stacks.zarr, 10 subdirectories will be created, each with two zarrs inside of shape (200, 200) corresponding to the labels and image layer.  This writer allows users to load stacks of associated images, label them, and then quickly save these stacks out into individual slices for easy loading, viewing and interaction. Its associated reader supports the loading into napari of the whole stack, all layers at one slice of the stack, and an individual layer of a given slice of the stack.  The writer currently supports only 3D layers, with the exception of RGB images of the form (z, y, x, 3), which are also supported.   ## Readers  Two readers are provided by this plugin for loading the formats saved by each writer. These are detailed below.  ### `get_zarr_labels`  This reader will open any zarr file with a .zarray at the top level in `path` as a labels layer. This is to be used in conjunction with `labels_to_zarr`.   ### `get_label_image_stack`  This reader will open any zarr containing a `.zmeta` file as layers into napari. Depending on what is being opened, the reader will either load a full stack of labels and images, one slice of a stack of images and labels or an individual layer within a slice. This is to be used in conjunction with `label_image_pairs_to_zarr`.  ## .zmeta  This metadata file contains information about the layer types in the stack and in each individual slice, as well as the number of image/label slices. This allows the reader plugin to load the correct layer types with appropriate names both at a stack level and at the individual slice level.  ### An example .zmeta specification  ```json {     \"meta\": {         \"stack\": 7                               # number of slices in the entire stack (1 for an individual slice, 0 for a layer within a slice)     },     \"data\": {         \"image\" : [                              # all image layers must be listed here             {                 \"name\": \"leaves_example_data\",                 \"shape\": [790, 790, 3],                 \"dtype\": \"uint8\",                 \"rgb\": true                      # where rgb is false the image will be loaded as greyscale (colormap support has not yet been implemented)             }         ],         \"labels\" : [             {                 \"name\": \"oak\",                 \"shape\": [790, 790],                 \"dtype\": \"int64\"             },             {                 \"name\": \"bg\",                 \"shape\": [790, 790],                 \"dtype\": \"int64\"             }         ]     } }  ```   ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/docs/plugins/index.html -->  ## Installation  You can install `napari-compressed-labels-io` via [pip]:      pip install napari-compressed-labels-io  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [MIT] license, \"napari-compressed-labels-io\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin [file an issue]: https://github.com/DragaDoncila/napari-compressed-labels-io/issues [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-compressed-labels-io      Description This napari plugin provides readers and writers for labels and their corresponding image layers into zarr format for compression and portability. Each reader/writer pair supports a round trip of saving and loading image and labels layers. Writers Two writers are provided by this plugin, each with its own reader. labels_to_zarr This writer is an alternative to napari's default label writer and will write an entire labels layer, regardless of its dimensions, into a single zarr file. This writer provides the best compression option and its associated reader get_zarr_labels will read the layer back into napari. This writer will be called when the user tries to save a selected labels layer into a path ending with .zarr label_image_pairs_to_zarr This writer will save 3-dimensional labels and image layers from the viewer into individual zarrs for portability and convenience. For example, given one labels and one image layer of the shape (10, 200, 200) saved to my_stacks.zarr, 10 subdirectories will be created, each with two zarrs inside of shape (200, 200) corresponding to the labels and image layer. This writer allows users to load stacks of associated images, label them, and then quickly save these stacks out into individual slices for easy loading, viewing and interaction. Its associated reader supports the loading into napari of the whole stack, all layers at one slice of the stack, and an individual layer of a given slice of the stack. The writer currently supports only 3D layers, with the exception of RGB images of the form (z, y, x, 3), which are also supported. Readers Two readers are provided by this plugin for loading the formats saved by each writer. These are detailed below. get_zarr_labels This reader will open any zarr file with a .zarray at the top level in path as a labels layer. This is to be used in conjunction with labels_to_zarr. get_label_image_stack This reader will open any zarr containing a .zmeta file as layers into napari. Depending on what is being opened, the reader will either load a full stack of labels and images, one slice of a stack of images and labels or an individual layer within a slice. This is to be used in conjunction with label_image_pairs_to_zarr. .zmeta This metadata file contains information about the layer types in the stack and in each individual slice, as well as the number of image/label slices. This allows the reader plugin to load the correct layer types with appropriate names both at a stack level and at the individual slice level. An example .zmeta specification ```json {     \"meta\": {         \"stack\": 7                               # number of slices in the entire stack (1 for an individual slice, 0 for a layer within a slice)     },     \"data\": {         \"image\" : [                              # all image layers must be listed here             {                 \"name\": \"leaves_example_data\",                 \"shape\": [790, 790, 3],                 \"dtype\": \"uint8\",                 \"rgb\": true                      # where rgb is false the image will be loaded as greyscale (colormap support has not yet been implemented)             }         ],         \"labels\" : [             {                 \"name\": \"oak\",                 \"shape\": [790, 790],                 \"dtype\": \"int64\"             },             {                 \"name\": \"bg\",                 \"shape\": [790, 790],                 \"dtype\": \"int64\"             }         ]     } } ```  This napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.  Installation You can install napari-compressed-labels-io via pip: pip install napari-compressed-labels-io  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the MIT license, \"napari-compressed-labels-io\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "napari-compressed-labels-io",
    "documentation": "",
    "first_released": "2021-02-23T01:34:32.478790Z",
    "license": "MIT",
    "name": "napari-compressed-labels-io",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader", "writer"],
    "project_site": "https://github.com/DragaDoncila/napari-compressed-labels-io",
    "python_version": ">=3.6",
    "reader_file_extensions": ["*"],
    "release_date": "2021-03-03T20:41:59.257726Z",
    "report_issues": "",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "zarr",
      "dask[complete]"
    ],
    "summary": "Plugin exploring different options for reading and writing compressed and portable labels layers in napari.",
    "support": "",
    "twitter": "",
    "version": "0.0.2",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": ["labels"]
  },
  {
    "authors": [{ "email": "jonsson@mpi-cbg.de", "name": "Joel Jonsson" }],
    "code_repository": null,
    "description": "# napari-apr-viewer  [![License](https://img.shields.io/pypi/l/napari-apr-viewer.svg?color=green)](https://github.com/AdaptiveParticles/napari-apr-viewer/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-apr-viewer.svg?color=green)](https://pypi.org/project/napari-apr-viewer) [![Python Version](https://img.shields.io/pypi/pyversions/napari-apr-viewer.svg?color=green)](https://python.org) [![tests](https://github.com/AdaptiveParticles/napari-apr-viewer/workflows/tests/badge.svg)](https://github.com/AdaptiveParticles/napari-apr-viewer/actions) [![codecov](https://codecov.io/gh/AdaptiveParticles/napari-apr-viewer/branch/main/graph/badge.svg)](https://codecov.io/gh/AdaptiveParticles/napari-apr-viewer) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-apr-viewer)](https://napari-hub.org/plugins/napari-apr-viewer)  A simple plugin to view APR images in napari  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.   ## Installation  You can install `napari-apr-viewer` via [pip]:      pip install napari-apr-viewer     ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [Apache Software License 2.0] license, \"napari-apr-viewer\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/  [file an issue]: https://github.com/AdaptiveParticles/napari-apr-viewer/issues ",
    "description_content_type": "text/markdown",
    "description_text": "napari-apr-viewer       A simple plugin to view APR images in napari  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template. Installation You can install napari-apr-viewer via pip: pip install napari-apr-viewer  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the Apache Software License 2.0 license, \"napari-apr-viewer\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari-apr-viewer",
    "documentation": "",
    "first_released": "2021-11-30T23:22:09.686972Z",
    "license": "Apache-2.0",
    "name": "napari-apr-viewer",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader", "writer", "widget"],
    "project_site": "",
    "python_version": ">=3.7",
    "reader_file_extensions": ["*"],
    "release_date": "2022-05-25T11:16:04.274323Z",
    "report_issues": "",
    "requirements": [
      "numpy",
      "pyapr (>=1.0.0rc1)",
      "napari",
      "napari-plugin-engine (>=0.1.4)",
      "qtpy",
      "magicgui"
    ],
    "summary": "A simple plugin to view APR images in napari",
    "support": "",
    "twitter": "",
    "version": "1.0.0",
    "writer_file_extensions": [],
    "writer_save_layers": ["image"]
  },
  {
    "authors": [{ "name": "Chi-Li Chiu" }],
    "code_repository": "https://github.com/chili-chiu/napari-bio-sample-data",
    "conda": [],
    "description": "# napari-bio-sample-data  [![License](https://img.shields.io/pypi/l/napari-bio-sample-data.svg?color=green)](https://github.com/chili-chiu/napari-bio-sample-data/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-bio-sample-data.svg?color=green)](https://pypi.org/project/napari-bio-sample-data) [![Python Version](https://img.shields.io/pypi/pyversions/napari-bio-sample-data.svg?color=green)](https://python.org) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-bio-sample-data)](https://napari-hub.org/plugins/napari-bio-sample-data)  a sample data plugin for bio-related demos  ---------------------------------- This plugin contains 5 sample datasets with additional napari layer types:  (1) 3D EM dataset (image + points + vectors)   Image credit: Alister Burt   The [original data](https://github.com/alisterburt/napari-cryo-et-demo) is down-sampled to have smaller file size.   <img width=\"300\" alt=\"image\" src=\"https://user-images.githubusercontent.com/89602983/178569428-7daa2eb8-a3ff-4c0e-8e5f-4f615a55684f.png\">  (2) 2D skin RGB dataset (image + shape)   Image credit: skimage.data.skin   <img width=\"300\" alt=\"image\" src=\"https://user-images.githubusercontent.com/89602983/178569580-bf77e55c-71cc-4883-9fe5-ed94e05f2a29.png\">    (3) 3D nuclei dataset (image + label + surface)   Image credit: skimage.data.cells3d   <img width=\"300\" alt=\"image\" src=\"https://user-images.githubusercontent.com/89602983/178569701-7c9b1cc3-c1c3-4e54-8ca0-fb2b530f858e.png\">  (4) 2D timelapse dataset (image + points + tracks)   Image credit: [Cell Tracking Challenge](http://celltrackingchallenge.net/2d-datasets/)   The original data is cropped to have smaller file size.   <img width=\"300\" alt=\"image\" src=\"https://user-images.githubusercontent.com/89602983/178569846-b995d1cb-c1ec-4363-ba1a-71243ffea4e0.png\">  (5) large multi-resolution 3D EM dataset   Image credit: [Janelia Open Organelle](https://openorganelle.janelia.org/datasets/jrc_hela-1)    This plugin only accesses 2 lower resolution levels.   <img width=\"300\" alt=\"image\" src=\"https://user-images.githubusercontent.com/89602983/178570136-6f59ba3c-d687-446c-9f5e-1df567a62948.png\">  Datasets (1)-(4) are stored locally.    Dataset (5) is downloaded and temporarily stored on RAM when accessed.      This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/plugins/index.html -->  ## Installation  You can install `napari-bio-sample-data` via [pip]:      pip install napari-bio-sample-data  To install latest development version :      pip install git+https://github.com/chili-chiu/napari-bio-sample-data.git   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-bio-sample-data\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/chili-chiu/napari-bio-sample-data/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-bio-sample-data     a sample data plugin for bio-related demos  This plugin contains 5 sample datasets with additional napari layer types: (1) 3D EM dataset (image + points + vectors) Image credit: Alister Burt The original data is down-sampled to have smaller file size.  (2) 2D skin RGB dataset (image + shape) Image credit: skimage.data.skin  (3) 3D nuclei dataset (image + label + surface) Image credit: skimage.data.cells3d  (4) 2D timelapse dataset (image + points + tracks) Image credit: Cell Tracking Challenge The original data is cropped to have smaller file size.  (5) large multi-resolution 3D EM dataset Image credit: Janelia Open Organelle  This plugin only accesses 2 lower resolution levels.  Datasets (1)-(4) are stored locally.  Dataset (5) is downloaded and temporarily stored on RAM when accessed.     This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install napari-bio-sample-data via pip: pip install napari-bio-sample-data  To install latest development version : pip install git+https://github.com/chili-chiu/napari-bio-sample-data.git  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-bio-sample-data\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari bio sample data",
    "documentation": "https://github.com/chili-chiu/napari-bio-sample-data#README.md",
    "first_released": "2022-07-12T19:19:01.431082Z",
    "license": "BSD-3-Clause",
    "name": "napari-bio-sample-data",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["sample_data"],
    "project_site": "https://github.com/chili-chiu/napari-bio-sample-data",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-07-13T21:13:29.844275Z",
    "report_issues": "https://github.com/chili-chiu/napari-bio-sample-data/issues",
    "requirements": [
      "numpy",
      "fsspec",
      "zarr (>=2.12.0)",
      "dask",
      "s3fs",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'"
    ],
    "summary": "a sample data plugin for bio-related demos",
    "support": "https://github.com/chili-chiu/napari-bio-sample-data/issues",
    "twitter": "",
    "version": "0.0.4",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "email": "loic.sauteur@unibas.ch", "name": "Loïc Sauteur" }],
    "code_repository": "https://github.com/loicsauteur/napari-annotator",
    "description": "# Description  This light-weight plugin provides additional control over label layers. It is intended to ease your work when annotating data manually. ![Example screenshot](../resources/image1.png) --> the image link needs to be updated... (to weblink)  <--  It provides you with a widget listing all individual labels. For each label, you can: - select it from the list to activate it for further drawing. - toggle the visibility of individual labels - locate the drawn label (i.e. move to the centroid location at the current zoom level) - change the label color with a color picker - erase the label (sets all the drawn pixels to the label layer background value, **not un-doable**)  # Intended Audience & Supported Data  Everyone that has 2D or 3D data and wants to annotate (or curate annotated data)  should find a useful extension with this plugin.  The plugin will recognise and work only on label layers.  **Note:** The \"locate center\" button will only work on 2D/3D label layers, i.e.: YX, ZYX, TYX, CYX.  Channels are considered a dimension.  # Quickstart  1. Start napari 2. Open an image you want to annotate     1. Best, an image with the same dimension as you labels layer should have    2. e.g. ``File > Open Sample > napari > Binary Blobs (3D)`` 3. Add (or load) a labels layer 4. Start the plugin ``Plugins > napari-annotator: Annotator`` 5. Make sure the labels layer is selected 6. Start drawing  #### Known limitations 1. Lag when drawing (see [GitHub README](https://github.com/loicsauteur/napari-annotator) for more info). 2. Maximum 255 labels supported (see [GitHub README](https://github.com/loicsauteur/napari-annotator) for more info).  # Getting Help  If you encounter bugs, please [file an issue] along with a detailed description. Or open a thread on [forum.image.sc](https://forum.image.sc) with a detailed description  and a [@loicsauteur](https://github.com/loicsauteur) tag.  For general help, reach out via the [forum.image.sc](https://forum.image.sc) with a tag [@loicsauteur](https://github.com/loicsauteur).  # How to Cite  No citation needed. Honorable mention welcome. ",
    "description_content_type": "text/markdown",
    "description_text": "Description This light-weight plugin provides additional control over label layers. It is intended to ease your work when annotating data manually.  --> the image link needs to be updated... (to weblink)  <-- It provides you with a widget listing all individual labels. For each label, you can: - select it from the list to activate it for further drawing. - toggle the visibility of individual labels - locate the drawn label (i.e. move to the centroid location at the current zoom level) - change the label color with a color picker - erase the label (sets all the drawn pixels to the label layer background value, not un-doable) Intended Audience & Supported Data Everyone that has 2D or 3D data and wants to annotate (or curate annotated data)  should find a useful extension with this plugin. The plugin will recognise and work only on label layers. Note: The \"locate center\" button will only work on 2D/3D label layers, i.e.: YX, ZYX, TYX, CYX. Channels are considered a dimension. Quickstart  Start napari Open an image you want to annotate  Best, an image with the same dimension as you labels layer should have e.g. File > Open Sample > napari > Binary Blobs (3D) Add (or load) a labels layer Start the plugin Plugins > napari-annotator: Annotator Make sure the labels layer is selected Start drawing  Known limitations  Lag when drawing (see GitHub README for more info). Maximum 255 labels supported (see GitHub README for more info).  Getting Help If you encounter bugs, please [file an issue] along with a detailed description. Or open a thread on forum.image.sc with a detailed description  and a @loicsauteur tag. For general help, reach out via the forum.image.sc with a tag @loicsauteur. How to Cite No citation needed. Honorable mention welcome.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "Annotator",
    "documentation": "https://github.com/loicsauteur/napari-annotator#README.md",
    "first_released": "2022-03-07T16:39:41.535325Z",
    "license": "BSD-3-Clause",
    "name": "napari-annotator",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/loicsauteur/napari-annotator",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-03-07T16:39:41.535325Z",
    "report_issues": "https://github.com/loicsauteur/napari-annotator/issues",
    "requirements": ["numpy", "scikit-image"],
    "summary": "A lightweight plugin extending label layer control",
    "support": "https://github.com/loicsauteur/napari-annotator/issues",
    "twitter": "",
    "version": "0.0.1",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Sander van Otterdijk" }],
    "code_repository": "https://github.com/SanderSMFISH/napari-buds",
    "description": "# napari-buds  [![License BSD-3](https://img.shields.io/pypi/l/napari-buds.svg?color=green)](https://github.com/SanderSMFISH/napari-buds/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-buds.svg?color=green)](https://pypi.org/project/napari-buds) [![Python Version](https://img.shields.io/pypi/pyversions/napari-buds.svg?color=green)](https://python.org) [![tests](https://github.com/SanderSMFISH/napari-buds/workflows/tests/badge.svg)](https://github.com/SanderSMFISH/napari-buds/actions) [![codecov](https://codecov.io/gh/SanderSMFISH/napari-buds/branch/main/graph/badge.svg)](https://codecov.io/gh/SanderSMFISH/napari-buds) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-buds)](https://napari-hub.org/plugins/napari-buds)  Random-forest automated bud annotation  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/stable/plugins/index.html -->  ## Installation  make sure you already have installed napari.   Next, You can install `napari-buds` via [pip]:      pip install napari-buds    To install latest development version :      pip install git+https://github.com/SanderSMFISH/napari-buds.git   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## Documentation Napari-Buds is a random forest based mother-bud annotation plugin for Napari devevoped by the TutucciLab (https://www.tutuccilab.com/) of the systems biology group at the Vrije Universiteit van Amsterdam. Mother-bud annotation requires single or multichannel 2D images of budding yeast and a fluorescent marker that localizes to the bud. In the example dataset provided smFISH DNA-probes were used as localized bud marker.The GUI layout for random forest based classification was inspired by ImageJ 'plugin Weka Segmentation' [1]. **Before installation make sure you have a working version of napari installed (pip install \"napari[all]\").** Napari-Buds is a random forest based mother-bud annotation plugin for Napari developed by the TutucciLab (https://www.tutuccilab.com/) of the systems biology group at the Vrije Universiteit van Amsterdam. Mother-bud annotation requires single or multichannel 2D images of budding yeast and a fluorescent marker that localizes to the bud. In the example dataset provided smFISH DNA-probes were used as localized bud marker.The GUI layout for random forest based classification was inspired by ImageJ 'plugin Weka Segmentation' [1].   Please follow the workflow described underneath to perform mother-bud annotation:  1. Open images in napari and create empty label layer. **Before starting the plugin it is required that a empty label layer is created**. For multichannel images each channel should be provided seperately to napari. An example (jupyter) notebook (Open Test Images Napari.ipynb) for loading test data in napari is provided in the notebooks folder.  Example dataset can be downloaded from https://zenodo.org/record/7004556#.YwM1_HZBztU.       2. If multichannel images are unaligned the  translate widget under Plugins>napari-buds>Translate can be used.  Select which layer should be translated to align to the layers in widget menu. Then use the aswd keys to translate (move) the selected layer.  To register changes and update coordinates of the translated image in napari press t.       ### Random forest classification 3. To open the mother-bud annotation plugin go to Plugins>napari-buds>bud annotation. **Before starting the plugin it is required that a empty label layer is created**.      4. To train a random forest classifier, in the created label layer draw examples of cells, buds and background (see tutorial gif below).  In the Define Label segment of the widget you define which label value (class #label_value) corresponds to cells, buds and background.  Currently, cells and backgrounds and buds **have to be defined in the Define Label segment**  if you want to be able to segment the classification as well. In the segment **Layers to extract Features from** we can select which layers will be used in training the random forest classifier.  Next press **Train classifier**. After training is completed a result layer is added to layer list.  Inspect the results carefully to asses classifier performance. The trained classifier can be saved using the **save classifier** button. Previously trained classifier can be loaded by pressing **Load classifier**. Loaded classifier can applied to new images by pressing **Classify**, resulting again in a results layer. It is possible to change the random forest parameters with **the Set random forest parameters** button and changing the values in the pop up menu. Press **Run** to register changed settings. For an example of the parameters used see:  https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html and  https://scikit-image.org/docs/stable/auto_examples/segmentation/plot_trainable_segmentation.html.       5. Next, we want to perfom watershed segmentation using the result layer. However, for watershed segmentation seeds (also called markers) are required (for an explanation of watershed segmenation see: https://en.wikipedia.org/wiki/Watershed_(image_processing)).  To define the seeds we can either simply threshold on one of the supplied image layers or we can use distance tranform (https://scikit-image.org/docs/stable/auto_examples/segmentation/plot_watershed.html#sphx-glr-auto   examples-segmentation-plot-watershed-py).The resulting seeds layer can be adjusted manually by editing in napari. A good seeds layers correspond to each cell having a single seed (buds are not single cells). To perform watershed segmentation press the **Segment** button.      6. Carefully inspect the resulting cell mask and bud layer. Correct the mistakes in both layers.  Bud label values should correspond to the label value of the cell mask of mother cell. To verify mother bud relations were drawn correctly press **Draw Mother-Bud relations**. If Mother-Bud relations are correct, you can save both label layers. Mother and buds simply share the same label number. Thus, either the mother or bud layer can be manually corrected for mistakes. Corrections can be checked by clicking **Draw Mother-Bud relations** again.  mother and buds layer can be saved manually in napari. When using Jupyter notebook mother and bud layers can be saved as shown in Open Test Images Napari.ipynb.  7. An example notebook for dataextraction of the created cell and bud masks can be found in the example notebooks folder (Extract_Mother_Buds_relations_from_Masks_and_intergrate_FQ_spot_data.ipynb).This notebooks relates RNA spots (smFISH data found on zenodo) to the mother or bud compartment.    See video for clarification:  ![Watch the video](https://github.com/SanderSMFISH/napari-buds/blob/main/videos/Napari_bud_gif.gif)  ## Similar Napari plugins   1-napari-accelerated-pixel-and-object-classification (APOC) by Robert Haase.  2-napari-feature-classifier.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-buds\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  ### Known Issues  If window geometry of the window is unable to be set, this might lead to issues in the display of the widget. For example, part of the widget might fall of the screen. In these cases, it might help to adjust in your display setting the display scaling to a lower setting.   [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/SanderSMFISH/napari-buds/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/  ## References 1. Arganda-Carreras, I., Kaynig, V., Rueden, C., Eliceiri, K. W., Schindelin, J., Cardona, A., & Sebastian Seung, H. (2017). Trainable Weka Segmentation: a machine learning tool for microscopy pixel classification. Bioinformatics, 33(15), 2424–2426. doi:10.1093/bioinformatics/btx180 ",
    "description_content_type": "text/markdown",
    "description_text": "napari-buds       Random-forest automated bud annotation  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation make sure you already have installed napari.  Next, You can install napari-buds via pip: pip install napari-buds  To install latest development version : pip install git+https://github.com/SanderSMFISH/napari-buds.git  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. Documentation Napari-Buds is a random forest based mother-bud annotation plugin for Napari devevoped by the TutucciLab (https://www.tutuccilab.com/) of the systems biology group at the Vrije Universiteit van Amsterdam. Mother-bud annotation requires single or multichannel 2D images of budding yeast and a fluorescent marker that localizes to the bud. In the example dataset provided smFISH DNA-probes were used as localized bud marker.The GUI layout for random forest based classification was inspired by ImageJ 'plugin Weka Segmentation' [1]. Before installation make sure you have a working version of napari installed (pip install \"napari[all]\"). Napari-Buds is a random forest based mother-bud annotation plugin for Napari developed by the TutucciLab (https://www.tutuccilab.com/) of the systems biology group at the Vrije Universiteit van Amsterdam. Mother-bud annotation requires single or multichannel 2D images of budding yeast and a fluorescent marker that localizes to the bud. In the example dataset provided smFISH DNA-probes were used as localized bud marker.The GUI layout for random forest based classification was inspired by ImageJ 'plugin Weka Segmentation' [1].  Please follow the workflow described underneath to perform mother-bud annotation:   Open images in napari and create empty label layer. Before starting the plugin it is required that a empty label layer is created. For multichannel images each channel should be provided seperately to napari. An example (jupyter) notebook (Open Test Images Napari.ipynb) for loading test data in napari is provided in the notebooks folder.  Example dataset can be downloaded from https://zenodo.org/record/7004556#.YwM1_HZBztU.    If multichannel images are unaligned the  translate widget under Plugins>napari-buds>Translate can be used.  Select which layer should be translated to align to the layers in widget menu. Then use the aswd keys to translate (move) the selected layer.  To register changes and update coordinates of the translated image in napari press t.    Random forest classification   To open the mother-bud annotation plugin go to Plugins>napari-buds>bud annotation. Before starting the plugin it is required that a empty label layer is created.   To train a random forest classifier, in the created label layer draw examples of cells, buds and background (see tutorial gif below).  In the Define Label segment of the widget you define which label value (class #label_value) corresponds to cells, buds and background.  Currently, cells and backgrounds and buds have to be defined in the Define Label segment  if you want to be able to segment the classification as well. In the segment Layers to extract Features from we can select which layers will be used in training the random forest classifier.  Next press Train classifier. After training is completed a result layer is added to layer list.  Inspect the results carefully to asses classifier performance. The trained classifier can be saved using the save classifier button. Previously trained classifier can be loaded by pressing Load classifier. Loaded classifier can applied to new images by pressing Classify, resulting again in a results layer. It is possible to change the random forest parameters with the Set random forest parameters button and changing the values in the pop up menu. Press Run to register changed settings. For an example of the parameters used see:  https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html and  https://scikit-image.org/docs/stable/auto_examples/segmentation/plot_trainable_segmentation.html.    Next, we want to perfom watershed segmentation using the result layer. However, for watershed segmentation seeds (also called markers) are required (for an explanation of watershed segmenation see: https://en.wikipedia.org/wiki/Watershed_(image_processing)).  To define the seeds we can either simply threshold on one of the supplied image layers or we can use distance tranform (https://scikit-image.org/docs/stable/auto_examples/segmentation/plot_watershed.html#sphx-glr-auto   examples-segmentation-plot-watershed-py).The resulting seeds layer can be adjusted manually by editing in napari. A good seeds layers correspond to each cell having a single seed (buds are not single cells). To perform watershed segmentation press the Segment button.   Carefully inspect the resulting cell mask and bud layer. Correct the mistakes in both layers.  Bud label values should correspond to the label value of the cell mask of mother cell. To verify mother bud relations were drawn correctly press Draw Mother-Bud relations. If Mother-Bud relations are correct, you can save both label layers. Mother and buds simply share the same label number. Thus, either the mother or bud layer can be manually corrected for mistakes. Corrections can be checked by clicking Draw Mother-Bud relations again.  mother and buds layer can be saved manually in napari. When using Jupyter notebook mother and bud layers can be saved as shown in Open Test Images Napari.ipynb.   An example notebook for dataextraction of the created cell and bud masks can be found in the example notebooks folder (Extract_Mother_Buds_relations_from_Masks_and_intergrate_FQ_spot_data.ipynb).This notebooks relates RNA spots (smFISH data found on zenodo) to the mother or bud compartment.    See video for clarification:  Similar Napari plugins 1-napari-accelerated-pixel-and-object-classification (APOC) by Robert Haase. 2-napari-feature-classifier. License Distributed under the terms of the BSD-3 license, \"napari-buds\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description. Known Issues If window geometry of the window is unable to be set, this might lead to issues in the display of the widget. For example, part of the widget might fall of the screen. In these cases, it might help to adjust in your display setting the display scaling to a lower setting.  References  Arganda-Carreras, I., Kaynig, V., Rueden, C., Eliceiri, K. W., Schindelin, J., Cardona, A., & Sebastian Seung, H. (2017). Trainable Weka Segmentation: a machine learning tool for microscopy pixel classification. Bioinformatics, 33(15), 2424–2426. doi:10.1093/bioinformatics/btx180 ",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari BudAnnotation",
    "documentation": "https://github.com/SanderSMFISH/napari-buds#README.md",
    "first_released": "2022-08-16T13:14:29.716611Z",
    "license": "BSD-3-Clause",
    "name": "napari-buds",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader", "writer", "widget", "sample_data"],
    "project_site": "https://github.com/SanderSMFISH/napari-buds",
    "python_version": ">=3.8",
    "reader_file_extensions": ["*.npy"],
    "release_date": "2022-11-08T11:55:49.657916Z",
    "report_issues": "https://github.com/SanderSMFISH/napari-buds/issues",
    "requirements": [
      "numpy",
      "magicgui",
      "qtpy",
      "pandas",
      "napari",
      "magic-class",
      "scipy",
      "scikit-learn",
      "scikit-image",
      "matplotlib",
      "joblib",
      "imageio-ffmpeg",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "summary": "Random-forest automated bud annotation",
    "support": "https://github.com/SanderSMFISH/napari-buds/issues",
    "twitter": "",
    "version": "0.1.2",
    "visibility": "public",
    "writer_file_extensions": [".npy"],
    "writer_save_layers": ["image", "image*", "labels*"]
  },
  {
    "authors": [{ "name": "Sebastian Rhode" }],
    "code_repository": "https://github.com/sebi06/napari-czann-segment",
    "conda": [{ "channel": "conda-forge", "package": "napari-czann-segment" }],
    "description": "# napari-czann-segment  [![License](https://img.shields.io/pypi/l/napari-czann-segment.svg?color=green)](https://github.com/sebi06/napari-czann-segment/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-czann-segment.svg?color=green)](https://pypi.org/project/napari-czann-segment) [![Python Version](https://img.shields.io/pypi/pyversions/napari-czann-segment.svg?color=green)](https://python.org) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-czann-segment)](https://napari-hub.org/plugins/napari-czann-segment)  Semantic Segmentation of multi-dimensional images using Deep Learning ONNX models packaged as *.czann files.  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  ![Train on APEER and use model in Napari](https://github.com/sebi06/napari-czann-segment/raw/main/readme_images/Train_APEER_run_Napari_CZANN_no_highlights_small.gif)  ## Installation  Before installing, please setup a conda environment. If you have never worked with conda environments, go through [this tutorial](https://biapol.github.io/blog/johannes_mueller/anaconda_getting_started/) first.  You can then install `napari-czann-segment` via [pip]:      pip install napari-czann-segment  ## What does the plugin do  The plugin allows you to:  - Use a *.czann file containing the Deep Neural Network (ONNX) for semantic segmentation and metadata - Segmentation will be applied per 2D plane for all dimensions - Processing larger multi-dimensional images it uses the [cztile] package to chunk the individual 2d arrays using a specific overlap. - multi-dimensional images will be processed plane-by-plane  ## What does the plugin NOT do  **Before one can actually use a model it needs to be trained, which is NOT done by this plugin**.  Therer two main ways hwo such a model can be created:  - Train the segmentation model fully automated on [APEER] and download the *.czann file - Train your model in a Jupyter notebook etc. and package it using the [czmodel] python package as an *.czann  ## Using this plugin  ### Sample Data  A test image and a *.czann model file can be downloaded [here](https://github.com/sebi06/napari-czann-segment/tree/main/src/napari_czann_segment/_data).  - `PGC_20X.ome.tiff` --> use `PGC_20X_nucleus_detector.czann` to segment  In order to use this plugin the user has to do the following things:  - Open the image using \"File - Open Files(s)\" (requires [napari-aicsimageio] plugin). - Click **napari-czann-segment: Segment with CZANN model** in the \"Plugins\" menu. - **Select a *.czann file** to use the model for segmentation. - metadata of the model will be shown (see example below)  | Parameter    | Value                                        | Explanation                                             | | :----------- | :------------------------------------------- | ------------------------------------------------------- | | model_type   | ModelType.SINGLE_CLASS_SEMANTIC_SEGMENTATION | see: [czmodel] for details                              | | input_shape  | [1024, 1024, 1]                              | tile dimensions of model input                          | | output_shape | [1024, 1024, 3]                              | tile dimensions of model output                         | | model_id     | ba32bc6d-6bc9-4774-8b47-20646c7cb838         | unique GUID for that model                              | | min_overlap  | [128, 128]                                   | tile overlap used during training (for this model)      | | classes      | ['background', 'grains', 'inclusions']       | availbale classes                                       | | model_name   | APEER-trained model                          | name of the model                                       |  ![Napari - Image loaded and czann selected](https://github.com/sebi06/napari-czann-segment/raw/main/readme_images/napari_czann1.png)  - Adjust the **minimum overlap** for the tiling (optional, see [cztile] for details). - Select the **layer** to be segmented. - Press **Segment Selected Image Layer** to run the segmentation.  ![Napari - Image successfully segmented](https://github.com/sebi06/napari-czann-segment/raw/main/readme_images/napari_czann2.png)  A successful is obviously only the starting point for further image analysis steps to extract the desired numbers from the segmented image. Another example is shown below demonstrating a simple \"Grain Size Analysis\" using a deep-learning model trained on [APEER] used in [napari]  ![Napari - Simple Grain Size Analysis](https://github.com/sebi06/napari-czann-segment/raw/main/readme_images/grainsize_czann_napari.png)  ### Remarks  > **IMPORTANT**: Currently the plugin only supports using models trained on a **single channel** image. Therefore make sure that during the training on [APEER] or somewhere else the correct inputs images are used. > It is quite simple to train an single RGB image, which actually has three channels, load this image in [napari] and notice only then that the model will not work, because the image will 3 channels inside [napari].  - Only the CPU will be used for the inference using the ONNX runtime for the [ONNX-CPU] runtime - GPUs are not supported yet and will require [ONNX-GPU] runtime  ## For developers  - **Please clone this repository first using your favorite tool.**  - **Ideally one creates a new [conda] environment or use an existing environment that already contains [Napari].**  Feel free to create a new environment using the [YAML](env_napari_czann_segment.yml) file at your own risk:      cd the-github-repo-with-YAML-file     conda env create --file conda_env_napari_czann_segment.yml     conda activate napari_czmodel  - **Install the plugin locally**  Please run the the following command:      pip install -e .  To install latest development version:      pip install git+https://github.com/sebi06/napari_czann_segment.git  ## Contributing  Contributions and Feedback are very welcome.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-czann-segment\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin [file an issue]: https://github.com/sebi06/napari-czann-segment/issues [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ [czmodel]: https://pypi.org/project/czmodel/ [cztile]: https://pypi.org/project/cztile/ [APEER]: https://www.apeer.com [napari-aicsimageio]: https://github.com/AllenCellModeling/napari-aicsimageio [ONNX-GPU]: https://pypi.org/project/onnxruntime-gpu/ [ONNX-CPU]: https://pypi.org/project/onnxruntime/ [conda]: https://docs.conda.io/projects/conda/en/latest/user-guide/getting-started.html ",
    "description_content_type": "text/markdown",
    "description_text": "napari-czann-segment     Semantic Segmentation of multi-dimensional images using Deep Learning ONNX models packaged as *.czann files.  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation Before installing, please setup a conda environment. If you have never worked with conda environments, go through this tutorial first. You can then install napari-czann-segment via pip: pip install napari-czann-segment  What does the plugin do The plugin allows you to:  Use a *.czann file containing the Deep Neural Network (ONNX) for semantic segmentation and metadata Segmentation will be applied per 2D plane for all dimensions Processing larger multi-dimensional images it uses the cztile package to chunk the individual 2d arrays using a specific overlap. multi-dimensional images will be processed plane-by-plane  What does the plugin NOT do Before one can actually use a model it needs to be trained, which is NOT done by this plugin. Therer two main ways hwo such a model can be created:  Train the segmentation model fully automated on APEER and download the *.czann file Train your model in a Jupyter notebook etc. and package it using the czmodel python package as an *.czann  Using this plugin Sample Data A test image and a *.czann model file can be downloaded here.  PGC_20X.ome.tiff --> use PGC_20X_nucleus_detector.czann to segment  In order to use this plugin the user has to do the following things:  Open the image using \"File - Open Files(s)\" (requires napari-aicsimageio plugin). Click napari-czann-segment: Segment with CZANN model in the \"Plugins\" menu. Select a *.czann file to use the model for segmentation. metadata of the model will be shown (see example below)  | Parameter    | Value                                        | Explanation                                             | | :----------- | :------------------------------------------- | ------------------------------------------------------- | | model_type   | ModelType.SINGLE_CLASS_SEMANTIC_SEGMENTATION | see: czmodel for details                              | | input_shape  | [1024, 1024, 1]                              | tile dimensions of model input                          | | output_shape | [1024, 1024, 3]                              | tile dimensions of model output                         | | model_id     | ba32bc6d-6bc9-4774-8b47-20646c7cb838         | unique GUID for that model                              | | min_overlap  | [128, 128]                                   | tile overlap used during training (for this model)      | | classes      | ['background', 'grains', 'inclusions']       | availbale classes                                       | | model_name   | APEER-trained model                          | name of the model                                       |   Adjust the minimum overlap for the tiling (optional, see cztile for details). Select the layer to be segmented. Press Segment Selected Image Layer to run the segmentation.   A successful is obviously only the starting point for further image analysis steps to extract the desired numbers from the segmented image. Another example is shown below demonstrating a simple \"Grain Size Analysis\" using a deep-learning model trained on APEER used in napari  Remarks  IMPORTANT: Currently the plugin only supports using models trained on a single channel image. Therefore make sure that during the training on APEER or somewhere else the correct inputs images are used. It is quite simple to train an single RGB image, which actually has three channels, load this image in napari and notice only then that the model will not work, because the image will 3 channels inside napari.   Only the CPU will be used for the inference using the ONNX runtime for the ONNX-CPU runtime GPUs are not supported yet and will require ONNX-GPU runtime  For developers   Please clone this repository first using your favorite tool.   Ideally one creates a new conda environment or use an existing environment that already contains Napari.   Feel free to create a new environment using the YAML file at your own risk: cd the-github-repo-with-YAML-file conda env create --file conda_env_napari_czann_segment.yml conda activate napari_czmodel   Install the plugin locally  Please run the the following command: pip install -e .  To install latest development version: pip install git+https://github.com/sebi06/napari_czann_segment.git  Contributing Contributions and Feedback are very welcome. License Distributed under the terms of the BSD-3 license, \"napari-czann-segment\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 3 - Alpha"],
    "display_name": "CZANN Segmentation",
    "documentation": "https://github.com/sebi06/napari-czann-segment#README.md",
    "first_released": "2022-07-11T12:10:58.248435Z",
    "license": "BSD-3-Clause",
    "name": "napari-czann-segment",
    "npe2": true,
    "operating_system": [
      "Operating System :: Microsoft :: Windows",
      "Operating System :: Unix"
    ],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/sebi06/napari-czann-segment",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-10-04T13:43:07.505874Z",
    "report_issues": "https://github.com/sebi06/napari-czann-segment/issues",
    "requirements": [
      "numpy",
      "magicgui",
      "qtpy",
      "napari",
      "cztile",
      "czmodel[pytorch]",
      "onnxruntime",
      "aicsimageio",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "summary": "Semantic Segmentation using Deep Learning ONNX models packaged as *.czann files",
    "support": "https://github.com/sebi06/napari-czann-segment/issues",
    "twitter": "",
    "version": "0.0.16",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      { "name": "Nathan Heath Patterson", "orcid": "0000-0002-0064-1583" },
      { "name": "Lukasz Migas", "orcid": "0000-0002-1884-6405" }
    ],
    "code_repository": "https://github.com/nhpatterson/napari-imsmicrolink",
    "description": "# napari-imsmicrolink ![microlink-logo-update](https://user-images.githubusercontent.com/17855764/146078168-dd557089-ff10-46d6-b24d-268f5d21a9ee.png)  [![License](https://img.shields.io/pypi/l/napari-imsmicrolink.svg?color=green)](https://github.com/nhpatterson/napari-imsmicrolink/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-imsmicrolink.svg?color=green)](https://pypi.org/project/napari-imsmicrolink) [![Python Version](https://img.shields.io/pypi/pyversions/napari-imsmicrolink.svg?color=green)](https://python.org) [![tests](https://github.com/nhpatterson/napari-imsmicrolink/workflows/tests/badge.svg)](https://github.com/nhpatterson/napari-imsmicrolink/actions)  [napari] plugin to perform MALDI IMS - microscopy registration using laser ablation marks as described in [Anal. Chem. 2018, 90, 21, 12395–12403](https://pubs.acs.org/doi/abs/10.1021/acs.analchem.8b02884). This plugin is a work-in-progress but is mostly functional.  __N.B.__ This tool is __NOT__ a general purpose registration framework to find transforms between IMS (MALDI or otherwise) and microscopy. It is built to align MALDI IMS pixels to their corresponding laser ablation marks as captured by microscopy AFTER the IMS experiment.  This approach has the advantage of providing direct evidence of registration performance as IMS pixels are aligned  to their _explicit spatial origin_ in microscopy space, improving overall accuracy and confidence of microscopy-driven IMS  data analysis.  ## Installation  You can install `napari-imsmicrolink` via [pip]:      pip install napari-imsmicrolink  ### Typical experiment workflow 1. Acquire pre-IMS microscopy (autofluorescence, brightfield) - _optional_ 2. Perform normal IMS sample preparation. 3. Acquire post-IMS microscopy (autofluorescence, brightfield) with matrix still on sample that reveals laser ablation marks.  4. Gather IMS data that contains XY integer coordinates for the IMS experiment    (.imzML, Bruker spotlist (.txt, .csv), Bruker peaks.sqlite (_FTICR_),    Bruker .tsf (TIMS qTOF only))  5. Run `napari-imsmicrolink` with data 3 and 4  6. Once registered, use `wsireg` to align other microscopy modalities to IMS-registered post-IMS microscopy  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/docs/plugins/index.html -->   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [MIT] license, \"napari-imsmicrolink\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin [file an issue]: https://github.com/nhpatterson/napari-imsmicrolink/issues [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-imsmicrolink      napari plugin to perform MALDI IMS - microscopy registration using laser ablation marks as described in Anal. Chem. 2018, 90, 21, 12395–12403. This plugin is a work-in-progress but is mostly functional. N.B. This tool is NOT a general purpose registration framework to find transforms between IMS (MALDI or otherwise) and microscopy. It is built to align MALDI IMS pixels to their corresponding laser ablation marks as captured by microscopy AFTER the IMS experiment.  This approach has the advantage of providing direct evidence of registration performance as IMS pixels are aligned  to their explicit spatial origin in microscopy space, improving overall accuracy and confidence of microscopy-driven IMS  data analysis. Installation You can install napari-imsmicrolink via pip: pip install napari-imsmicrolink  Typical experiment workflow  Acquire pre-IMS microscopy (autofluorescence, brightfield) - optional Perform normal IMS sample preparation.  Acquire post-IMS microscopy (autofluorescence, brightfield) with matrix still on sample that reveals laser ablation marks.   Gather IMS data that contains XY integer coordinates for the IMS experiment    (.imzML, Bruker spotlist (.txt, .csv), Bruker peaks.sqlite (FTICR),    Bruker .tsf (TIMS qTOF only))   Run napari-imsmicrolink with data 3 and 4   Once registered, use wsireg to align other microscopy modalities to IMS-registered post-IMS microscopy    This napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the MIT license, \"napari-imsmicrolink\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 3 - Alpha"],
    "display_name": "napari-imsmicrolink",
    "documentation": "https://napari-imsmicrolink.readthedocs.io/en/latest/",
    "first_released": "2021-12-14T20:51:38.664548Z",
    "license": "MIT",
    "name": "napari-imsmicrolink",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://napari-imsmicrolink.readthedocs.io/en/latest/",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-11-18T21:33:33.140168Z",
    "report_issues": "https://github.com/nhpatterson/napari-imsmicrolink/issues",
    "requirements": [
      "numpy",
      "tifffile",
      "dask",
      "zarr (>=2.10.3)",
      "qtpy",
      "aicsimageio[bioformats]",
      "bioformats-jar",
      "SimpleITK",
      "pandas",
      "h5py",
      "opencv-python",
      "czifile",
      "imagecodecs"
    ],
    "summary": "Plugin to perform IMS to microscopy registration using laser ablation marks.",
    "support": "https://github.com/nhpatterson/napari-imsmicrolink/issues",
    "twitter": "https://twitter.com/nheathpatterson/",
    "version": "0.1.8",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Lorenzo Gaifas" }],
    "code_repository": "https://github.com/brisvag/napari-help",
    "conda": [],
    "description": "# napari-help  [![License GNU GPL v3.0](https://img.shields.io/pypi/l/napari-help.svg?color=green)](https://github.com/brisvag/napari-help/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-help.svg?color=green)](https://pypi.org/project/napari-help) [![Python Version](https://img.shields.io/pypi/pyversions/napari-help.svg?color=green)](https://python.org) [![tests](https://github.com/brisvag/napari-help/workflows/tests/badge.svg)](https://github.com/brisvag/napari-help/actions) [![codecov](https://codecov.io/gh/brisvag/napari-help/branch/main/graph/badge.svg)](https://codecov.io/gh/brisvag/napari-help) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-help)](https://napari-hub.org/plugins/napari-help)  Helpful tooltips for napari.  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/stable/plugins/index.html -->  ## Installation  You can install `napari-help` via [pip]:      pip install napari-help    To install latest development version :      pip install git+https://github.com/brisvag/napari-help.git   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [GNU GPL v3.0] license, \"napari-help\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/brisvag/napari-help/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-help       Helpful tooltips for napari.  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install napari-help via pip: pip install napari-help  To install latest development version : pip install git+https://github.com/brisvag/napari-help.git  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the GNU GPL v3.0 license, \"napari-help\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari Help",
    "documentation": "https://github.com/brisvag/napari-help#README.md",
    "first_released": "2022-07-28T20:10:18.515120Z",
    "license": "GPL-3.0",
    "name": "napari-help",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/brisvag/napari-help",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-07-28T20:10:18.515120Z",
    "report_issues": "https://github.com/brisvag/napari-help/issues",
    "requirements": [
      "napari",
      "qtpy",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "tox ; extra == 'testing'"
    ],
    "summary": "Helpful tooltips for napari.",
    "support": "https://github.com/brisvag/napari-help/issues",
    "twitter": "",
    "version": "0.1.0",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Jessy Lauer" }],
    "code_repository": "https://github.com/DeepLabCut/napari-deeplabcut",
    "description": "# napari-deeplabcut   <img src=\"https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1d409ffe-c9f4-47e1-bde2-3010c1c40455/naparidlc.png?format=750w\" width=\"250\" title=\"napari-deeplabcut\" alt=\"napari+deeplabcut\" align=\"right\" vspace = \"80\">  [![License: BSD-3](https://img.shields.io/badge/License-BSD3-blue.svg)](https://www.gnu.org/licenses/bsd3) [![PyPI](https://img.shields.io/pypi/v/napari-deeplabcut.svg?color=green)](https://pypi.org/project/napari-deeplabcut) [![Python Version](https://img.shields.io/pypi/pyversions/napari-deeplabcut.svg?color=green)](https://python.org) [![tests](https://github.com/DeepLabCut/napari-deeplabcut/workflows/tests/badge.svg)](https://github.com/DeepLabCut/napari-deeplabcut/actions) [![codecov](https://codecov.io/gh/DeepLabCut/napari-deeplabcut/branch/main/graph/badge.svg)](https://codecov.io/gh/DeepLabCut/napari-deeplabcut) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-deeplabcut)](https://napari-hub.org/plugins/napari-deeplabcut)  A napari plugin for keypoint annotation with DeepLabCut   ## Installation  Start by installing PySide6 with `pip install \"pyside6<6.3.2\"`; this is the library we now use to build GUIs.  You can then install `napari-deeplabcut` via [pip]:      pip install napari-deeplabcut    Alternatively, to install the latest development version, run:      pip install git+https://github.com/DeepLabCut/napari-deeplabcut.git   ## Usage  To use the plugin, please run:      napari  Then, activate the plugin in Plugins > napari-deeplabcut: Keypoint controls.  All accepted files (config.yaml, images, h5 data files) can be loaded either by dropping them directly onto the canvas or via the File menu.  The easiest way to get started is to drop a folder (typically a folder from within a DeepLabCut's `labeled-data` directory), and, if labeling from scratch, drop the corresponding `config.yaml` to automatically add a `Points layer` and populate the dropdown menus.  **Tools & shortcuts are:**  - `2` and `3`, to easily switch between labeling and selection mode - `4`, to enable pan & zoom (which is achieved using the mouse wheel or finger scrolling on the Trackpad) - `M`, to cycle through regular (sequential), quick, and cycle annotation mode (see the description [here](https://github.com/DeepLabCut/DeepLabCut-label/blob/ee71b0e15018228c98db3b88769e8a8f4e2c0454/dlclabel/layers.py#L9-L19)) - `E`, to enable edge coloring (by default, if using this in refinement GUI mode, points with a confidence lower than 0.6 are marked in red) - `F`, to toggle between animal and body part color scheme. - `backspace` to delete a point. - Check the box \"display text\" to show the label names on the canvas. - To move to another folder, be sure to save (Ctrl+S), then delete the layers, and re-drag/drop the next folder.   ### Save Layers  Annotations and segmentations are saved with `File > Save Selected Layer(s)...` (or its shortcut `Ctrl+S`). Only when saving segmentation masks does a save file dialog pop up to name the destination folder; keypoint annotations are otherwise automatically saved in the corresponding folder as `CollectedData_<ScorerName>.h5`. - As a reminder, DLC will only use the H5 file; so be sure if you open already labeled images you save/overwrite the H5. - Note, before saving a layer, make sure the points layer is selected. If the user clicked on the image(s) layer first, does `Save As`, then closes the window, any labeling work during that session will be lost!   ### Video frame extraction and prediction refinement  Since v0.0.4, videos can be viewed in the GUI.  Since v0.0.5, trailing points can be visualized; e.g., helping in the identification of swaps or outlier, jittery predictions.  Loading a video (and its corresponding output h5 file) will enable the video actions at the top of the dock widget: they offer the option to manually extract video frames from the GUI, or to define cropping coordinates. Note that keypoints can be displaced and saved, as when annotating individual frames.   ## Workflow  Suggested workflows, depending on the image folder contents:  1. **Labeling from scratch** – the image folder does not contain `CollectedData_<ScorerName>.h5` file.      Open *napari* as described in [Usage](#usage) and open an image folder together with the DeepLabCut project's `config.yaml`.     The image folder creates an *image layer* with the images to label.     Supported image formats are: `jpg`, `jpeg`, `png`.     The `config.yaml` file creates a *Points layer*, which holds metadata (such as keypoints read from the config file) necessary for labeling.     Select the *Points layer* in the layer list (lower left pane on the GUI) and click on the *+*-symbol in the layer controls menu (upper left pane) to start labeling.     The current keypoint can be viewed/selected in the keypoints dropdown menu (right pane).     The slider below the displayed image (or the left/right arrow keys) allows selecting the image to label.      To save the labeling progress refer to [Save Layers](#save-layers).     `Data successfully saved` should be shown in the status bar, and the image folder should now contain a `CollectedData_<ScorerName>.h5` file.     (Note: For convenience, a CSV file with the same name is also saved.)  2. **Resuming labeling** – the image folder contains a `CollectedData_<ScorerName>.h5` file.      Open *napari* and open an image folder (which needs to contain a `CollectedData_<ScorerName>.h5` file).     In this case, it is not necessary to open the DLC project's `config.yaml` file, as all necessary metadata is read from the `h5` data file.      Saving works as described in *1*.  3. **Refining labels** – the image folder contains a `machinelabels-iter<#>.h5` file.      The process is analog to *2*.  4. **Drawing segmentation masks**      Drop an image folder as in *1*, manually add a *shapes layer*. Then select the *rectangle* in the layer controls (top left pane),     and start drawing rectangles over the images. Masks and rectangle vertices are saved as described in [Save Layers](#save-layers).     Note that masks can be reloaded and edited at a later stage by dropping the `vertices.csv` file onto the canvas.   ### Labeling multiple image folders  Labeling multiple image folders has to be done in sequence; i.e., only one image folder can be opened at a time. After labeling the images of a particular folder is done and the associated *Points layer* has been saved, *all* layers should be removed from the layers list (lower left pane on the GUI) by selecting them and clicking on the trashcan icon. Now, another image folder can be labeled, following the process described in *1*, *2*, or *3*, depending on the particular image folder.   ### Defining cropping coordinates  Prior to defining cropping coordinates, two elements should be loaded in the GUI: a video and the DLC project's `config.yaml` file (into which the crop dimensions will be stored). Then it suffices to add a `Shapes layer`, draw a `rectangle` in it with the desired area, and hit the button `Store crop coordinates`; coordinates are automatically written to the configuration file.   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  To locally install the code, please git clone the repo and then run `pip install -e .`  ## License  Distributed under the terms of the [BSD-3] license, \"napari-deeplabcut\" is free and open source software.  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [file an issue]: https://github.com/DeepLabCut/napari-deeplabcut/issues   ## Acknowledgements   This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template. We thank the Chan Zuckerberg Initiative (CZI) for funding this work!  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/plugins/stable/index.html -->   [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-deeplabcut        A napari plugin for keypoint annotation with DeepLabCut Installation Start by installing PySide6 with pip install \"pyside6<6.3.2\"; this is the library we now use to build GUIs. You can then install napari-deeplabcut via pip: pip install napari-deeplabcut  Alternatively, to install the latest development version, run: pip install git+https://github.com/DeepLabCut/napari-deeplabcut.git  Usage To use the plugin, please run: napari  Then, activate the plugin in Plugins > napari-deeplabcut: Keypoint controls. All accepted files (config.yaml, images, h5 data files) can be loaded either by dropping them directly onto the canvas or via the File menu. The easiest way to get started is to drop a folder (typically a folder from within a DeepLabCut's labeled-data directory), and, if labeling from scratch, drop the corresponding config.yaml to automatically add a Points layer and populate the dropdown menus. Tools & shortcuts are:  2 and 3, to easily switch between labeling and selection mode 4, to enable pan & zoom (which is achieved using the mouse wheel or finger scrolling on the Trackpad) M, to cycle through regular (sequential), quick, and cycle annotation mode (see the description here) E, to enable edge coloring (by default, if using this in refinement GUI mode, points with a confidence lower than 0.6 are marked in red) F, to toggle between animal and body part color scheme. backspace to delete a point. Check the box \"display text\" to show the label names on the canvas. To move to another folder, be sure to save (Ctrl+S), then delete the layers, and re-drag/drop the next folder.  Save Layers Annotations and segmentations are saved with File > Save Selected Layer(s)... (or its shortcut Ctrl+S). Only when saving segmentation masks does a save file dialog pop up to name the destination folder; keypoint annotations are otherwise automatically saved in the corresponding folder as CollectedData_<ScorerName>.h5. - As a reminder, DLC will only use the H5 file; so be sure if you open already labeled images you save/overwrite the H5. - Note, before saving a layer, make sure the points layer is selected. If the user clicked on the image(s) layer first, does Save As, then closes the window, any labeling work during that session will be lost! Video frame extraction and prediction refinement Since v0.0.4, videos can be viewed in the GUI. Since v0.0.5, trailing points can be visualized; e.g., helping in the identification of swaps or outlier, jittery predictions. Loading a video (and its corresponding output h5 file) will enable the video actions at the top of the dock widget: they offer the option to manually extract video frames from the GUI, or to define cropping coordinates. Note that keypoints can be displaced and saved, as when annotating individual frames. Workflow Suggested workflows, depending on the image folder contents:   Labeling from scratch – the image folder does not contain CollectedData_<ScorerName>.h5 file. Open napari as described in Usage and open an image folder together with the DeepLabCut project's config.yaml. The image folder creates an image layer with the images to label. Supported image formats are: jpg, jpeg, png. The config.yaml file creates a Points layer, which holds metadata (such as keypoints read from the config file) necessary for labeling. Select the Points layer in the layer list (lower left pane on the GUI) and click on the +-symbol in the layer controls menu (upper left pane) to start labeling. The current keypoint can be viewed/selected in the keypoints dropdown menu (right pane). The slider below the displayed image (or the left/right arrow keys) allows selecting the image to label. To save the labeling progress refer to Save Layers. Data successfully saved should be shown in the status bar, and the image folder should now contain a CollectedData_<ScorerName>.h5 file. (Note: For convenience, a CSV file with the same name is also saved.)   Resuming labeling – the image folder contains a CollectedData_<ScorerName>.h5 file. Open napari and open an image folder (which needs to contain a CollectedData_<ScorerName>.h5 file). In this case, it is not necessary to open the DLC project's config.yaml file, as all necessary metadata is read from the h5 data file. Saving works as described in 1.   Refining labels – the image folder contains a machinelabels-iter<#>.h5 file. The process is analog to 2.   Drawing segmentation masks Drop an image folder as in 1, manually add a shapes layer. Then select the rectangle in the layer controls (top left pane), and start drawing rectangles over the images. Masks and rectangle vertices are saved as described in Save Layers. Note that masks can be reloaded and edited at a later stage by dropping the vertices.csv file onto the canvas.   Labeling multiple image folders Labeling multiple image folders has to be done in sequence; i.e., only one image folder can be opened at a time. After labeling the images of a particular folder is done and the associated Points layer has been saved, all layers should be removed from the layers list (lower left pane on the GUI) by selecting them and clicking on the trashcan icon. Now, another image folder can be labeled, following the process described in 1, 2, or 3, depending on the particular image folder. Defining cropping coordinates Prior to defining cropping coordinates, two elements should be loaded in the GUI: a video and the DLC project's config.yaml file (into which the crop dimensions will be stored). Then it suffices to add a Shapes layer, draw a rectangle in it with the desired area, and hit the button Store crop coordinates; coordinates are automatically written to the configuration file. Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. To locally install the code, please git clone the repo and then run pip install -e . License Distributed under the terms of the BSD-3 license, \"napari-deeplabcut\" is free and open source software. Issues If you encounter any problems, please file an issue along with a detailed description. Acknowledgements This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template. We thank the Chan Zuckerberg Initiative (CZI) for funding this work! ",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari DeepLabCut",
    "documentation": "https://github.com/DeepLabCut/napari-deeplabcut#README.md",
    "first_released": "2022-06-03T18:54:47.600892Z",
    "license": "BSD-3-Clause",
    "name": "napari-deeplabcut",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader", "writer", "widget"],
    "project_site": "https://github.com/DeepLabCut/napari-deeplabcut",
    "python_version": ">=3.8",
    "reader_file_extensions": [
      "*.avi",
      "*.yaml",
      "*",
      "*.mp4",
      "*.jpg",
      "*.jpeg",
      "*.h5",
      "*.png",
      "*.mov"
    ],
    "release_date": "2022-12-06T10:12:20.065869Z",
    "report_issues": "https://github.com/DeepLabCut/napari-deeplabcut/issues",
    "requirements": [
      "dask-image",
      "napari (==0.4.17rc8)",
      "numpy",
      "opencv-python-headless",
      "pandas",
      "pyyaml",
      "qtpy",
      "scikit-image",
      "tables",
      "napari ; extra == 'testing'",
      "pyside6 (<6.3.2) ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "tox ; extra == 'testing'"
    ],
    "summary": "napari + DeepLabCut annotation tool",
    "support": "https://github.com/DeepLabCut/napari-deeplabcut/issues",
    "twitter": "",
    "version": "0.0.9",
    "visibility": "public",
    "writer_file_extensions": [".h5", ".csv"],
    "writer_save_layers": ["points{1}", "shapes{1}"]
  },
  {
    "authors": [{ "name": "Jonas Windhager" }],
    "code_repository": "https://github.com/BodenmillerGroup/napari-hierarchical",
    "description": "# napari-hierarchical  [![License MIT](https://img.shields.io/pypi/l/napari-hierarchical.svg?color=green)](https://github.com/BodenmillerGroup/napari-hierarchical/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-hierarchical.svg?color=green)](https://pypi.org/project/napari-hierarchical) [![Python Version](https://img.shields.io/pypi/pyversions/napari-hierarchical.svg?color=green)](https://python.org) [![tests](https://github.com/BodenmillerGroup/napari-hierarchical/workflows/tests/badge.svg)](https://github.com/BodenmillerGroup/napari-hierarchical/actions) [![codecov](https://codecov.io/gh/BodenmillerGroup/napari-hierarchical/branch/main/graph/badge.svg)](https://codecov.io/gh/BodenmillerGroup/napari-hierarchical) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-hierarchical)](https://napari-hub.org/plugins/napari-hierarchical)  Hierarchical file format support for napari  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.   ## Installation  You can install `napari-hierarchical` via [pip]:      pip install \"napari-hierarchical[all]\"  To install latest development version :      pip install \"git+https://github.com/BodenmillerGroup/napari-hierarchical.git#egg=napari-hierarchical[all]\"  ## Usage  The plugin enables the reading, editing and writing of container formats. In the plugin, *groups* represent hierarchically structured collections of *arrays*. Each group can hold zero or more arrays and can have zero or more child groups (hierarchical structure). An array is a logical representation of (image) data on disk and directly corresponds to a napari layer when loaded.  Files can be opened through napari (e.g. `File -> Open File(s)` menu, `Viewer.open(...)` function), as the plugin implements napari's file reader hook. Upon opening a hierarchically structured file, the *Groups* and *Arrays* widgets are displayed. The *Groups* widget allows to browse and restructure the groups tree, while the *Arrays* widget groups arrays from the selected groups by file format-specific metadata (e.g. channel name for MCD files). Selecting arrays also selects the corresponding napari layers, allowing to adjust their properties.  Arrays can be loaded individually by toggling their *loaded* state (circular button), which will add napari layers for the corresponding arrays. Similarly, loaded arrays can be shown or hidden by toggling their *visible* state (eye button), which will toggle the visibility of the associated napari layers. The loaded/visible states of groups (collections of arrays) can be toggled in a similar fashion. Arrays are always loaded into memory (no memory mapping), to allow for editing the tree structure. Loaded root groups can be exported to supported hierarchical file formats.  Currently, reading/writing of HDF5 and Zarr (not: OME-NGFF) files are supported out of the box, as well as reading imaging mass cytometry (IMC) data (i.e., MCD files). For these file formats, sample data is available through the plugin. Additional readers/writers can be implemented using a pluggy-based interface, similar to the first generation `napari-plugin-engine`.  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.   ## License  Distributed under the terms of the [MIT] license, \"napari-hierarchical\" is free and open source software   ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/BodenmillerGroup/napari-hierarchical/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-hierarchical       Hierarchical file format support for napari  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template. Installation You can install napari-hierarchical via pip: pip install \"napari-hierarchical[all]\"  To install latest development version : pip install \"git+https://github.com/BodenmillerGroup/napari-hierarchical.git#egg=napari-hierarchical[all]\"  Usage The plugin enables the reading, editing and writing of container formats. In the plugin, groups represent hierarchically structured collections of arrays. Each group can hold zero or more arrays and can have zero or more child groups (hierarchical structure). An array is a logical representation of (image) data on disk and directly corresponds to a napari layer when loaded. Files can be opened through napari (e.g. File -> Open File(s) menu, Viewer.open(...) function), as the plugin implements napari's file reader hook. Upon opening a hierarchically structured file, the Groups and Arrays widgets are displayed. The Groups widget allows to browse and restructure the groups tree, while the Arrays widget groups arrays from the selected groups by file format-specific metadata (e.g. channel name for MCD files). Selecting arrays also selects the corresponding napari layers, allowing to adjust their properties. Arrays can be loaded individually by toggling their loaded state (circular button), which will add napari layers for the corresponding arrays. Similarly, loaded arrays can be shown or hidden by toggling their visible state (eye button), which will toggle the visibility of the associated napari layers. The loaded/visible states of groups (collections of arrays) can be toggled in a similar fashion. Arrays are always loaded into memory (no memory mapping), to allow for editing the tree structure. Loaded root groups can be exported to supported hierarchical file formats. Currently, reading/writing of HDF5 and Zarr (not: OME-NGFF) files are supported out of the box, as well as reading imaging mass cytometry (IMC) data (i.e., MCD files). For these file formats, sample data is available through the plugin. Additional readers/writers can be implemented using a pluggy-based interface, similar to the first generation napari-plugin-engine. Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the MIT license, \"napari-hierarchical\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": [],
    "display_name": "napari-hierarchical",
    "documentation": "https://github.com/BodenmillerGroup/napari-hierarchical#README.md",
    "first_released": "2022-12-21T16:24:46.716911Z",
    "license": "MIT",
    "name": "napari-hierarchical",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader", "widget", "sample_data"],
    "project_site": "https://github.com/BodenmillerGroup/napari-hierarchical",
    "python_version": "<3.11,>=3.8",
    "reader_file_extensions": ["*"],
    "release_date": "2022-12-21T16:24:46.716911Z",
    "report_issues": "https://github.com/BodenmillerGroup/napari-hierarchical/issues",
    "requirements": [
      "napari (<0.4.18,>=0.4.17)",
      "pluggy",
      "qtpy",
      "dask ; extra == 'all'",
      "h5py ; extra == 'all'",
      "readimc ; extra == 'all'",
      "s3fs ; extra == 'all'",
      "zarr ; extra == 'all'",
      "dask ; extra == 'hdf5'",
      "h5py ; extra == 'hdf5'",
      "dask ; extra == 'imc'",
      "readimc ; extra == 'imc'",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'",
      "dask ; extra == 'zarr'",
      "s3fs ; extra == 'zarr'",
      "zarr ; extra == 'zarr'"
    ],
    "summary": "Hierarchical file format support for napari",
    "support": "https://github.com/BodenmillerGroup/napari-hierarchical/issues",
    "twitter": "",
    "version": "0.1.0",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      { "email": "robert.haase@tu-dresden.de", "name": "Robert Haase" }
    ],
    "code_repository": "https://github.com/haesleinhuepf/napari-folder-browser",
    "description": "# napari-folder-browser  [![License](https://img.shields.io/pypi/l/napari-folder-browser.svg?color=green)](https://github.com/haesleinhuepf/napari-folder-browser/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-folder-browser.svg?color=green)](https://pypi.org/project/napari-folder-browser) [![Python Version](https://img.shields.io/pypi/pyversions/napari-folder-browser.svg?color=green)](https://python.org) [![tests](https://github.com/haesleinhuepf/napari-folder-browser/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-folder-browser/actions) [![codecov](https://codecov.io/gh/haesleinhuepf/napari-folder-browser/branch/master/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-folder-browser)  Browse folders of images and open them using double-click or <ENTER>. You can also navigate through the list using arrow up/down keys.  ![](https://github.com/haesleinhuepf/napari-folder-browser/raw/main/docs/napari-folder-browser.gif)  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.  ## Installation  You can install `napari-folder-browser` from within napari by clicking menu `Plugins > Install/uninstall Plugins...` and entering here: ![img.png](https://github.com/haesleinhuepf/napari-folder-browser/raw/main/docs/install.png)  You can install `napari-folder-browser` via [pip]:      pip install napari-folder-browser  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-folder-browser\" is free and open source software  ## Issues  If you encounter any problems, please create a thread on [image.sc] along with a detailed description and tag [@haesleinhuepf].  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/haesleinhuepf/napari-folder-browser/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ [image.sc]: https://image.sc [@haesleinhuepf]: https://twitter.com/haesleinhuepf    ",
    "description_content_type": "text/markdown",
    "description_text": "napari-folder-browser      Browse folders of images and open them using double-click or . You can also navigate through the list using arrow up/down keys.   This napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template. Installation You can install napari-folder-browser from within napari by clicking menu Plugins > Install/uninstall Plugins... and entering here:  You can install napari-folder-browser via pip: pip install napari-folder-browser  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-folder-browser\" is free and open source software Issues If you encounter any problems, please create a thread on image.sc along with a detailed description and tag @haesleinhuepf.",
    "development_status": ["Development Status :: 3 - Alpha"],
    "display_name": "napari-folder-browser",
    "documentation": "https://github.com/haesleinhuepf/napari-folder-browser#README.md",
    "first_released": "2021-10-03T13:57:14.143133Z",
    "license": "BSD-3-Clause",
    "name": "napari-folder-browser",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/haesleinhuepf/napari-folder-browser",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-01-01T15:55:38.504053Z",
    "report_issues": "https://github.com/haesleinhuepf/napari-folder-browser/issues",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "napari-tools-menu"
    ],
    "summary": "Browse folders of images and open them using double-click",
    "support": "https://github.com/haesleinhuepf/napari-folder-browser/issues",
    "twitter": "",
    "version": "0.1.3",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Anna Medyukhina", "orcid": "0000-0001-5268-713X" }],
    "code_repository": "https://github.com/amedyukhina/napari-filament-annotator",
    "conda": [],
    "description": "# 3D Filament Annotator  ## Summary  3D Filament Annotator is a tool for annotating filaments and other curvilinear structures in 3D.  The 3D annotation is done by annotating the filament in two different projections,  calculating intersection, and refining the filament position with active contours.  ![demo](https://raw.githubusercontent.com/amedyukhina/napari-filament-annotator/main/docs/demo_09.gif)  ## Intended Audience & Supported Data  The plugin is intended for annotation of filamentous structures from a 3D view.  The main use-case are structures that are not visible in a single-slice image due to being too thin or too low intensity.   No expertise in image analysis is required to use this plugin, though a basic knowledge of  active contours would be helpful to set the parameters.  The plugin expects single-channel 3D images as input. Time-series data are not yet supported.  ## Quickstart  **1. Open example image**  ![Open example image](https://raw.githubusercontent.com/amedyukhina/napari-filament-annotator/main/docs/demo_01.png)  **2. Start the 3D annotator plugin**  ![Start 3D annotator plugin](https://raw.githubusercontent.com/amedyukhina/napari-filament-annotator/main/docs/demo_02.png)  **3. Adjust image parameters**  Adjust the voxel size of the image and the Gaussian-smoothing sigma that will be used to smooth the image for active contour refinement of filament position.  - Voxel size in xy and z - Sigma um: smoothing sigma, microns (or the same units as used for the voxel size)  ![Adjust image parameters](https://raw.githubusercontent.com/amedyukhina/napari-filament-annotator/main/docs/demo_03.png)  **4. Add annotation layer**  Click the \"Add annotation layer\" button to add a new Shapes layer for annotation.  This step might take several seconds, depending on the image size, due to some filtering  that is performed behind the scenes.  ![Add annotation layer](https://raw.githubusercontent.com/amedyukhina/napari-filament-annotator/main/docs/demo_05.png)  **5. Annotate filaments**  1. Rotate the image to find a position, where the filament is clearly visible 2. Draw a line over the filament, by holding \"Control\" (or \"Command\" on macOS) and clicking with the mouse:    this will draw a polygon with potential filament locations 3. Rotate the image to view the filament from another angle and repeat step 2 4. Rotate the image again: this will calculate the filament position from the intersection of the two polygons 5. Repeat steps 1-4 for other filaments  Hot keys to edit the annotations:  - `p`: delete the last added point (during the polygon drawing) - `d`: delete the last added shape (polygon or filament) - `f`: delete the first point of the last added filament - `l`: delete the last point of the last added filament  ![Annotate](https://raw.githubusercontent.com/amedyukhina/napari-filament-annotator/main/docs/demo_09.gif)  **6. Save annotations**  Save final or intermediate annotations to a csv file.  There is an option to load previously annotated filaments and continue the annotation.  ![Save annotations](https://raw.githubusercontent.com/amedyukhina/napari-filament-annotator/main/docs/demo_10.png)  ## Documentation   For more details on the plugin functionality and tips for adjusting parameters, please refer to the [Filamenter Annotator Tutorial](https://github.com/amedyukhina/napari-filament-annotator/blob/main/docs/tutorial.md)  ## Getting Help  If you encounter any problems, please  [file an issue](https://github.com/amedyukhina/napari-filament-annotator/issues)  along with a detailed description.  If you have a question, you can ask it in the  [Discussion tab](https://github.com/amedyukhina/napari-filament-annotator/discussions)  of the project.  ## How to Cite  Anna Medyukhina. (2022). amedyukhina/napari-filament-annotator: Version 0.1 (v0.1). Zenodo.  https://doi.org/10.5281/zenodo.7145278  ",
    "description_content_type": "text/markdown",
    "description_text": "3D Filament Annotator Summary 3D Filament Annotator is a tool for annotating filaments and other curvilinear structures in 3D.  The 3D annotation is done by annotating the filament in two different projections,  calculating intersection, and refining the filament position with active contours.  Intended Audience & Supported Data The plugin is intended for annotation of filamentous structures from a 3D view.  The main use-case are structures that are not visible in a single-slice image due to being too thin or too low intensity.  No expertise in image analysis is required to use this plugin, though a basic knowledge of  active contours would be helpful to set the parameters. The plugin expects single-channel 3D images as input. Time-series data are not yet supported. Quickstart 1. Open example image  2. Start the 3D annotator plugin  3. Adjust image parameters Adjust the voxel size of the image and the Gaussian-smoothing sigma that will be used to smooth the image for active contour refinement of filament position.  Voxel size in xy and z Sigma um: smoothing sigma, microns (or the same units as used for the voxel size)   4. Add annotation layer Click the \"Add annotation layer\" button to add a new Shapes layer for annotation. This step might take several seconds, depending on the image size, due to some filtering  that is performed behind the scenes.  5. Annotate filaments  Rotate the image to find a position, where the filament is clearly visible Draw a line over the filament, by holding \"Control\" (or \"Command\" on macOS) and clicking with the mouse:    this will draw a polygon with potential filament locations Rotate the image to view the filament from another angle and repeat step 2 Rotate the image again: this will calculate the filament position from the intersection of the two polygons Repeat steps 1-4 for other filaments  Hot keys to edit the annotations:  p: delete the last added point (during the polygon drawing) d: delete the last added shape (polygon or filament) f: delete the first point of the last added filament l: delete the last point of the last added filament   6. Save annotations Save final or intermediate annotations to a csv file. There is an option to load previously annotated filaments and continue the annotation.  Documentation For more details on the plugin functionality and tips for adjusting parameters, please refer to the Filamenter Annotator Tutorial Getting Help If you encounter any problems, please  file an issue  along with a detailed description. If you have a question, you can ask it in the  Discussion tab  of the project. How to Cite Anna Medyukhina. (2022). amedyukhina/napari-filament-annotator: Version 0.1 (v0.1). Zenodo.  https://doi.org/10.5281/zenodo.7145278",
    "development_status": ["Development Status :: 3 - Alpha"],
    "display_name": "napari 3D filament annotator",
    "documentation": "https://github.com/amedyukhina/napari-filament-annotator#README.md",
    "first_released": "2022-10-04T21:47:29.936695Z",
    "license": "Apache-2.0",
    "name": "napari-filament-annotator",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget", "sample_data"],
    "project_site": "https://github.com/amedyukhina/napari-filament-annotator",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-10-06T19:55:40.774807Z",
    "report_issues": "https://github.com/amedyukhina/napari-filament-annotator/issues",
    "requirements": [
      "Geometry3D",
      "networkx",
      "numpy",
      "magicgui",
      "pandas",
      "qtpy",
      "scipy",
      "sklearn",
      "imageio (!=2.22.1)",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "summary": "Annotation of filaments / curvilinear structures in 3D",
    "support": "https://github.com/amedyukhina/napari-filament-annotator/issues",
    "twitter": "",
    "version": "0.1.2",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Lorenzo Gaifas" }],
    "code_repository": "https://github.com/brisvag/napari-gruvbox",
    "description": "# napari-gruvbox  [![License GNU GPL v3.0](https://img.shields.io/pypi/l/napari-gruvbox.svg?color=green)](https://github.com/brisvag/napari-gruvbox/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-gruvbox.svg?color=green)](https://pypi.org/project/napari-gruvbox) [![Python Version](https://img.shields.io/pypi/pyversions/napari-gruvbox.svg?color=green)](https://python.org) [![tests](https://github.com/brisvag/napari-gruvbox/workflows/tests/badge.svg)](https://github.com/brisvag/napari-gruvbox/actions) [![codecov](https://codecov.io/gh/brisvag/napari-gruvbox/branch/main/graph/badge.svg)](https://codecov.io/gh/brisvag/napari-gruvbox) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-gruvbox)](https://napari-hub.org/plugins/napari-gruvbox)  Gruvbox theme for napari. Colors are taken from the palette in https://github.com/morhetz/gruvbox.  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/stable/plugins/index.html -->  ## Installation  You can install `napari-gruvbox` via [pip]:      pip install napari-gruvbox    To install latest development version :      pip install git+https://github.com/brisvag/napari-gruvbox.git   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [GNU GPL v3.0] license, \"napari-gruvbox\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/brisvag/napari-gruvbox/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-gruvbox       Gruvbox theme for napari. Colors are taken from the palette in https://github.com/morhetz/gruvbox.  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install napari-gruvbox via pip: pip install napari-gruvbox  To install latest development version : pip install git+https://github.com/brisvag/napari-gruvbox.git  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the GNU GPL v3.0 license, \"napari-gruvbox\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari Gruvbox",
    "documentation": "https://github.com/brisvag/napari-gruvbox#README.md",
    "first_released": "2022-12-14T13:17:54.884662Z",
    "license": "GPL-3.0",
    "name": "napari-gruvbox",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["theme"],
    "project_site": "https://github.com/brisvag/napari-gruvbox",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-12-14T13:17:54.884662Z",
    "report_issues": "https://github.com/brisvag/napari-gruvbox/issues",
    "requirements": null,
    "summary": "Gruvbox theme for napari.",
    "support": "https://github.com/brisvag/napari-gruvbox/issues",
    "twitter": "",
    "version": "0.1.0",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Guillaume Witz" }],
    "code_repository": "https://github.com/guiwitz/napari-steinpose",
    "description": "# napari-steinpose  [![License BSD-3](https://img.shields.io/pypi/l/napari-steinpose.svg?color=green)](https://github.com/guiwitz/napari-steinpose/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-steinpose.svg?color=green)](https://pypi.org/project/napari-steinpose) [![Python Version](https://img.shields.io/pypi/pyversions/napari-steinpose.svg?color=green)](https://python.org) [![tests](https://github.com/guiwitz/napari-steinpose/workflows/tests/badge.svg)](https://github.com/guiwitz/napari-steinpose/actions) [![codecov](https://codecov.io/gh/guiwitz/napari-steinpose/branch/main/graph/badge.svg)](https://codecov.io/gh/guiwitz/napari-steinpose) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-steinpose)](https://napari-hub.org/plugins/napari-steinpose)  This napari plugin allows to segment and extract information from Imaging Mass Cytometry data by combining the [cellpose](http://www.cellpose.org/) and [steinbock](https://bodenmillergroup.github.io/steinbock/v0.14.2/) tools.  ## Installation  In order to use this plugin, whe highly recommend to create a specific environment and to install the required software in it. You can create a conda environment using:      conda create -n steinpose python=3.8.5 napari -c conda-forge  Then activate it and install the plugin:          conda activate steinpose     pip install napari-steinpose  ### Potential issue with PyTorch  Cellpose and therefore the plugin and napari can crash without warning in some cases with ```torch==1.12.0```. This can be fixed by reverting to an earlier version using:          pip install torch==1.11.0  ### GPU  In order to use a GPU:  1. Uninstall the PyTorch version that gets installed by default with Cellpose:          pip uninstall torch  2. Make sure your have up-to-date drivers for your NVIDIA card installed.  3. Re-install a GPU version of PyTorch via conda using a command that you can find [here](https://pytorch.org/get-started/locally/) (this takes care of the cuda toolkit, cudnn etc. so **no need to install manually anything more than the driver**). The command will look like this:          conda install pytorch torchvision cudatoolkit=11.3 -c pytorch  ### Plugin Updates  To update the plugin, you only need to activate the existing environment and install the new version:      conda activate steinpose     pip install git+https://github.com/guiwitz/napari-steinpose.git -U  ## Usage  Here is a short summary on how to proceed to use the plugin. For more detailed information, please visit [this page](https://guiwitz.github.io/napari-steinpose).  ### Load data  Using the \"Select data folder\" button, select a folder containing your .mcd files. The contents of the folder will appear in the List of images box. When you select one of the files it is loaded in the viewer. Using the ROI spinpox, you can change the roi (or acquisition) to be visualized. ### Segmentation  1. In the channels tab, choose the combination of channels to use to define images to segment. You can choose what type of projection (mean, min etc.) is used to combine channels. You can either select channels defining both cells and nuclei or just a single channel. **Note that if you want to just segment nuclei, you need to select them as \"cell channel\".**  2. To save the output, select a folder using the \"Select output folder\" button.  3. In the segmentation tab, pick a cellpose model to use. If you use one of the built-in models, you can specify the average diameter of objects to detect.  4. In the Options tab, you can set a few more options:    - cellpose options: you can adjust the flow threshold and cell probabilities. If cells are missing try to use higher values of flow threshold (close to 1) and lower values for the cell probabilities (around -6)    - segmentation options: you can decide to remove segmentation touching the image border, and you can also decide to expand the segmented objects by a fixed number of pixels. If a segmentation is displayed in the viewer, adjusting this parameter will live-adjust the mask.  5. You can first test the segmentation using the \"Run on current image\" button. Once segmentation is done, the corresponding mask is displayed. You can then run the segmentation over all ROIs of **all .mcd files** present in the folder by using the \"Run on folder\" button.  ### Post-processing  In the Segmentation tab, if you tick the box \"Run steinbock post-processing\", information will directly be extracted from images and masks at the end of segmentation. Processing is done via steinbock and generates files compatible with further downstream processing.  In the Export tab, you can select what type of information to export: object intensities, geometric properties and object neighbourhood. Note that if you have performed a segmentation without post-processing, you can still run post-processing using the \"Run steinbock postproc\" button.  ### Saving settings  To avoid having to re-type the same settings repeatedly, you can export a give configuration using the \"Export config\" button in the Options tab. This generates a human readable .yml file with: - segmentation options - channels selected for projections  The file is saved in the output folder. You can just copy the file in a new empty output folder to use it for an other analysis run. Once you select that folder containing a configuration file, you can import it with the \"Import config\" button. **Note that you need to have an image opened so that channels can be selected properly.** ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-steinpose\" is free and open source software  ## Authors  The author of this plugin is Guillaume Witz, Data Science Lab and Microscopy Imaging Center, University of Bern. This plugin is the result of a collaboration with the Imaging Mass Cytometry and Mass Cytometry Platform, University of Bern.  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/guiwitz/napari-steinpose/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-steinpose       This napari plugin allows to segment and extract information from Imaging Mass Cytometry data by combining the cellpose and steinbock tools. Installation In order to use this plugin, whe highly recommend to create a specific environment and to install the required software in it. You can create a conda environment using: conda create -n steinpose python=3.8.5 napari -c conda-forge  Then activate it and install the plugin: conda activate steinpose pip install napari-steinpose  Potential issue with PyTorch Cellpose and therefore the plugin and napari can crash without warning in some cases with torch==1.12.0. This can be fixed by reverting to an earlier version using: pip install torch==1.11.0  GPU In order to use a GPU:   Uninstall the PyTorch version that gets installed by default with Cellpose: pip uninstall torch    Make sure your have up-to-date drivers for your NVIDIA card installed.   Re-install a GPU version of PyTorch via conda using a command that you can find here (this takes care of the cuda toolkit, cudnn etc. so no need to install manually anything more than the driver). The command will look like this: conda install pytorch torchvision cudatoolkit=11.3 -c pytorch    Plugin Updates To update the plugin, you only need to activate the existing environment and install the new version: conda activate steinpose pip install git+https://github.com/guiwitz/napari-steinpose.git -U  Usage Here is a short summary on how to proceed to use the plugin. For more detailed information, please visit this page. Load data Using the \"Select data folder\" button, select a folder containing your .mcd files. The contents of the folder will appear in the List of images box. When you select one of the files it is loaded in the viewer. Using the ROI spinpox, you can change the roi (or acquisition) to be visualized. Segmentation   In the channels tab, choose the combination of channels to use to define images to segment. You can choose what type of projection (mean, min etc.) is used to combine channels. You can either select channels defining both cells and nuclei or just a single channel. Note that if you want to just segment nuclei, you need to select them as \"cell channel\".   To save the output, select a folder using the \"Select output folder\" button.   In the segmentation tab, pick a cellpose model to use. If you use one of the built-in models, you can specify the average diameter of objects to detect.   In the Options tab, you can set a few more options:  cellpose options: you can adjust the flow threshold and cell probabilities. If cells are missing try to use higher values of flow threshold (close to 1) and lower values for the cell probabilities (around -6)  segmentation options: you can decide to remove segmentation touching the image border, and you can also decide to expand the segmented objects by a fixed number of pixels. If a segmentation is displayed in the viewer, adjusting this parameter will live-adjust the mask.   You can first test the segmentation using the \"Run on current image\" button. Once segmentation is done, the corresponding mask is displayed. You can then run the segmentation over all ROIs of all .mcd files present in the folder by using the \"Run on folder\" button.   Post-processing In the Segmentation tab, if you tick the box \"Run steinbock post-processing\", information will directly be extracted from images and masks at the end of segmentation. Processing is done via steinbock and generates files compatible with further downstream processing. In the Export tab, you can select what type of information to export: object intensities, geometric properties and object neighbourhood. Note that if you have performed a segmentation without post-processing, you can still run post-processing using the \"Run steinbock postproc\" button. Saving settings To avoid having to re-type the same settings repeatedly, you can export a give configuration using the \"Export config\" button in the Options tab. This generates a human readable .yml file with: - segmentation options - channels selected for projections The file is saved in the output folder. You can just copy the file in a new empty output folder to use it for an other analysis run. Once you select that folder containing a configuration file, you can import it with the \"Import config\" button. Note that you need to have an image opened so that channels can be selected properly. Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-steinpose\" is free and open source software Authors The author of this plugin is Guillaume Witz, Data Science Lab and Microscopy Imaging Center, University of Bern. This plugin is the result of a collaboration with the Imaging Mass Cytometry and Mass Cytometry Platform, University of Bern. Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari Steinpose",
    "documentation": "https://github.com/guiwitz/napari-steinpose#README.md",
    "first_released": "2022-11-17T13:54:34.626169Z",
    "license": "BSD-3-Clause",
    "name": "napari-steinpose",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader", "widget"],
    "project_site": "https://github.com/guiwitz/napari-steinpose",
    "python_version": ">=3.8",
    "reader_file_extensions": ["*.mcd"],
    "release_date": "2022-11-17T13:54:34.626169Z",
    "report_issues": "https://github.com/guiwitz/napari-steinpose/issues",
    "requirements": [
      "torch (==1.11.0)",
      "cellpose",
      "numpy",
      "magicgui",
      "qtpy",
      "matplotlib",
      "readimc",
      "steinbock",
      "pandas",
      "aicsimageio",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'",
      "pytest-order ; extra == 'testing'"
    ],
    "summary": "A plugin to process Imaging Mass Cytometry data with cellpose and steinbock",
    "support": "https://github.com/guiwitz/napari-steinpose/issues",
    "twitter": "",
    "version": "0.1.0",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "email": "emma@emmazhou.com", "name": "Emma Zhou" }],
    "code_repository": "https://github.com/emmazhou/napari-dvid",
    "description": "# napari-dvid  [![License](https://img.shields.io/pypi/l/napari-dvid.svg?color=green)](https://github.com/emmazhou/napari-dvid/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-dvid.svg?color=green)](https://pypi.org/project/napari-dvid) [![Python Version](https://img.shields.io/pypi/pyversions/napari-dvid.svg?color=green)](https://python.org) [![tests](https://github.com/emmazhou/napari-dvid/workflows/tests/badge.svg)](https://github.com/emmazhou/napari-dvid/actions) [![codecov](https://codecov.io/gh/emmazhou/napari-dvid/branch/master/graph/badge.svg)](https://codecov.io/gh/emmazhou/napari-dvid)  DVID loader for napari, from a url  ---  This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/docs/plugins/index.html -->  ## Installation  You can install `napari-dvid` via [pip]:      pip install napari-dvid  ## Examples  Once installed, run `napari --with napari-dvid` to get the plugin sidebar:  ![Screenshot](screenshot.png)  Paste in a URL to a DVID volume and hit \"Load\" to load the volume! As an example, try:  `https://emdata.janelia.org/api/node/ab6e610d4/grayscale/raw/0_1_2/256_256_256/7500_7000_4400`  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [MIT] license, \"napari-dvid\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [mit]: http://opensource.org/licenses/MIT [bsd-3]: http://opensource.org/licenses/BSD-3-Clause [gnu gpl v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [gnu lgpl v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [apache software license 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [mozilla public license 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin [file an issue]: https://github.com/emmazhou/napari-dvid/issues [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [pypi]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-dvid      DVID loader for napari, from a url  This napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.  Installation You can install napari-dvid via pip: pip install napari-dvid  Examples Once installed, run napari --with napari-dvid to get the plugin sidebar:  Paste in a URL to a DVID volume and hit \"Load\" to load the volume! As an example, try: https://emdata.janelia.org/api/node/ab6e610d4/grayscale/raw/0_1_2/256_256_256/7500_7000_4400 Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the MIT license, \"napari-dvid\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari-dvid",
    "documentation": "https://github.com/emmazhou/napari-dvid#README.md",
    "first_released": "2021-06-09T21:18:41.905660Z",
    "license": "MIT",
    "name": "napari-dvid",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader", "widget"],
    "project_site": "https://github.com/emmazhou/napari-dvid",
    "python_version": ">=3.7",
    "reader_file_extensions": ["*"],
    "release_date": "2021-06-09T21:45:26.613386Z",
    "report_issues": "https://github.com/emmazhou/napari-dvid/issues",
    "requirements": ["napari-plugin-engine (>=0.1.4)", "numpy", "requests"],
    "summary": "DVID loader for napari, from a url",
    "support": "https://github.com/emmazhou/napari-dvid/issues",
    "twitter": "",
    "version": "0.2.0",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Hanjin Liu" }],
    "code_repository": "https://github.com/hanjinliu/napari-filaments",
    "conda": [{ "channel": "conda-forge", "package": "napari-filaments" }],
    "description": "# napari-filaments  [![License BSD-3](https://img.shields.io/pypi/l/napari-filaments.svg?color=green)](https://github.com/hanjinliu/napari-filaments/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-filaments.svg?color=green)](https://pypi.org/project/napari-filaments) [![Python Version](https://img.shields.io/pypi/pyversions/napari-filaments.svg?color=green)](https://python.org) [![tests](https://github.com/hanjinliu/napari-filaments/workflows/tests/badge.svg)](https://github.com/hanjinliu/napari-filaments/actions) [![codecov](https://codecov.io/gh/hanjinliu/napari-filaments/branch/main/graph/badge.svg)](https://codecov.io/gh/hanjinliu/napari-filaments) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-filaments)](https://napari-hub.org/plugins/napari-filaments)  A napari plugin for filament analysis.  This plugin helps you to manually track filaments using path shapes of `Shapes` layer.  ![](https://github.com/hanjinliu/napari-filaments/raw/main/resources/fit.gif)  Currently implemented functions  - Fit paths to filaments in an image as a 2-D spline curve. - Clip/extend paths. - Measurement of filament length at sub-pixel precision. - Basic quantification (mean, std, etc.) along paths. - Import paths from ImageJ ROI file.  Basic Usage -----------  Click `Layers > open image` to open a tif file. You'll find the image you chose and a shapes layer are added to the layer list.  ![](https://github.com/hanjinliu/napari-filaments/raw/main/resources/fig.png)  - The \"target filaments\" box shows the working shapes layer. - The \"target image\" box shows the image layer on which fitting and quantification will be conducted. - The \"filament\" box shows currently selected shape (initially this box is empty).  Add path shapes and push key `F1` to fit the shape to the filament in the target image layer.   ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.   ## Installation  You can install `napari-filaments` via [pip]:      pip install napari-filaments    To install latest development version :      pip install git+https://github.com/hanjinliu/napari-filaments.git   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-filaments\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/hanjinliu/napari-filaments/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-filaments       A napari plugin for filament analysis. This plugin helps you to manually track filaments using path shapes of Shapes layer.  Currently implemented functions  Fit paths to filaments in an image as a 2-D spline curve. Clip/extend paths. Measurement of filament length at sub-pixel precision. Basic quantification (mean, std, etc.) along paths. Import paths from ImageJ ROI file.  Basic Usage Click Layers > open image to open a tif file. You'll find the image you chose and a shapes layer are added to the layer list.   The \"target filaments\" box shows the working shapes layer. The \"target image\" box shows the image layer on which fitting and quantification will be conducted. The \"filament\" box shows currently selected shape (initially this box is empty).  Add path shapes and push key F1 to fit the shape to the filament in the target image layer.  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template. Installation You can install napari-filaments via pip: pip install napari-filaments  To install latest development version : pip install git+https://github.com/hanjinliu/napari-filaments.git  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-filaments\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari filaments",
    "documentation": "https://github.com/hanjinliu/napari-filaments#README.md",
    "first_released": "2022-07-01T08:02:15.100050Z",
    "license": "BSD-3-Clause",
    "name": "napari-filaments",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/hanjinliu/napari-filaments",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-07-16T06:24:33.490837Z",
    "report_issues": "https://github.com/hanjinliu/napari-filaments/issues",
    "requirements": [
      "magic-class (>=0.6.7)",
      "magicgui",
      "matplotlib",
      "numpy",
      "qtpy",
      "scipy",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "tox ; extra == 'testing'"
    ],
    "summary": "A napari plugin for filament analysis",
    "support": "https://github.com/hanjinliu/napari-filaments/issues",
    "twitter": "",
    "version": "0.2.1",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Hiroki Kawai", "orcid": "0000-0002-7129-2384" }],
    "code_repository": "https://github.com/hiroalchem/napari-labelimg4classification",
    "description": "# napari-labelimg4classification  [![License](https://img.shields.io/pypi/l/napari-labelimg4classification.svg?color=green)](https://github.com/hiroalchem/napari-labelimg4classification/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-labelimg4classification.svg?color=green)](https://pypi.org/project/napari-labelimg4classification) [![Python Version](https://img.shields.io/pypi/pyversions/napari-labelimg4classification.svg?color=green)](https://python.org) [![tests](https://github.com/hiroalchem/napari-labelimg4classification/workflows/tests/badge.svg)](https://github.com/hiroalchem/napari-labelimg4classification/actions) [![codecov](https://codecov.io/gh/hiroalchem/napari-labelimg4classification/branch/main/graph/badge.svg)](https://codecov.io/gh/hiroalchem/napari-labelimg4classification) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-labelimg4classification)](https://napari-hub.org/plugins/napari-labelimg4classification)  A simple image-level annotation tool supporting multi-channel images for napari.  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  ## Usage Start the labeling tool from the menu `Utilities > label tool for classification`.    First, click on the Choose directory button to open the folder selection window, and select the folder that contains the  images you want to label and annotate.    It will automatically list and display the images of tif, png, jpg, and bmp formats. If you want to view the channels of a multi-channel image separately, check the split channels checkbox. ![](https://github.com/hiroalchem/napari-labelimg4classification/raw/main/docs/open.gif)  Initially, all channels will be opened in grayscale, but the pseudo-color and contrast adjustments you specified will be  carried over when you open the next image.    Thanks to napari, you can freely merge channels and turn them on and off.    Label classes can be added, and can be removed by typing the same name as an already added class. ![](https://github.com/hiroalchem/napari-labelimg4classification/raw/main/docs/color_and_label.gif)   It will automatically save the labels.csv file with the image path and label, and the class.txt file with the class of the label. ![](https://github.com/hiroalchem/napari-labelimg4classification/raw/main/docs/class_and_labels.png)  If labels.csv and class.txt are already in the folder, they will be loaded and reflected automatically. ![](https://github.com/hiroalchem/napari-labelimg4classification/raw/main/docs/reopen.gif)  ## Installation  You can install `napari-labelimg4classification` via [pip]:      pip install napari-labelimg4classification    To install latest development version :      pip install git+https://github.com/hiroalchem/napari-labelimg4classification.git   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [MIT] license, \"napari-labelimg4classification\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/hiroalchem/napari-labelimg4classification/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-labelimg4classification       A simple image-level annotation tool supporting multi-channel images for napari.  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template. Usage Start the labeling tool from the menu Utilities > label tool for classification.  First, click on the Choose directory button to open the folder selection window, and select the folder that contains the  images you want to label and annotate.  It will automatically list and display the images of tif, png, jpg, and bmp formats. If you want to view the channels of a multi-channel image separately, check the split channels checkbox.  Initially, all channels will be opened in grayscale, but the pseudo-color and contrast adjustments you specified will be  carried over when you open the next image.  Thanks to napari, you can freely merge channels and turn them on and off.  Label classes can be added, and can be removed by typing the same name as an already added class.  It will automatically save the labels.csv file with the image path and label, and the class.txt file with the class of the label.  If labels.csv and class.txt are already in the folder, they will be loaded and reflected automatically.  Installation You can install napari-labelimg4classification via pip: pip install napari-labelimg4classification  To install latest development version : pip install git+https://github.com/hiroalchem/napari-labelimg4classification.git  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the MIT license, \"napari-labelimg4classification\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari-labelimg4classification",
    "documentation": "https://github.com/hiroalchem/napari-labelimg4classification#README.md",
    "first_released": "2021-12-02T02:23:27.798505Z",
    "license": "MIT",
    "name": "napari-labelimg4classification",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/hiroalchem/napari-labelimg4classification",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2021-12-03T01:44:21.284216Z",
    "report_issues": "https://github.com/hiroalchem/napari-labelimg4classification/issues",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "napari",
      "numpy",
      "napari-tools-menu",
      "pandas"
    ],
    "summary": "A simple image-level annotation tool supporting multi-channel images.",
    "support": "https://github.com/hiroalchem/napari-labelimg4classification/issues",
    "twitter": "",
    "version": "0.1.1",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Robin Koch" }, { "name": "Marc Boucsein" }],
    "code_repository": "https://github.com/RobAnKo/napari-image-stacker",
    "conda": [{ "channel": "conda-forge", "package": "napari-image-stacker" }],
    "description": "# napari-image-stacker  [![License](https://img.shields.io/pypi/l/napari-image-stacker.svg?color=green)](https://github.com/RobAnKo/napari-image-stacker/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-image-stacker.svg?color=green)](https://pypi.org/project/napari-image-stacker) [![Python Version](https://img.shields.io/pypi/pyversions/napari-image-stacker.svg?color=green)](https://python.org) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-image-stacker)](https://napari-hub.org/plugins/napari-image-stacker)  A plugin designed to convert multiple open layers into a stack or vice versa  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.   ## Installation  You can install `napari-image-stacker` via [pip]:      pip install napari-image-stacker    To install latest development version :      pip install git+https://github.com/RobAnKo/napari-image-stacker.git   ## Contributing  Contributions are very welcome. Tests can be run with [tox].  ## License  Distributed under the terms of the [BSD-3] license, \"napari-image-stacker\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin [file an issue]: https://github.com/RobAnKo/napari-image-stacker/issues [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-image-stacker     A plugin designed to convert multiple open layers into a stack or vice versa  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template. Installation You can install napari-image-stacker via pip: pip install napari-image-stacker  To install latest development version : pip install git+https://github.com/RobAnKo/napari-image-stacker.git  Contributing Contributions are very welcome. Tests can be run with tox. License Distributed under the terms of the BSD-3 license, \"napari-image-stacker\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari-image-stacker",
    "documentation": "https://github.com/RobAnKo/napari-image-stacker#README.md",
    "first_released": "2022-01-11T16:09:57.895727Z",
    "license": "BSD-3-Clause",
    "name": "napari-image-stacker",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/RobAnKo/napari-image-stacker",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-02-04T13:59:14.581478Z",
    "report_issues": "https://github.com/RobAnKo/napari-image-stacker/issues",
    "requirements": ["napari-plugin-engine (>=0.1.4)", "numpy"],
    "summary": "A plugin designed to convert multiple open layers into a stack or vice versa",
    "support": "https://github.com/RobAnKo/napari-image-stacker/issues",
    "twitter": "",
    "version": "0.1.10",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      { "name": "Pranjal Dhole" },
      { "name": "Duway Nicolas Lesmes Leon" }
    ],
    "code_repository": "https://github.com/yapic/napari-hdf5-labels-io",
    "description": "# Description  This IO plugin lets you to store your progress in a single file (.h5 extension). It stores not only the layer's data but also its metadata, meaning that in some way, this IO can be seen as a project file generator.  The current supported layer types are: images, labels and dots.  This plugin was developed to create a connection between napari (for labeling) and YAPiC (a deep learning segmentation tool).  # Who is this for?  This plugin is meant to be used for any napari user wanting to store their progress in a single file and for those which use napari as a labeling tool.  It supports any data dimensionality and it was designed to improve the memory efficiency when storing label layers.  Additionally, YAPiC supports the files generated by this IO to perform image segmentation.  # Quick start  ## Saving .h5 files  With `napari-hdf5-labels-io installed`, use napari as alway. once you are done, click in File, click in Save Selected Layer(s)... (Ctrl+S) or Save All Layers... (Ctrl+Shift+S) and write the output file name as `filename.h5`. Including the \".h5\" extension at the end of the name will automatically activate the plugin.  ## Opening .h5 files  To open a .h5 file written with this plugin, you can open this file as any other (either by the Open File option in the File menu or dragging it to the main window).",
    "description_content_type": "text/markdown",
    "description_text": "Description This IO plugin lets you to store your progress in a single file (.h5 extension). It stores not only the layer's data but also its metadata, meaning that in some way, this IO can be seen as a project file generator. The current supported layer types are: images, labels and dots. This plugin was developed to create a connection between napari (for labeling) and YAPiC (a deep learning segmentation tool). Who is this for? This plugin is meant to be used for any napari user wanting to store their progress in a single file and for those which use napari as a labeling tool. It supports any data dimensionality and it was designed to improve the memory efficiency when storing label layers. Additionally, YAPiC supports the files generated by this IO to perform image segmentation. Quick start Saving .h5 files With napari-hdf5-labels-io installed, use napari as alway. once you are done, click in File, click in Save Selected Layer(s)... (Ctrl+S) or Save All Layers... (Ctrl+Shift+S) and write the output file name as filename.h5. Including the \".h5\" extension at the end of the name will automatically activate the plugin. Opening .h5 files To open a .h5 file written with this plugin, you can open this file as any other (either by the Open File option in the File menu or dragging it to the main window).",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "napari-hdf5-labels-io",
    "documentation": "https://yapic.github.io/napari-hdf5-labels-io/",
    "first_released": "2021-03-04T09:44:01.947204Z",
    "license": "GPL-3.0",
    "name": "napari-hdf5-labels-io",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader", "writer"],
    "project_site": "https://yapic.github.io/napari-hdf5-labels-io/",
    "python_version": "<3.9",
    "reader_file_extensions": ["*"],
    "release_date": "2021-11-30T06:00:41.776331Z",
    "report_issues": "https://github.com/yapic/napari-hdf5-labels-io/issues",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "typing",
      "numpy",
      "sparse",
      "h5py (==2.10.0)",
      "zarr"
    ],
    "summary": "Napari plugin to store set of layers in a .h5 file. Label layer are stored in a sparse representation.",
    "support": "",
    "twitter": "",
    "version": "0.3.dev16",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": ["labels", "points", "image"]
  },
  {
    "authors": [{ "email": "trevor.j.manz@gmail.com", "name": "Trevor Manz" }],
    "code_repository": "https://github.com/manzt/napari-dzi-zarr",
    "description": "# napari-dzi-zarr  [![License](https://img.shields.io/pypi/l/napari-dzi-zarr.svg?color=green)](https://github.com/napari/napari-dzi-zarr/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-dzi-zarr.svg?color=green)](https://pypi.org/project/napari-dzi-zarr) [![Python Version](https://img.shields.io/pypi/pyversions/napari-dzi-zarr.svg?color=green)](https://python.org) [![tests](https://github.com/manzt/napari-dzi-zarr/workflows/tests/badge.svg)](https://github.com/manzt/napari-dzi-zarr/actions)  An experimental plugin for viewing Deep Zoom Images (DZI) with napari + zarr + dask.  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.  ## Description   The [DZI File Format](https://github.com/openseadragon/openseadragon/wiki/The-DZI-File-Format)  is a pyramidal tile source specification where individual tiles are RGB/RGBA JPEG/PNG images.  DZI is a very popular tile source for zoomable web-viewers like  [OpenSeadragon](https://openseadragon.github.io/), and as a result many tile sources are available over  HTTP. This plugin wraps a DZI tile source (local or remote) as a multiscale Zarr, where each pyramidal level is a `zarr.Array` of shape `(level_height, level_width, 3/4)`, allowing the same images to be viewed  in `napari` + `dask`.  ## Installation  You can install `napari-dzi-zarr` via [pip]:      pip install napari-dzi-zarr   ## Usage  This high-resolution scan of Rembrandt's Night Watch is available thanks to [R.G Erdmann](https://twitter.com/erdmann). More examples can be found on [boschproject.org](http://boschproject.org).      $ napari http://hyper-resolution.org/dzi/Rijksmuseum/SK-C-5/SK-C-5_VIS_20-um_2019-12-21.dzi  ![Rembrandt's Night Watch in napari](./night_watch_napari.png)  ## Contributing  Contributions are very welcome. Tests can be run with [tox].  ## License  Distributed under the terms of the [BSD-3] license, \"napari-dzi-zarr\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin [file an issue]: https://github.com/manzt/napari-dzi-zarr/issues [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-dzi-zarr     An experimental plugin for viewing Deep Zoom Images (DZI) with napari + zarr + dask.  This napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template. Description The DZI File Format  is a pyramidal tile source specification where individual tiles are RGB/RGBA JPEG/PNG images.  DZI is a very popular tile source for zoomable web-viewers like  OpenSeadragon, and as a result many tile sources are available over  HTTP. This plugin wraps a DZI tile source (local or remote) as a multiscale Zarr, where each pyramidal level is a zarr.Array of shape (level_height, level_width, 3/4), allowing the same images to be viewed  in napari + dask. Installation You can install napari-dzi-zarr via pip: pip install napari-dzi-zarr  Usage This high-resolution scan of Rembrandt's Night Watch is available thanks to R.G Erdmann. More examples can be found on boschproject.org. $ napari http://hyper-resolution.org/dzi/Rijksmuseum/SK-C-5/SK-C-5_VIS_20-um_2019-12-21.dzi   Contributing Contributions are very welcome. Tests can be run with tox. License Distributed under the terms of the BSD-3 license, \"napari-dzi-zarr\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "napari-dzi-zarr",
    "documentation": "",
    "first_released": "2020-08-20T17:18:30.022784Z",
    "license": "BSD-3-Clause",
    "name": "napari-dzi-zarr",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader"],
    "project_site": "https://github.com/manzt/napari-dzi-zarr",
    "python_version": ">=3.6",
    "reader_file_extensions": ["*"],
    "release_date": "2021-04-06T14:13:16.654713Z",
    "report_issues": "",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy (>=0.1.19)",
      "zarr (>=0.2.4)",
      "dask[array] (>=2.23.0)",
      "fsspec (>=0.8.0)",
      "requests (>=2.24.0)",
      "aiohttp (>=3.6.2)",
      "imageio (>=2.9.0)"
    ],
    "summary": "An experimental plugin for viewing Deep Zoom Images (DZI) with napari and zarr.",
    "support": "",
    "twitter": "",
    "version": "0.1.2",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Tim Morello" }],
    "code_repository": "https://github.com/tdmorello/napari-geojson",
    "description": "# napari-geojson  [![License](https://img.shields.io/pypi/l/napari-geojson.svg?color=green)](https://github.com/tdmorello/napari-geojson/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-geojson.svg?color=green)](https://pypi.org/project/napari-geojson) [![Python Version](https://img.shields.io/pypi/pyversions/napari-geojson.svg?color=green)](https://python.org) [![tests](https://github.com/tdmorello/napari-geojson/workflows/tests/badge.svg)](https://github.com/tdmorello/napari-geojson/actions) [![codecov](https://codecov.io/gh/tdmorello/napari-geojson/branch/main/graph/badge.svg)](https://codecov.io/gh/tdmorello/napari-geojson) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-geojson)](https://napari-hub.org/plugins/napari-geojson)  Read and write geojson files in napari.  ![](https://github.com/tdmorello/napari-geojson/raw/main/resources/output.gif)  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/plugins/stable/index.html -->  ## Installation  You can install `napari-geojson` via [pip]:      pip install napari-geojson    To install latest development version :      pip install git+https://github.com/tdmorello/napari-geojson.git   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-geojson\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/tdmorello/napari-geojson/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-geojson       Read and write geojson files in napari.   This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install napari-geojson via pip: pip install napari-geojson  To install latest development version : pip install git+https://github.com/tdmorello/napari-geojson.git  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-geojson\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 3 - Alpha"],
    "display_name": "napari-geojson",
    "documentation": "https://github.com/tdmorello/napari-geojson#README.md",
    "first_released": "2021-12-27T22:49:46.771804Z",
    "license": "BSD-3-Clause",
    "name": "napari-geojson",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader", "writer"],
    "project_site": "https://github.com/tdmorello/napari-geojson",
    "python_version": ">=3.8",
    "reader_file_extensions": ["*.geojson"],
    "release_date": "2022-11-10T15:27:48.589387Z",
    "report_issues": "https://github.com/tdmorello/napari-geojson/issues",
    "requirements": [
      "geojson",
      "numpy",
      "black ; extra == 'dev'",
      "flake8 ; extra == 'dev'",
      "flake8-black ; extra == 'dev'",
      "flake8-docstrings ; extra == 'dev'",
      "flake8-isort ; extra == 'dev'",
      "isort ; extra == 'dev'",
      "mypy ; extra == 'dev'",
      "pre-commit ; extra == 'dev'",
      "pytest ; extra == 'dev'",
      "pytest-cov ; extra == 'dev'",
      "pytest-qt ; extra == 'dev'",
      "tox ; extra == 'dev'",
      "napari[all] ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "tox ; extra == 'testing'"
    ],
    "summary": "Read and write geojson files in napari",
    "support": "https://github.com/tdmorello/napari-geojson/issues",
    "twitter": "",
    "version": "0.1.3",
    "visibility": "public",
    "writer_file_extensions": [".geojson"],
    "writer_save_layers": ["points*", "shapes*"]
  },
  {
    "authors": [{ "name": "Robert Haase" }],
    "category": {
      "Supported data": ["2D", "3D"],
      "Workflow step": [
        "Image Segmentation",
        "Image correction",
        "Image reconstruction",
        "Image enhancement",
        "Object feature extraction",
        "Morphological operations",
        "Image feature detection"
      ]
    },
    "category_hierarchy": {
      "Supported data": [["2D"], ["3D"]],
      "Workflow step": [
        ["Image Segmentation", "Cell segmentation"],
        ["Image Segmentation", "Connected-component analysis"],
        ["Image correction", "Illumination correction"],
        ["Image correction"],
        ["Image reconstruction", "Image denoising"],
        ["Image enhancement", "Image denoising"],
        ["Image Segmentation", "Image thresholding"],
        ["Image Segmentation"],
        ["Object feature extraction"],
        ["Image enhancement", "Smoothing"],
        ["Morphological operations"],
        ["Image feature detection"],
        ["Image Segmentation", "Semi-automatic segmentation"],
        ["Image feature detection", "Edge detection"],
        ["Morphological operations", "Top-hat transform"],
        ["Morphological operations", "Closing"],
        ["Morphological operations", "Dilation"],
        ["Morphological operations", "Opening"],
        ["Morphological operations", "Erosion"],
        ["Image enhancement"]
      ]
    },
    "code_repository": "https://github.com/haesleinhuepf/napari-simpleitk-image-processing",
    "conda": [
      {
        "channel": "conda-forge",
        "package": "napari-simpleitk-image-processing"
      }
    ],
    "description": "# napari-simpleitk-image-processing (n-SimpleITK)  [![License](https://img.shields.io/pypi/l/napari-simpleitk-image-processing.svg?color=green)](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-simpleitk-image-processing.svg?color=green)](https://pypi.org/project/napari-simpleitk-image-processing) [![Python Version](https://img.shields.io/pypi/pyversions/napari-simpleitk-image-processing.svg?color=green)](https://python.org) [![tests](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/actions) [![codecov](https://codecov.io/gh/haesleinhuepf/napari-simpleitk-image-processing/branch/main/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-simpleitk-image-processing) [![Development Status](https://img.shields.io/pypi/status/napari-simpleitk-image-processing.svg)](https://en.wikipedia.org/wiki/Software_release_life_cycle#Alpha) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-simpleitk-image-processing)](https://napari-hub.org/plugins/napari-simpleitk-image-processing) [![DOI](https://zenodo.org/badge/432729955.svg)](https://zenodo.org/badge/latestdoi/432729955)  Process images using [SimpleITK](https://simpleitk.org/) in [napari]  ## Usage  Filters, segmentation algorithms and measurements provided by this napari plugin can be found in the `Tools` menu.  You can recognize them with their suffix `(n-SimpleITK)` in brackets. Furthermore, it can be used from the [napari-assistant](https://www.napari-hub.org/plugins/napari-assistant) graphical user interface.  Therefore, just click the menu `Tools > Utilities > Assistant (na)` or run `naparia` from the command line.  ![img.png](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/raw/main/docs/screenshot_with_assistant.png)  All filters implemented in this napari plugin are also demonstrated in [this notebook](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/blob/main/docs/demo.ipynb).  ### Gaussian blur  Applies a [Gaussian blur](https://en.wikipedia.org/wiki/Gaussian_blur) to an image. This might be useful for denoising, e.g. before applying the Threshold-Otsu method.  ![img.png](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/raw/main/docs/gaussian_blur.png)  ### Median filter  Applies a [median filter](https://en.wikipedia.org/wiki/Median_filter) to an image.  Compared to the Gaussian blur this method preserves edges in the image better.  It also performs slower.  ![img.png](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/raw/main/docs/median_filter.png)  ### Bilateral filter  The [bilateral filter](https://en.wikipedia.org/wiki/Bilateral_filter) allows denoising an image while preserving edges.  ![img.png](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/raw/main/docs/bilateral.png)  ### Threshold Otsu  Binarizes an image using [Otsu's method](https://ieeexplore.ieee.org/document/4310076).  ![img.png](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/raw/main/docs/threshold_otsu.png)  ### Connected Component Labeling  Takes a binary image and labels all objects with individual numbers to produce a label image.  ![img.png](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/raw/main/docs/connected_component_labeling.png)  ### Measurements  This function allows determining intensity and shape statistics from labeled images.  ![img.png](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/raw/main/docs/measurements.png)  ### Signed Maurer distance map  A distance map (more precise: [Signed Maurer Distance Map](https://itk.org/ITKExamples/src/Filtering/DistanceMap/MaurerDistanceMapOfBinary/Documentation.html)) can be useful for visualizing distances within binary images between black/white borders.  Positive values in this image correspond to a white (value=1) pixel's distance to the next black pixel. Black pixel's (value=0) distance to the next white pixel are represented in this map with negative values.  ![img.png](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/raw/main/docs/signed_maured_distance_map.png)  ### Binary fill holes  Fills holes in a binary image.  ![img.png](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/raw/main/docs/binary_fill_holes.png)  ### Touching objects labeling  Starting from a binary image, touching objects can be splits into multiple regions, similar to the [Watershed segmentation in ImageJ](https://imagej.net/plugins/classic-watershed).  ![img.png](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/raw/main/docs/Touching_object_labeling.png)  ### Morphological Watershed  The [morhological watershed](http://insightsoftwareconsortium.github.io/SimpleITK-Notebooks/Python_html/32_Watersheds_Segmentation.html) allows to segment images showing membranes. Before segmentation, a filter such as the Gaussian blur or a median filter should be used to eliminate noise. It also makes sense to increase the thickness of membranes using a maximum filter.  See [this notebook](https://github.com/clEsperanto/pyclesperanto_prototype/blob/master/demo/segmentation/segmentation_2d_membranes.ipynb) for details.  ![img.png](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/raw/main/docs/morphological_watershed.png)  ### Watershed-Otsu-Labeling  This algorithm uses [Otsu's thresholding method](https://ieeexplore.ieee.org/document/4310076) in combination with  [Gaussian blur](https://en.wikipedia.org/wiki/Gaussian_blur) and the  [Watershed-algorithm](https://en.wikipedia.org/wiki/Watershed_(image_processing))  approach to label bright objects such as nuclei in an intensity image. The alogrithm has two sigma parameters and a  level parameter which allow you to fine-tune where objects should be cut (`spot_sigma`) and how smooth outlines  should be (`outline_sigma`). The `watershed_level` parameter determines how deep an intensity valley between two maxima  has to be to differentiate the two maxima.  This implementation is similar to [Voronoi-Otsu-Labeling in clesperanto](https://github.com/clEsperanto/pyclesperanto_prototype/blob/master/demo/segmentation/voronoi_otsu_labeling.ipynb).   ![img.png](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/raw/main/docs/watershed_otsu_labeling.png)  ### Richardson-Lucy Deconvolution  [Richardson-Lucy deconvolution](https://en.wikipedia.org/wiki/Richardson%E2%80%93Lucy_deconvolution) allows to restore image quality if the point-spread-function of the optical system used  for acquisition is known or can be approximated.  ![img.png](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/raw/main/docs/Richardson-Lucy-Deconvolution.png)   ## Installation  You can install `napari-simpleitk-image-processing` via using `conda` and `pip`. If you have never used `conda` before, please go through [this tutorial](https://biapol.github.io/blog/johannes_mueller/anaconda_getting_started/) first.      conda install -c conda-forge napari     pip install napari-simpleitk-image-processing  ## See also  There are other napari plugins with similar functionality for processing images and extracting features: * [morphometrics](https://www.napari-hub.org/plugins/morphometrics) * [PartSeg](https://www.napari-hub.org/plugins/PartSeg) * [napari-skimage-regionprops](https://www.napari-hub.org/plugins/napari-skimage-regionprops) * [napari-cupy-image-processing](https://www.napari-hub.org/plugins/napari-cupy-image-processing) * [napari-pyclesperanto-assistant](https://www.napari-hub.org/plugins/napari-pyclesperanto-assistant) * [napari-allencell-segmenter](https://napari-hub.org/plugins/napari-allencell-segmenter) * [RedLionfish](https://www.napari-hub.org/plugins/RedLionfish) * [bbii-decon](https://www.napari-hub.org/plugins/bbii-decon)   * [napari-segment-blobs-and-things-with-membranes](https://www.napari-hub.org/plugins/napari-segment-blobs-and-things-with-membranes)  Furthermore, there are plugins for postprocessing extracted measurements * [napari-feature-classifier](https://www.napari-hub.org/plugins/napari-feature-classifier) * [napari-clusters-plotter](https://www.napari-hub.org/plugins/napari-clusters-plotter)  ## Contributing  Contributions are very welcome. There are many useful algorithms available in  [SimpleITK](https://simpleitk.org/). If you want another one available here in this napari plugin, don't hesitate to send a [pull-request](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/pulls). This repository just holds wrappers for SimpleITK-functions, see [this file](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/raw/main/src/napari_simpleitk_image_processing/_simpleitk_image_processing.py#L51) for how those wrappers can be written.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-simpleitk-image-processing\" is free and open source software  ## Citation  For implementing this napari plugin, the  [SimpleITK python notebooks](https://insightsoftwareconsortium.github.io/SimpleITK-Notebooks/) were very helpful.  Thus, if you find the plugin useful, consider citing the SimpleITK notebooks:  Z. Yaniv, B. C. Lowekamp, H. J. Johnson, R. Beare,  \"SimpleITK Image-Analysis Notebooks: a Collaborative Environment for Education and Reproducible Research\", \\\\ J Digit Imaging., 31(3): 290-303, 2018, [https://doi.org/10.1007/s10278-017-0037-8](https://doi.org/10.1007/s10278-017-0037-8).  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/haesleinhuepf/napari-simpleitk-image-processing/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-simpleitk-image-processing (n-SimpleITK)         Process images using SimpleITK in napari Usage Filters, segmentation algorithms and measurements provided by this napari plugin can be found in the Tools menu.  You can recognize them with their suffix (n-SimpleITK) in brackets. Furthermore, it can be used from the napari-assistant graphical user interface.  Therefore, just click the menu Tools > Utilities > Assistant (na) or run naparia from the command line.  All filters implemented in this napari plugin are also demonstrated in this notebook. Gaussian blur Applies a Gaussian blur to an image. This might be useful for denoising, e.g. before applying the Threshold-Otsu method.  Median filter Applies a median filter to an image.  Compared to the Gaussian blur this method preserves edges in the image better.  It also performs slower.  Bilateral filter The bilateral filter allows denoising an image while preserving edges.  Threshold Otsu Binarizes an image using Otsu's method.  Connected Component Labeling Takes a binary image and labels all objects with individual numbers to produce a label image.  Measurements This function allows determining intensity and shape statistics from labeled images.  Signed Maurer distance map A distance map (more precise: Signed Maurer Distance Map) can be useful for visualizing distances within binary images between black/white borders.  Positive values in this image correspond to a white (value=1) pixel's distance to the next black pixel. Black pixel's (value=0) distance to the next white pixel are represented in this map with negative values.  Binary fill holes Fills holes in a binary image.  Touching objects labeling Starting from a binary image, touching objects can be splits into multiple regions, similar to the Watershed segmentation in ImageJ.  Morphological Watershed The morhological watershed allows to segment images showing membranes. Before segmentation, a filter such as the Gaussian blur or a median filter should be used to eliminate noise. It also makes sense to increase the thickness of membranes using a maximum filter.  See this notebook for details.  Watershed-Otsu-Labeling This algorithm uses Otsu's thresholding method in combination with  Gaussian blur and the  Watershed-algorithm  approach to label bright objects such as nuclei in an intensity image. The alogrithm has two sigma parameters and a  level parameter which allow you to fine-tune where objects should be cut (spot_sigma) and how smooth outlines  should be (outline_sigma). The watershed_level parameter determines how deep an intensity valley between two maxima  has to be to differentiate the two maxima.  This implementation is similar to Voronoi-Otsu-Labeling in clesperanto.  Richardson-Lucy Deconvolution Richardson-Lucy deconvolution allows to restore image quality if the point-spread-function of the optical system used  for acquisition is known or can be approximated.  Installation You can install napari-simpleitk-image-processing via using conda and pip. If you have never used conda before, please go through this tutorial first. conda install -c conda-forge napari pip install napari-simpleitk-image-processing  See also There are other napari plugins with similar functionality for processing images and extracting features: * morphometrics * PartSeg * napari-skimage-regionprops * napari-cupy-image-processing * napari-pyclesperanto-assistant * napari-allencell-segmenter * RedLionfish * bbii-decon * napari-segment-blobs-and-things-with-membranes Furthermore, there are plugins for postprocessing extracted measurements * napari-feature-classifier * napari-clusters-plotter Contributing Contributions are very welcome. There are many useful algorithms available in  SimpleITK. If you want another one available here in this napari plugin, don't hesitate to send a pull-request. This repository just holds wrappers for SimpleITK-functions, see this file for how those wrappers can be written. License Distributed under the terms of the BSD-3 license, \"napari-simpleitk-image-processing\" is free and open source software Citation For implementing this napari plugin, the  SimpleITK python notebooks were very helpful.  Thus, if you find the plugin useful, consider citing the SimpleITK notebooks: Z. Yaniv, B. C. Lowekamp, H. J. Johnson, R. Beare,  \"SimpleITK Image-Analysis Notebooks: a Collaborative Environment for Education and Reproducible Research\", \\\\ J Digit Imaging., 31(3): 290-303, 2018, https://doi.org/10.1007/s10278-017-0037-8. Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 3 - Alpha"],
    "display_name": "napari-simpleitk-image-processing",
    "documentation": "https://github.com/haesleinhuepf/napari-simpleitk-image-processing#README.md",
    "first_released": "2021-11-28T15:30:28.098377Z",
    "license": "BSD-3-Clause",
    "name": "napari-simpleitk-image-processing",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/haesleinhuepf/napari-simpleitk-image-processing",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-09-24T17:48:00.598265Z",
    "report_issues": "https://github.com/haesleinhuepf/napari-simpleitk-image-processing/issues",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "simpleitk",
      "napari-tools-menu (>=0.1.17)",
      "napari-time-slicer",
      "napari-skimage-regionprops (>=0.5.1)",
      "napari-assistant (>=0.3.10)",
      "pandas",
      "stackview (>=0.3.2)"
    ],
    "summary": "Process and analyze images using SimpleITK in napari",
    "support": "https://github.com/haesleinhuepf/napari-simpleitk-image-processing/issues",
    "twitter": "",
    "version": "0.4.4",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Lorenzo Gaifas" }],
    "code_repository": "https://github.com/brisvag/napari-label-interpolator",
    "conda": [],
    "description": "# napari-label-interpolator  [![License GNU GPL v3.0](https://img.shields.io/pypi/l/napari-label-interpolator.svg?color=green)](https://github.com/brisvag/napari-label-interpolator/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-label-interpolator.svg?color=green)](https://pypi.org/project/napari-label-interpolator) [![Python Version](https://img.shields.io/pypi/pyversions/napari-label-interpolator.svg?color=green)](https://python.org) [![tests](https://github.com/brisvag/napari-label-interpolator/workflows/tests/badge.svg)](https://github.com/brisvag/napari-label-interpolator/actions) [![codecov](https://codecov.io/gh/brisvag/napari-label-interpolator/branch/main/graph/badge.svg)](https://codecov.io/gh/brisvag/napari-label-interpolator) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-label-interpolator)](https://napari-hub.org/plugins/napari-label-interpolator)  A napari plugin to interpolate any number of (n-1)d-labels across a single dimension.  To use, simply label a few slices along the desired dimension, then use the widget to interpolate along the desired axis.  ![https://user-images.githubusercontent.com/23482191/189153632-40ef38b7-be89-40b3-b583-b17f3241c67b.png]()  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/plugins/index.html -->  ## Installation  You can install `napari-label-interpolator` via [pip]:      pip install napari-label-interpolator    To install latest development version :      pip install git+https://github.com/brisvag/napari-label-interpolator.git   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [GNU GPL v3.0] license, \"napari-label-interpolator\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/brisvag/napari-label-interpolator/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-label-interpolator       A napari plugin to interpolate any number of (n-1)d-labels across a single dimension. To use, simply label a few slices along the desired dimension, then use the widget to interpolate along the desired axis.   This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install napari-label-interpolator via pip: pip install napari-label-interpolator  To install latest development version : pip install git+https://github.com/brisvag/napari-label-interpolator.git  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the GNU GPL v3.0 license, \"napari-label-interpolator\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari label interpolator",
    "documentation": "https://github.com/brisvag/napari-label-interpolator#README.md",
    "first_released": "2022-09-08T15:27:52.824433Z",
    "license": "GPL-3.0",
    "name": "napari-label-interpolator",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/brisvag/napari-label-interpolator",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-09-08T15:27:52.824433Z",
    "report_issues": "https://github.com/brisvag/napari-label-interpolator/issues",
    "requirements": [
      "magicgui",
      "edt",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "summary": "A napari plugin to interpolate any number of (n-1)d-labels across a single dimension.",
    "support": "https://github.com/brisvag/napari-label-interpolator/issues",
    "twitter": "",
    "version": "0.1.0",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Andrea Bassi" }, { "name": "Mark Neil" }],
    "category": {
      "Image modality": ["Fluorescence microscopy"],
      "Supported data": ["3D"],
      "Workflow step": ["Image reconstruction"]
    },
    "category_hierarchy": {
      "Image modality": [
        ["Fluorescence microscopy", "Super-resolution microscopy"]
      ],
      "Supported data": [["3D"]],
      "Workflow step": [
        ["Image reconstruction", "Structured illumination reconstruction"]
      ]
    },
    "code_repository": "https://github.com/andreabassi78/napari-sim-processor",
    "conda": [{ "channel": "conda-forge", "package": "napari-sim-processor" }],
    "description": "# napari-sim-processor  [![License](https://img.shields.io/pypi/l/napari-sim-processor.svg?color=green)](https://github.com/andreabassi78/napari-sim-processor/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-sim-processor.svg?color=green)](https://pypi.org/project/napari-sim-processor) [![Python Version](https://img.shields.io/pypi/pyversions/napari-sim-processor.svg?color=green)](https://python.org) [![tests](https://github.com/andreabassi78/napari-sim-processor/workflows/tests/badge.svg)](https://github.com/andreabassi78/napari-sim-processor/actions) [![codecov](https://codecov.io/gh/andreabassi78/napari-sim-processor/branch/main/graph/badge.svg)](https://codecov.io/gh/andreabassi78/napari-sim-processor) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-sim-processor)](https://napari-hub.org/plugins/napari-sim-processor)  A Napari plugin for the reconstruction of Structured Illumination Microscopy (SIM) with GPU acceleration (pytorch/cupy if installed). Currently supports:        - conventional SIM data with a generic number of angles and phases (typically, 3 angles and 3 phases are used for resolution improvement in 2D, but any combination can be processed by the widget)    - hexagonal SIM data with 7 phases.  The SIM processing widget accepts image stacks organized in 5D (`angle`,`phase`,`z`,`y`,`x`).  The reshape widget can be used to easily reshape the data if they are not organized as 5D (angle,phase,z,y,x). Currently only square images are supported (`x`=`y`)  For 3D stacks (raw images) with multiple z-frames, a batch reconstruction method is available, as described here: \\thttps://doi.org/10.1098/rsta.2020.0162          Support for 3D SIM with enhanced resolution in all directions is not yet available. Multicolor reconstruction is not yet available.    ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/plugins/index.html -->  ## Installation  You can install `napari-sim-processor` via [pip]:      pip install napari-sim-processor   To install latest development version :      pip install git+https://github.com/andreabassi78/napari-sim-processor.git   ## Usage  1) Open napari.   2) Launch the reshape and sim-processor widgets.  3) Open your raw image stack (using the napari built-in or your own file openers).  ![raw](https://github.com/andreabassi78/napari-sim-processor/raw/main/images/Picture1.png)  4) If your image is ordered as a 5D stack (angle, phase, z-frame, y, x) go to point 6.   5) In the reshape widget, select the actual number of acquired angles, phases, and frames (red arrow) and press `Reshape Stack`.  Note that the label axis of the viewer will be updated (green arrow).  ![raw](https://github.com/andreabassi78/napari-sim-processor/raw/main/images/Picture1b.png)  6) In the sim-reconstruction widget press the Select image layer button. Note that the number of phases and angles will be updated (blue arrow).   7) Choose the correct parameters of the SIM acquisition (`NA`, `pixelsize`, `M`, etc.) and processing parameters (`alpha`, `beta`, w, `eta`, `group`):    - `w`: parameter of the Weiner filter.    - `eta`: constant used for calibration. It should be slightly smaller than the carrier frequency (in pupil radius units).    - `group`: for stacks with multiple z-frames, it is the number of frames that are used together for the calibration process. \\t For details on the other parameters see https://doi.org/10.1098/rsta.2020.0162.  8) Calibrate the SIM processor, pressing the `Calibrate` button. This will find the carrier frequencies (red circles if the `Show Carrier` checkbox is selected), the modulation amplitude and the phase, using cross correlation analysis.  9) Click on the checkboxes to show the power spectrum of the raw image (`Show power spectrum`) or the cross-correlation (`Show Xcorr`), to see if the found carrier frequency is correct.  ![raw](https://github.com/andreabassi78/napari-sim-processor/raw/main/images/Picture2b.png) **Napari viewer showing the power spectrum of the raw stack. The pupil circle is in blue. A circle corresponding to `eta` is shown in green.**  ![raw](https://github.com/andreabassi78/napari-sim-processor/raw/main/images/Picture2.png) **Napari viewer showing the cross-correlation of the raw stack. The red circles indicate the found carrier frequencies**  10) Run the reconstruction of a single plane (`SIM reconstruction`) or of a stack (`Stack reconstruction`). After execution, a new image_layer will be added to the napari viewer. Click on the `Batch reconstruction` checkbox in order to process an entire stack in one shot. Click on the pytorch checkbox for gpu acceleration.  ![raw](https://github.com/andreabassi78/napari-sim-processor/raw/main/images/Picture3b.png) **Napari viewer with widgets showing a pseudo-widefield reconstruction**  ![raw](https://github.com/andreabassi78/napari-sim-processor/raw/main/images/Picture3.png) **Napari viewer with widgets showing a SIM reconstruction**  ## GPU processing  The underlying processing classes will use numpy (and FFTW if available) for  its calculations. For GPU accelerated processing you need to have either the  PyTorch (tested with torch v1.11.0+cu113) or the CuPy (tested with cupy-cuda113  v10.4.0) package installed.  Make sure to match the package cuda version to the CUDA library  installed on your system otherwise PyTorch will default to CPU and CuPy will not work at all.    Both packages give significant speedup on even relatively modest CUDA GPUs compared  to Numpy, and PyTorch running on the CPU only can show improvements relative to numpy  and FFTW. Selection of which processing package to use is via a ComboBox in the  napari_sim_processor widget.  Only available packages are shown.   Other than requiring a CUDA GPU it is advisable to have significant GPU memory  available, particularly when processing large datasets.  Batch processing is the  most memory hungry of the methods, but can process 280x512x512 datasets on a 4GB GPU.  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-sim-processor\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/andreabassi78/napari-sim-processor/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-sim-processor       A Napari plugin for the reconstruction of Structured Illumination Microscopy (SIM) with GPU acceleration (pytorch/cupy if installed). Currently supports:      - conventional SIM data with a generic number of angles and phases (typically, 3 angles and 3 phases are used for resolution improvement in 2D, but any combination can be processed by the widget)    - hexagonal SIM data with 7 phases. The SIM processing widget accepts image stacks organized in 5D (angle,phase,z,y,x). The reshape widget can be used to easily reshape the data if they are not organized as 5D (angle,phase,z,y,x). Currently only square images are supported (x=y) For 3D stacks (raw images) with multiple z-frames, a batch reconstruction method is available, as described here:     https://doi.org/10.1098/rsta.2020.0162 Support for 3D SIM with enhanced resolution in all directions is not yet available. Multicolor reconstruction is not yet available.    This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install napari-sim-processor via pip: pip install napari-sim-processor  To install latest development version : pip install git+https://github.com/andreabassi78/napari-sim-processor.git  Usage 1) Open napari.  2) Launch the reshape and sim-processor widgets. 3) Open your raw image stack (using the napari built-in or your own file openers).  4) If your image is ordered as a 5D stack (angle, phase, z-frame, y, x) go to point 6.  5) In the reshape widget, select the actual number of acquired angles, phases, and frames (red arrow) and press Reshape Stack.  Note that the label axis of the viewer will be updated (green arrow).  6) In the sim-reconstruction widget press the Select image layer button. Note that the number of phases and angles will be updated (blue arrow).  7) Choose the correct parameters of the SIM acquisition (NA, pixelsize, M, etc.) and processing parameters (alpha, beta, w, eta, group):    - w: parameter of the Weiner filter.    - eta: constant used for calibration. It should be slightly smaller than the carrier frequency (in pupil radius units).    - group: for stacks with multiple z-frames, it is the number of frames that are used together for the calibration process. For details on the other parameters see https://doi.org/10.1098/rsta.2020.0162. 8) Calibrate the SIM processor, pressing the Calibrate button. This will find the carrier frequencies (red circles if the Show Carrier checkbox is selected), the modulation amplitude and the phase, using cross correlation analysis. 9) Click on the checkboxes to show the power spectrum of the raw image (Show power spectrum) or the cross-correlation (Show Xcorr), to see if the found carrier frequency is correct.  Napari viewer showing the power spectrum of the raw stack. The pupil circle is in blue. A circle corresponding to eta is shown in green.  Napari viewer showing the cross-correlation of the raw stack. The red circles indicate the found carrier frequencies 10) Run the reconstruction of a single plane (SIM reconstruction) or of a stack (Stack reconstruction). After execution, a new image_layer will be added to the napari viewer. Click on the Batch reconstruction checkbox in order to process an entire stack in one shot. Click on the pytorch checkbox for gpu acceleration.  Napari viewer with widgets showing a pseudo-widefield reconstruction  Napari viewer with widgets showing a SIM reconstruction GPU processing The underlying processing classes will use numpy (and FFTW if available) for  its calculations. For GPU accelerated processing you need to have either the  PyTorch (tested with torch v1.11.0+cu113) or the CuPy (tested with cupy-cuda113  v10.4.0) package installed.  Make sure to match the package cuda version to the CUDA library  installed on your system otherwise PyTorch will default to CPU and CuPy will not work at all.   Both packages give significant speedup on even relatively modest CUDA GPUs compared  to Numpy, and PyTorch running on the CPU only can show improvements relative to numpy  and FFTW. Selection of which processing package to use is via a ComboBox in the  napari_sim_processor widget.  Only available packages are shown.  Other than requiring a CUDA GPU it is advisable to have significant GPU memory  available, particularly when processing large datasets.  Batch processing is the  most memory hungry of the methods, but can process 280x512x512 datasets on a 4GB GPU. Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-sim-processor\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari SIM processor",
    "documentation": "https://github.com/andreabassi78/napari-sim-processor#README.md",
    "first_released": "2022-05-04T16:42:50.109160Z",
    "license": "BSD-3-Clause",
    "name": "napari-sim-processor",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/andreabassi78/napari-sim-processor",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-07-07T14:30:29.235633Z",
    "report_issues": "https://github.com/andreabassi78/napari-sim-processor/issues",
    "requirements": [
      "numpy",
      "magicgui",
      "qtpy",
      "matplotlib",
      "superqt (>=0.3.2)",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'",
      "matplotlib ; extra == 'testing'"
    ],
    "summary": "A plugin to process Structured Illumination Microscopy data with gpu acceleration",
    "support": "https://github.com/andreabassi78/napari-sim-processor/issues",
    "twitter": "",
    "version": "0.0.9",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Kevin Yamauchi" }],
    "code_repository": "https://github.com/morphometrics/morphometrics-engine",
    "conda": [],
    "description": "# morphometrics-engine  [![License BSD-3](https://img.shields.io/pypi/l/morphometrics-engine.svg?color=green)](https://github.com/morphometrics/morphometrics-engine/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/morphometrics-engine.svg?color=green)](https://pypi.org/project/morphometrics-engine) [![Python Version](https://img.shields.io/pypi/pyversions/morphometrics-engine.svg?color=green)](https://python.org) [![tests](https://github.com/morphometrics/morphometrics-engine/workflows/tests/badge.svg)](https://github.com/morphometrics/morphometrics-engine/actions) [![codecov](https://codecov.io/gh/morphometrics/morphometrics-engine/branch/main/graph/badge.svg)](https://codecov.io/gh/morphometrics/morphometrics-engine) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/morphometrics-engine)](https://napari-hub.org/plugins/morphometrics-engine)  A morphometrics measurement engine.  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/stable/plugins/index.html -->  ## Installation  You can install `morphometrics-engine` via [pip]:      pip install morphometrics-engine    To install latest development version :      pip install git+https://github.com/morphometrics/morphometrics-engine.git   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"morphometrics-engine\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/morphometrics/morphometrics-engine/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "morphometrics-engine       A morphometrics measurement engine.  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install morphometrics-engine via pip: pip install morphometrics-engine  To install latest development version : pip install git+https://github.com/morphometrics/morphometrics-engine.git  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"morphometrics-engine\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "morphometrics engine",
    "documentation": "https://github.com/morphometrics/morphometrics-engine#README.md",
    "first_released": "2022-10-23T08:44:53.733804Z",
    "license": "BSD-3-Clause",
    "name": "morphometrics-engine",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/morphometrics/morphometrics-engine",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-10-23T08:44:53.733804Z",
    "report_issues": "https://github.com/morphometrics/morphometrics-engine/issues",
    "requirements": [
      "napari",
      "napari-skimage-regionprops",
      "numpy",
      "magicgui",
      "pandas",
      "qtpy",
      "superqt",
      "tqdm",
      "toolz",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "summary": "A morphometrics measurement engine.",
    "support": "https://github.com/morphometrics/morphometrics-engine/issues",
    "twitter": "",
    "version": "0.0.1",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      { "email": "dhole.pranjal@gmail.com", "name": "Pranjal Dhole" }
    ],
    "code_repository": "https://github.com/pranjaldhole/napari-labelling-assistant",
    "description": "# napari-labelling-assistant  [![License](https://img.shields.io/pypi/l/napari-labelling-assistant.svg?color=green)](https://github.com/pranjaldhole/napari-labelling-assistant/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-labelling-assistant.svg?color=green)](https://pypi.org/project/napari-labelling-assistant) [![Python Version](https://img.shields.io/pypi/pyversions/napari-labelling-assistant.svg?color=green)](https://python.org) [![tests](https://github.com/pranjaldhole/napari-labelling-assistant/workflows/tests/badge.svg)](https://github.com/pranjaldhole/napari-labelling-assistant/actions) [![codecov](https://codecov.io/gh/pranjaldhole/napari-labelling-assistant/branch/main/graph/badge.svg)](https://codecov.io/gh/pranjaldhole/napari-labelling-assistant) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-labelling-assistant)](https://napari-hub.org/plugins/napari-labelling-assistant)  A lightweight plugin for visualizing labelling statistics.  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/plugins/stable/index.html -->  ## Installation  You can install `napari-labelling-assistant` via [pip]:      pip install napari-labelling-assistant    To install latest development version :      pip install git+https://github.com/pranjaldhole/napari-labelling-assistant.git   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [MIT] license, \"napari-labelling-assistant\" is free and open source software.  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/pranjaldhole/napari-labelling-assistant/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-labelling-assistant       A lightweight plugin for visualizing labelling statistics.  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install napari-labelling-assistant via pip: pip install napari-labelling-assistant  To install latest development version : pip install git+https://github.com/pranjaldhole/napari-labelling-assistant.git  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the MIT license, \"napari-labelling-assistant\" is free and open source software. Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari-labelling-assistant",
    "documentation": "https://github.com/pranjaldhole/napari-labelling-assistant#README.md",
    "first_released": "2022-01-19T10:01:33.353562Z",
    "license": "MIT",
    "name": "napari-labelling-assistant",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/pranjaldhole/napari-labelling-assistant",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-01-24T10:04:52.881161Z",
    "report_issues": "https://github.com/pranjaldhole/napari-labelling-assistant/issues",
    "requirements": ["napari-plugin-engine (>=0.1.4)", "numpy", "matplotlib"],
    "summary": "A lightweight plugin for visualizing labelling statistics.",
    "support": "https://github.com/pranjaldhole/napari-labelling-assistant/issues",
    "twitter": "",
    "version": "0.0.5",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Robert Haase" }],
    "code_repository": "https://github.com/haesleinhuepf/napari-time-slicer",
    "conda": [{ "channel": "conda-forge", "package": "napari-time-slicer" }],
    "description": "# napari-time-slicer  [![License](https://img.shields.io/pypi/l/napari-time-slicer.svg?color=green)](https://github.com/haesleinhuepf/napari-time-slicer/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-time-slicer.svg?color=green)](https://pypi.org/project/napari-time-slicer) [![Python Version](https://img.shields.io/pypi/pyversions/napari-time-slicer.svg?color=green)](https://python.org) [![tests](https://github.com/haesleinhuepf/napari-time-slicer/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-time-slicer/actions) [![codecov](https://codecov.io/gh/haesleinhuepf/napari-time-slicer/branch/main/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-time-slicer) [![Development Status](https://img.shields.io/pypi/status/napari-time-slicer.svg)](https://en.wikipedia.org/wiki/Software_release_life_cycle#Alpha) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-time-slicer)](https://napari-hub.org/plugins/napari-time-slicer)  A meta plugin for processing timelapse data timepoint by timepoint. It  enables a list of napari plugins to process 2D+t or 3D+t data step by step when the user goes  through the timelapse. Currently, these plugins are using `napari-time-slicer`: * [napari-segment-blobs-and-things-with-membranes](https://www.napari-hub.org/plugins/napari-segment-blobs-and-things-with-membranes) * [napari-cupy-image-processing](https://www.napari-hub.org/plugins/napari-cupy-image-processing) * [napari-pyclesperanto-assistant](https://www.napari-hub.org/plugins/napari-pyclesperanto-assistant) * [napari-accelerated-pixel-and-object-classification](https://www.napari-hub.org/plugins/napari-accelerated-pixel-and-object-classification) * [napari-simpleitk-image-processing](https://www.napari-hub.org/plugins/napari-simpleitk-image-processing) * [napari-stress](https://www.napari-hub.org/plugins/napari-stress) * [napari-process-points-and-surfaces](https://www.napari-hub.org/plugins/napari-process-points-and-surfaces)  `napari-time-slicer` enables inter-plugin communication, e.g. allowing to combine the plugins listed above in  one image processing workflow for segmenting a timelapse dataset:  ![](https://github.com/haesleinhuepf/napari-time-slicer/raw/main/images/screencast1.gif)  The workflow can then also be exported as a script. The 'Generate Code' button can be found in the [Workflow Inspector](https://www.napari-hub.org/plugins/napari-workflow-inspector)   If you want to convert a 3D dataset into a 2D + time dataset, use the  menu `Tools > Utilities > Convert 3D stack to 2D timelapse (time-slicer)`. It will turn the 3D dataset to a 4D datset where the Z-dimension (index 1) has only 1 element, which will in napari be displayed with a time-slider. Note: It is  recommended to remove the original 3D dataset after this conversion.  ## Working with large on-the-fly processed datasets  Using the [napari-assistant](https://www.napari-hub.org/plugins/napari-assistant) complex image processing workflows on timelapse datasets can be setup.  In combination with the time-slicer it is possible to process time-lapse data that is larger than available computer memory. In case the workflow only consists of images and label-images and out-of-memory issues arise, consider storing intermediate results on disk following this procedure:  After setting up the workflow and testing it on a couple of selected frames, store the entire processed timelapse dataset to disk  using the menu `Tools > Utilities > Convert to file-backed timelapse data (time-slicer)`. It will open this dialog, where you can select  ![img.png](https://github.com/haesleinhuepf/napari-time-slicer/raw/main/images/convert_to_file_backed_timelapse.png)  It is recommended to enter a folder location in the text field.  If not provided, a temporary folder will be created, typically in the User's temp folder in the home directory.  The user is responsible for emptying this folder from time to time. The data stored in this folder can also be loaded into napari using its `File > Open Folder...` menu.  Executing this operation can take time as every timepoint of the timelapse is computed.  Afterwards, there will be another layer available in napari, which is typically faster to navigate through.  Consider removing the layer(s) that were only needed to determine the new file-backed layer.  ![img.png](https://github.com/haesleinhuepf/napari-time-slicer/raw/main/images/new_file_backed_layer.png)  ## Usage for plugin developers  Plugins which implement the `napari_experimental_provide_function` hook can make use of the `@time_slicer`. At the moment, only functions which take `napari.types.ImageData`, `napari.types.LabelsData` and basic python types such as `int`  and `float` are supported. If you annotate such a function with `@time_slicer` it will internally convert any 4D dataset to a 3D dataset according to the timepoint currently selected in napari. Furthermore, when the napari user changes the current timepoint or the input data of the function changes, a re-computation is invoked. Thus, it is recommended to  only use the `time_slicer` for functions which can provide [almost] real-time performance. Another constraint is that  these annotated functions have to have a `viewer` parameter. This is necessary to read the current timepoint from the  viewer when invoking the re-computions.  Example ```python import napari from napari_time_slicer import time_slicer  @time_slicer def threshold_otsu(image:napari.types.ImageData, viewer: napari.Viewer = None) -> napari.types.LabelsData:     # ... ```  You can see a full implementations of this concept in the napari plugins listed above.  If you want to combine slicing in time and processing z-stack images slice-by-slice, you can use the `@slice_by_slice` annotation. Make sure, to insert it after `@time_slicer` as shown below and implemented in [napari-pillow-image-processing](https://github.com/haesleinhuepf/napari-pillow-image-processing/blob/4d846b226739843124953f16059241d917cde8e1/src/napari_pillow_image_processing/__init__.py#L151)  ```python from napari_time_slicer import slice_by_slice  @time_slicer @slice_by_slice def blur_2d(image:napari.types.ImageData, sigma:float = 1, viewer: napari.Viewer = None) -> napari.types.LabelsData:     # ... ```  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  ## Installation  You can install `napari-time-slicer` via [pip]:      pip install napari-time-slicer    To install latest development version :      pip install git+https://github.com/haesleinhuepf/napari-time-slicer.git   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-time-slicer\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/haesleinhuepf/napari-time-slicer/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-time-slicer        A meta plugin for processing timelapse data timepoint by timepoint. It  enables a list of napari plugins to process 2D+t or 3D+t data step by step when the user goes  through the timelapse. Currently, these plugins are using napari-time-slicer: * napari-segment-blobs-and-things-with-membranes * napari-cupy-image-processing * napari-pyclesperanto-assistant * napari-accelerated-pixel-and-object-classification * napari-simpleitk-image-processing * napari-stress * napari-process-points-and-surfaces napari-time-slicer enables inter-plugin communication, e.g. allowing to combine the plugins listed above in  one image processing workflow for segmenting a timelapse dataset:  The workflow can then also be exported as a script. The 'Generate Code' button can be found in the Workflow Inspector If you want to convert a 3D dataset into a 2D + time dataset, use the  menu Tools > Utilities > Convert 3D stack to 2D timelapse (time-slicer). It will turn the 3D dataset to a 4D datset where the Z-dimension (index 1) has only 1 element, which will in napari be displayed with a time-slider. Note: It is  recommended to remove the original 3D dataset after this conversion. Working with large on-the-fly processed datasets Using the napari-assistant complex image processing workflows on timelapse datasets can be setup.  In combination with the time-slicer it is possible to process time-lapse data that is larger than available computer memory. In case the workflow only consists of images and label-images and out-of-memory issues arise, consider storing intermediate results on disk following this procedure:  After setting up the workflow and testing it on a couple of selected frames, store the entire processed timelapse dataset to disk  using the menu Tools > Utilities > Convert to file-backed timelapse data (time-slicer). It will open this dialog, where you can select   It is recommended to enter a folder location in the text field.  If not provided, a temporary folder will be created, typically in the User's temp folder in the home directory.  The user is responsible for emptying this folder from time to time. The data stored in this folder can also be loaded into napari using its File > Open Folder... menu. Executing this operation can take time as every timepoint of the timelapse is computed.  Afterwards, there will be another layer available in napari, which is typically faster to navigate through.  Consider removing the layer(s) that were only needed to determine the new file-backed layer.  Usage for plugin developers Plugins which implement the napari_experimental_provide_function hook can make use of the @time_slicer. At the moment, only functions which take napari.types.ImageData, napari.types.LabelsData and basic python types such as int  and float are supported. If you annotate such a function with @time_slicer it will internally convert any 4D dataset to a 3D dataset according to the timepoint currently selected in napari. Furthermore, when the napari user changes the current timepoint or the input data of the function changes, a re-computation is invoked. Thus, it is recommended to  only use the time_slicer for functions which can provide [almost] real-time performance. Another constraint is that  these annotated functions have to have a viewer parameter. This is necessary to read the current timepoint from the  viewer when invoking the re-computions. Example ```python import napari from napari_time_slicer import time_slicer @time_slicer def threshold_otsu(image:napari.types.ImageData, viewer: napari.Viewer = None) -> napari.types.LabelsData:     # ... ``` You can see a full implementations of this concept in the napari plugins listed above. If you want to combine slicing in time and processing z-stack images slice-by-slice, you can use the @slice_by_slice annotation. Make sure, to insert it after @time_slicer as shown below and implemented in napari-pillow-image-processing ```python from napari_time_slicer import slice_by_slice @time_slicer @slice_by_slice def blur_2d(image:napari.types.ImageData, sigma:float = 1, viewer: napari.Viewer = None) -> napari.types.LabelsData:     # ... ```  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template. Installation You can install napari-time-slicer via pip: pip install napari-time-slicer  To install latest development version : pip install git+https://github.com/haesleinhuepf/napari-time-slicer.git  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-time-slicer\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 3 - Alpha"],
    "display_name": "napari-time-slicer",
    "documentation": "https://github.com/haesleinhuepf/napari-time-slicer#README.md",
    "first_released": "2021-11-12T21:02:28.253302Z",
    "license": "BSD-3-Clause",
    "name": "napari-time-slicer",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/haesleinhuepf/napari-time-slicer",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-07-10T14:24:14.887100Z",
    "report_issues": "https://github.com/haesleinhuepf/napari-time-slicer/issues",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "toolz",
      "napari-tools-menu",
      "napari-workflows"
    ],
    "summary": "A meta plugin for processing timelapse data in napari timepoint by timepoint",
    "support": "https://github.com/haesleinhuepf/napari-time-slicer/issues",
    "twitter": "",
    "version": "0.4.9",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Niklas Netter" }],
    "code_repository": "https://github.com/gatoniel/napari-merge-stardist-masks",
    "description": "# napari-merge-stardist-masks  [![License BSD-3](https://img.shields.io/pypi/l/napari-merge-stardist-masks.svg?color=green)](https://github.com/gatoniel/napari-merge-stardist-masks/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-merge-stardist-masks.svg?color=green)](https://pypi.org/project/napari-merge-stardist-masks) [![Python Version](https://img.shields.io/pypi/pyversions/napari-merge-stardist-masks.svg?color=green)](https://python.org) [![tests](https://github.com/gatoniel/napari-merge-stardist-masks/workflows/tests/badge.svg)](https://github.com/gatoniel/napari-merge-stardist-masks/actions) [![codecov](https://codecov.io/gh/gatoniel/napari-merge-stardist-masks/branch/main/graph/badge.svg)](https://codecov.io/gh/gatoniel/napari-merge-stardist-masks) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-merge-stardist-masks)](https://napari-hub.org/plugins/napari-merge-stardist-masks)  Segment non-star-convex objects with StarDist by merging masks.  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/stable/plugins/index.html -->   ## Run  In PowerShell, when you do not have sufficient GPU support, run napari without CUDA support, i.e.,: ``` $env:CUDA_VISIBLE_DEVICES=-1; napari ```   ## Installation  You can install `napari-merge-stardist-masks` via [pip]:      pip install napari-merge-stardist-masks    To install latest development version :      pip install git+https://github.com/gatoniel/napari-merge-stardist-masks.git   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-merge-stardist-masks\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/gatoniel/napari-merge-stardist-masks/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-merge-stardist-masks       Segment non-star-convex objects with StarDist by merging masks.  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Run In PowerShell, when you do not have sufficient GPU support, run napari without CUDA support, i.e.,: $env:CUDA_VISIBLE_DEVICES=-1; napari Installation You can install napari-merge-stardist-masks via pip: pip install napari-merge-stardist-masks  To install latest development version : pip install git+https://github.com/gatoniel/napari-merge-stardist-masks.git  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-merge-stardist-masks\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "Merge StarDist Masks",
    "documentation": "https://github.com/gatoniel/napari-merge-stardist-masks#README.md",
    "first_released": "2022-08-31T20:01:16.713372Z",
    "license": "BSD-3-Clause",
    "name": "napari-merge-stardist-masks",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget", "sample_data"],
    "project_site": "https://github.com/gatoniel/napari-merge-stardist-masks",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-11-23T15:26:10.873080Z",
    "report_issues": "https://github.com/gatoniel/napari-merge-stardist-masks/issues",
    "requirements": [
      "numpy",
      "magicgui",
      "qtpy",
      "stardist-napari (>=2022.7.5)",
      "merge-stardist-masks (>=0.1.0)",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "summary": "Segment non-star-convex objects with StarDist by merging masks.",
    "support": "https://github.com/gatoniel/napari-merge-stardist-masks/issues",
    "twitter": "",
    "version": "0.0.3",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Uwe Schmidt" }, { "name": "Martin Weigert" }],
    "category": {
      "Image modality": ["Medical imaging"],
      "Supported data": ["2D", "3D", "Time series", "Multi-channel"],
      "Workflow step": ["Image Segmentation"]
    },
    "category_hierarchy": {
      "Image modality": [["Medical imaging"]],
      "Supported data": [["2D"], ["3D"], ["Time series"], ["Multi-channel"]],
      "Workflow step": [
        ["Image Segmentation", "Cell segmentation"],
        ["Image Segmentation", "Model-based segmentation"]
      ]
    },
    "code_repository": "https://github.com/stardist/stardist-napari",
    "description": "# StarDist Napari Plugin  [![PyPI version](https://img.shields.io/pypi/v/stardist-napari.svg)](https://pypi.org/project/stardist-napari) [![Anaconda-Server Badge](https://anaconda.org/conda-forge/stardist-napari/badges/version.svg)](https://anaconda.org/conda-forge/stardist-napari) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/stardist-napari)](https://napari-hub.org/plugins/stardist-napari) [![Image.sc forum](https://img.shields.io/badge/dynamic/json.svg?label=forum&url=https%3A%2F%2Fforum.image.sc%2Ftags%2Fstardist.json&query=%24.topic_list.tags.0.topic_count&colorB=brightgreen&suffix=%20topics&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA4AAAAOCAYAAAAfSC3RAAABPklEQVR42m3SyyqFURTA8Y2BER0TDyExZ+aSPIKUlPIITFzKeQWXwhBlQrmFgUzMMFLKZeguBu5y+//17dP3nc5vuPdee6299gohUYYaDGOyyACq4JmQVoFujOMR77hNfOAGM+hBOQqB9TjHD36xhAa04RCuuXeKOvwHVWIKL9jCK2bRiV284QgL8MwEjAneeo9VNOEaBhzALGtoRy02cIcWhE34jj5YxgW+E5Z4iTPkMYpPLCNY3hdOYEfNbKYdmNngZ1jyEzw7h7AIb3fRTQ95OAZ6yQpGYHMMtOTgouktYwxuXsHgWLLl+4x++Kx1FJrjLTagA77bTPvYgw1rRqY56e+w7GNYsqX6JfPwi7aR+Y5SA+BXtKIRfkfJAYgj14tpOF6+I46c4/cAM3UhM3JxyKsxiOIhH0IO6SH/A1Kb1WBeUjbkAAAAAElFTkSuQmCC)](https://forum.image.sc/tags/stardist)  This project provides the [napari](https://napari.org/) plugin for [StarDist](https://github.com/stardist/stardist), a deep learning based 2D and 3D object detection method with star-convex shapes. StarDist has originally been developed (see [papers](https://github.com/stardist/stardist#stardist---object-detection-with-star-convex-shapes)) for the segmentation of densely packed cell nuclei in challenging images with low signal-to-noise ratios. The plugin allows to apply pretrained and custom trained models from within napari.  If you use this plugin for your research, please [cite us](https://github.com/stardist/stardist#how-to-cite).  ![Screenshot](https://github.com/stardist/stardist-napari/raw/main/images/stardist_napari_screenshot_small.png)   ## Installation  Install the plugin with `pip install stardist-napari` or from within napari via `Plugins > Install/Uninstall Plugins…`. If you want GPU-accelerated prediction, please read the more detailed [installation instructions](https://github.com/stardist/stardist#installation) for StarDist.  - You can activate the plugin in napari via `Plugins > stardist-napari: StarDist`. - Example images for testing are provided via `File > Open Sample > stardist-napari`.   ## Documentation  The two main buttons at the bottom of the plugin are (see right side of screenshot above):  **Restore Defaults**: Restore default values for [inputs](#inputs) (exceptions: *Input Image*, *Image Axes*, *Custom Model*).  **Run**: Start the prediction with the selected inputs and create the [outputs](#outputs) when done.  All plugin activity is shown in the napari *activity dock*, which can be shown/hidden by clicking on the word `activity` next to the little arrow at the bottom right of the napari window.  ### Inputs  The plugin does perform input validation, i.e. it will disable the `Run` button if it detects a problem with the selected inputs. Problematic input fields are highlighted with a \"lightcoral\" background color ![](https://via.placeholder.com/15/f08080/f08080.png), and their [*tooltips*](https://en.wikipedia.org/wiki/Tooltip) typically explain what the problem is. Some error messages are shown at the bottom in napari's status bar, such as for incompatibilities between multiple input fields. Input fields with warnings (also explained via tooltips) are highlighted with an orange background color ![](https://via.placeholder.com/15/ffa500/ffa500.png).  **Input Image**: Select a napari layer of type `Image` as the input.   *Tooltip:* Shows the shape of the image.  **Image Axes**: String that describes the semantic image axes and their order, e.g. `YX` for a 2D image. This parameter is automatically chosen (i.e. guessed) when a new input image is selected and should work in most cases. Permissible axis values are: `X` (width/columns), `Y` (height/rows), `Z` (depth/planes), `C` (channels), `T` (frames/time).   *Tooltip:* Shows the mapping of semantic axes to the shape of the selected input image.  **Predict on field of view (only for 2D models in 2D view)**: If enabled, the StarDist prediction is only applied to the current field of view of the napari viewer. As the name of this checkbox indicates, this only works for 2D StarDist models and when the napari viewer is in 2D viewing mode. The checkbox is not even shown if those conditions are not met.  #### *Neural Network Prediction*  **Model Type**: Choice whether to use registered pre-trained models (`2D`, `3D`) or provide a path to a model folder (`Custom 2D/3D`). Based on this choice, either the input for *Pre-trained Model* or *Custom Model* is shown below.   (Further information regarding pre-trained models: [how to register your own model](https://nbviewer.org/github/CSBDeep/CSBDeep/blob/master/examples/other/technical.ipynb#Registry-for-pretrained-models), [model registration in StarDist](https://github.com/stardist/stardist/blob/f73cdc44f718d36844b38c1f1662dbb66d157182/stardist/models/__init__.py#L17-L29).)  **Pre-trained Model**: Select a registered pre-trained model from a list. The first time a model is selected, it is downloaded and cached locally.  **Custom Model**: Provide a path to a StarDist model folder, containing at least `config.json` and a compatible neural network weights file (with suffix `.h5` or `.hdf5`). If present, `thresholds.json` is also loaded and its values can be used via the button *Set optimized postprocessing thresholds (for selected model)*.  **Model Axes**: A read-only text field that shows the semantic axes that the currently selected model expects as input. Additionally, we show the number of expected input channels, e.g. `YXC[2]` to indicate that the model expects a 2D input image with 2 channels. Seeing the model axes is helpful to understand whether the axes of the input image are compatible or not.  **Normalize Image**: A checkbox to indicate whether to perform [percentile-based input image normalization](https://forum.image.sc/t/normalization-in-stardist/41696/2) or not. This should be checked if the input image wasn't [manually normalized](https://forum.image.sc/t/stardist-extension/37696/7) such that most pixel values are in the range 0 to 1. If unchecked, inputs *Percentile low* and *Percentile high* are hidden.  **Percentile low**: Percentile value of input pixel distribution that is mapped to 0 (~min value). If there aren't any outlier pixels in your image, you may use percentile `0` to do a standard [min-max image normalization](https://www.codecademy.com/article/normalization).  **Percentile high**: Percentile value of input pixel distribution that is mapped to 1 (~max value). If there aren't any outlier pixels in your image, you may use percentile `100` to do a standard [min-max image normalization](https://www.codecademy.com/article/normalization).  **Input image scaling**: Number or list of numbers (one per input axis) to scale the input image before prediction and rescale the output accordingly. For example, a value of `0.5` indicates that all spatial axes are downscaled to half their size before prediction, and that the outputs are scaled to double their size. This is useful to adapt to different object sizes in the input image.   *Tooltip:* Shows the mapping of scale values to the semantic axes of the selected input image.  #### *NMS Postprocessing*  **Probability/Score Threshold**: Determine the number of object candidates to enter non-maximum suppression. Higher values lead to fewer segmented objects, but will likely avoid false positives. The selected model may have an associated threshold value, which can be loaded via the *Set optimized postprocessing thresholds (for selected model)* button.  **Overlap Threshold**: Determine when two objects are considered the same during non-maximum suppression. Higher values allow segmented objects to overlap substantially. The selected model may have an associated threshold value, which can be loaded via the *Set optimized postprocessing thresholds (for selected model)* button.  **Output Type**: Choose format of [outputs](#outputs) (see below for details). Selecting `Label Image` will create the outputs *StarDist labels* and *StarDist class labels* (for multi-class models only) as napari `Labels` layers. Selecting `Polygons / Polyhedra` will instead return the output *StarDist polygons* as a napari `Shapes` layer for a 2D model, or *StarDist polyhedra* as a napari `Surface` layer for a 3D model. Selecting `Both` will return both types of outputs.  #### *Advanced Options*  **Number of Tiles**: String `None` (to disable tiling) or list of integer numbers (one per axis of input image) to determine how the input image is tiled before the CNN prediction is computed on each tile individually. This is needed to avoid (GPU) memory issues that can occur for large input images. Note that the NMS postprocessing is still run only once with candidates from the predictions of all image tiles.   *Tooltip:* Shows the mapping of tile values to the semantic axes of the selected input image.  **Normalization Axes**: String of semantic axes which are jointly normalized (if they are present in the input image). For example, the default value `ZYX` indicates that all spatial axes are always normalized together; if an image has multiple channels, the pixels will be normalized separately per channel (e.g. this is what typically makes sense for fluorescence microscopy where channels are independent). On the other hand, the channels in RGB color images typically need to be normalized jointly, hence using `ZYXC` makes sense in this case. Note: if an image is explicitly opened with `rgb=True` in napari, the channels are automatically normalized together.   *Tooltip:* Shows a brief explanation.  **Time-lapse Labels**: If the input is a time-lapse/movie, each frame is first independently processed by StarDist. If `Separate per frame (no processing)` is chosen, the object ids in the label images of each frame are not modified, i.e. they are consecutive integers that always start at 1. Selecting `Unique through time` will cause object ids to be unique over time, i.e. the smallest object id in a given frame is larger than the largest object id of the previous frame. Finally, choosing `Match to previous frame (via overlap)` will perform a simple form of [greedy](https://en.wikipedia.org/wiki/Greedy_algorithm) matching/tracking, where object ids are propagated from one frame to the next based on object overlap.  **Show CNN Output**: Create additional [outputs](#outputs) (see below for details) *StarDist probability* and *StarDist distances* that show the direct results of the CNN prediction which are the inputs to the NMS postprocessing. Additionally, *StarDist class probabilities* is created for multi-class models.  **Set optimized postprocessing thresholds (for selected model)**: Button to set *Probability/Score Threshold* and *Overlap Threshold* to the values provided by the selected model. Nothing is changed if the model does not provide threshold values.  ### Outputs  **StarDist polygons**: The detected/segmented 2D objects as polygons (napari `Shapes` layer).  **StarDist polyhedra**: The detected/segmented 3D objects as surfaces (napari `Surface` layer).  **StarDist labels**: The detected/segmented 2D/3D objects as a *label image* (napari `Labels` layer). In an integer-valued label image, the value of a given pixel denotes the id of the object that it belongs to. For example, all pixels with value 5 belong to the object with id 5. All background pixels (that don't belong to any object) have value 0.  **StarDist class labels** ([multi-class models](https://nbviewer.org/github/stardist/stardist/blob/master/examples/other2D/multiclass.ipynb) only): The classes of detected/segmented 2D/3D objects as a *semantic segmentation labeling* (napari `Labels` layer). The integer value of a given pixel denotes the class id of the object that it belongs to. For example, all pixels with value 3 belong to the object class 3. Note that all pixels that belong to a specific object instance (as returned by *StarDist labels*) do have the same object class here. All background pixels (that don't belong to an object class) have value 0.  **StarDist probability**: The object probabilities predicted by the neural network as a single-channel image (napari `Image` layer).  **StarDist distances**: The radial distances predicted by the neural network as a multi-channel image (napari `Image` layer).  **StarDist class probabilities** ([multi-class models](https://nbviewer.org/github/stardist/stardist/blob/master/examples/other2D/multiclass.ipynb) only): The object class probabilities predicted by the neural network as a multi-channel image (napari `Image` layer).   ## Troubleshooting & Support  - The [image.sc forum](https://forum.image.sc/tag/stardist) is the best place to start getting help and support. Make sure to use the tag `stardist`, since we are monitoring all questions with this tag. - For general questions about StarDist, it's worth taking a look at the [frequently asked questions (FAQ)]( https://stardist.net/docs/faq.html). - If you have technical questions or found a bug, feel free to [open an issue](https://github.com/stardist/stardist-napari/issues).   ## Other resources  A demonstration of an earlier version of the plugin is shown in [this video](https://www.youtube.com/watch?v=Km1_TnUQ4FM&list=PLilvrWT8aLuZCaOkjucLjvDu2YRtCS-JT&index=5).  Many of the parameters are identical to those of our [StarDist ImageJ/Fiji plugin](https://github.com/stardist/stardist-imagej), which are documented [here](https://imagej.net/plugins/stardist#usage). ",
    "description_content_type": "text/markdown",
    "description_text": "StarDist Napari Plugin     This project provides the napari plugin for StarDist, a deep learning based 2D and 3D object detection method with star-convex shapes. StarDist has originally been developed (see papers) for the segmentation of densely packed cell nuclei in challenging images with low signal-to-noise ratios. The plugin allows to apply pretrained and custom trained models from within napari. If you use this plugin for your research, please cite us.  Installation Install the plugin with pip install stardist-napari or from within napari via Plugins > Install/Uninstall Plugins…. If you want GPU-accelerated prediction, please read the more detailed installation instructions for StarDist.  You can activate the plugin in napari via Plugins > stardist-napari: StarDist. Example images for testing are provided via File > Open Sample > stardist-napari.  Documentation The two main buttons at the bottom of the plugin are (see right side of screenshot above): Restore Defaults: Restore default values for inputs (exceptions: Input Image, Image Axes, Custom Model). Run: Start the prediction with the selected inputs and create the outputs when done. All plugin activity is shown in the napari activity dock, which can be shown/hidden by clicking on the word activity next to the little arrow at the bottom right of the napari window. Inputs The plugin does perform input validation, i.e. it will disable the Run button if it detects a problem with the selected inputs. Problematic input fields are highlighted with a \"lightcoral\" background color , and their tooltips typically explain what the problem is. Some error messages are shown at the bottom in napari's status bar, such as for incompatibilities between multiple input fields. Input fields with warnings (also explained via tooltips) are highlighted with an orange background color . Input Image: Select a napari layer of type Image as the input. Tooltip: Shows the shape of the image. Image Axes: String that describes the semantic image axes and their order, e.g. YX for a 2D image. This parameter is automatically chosen (i.e. guessed) when a new input image is selected and should work in most cases. Permissible axis values are: X (width/columns), Y (height/rows), Z (depth/planes), C (channels), T (frames/time). Tooltip: Shows the mapping of semantic axes to the shape of the selected input image. Predict on field of view (only for 2D models in 2D view): If enabled, the StarDist prediction is only applied to the current field of view of the napari viewer. As the name of this checkbox indicates, this only works for 2D StarDist models and when the napari viewer is in 2D viewing mode. The checkbox is not even shown if those conditions are not met. Neural Network Prediction Model Type: Choice whether to use registered pre-trained models (2D, 3D) or provide a path to a model folder (Custom 2D/3D). Based on this choice, either the input for Pre-trained Model or Custom Model is shown below. (Further information regarding pre-trained models: how to register your own model, model registration in StarDist.) Pre-trained Model: Select a registered pre-trained model from a list. The first time a model is selected, it is downloaded and cached locally. Custom Model: Provide a path to a StarDist model folder, containing at least config.json and a compatible neural network weights file (with suffix .h5 or .hdf5). If present, thresholds.json is also loaded and its values can be used via the button Set optimized postprocessing thresholds (for selected model). Model Axes: A read-only text field that shows the semantic axes that the currently selected model expects as input. Additionally, we show the number of expected input channels, e.g. YXC[2] to indicate that the model expects a 2D input image with 2 channels. Seeing the model axes is helpful to understand whether the axes of the input image are compatible or not. Normalize Image: A checkbox to indicate whether to perform percentile-based input image normalization or not. This should be checked if the input image wasn't manually normalized such that most pixel values are in the range 0 to 1. If unchecked, inputs Percentile low and Percentile high are hidden. Percentile low: Percentile value of input pixel distribution that is mapped to 0 (~min value). If there aren't any outlier pixels in your image, you may use percentile 0 to do a standard min-max image normalization. Percentile high: Percentile value of input pixel distribution that is mapped to 1 (~max value). If there aren't any outlier pixels in your image, you may use percentile 100 to do a standard min-max image normalization. Input image scaling: Number or list of numbers (one per input axis) to scale the input image before prediction and rescale the output accordingly. For example, a value of 0.5 indicates that all spatial axes are downscaled to half their size before prediction, and that the outputs are scaled to double their size. This is useful to adapt to different object sizes in the input image. Tooltip: Shows the mapping of scale values to the semantic axes of the selected input image. NMS Postprocessing Probability/Score Threshold: Determine the number of object candidates to enter non-maximum suppression. Higher values lead to fewer segmented objects, but will likely avoid false positives. The selected model may have an associated threshold value, which can be loaded via the Set optimized postprocessing thresholds (for selected model) button. Overlap Threshold: Determine when two objects are considered the same during non-maximum suppression. Higher values allow segmented objects to overlap substantially. The selected model may have an associated threshold value, which can be loaded via the Set optimized postprocessing thresholds (for selected model) button. Output Type: Choose format of outputs (see below for details). Selecting Label Image will create the outputs StarDist labels and StarDist class labels (for multi-class models only) as napari Labels layers. Selecting Polygons / Polyhedra will instead return the output StarDist polygons as a napari Shapes layer for a 2D model, or StarDist polyhedra as a napari Surface layer for a 3D model. Selecting Both will return both types of outputs. Advanced Options Number of Tiles: String None (to disable tiling) or list of integer numbers (one per axis of input image) to determine how the input image is tiled before the CNN prediction is computed on each tile individually. This is needed to avoid (GPU) memory issues that can occur for large input images. Note that the NMS postprocessing is still run only once with candidates from the predictions of all image tiles. Tooltip: Shows the mapping of tile values to the semantic axes of the selected input image. Normalization Axes: String of semantic axes which are jointly normalized (if they are present in the input image). For example, the default value ZYX indicates that all spatial axes are always normalized together; if an image has multiple channels, the pixels will be normalized separately per channel (e.g. this is what typically makes sense for fluorescence microscopy where channels are independent). On the other hand, the channels in RGB color images typically need to be normalized jointly, hence using ZYXC makes sense in this case. Note: if an image is explicitly opened with rgb=True in napari, the channels are automatically normalized together. Tooltip: Shows a brief explanation. Time-lapse Labels: If the input is a time-lapse/movie, each frame is first independently processed by StarDist. If Separate per frame (no processing) is chosen, the object ids in the label images of each frame are not modified, i.e. they are consecutive integers that always start at 1. Selecting Unique through time will cause object ids to be unique over time, i.e. the smallest object id in a given frame is larger than the largest object id of the previous frame. Finally, choosing Match to previous frame (via overlap) will perform a simple form of greedy matching/tracking, where object ids are propagated from one frame to the next based on object overlap. Show CNN Output: Create additional outputs (see below for details) StarDist probability and StarDist distances that show the direct results of the CNN prediction which are the inputs to the NMS postprocessing. Additionally, StarDist class probabilities is created for multi-class models. Set optimized postprocessing thresholds (for selected model): Button to set Probability/Score Threshold and Overlap Threshold to the values provided by the selected model. Nothing is changed if the model does not provide threshold values. Outputs StarDist polygons: The detected/segmented 2D objects as polygons (napari Shapes layer). StarDist polyhedra: The detected/segmented 3D objects as surfaces (napari Surface layer). StarDist labels: The detected/segmented 2D/3D objects as a label image (napari Labels layer). In an integer-valued label image, the value of a given pixel denotes the id of the object that it belongs to. For example, all pixels with value 5 belong to the object with id 5. All background pixels (that don't belong to any object) have value 0. StarDist class labels (multi-class models only): The classes of detected/segmented 2D/3D objects as a semantic segmentation labeling (napari Labels layer). The integer value of a given pixel denotes the class id of the object that it belongs to. For example, all pixels with value 3 belong to the object class 3. Note that all pixels that belong to a specific object instance (as returned by StarDist labels) do have the same object class here. All background pixels (that don't belong to an object class) have value 0. StarDist probability: The object probabilities predicted by the neural network as a single-channel image (napari Image layer). StarDist distances: The radial distances predicted by the neural network as a multi-channel image (napari Image layer). StarDist class probabilities (multi-class models only): The object class probabilities predicted by the neural network as a multi-channel image (napari Image layer). Troubleshooting & Support  The image.sc forum is the best place to start getting help and support. Make sure to use the tag stardist, since we are monitoring all questions with this tag. For general questions about StarDist, it's worth taking a look at the frequently asked questions (FAQ). If you have technical questions or found a bug, feel free to open an issue.  Other resources A demonstration of an earlier version of the plugin is shown in this video. Many of the parameters are identical to those of our StarDist ImageJ/Fiji plugin, which are documented here.",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "stardist-napari",
    "documentation": "https://github.com/stardist/stardist-napari",
    "first_released": "2021-06-01T13:33:52.440542Z",
    "license": "BSD-3-Clause",
    "name": "stardist-napari",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget", "sample_data"],
    "project_site": "https://github.com/stardist/stardist",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-12-06T10:44:01.951663Z",
    "report_issues": "https://github.com/stardist/stardist-napari/issues",
    "requirements": [
      "stardist (>=0.8.3)",
      "napari (>=0.4.13)",
      "magicgui (>=0.4.0)",
      "tensorflow ; platform_system != \"Darwin\" or platform_machine != \"arm64\"",
      "tensorflow-macos ; platform_system == \"Darwin\" and platform_machine == \"arm64\"",
      "tensorflow-metal ; platform_system == \"Darwin\" and platform_machine == \"arm64\"",
      "pytest ; extra == 'test'",
      "pytest-qt ; extra == 'test'",
      "napari[pyqt] (>=0.4.13) ; extra == 'test'"
    ],
    "summary": "Object Detection with Star-convex Shapes",
    "support": "https://forum.image.sc/tag/stardist",
    "twitter": "https://twitter.com/martweig",
    "version": "2022.12.6",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Robert Haase" }, { "name": "Martin Schätz" }],
    "code_repository": "https://github.com/haesleinhuepf/the-segmentation-game",
    "conda": [{ "channel": "conda-forge", "package": "the-segmentation-game" }],
    "description": "# The segmentation game - for napari  [![License](https://img.shields.io/pypi/l/the-segmentation-game.svg?color=green)](https://github.com/haesleinhuepf/the-segmentation-game/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/the-segmentation-game.svg?color=green)](https://pypi.org/project/the-segmentation-game) [![Python Version](https://img.shields.io/pypi/pyversions/the-segmentation-game.svg?color=green)](https://python.org) [![tests](https://github.com/haesleinhuepf/the-segmentation-game/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/the-segmentation-game/actions) [![codecov](https://codecov.io/gh/haesleinhuepf/the-segmentation-game/branch/main/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/the-segmentation-game) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/the-segmentation-game)](https://napari-hub.org/plugins/the-segmentation-game) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.6588373.svg)](https://doi.org/10.5281/zenodo.6588373)  Gamified image segmentation quality estimation  ![img.png](https://github.com/haesleinhuepf/the-segmentation-game/raw/main/images/screencast.gif)  ----------------------------------  ## Usage  The Segmentation Game allows to quantitatively compare segmentation results to a given ground truth annotation. This allows fine-tuning parameters of image processing workflows to get optimal segmentation quality.  It also allows comparing different segmentation algorithms and identify which algorithm performs best objectively.  The game can be found in napari's `Tools > Games > The Segmentation Game` menu.  ### Ground Truth Annotation  Before you can start playing the game, some annotated cells/nuclei are necessary to later compute segmentation quality from. Depending on the used metric, it might be sufficient to annotate a hand full of objects.  Use napari's annotation tools as shown below.  Use the `+` and `-` keys on your keyboard to increase and decrease the label number that is currently drawn. Note: Avoid label gaps. The labels must be continuously subsequent. If there are pixels annotated with value 2, there must be pixels annotated with value 1.  ![](https://github.com/haesleinhuepf/the-segmentation-game/raw/main/images/annotation.gif)  ### Parameter tuning  If you work with one of [napari's segmentation plugins](https://www.napari-hub.org/?search=segmentation&sort=relevance&page=1) that produce labels layers, you can tune their parameters and see how this influences segmentation quality.  ![](https://github.com/haesleinhuepf/the-segmentation-game/raw/main/images/parameter_tuning.gif)  ### Segmentation algorithm comparison  If you aim at comparing different segmentation algorithms, you can collect their results in label layers in the napari viewer. You can then select the segmentation result from the corresponding pulldown and save quantitative comparison results in the Highscore table.  ![](https://github.com/haesleinhuepf/the-segmentation-game/raw/main/images/algorithm_comparison.gif)  ## Metrics  Currently, these metrics are implemented: * Jaccard Index (sparse): The [Jaccard Index](https://en.wikipedia.org/wiki/Jaccard_index) is a measure of overlap.    It lies between 0 (no overlap) and 1 (perfect overlap).    For each annotated ground truth label, the maximum overlap of any segmented label is determined.    The mean overlap of all annotated labels serves as metric result. * Jaccard Index (binary): The annotated ground truth labels and the segmentation result are first binarized considering all annotated pixels as positive and all other labels as negative.   Afterwards, the overlap between the two binary images is computed. This allows comparing binary segmentation algorithms, such as thresholding techniques. * Jaccard Index (binary, sparse): The annotated ground truth image can contain values 1 (negative, false) and 2 (positive, true) and   the segmentation result image will be binarized (0: False, otherwise: True). This allows comparing object/no-object annotations with label images.     Receiver operating characteristic ([ROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic))    Consider a two-class thresholding problem (binary pixel-wise classification object/background), in which the outcomes are labeled either as positive (p) or negative (n). There are four possible outcomes from a binary classifier. If the outcome from a prediction is p and the actual value is also p, then it is called a true positive (TP); however if the actual value is n then it is said to be a false positive (FP). Conversely, a true negative (TN) has occurred when both the prediction outcome and the actual value are n, and false negative (FN) is when the prediction outcome is n while the actual value is p. We can organize result in table called confusion matrix, based on positive/neagtive results in row and true and false result in columns. From the confucsion matrix we can get many metrics with various usefulness. The curently implemented used for classification evaluation are:  * Sensitivity, recall, hit rate, or true positive rate (TPR): (TP)/ (TP + FP), Sensitivity and specificity mathematically describe the accuracy of a test which reports the presence or absence of a condition. Individuals for which the condition is satisfied are considered \"positive\" and those for which it is not are considered \"negative\". * Specificity, selectivity or true negative rate (TNR): (TN)/ (TN + FN), Sensitivity and specificity mathematically describe the accuracy of a test which reports the presence or absence of a condition. Individuals for which the condition is satisfied are considered \"positive\" and those for which it is not are considered \"negative\". * Precision or positive predictive value (PPV): (TP)/ (TP + FP), in computing and information science is the process of obtaining information system resources that are relevant to an information need from a collection of those resources. It is quantification for the TP events. * Accuracy: (TP + TN)/ (TP + FP + TN + FN), Accuracy measures observational error. Accuracy is how close or far off a given set of measurements are to their true value. However, it usually fails in imbalanced sets. * Balanced Accuracy: (TP/(TP+FN) + TN/(TN+FP))/2, Balanced Accuracy is trying to even out problems of accuracy in imbalanced sets. * F1 Score: 2*TP/(2*TP + FP + TN + FN), In statistical analysis of binary classification, the F-score or F-measure is a measure of a test's accuracy. It is calculated from the precision and recall of the test, where the precision is the number of true positive results divided by the number of all positive results, including those not identified. * Threat score (TS) or critical success index (CSI): TP/(TP + FP + FN), TC is another name for Jaccard Index (binary).  The ROC measures or confusion matrix is invaluable in cases when when our binary classifier is not ideal (which is often) and we are aiming to not get a general good result but specified low error. In that case we usually need to decide for some trade off, for example we need all (as many as possible) classified true positive objects, but we do not mind getting (usually as few as possible) false positive objects.  **What we want to achieve**  ![Precision-versus-accuracy, source: 10.13140/RG.2.1.1668.7603](https://github.com/martinschatz-cz/the-segmentation-game/blob/main/images/Precision-versus-accuracy.png)  When we are doing semantic segmentation, we are aiming to classify each pixel (ideally correctly) to each of our classes. But that can be hugr ammount of information, and our object might have significantly much less pixels then number of pixels belonging to background and/or other classes. Before choosing right metrics, we need to set up goal for our classification results. Idealy, we would like to have high accuracy and precission for ach class (as is on pictur above), but we might be happy getting high accuracy with good precision. Realisticaly we might need to be more specific, as to choose how big error we are prepared to accept, or decide if it is acceptable to have FN findings but no FP.  Picking up a metric for highly unbalanced classification as in semantic segmentation is challenging. Most of the classic metrics wil fail (but they are stil usable object-wise). And we usually stick up with Jaccard Index/Threat score, F1 Score or anything that will tell us result for TP rate (as we expect we will have less pixels for objects then background and/or other classes).  ## Literature recommendation  How to choose the right metric for comparing segmentation results is explained in this paper: * [Metrics reloaded: Pitfalls and recommendations for image analysis validation. Maier-Hein L. and Reinke A. et al.](https://arxiv.org/abs/2206.01653)  ## Related plugins  If you aim at automatically optimizing segmentation quality, there are also napari plugins available with this capability:  * [napari-accelerated-pixel-and-object-classification](https://www.napari-hub.org/plugins/napari-accelerated-pixel-and-object-classification) * [napari-workflow-optimizer](https://www.napari-hub.org/plugins/napari-workflow-optimizer)  ## Installation  You can install `the-segmentation-game` via [pip]:      pip install the-segmentation-game  ## Contributing  Contributions - especially new image segmentation quality metrics - are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"the-segmentation-game\" is free and open source software  ## Issues  If you encounter any problems, please open a thread on [image.sc](https://image.sc) along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/haesleinhuepf/the-segmentation-game/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "The segmentation game - for napari        Gamified image segmentation quality estimation   Usage The Segmentation Game allows to quantitatively compare segmentation results to a given ground truth annotation. This allows fine-tuning parameters of image processing workflows to get optimal segmentation quality.  It also allows comparing different segmentation algorithms and identify which algorithm performs best objectively. The game can be found in napari's Tools > Games > The Segmentation Game menu. Ground Truth Annotation Before you can start playing the game, some annotated cells/nuclei are necessary to later compute segmentation quality from. Depending on the used metric, it might be sufficient to annotate a hand full of objects.  Use napari's annotation tools as shown below.  Use the + and - keys on your keyboard to increase and decrease the label number that is currently drawn. Note: Avoid label gaps. The labels must be continuously subsequent. If there are pixels annotated with value 2, there must be pixels annotated with value 1.  Parameter tuning If you work with one of napari's segmentation plugins that produce labels layers, you can tune their parameters and see how this influences segmentation quality.  Segmentation algorithm comparison If you aim at comparing different segmentation algorithms, you can collect their results in label layers in the napari viewer. You can then select the segmentation result from the corresponding pulldown and save quantitative comparison results in the Highscore table.  Metrics Currently, these metrics are implemented: * Jaccard Index (sparse): The Jaccard Index is a measure of overlap.    It lies between 0 (no overlap) and 1 (perfect overlap).    For each annotated ground truth label, the maximum overlap of any segmented label is determined.    The mean overlap of all annotated labels serves as metric result. * Jaccard Index (binary): The annotated ground truth labels and the segmentation result are first binarized considering all annotated pixels as positive and all other labels as negative.   Afterwards, the overlap between the two binary images is computed. This allows comparing binary segmentation algorithms, such as thresholding techniques. * Jaccard Index (binary, sparse): The annotated ground truth image can contain values 1 (negative, false) and 2 (positive, true) and   the segmentation result image will be binarized (0: False, otherwise: True). This allows comparing object/no-object annotations with label images. Receiver operating characteristic (ROC) Consider a two-class thresholding problem (binary pixel-wise classification object/background), in which the outcomes are labeled either as positive (p) or negative (n). There are four possible outcomes from a binary classifier. If the outcome from a prediction is p and the actual value is also p, then it is called a true positive (TP); however if the actual value is n then it is said to be a false positive (FP). Conversely, a true negative (TN) has occurred when both the prediction outcome and the actual value are n, and false negative (FN) is when the prediction outcome is n while the actual value is p. We can organize result in table called confusion matrix, based on positive/neagtive results in row and true and false result in columns. From the confucsion matrix we can get many metrics with various usefulness. The curently implemented used for classification evaluation are:  Sensitivity, recall, hit rate, or true positive rate (TPR): (TP)/ (TP + FP), Sensitivity and specificity mathematically describe the accuracy of a test which reports the presence or absence of a condition. Individuals for which the condition is satisfied are considered \"positive\" and those for which it is not are considered \"negative\". Specificity, selectivity or true negative rate (TNR): (TN)/ (TN + FN), Sensitivity and specificity mathematically describe the accuracy of a test which reports the presence or absence of a condition. Individuals for which the condition is satisfied are considered \"positive\" and those for which it is not are considered \"negative\". Precision or positive predictive value (PPV): (TP)/ (TP + FP), in computing and information science is the process of obtaining information system resources that are relevant to an information need from a collection of those resources. It is quantification for the TP events. Accuracy: (TP + TN)/ (TP + FP + TN + FN), Accuracy measures observational error. Accuracy is how close or far off a given set of measurements are to their true value. However, it usually fails in imbalanced sets. Balanced Accuracy: (TP/(TP+FN) + TN/(TN+FP))/2, Balanced Accuracy is trying to even out problems of accuracy in imbalanced sets. F1 Score: 2TP/(2TP + FP + TN + FN), In statistical analysis of binary classification, the F-score or F-measure is a measure of a test's accuracy. It is calculated from the precision and recall of the test, where the precision is the number of true positive results divided by the number of all positive results, including those not identified. Threat score (TS) or critical success index (CSI): TP/(TP + FP + FN), TC is another name for Jaccard Index (binary).  The ROC measures or confusion matrix is invaluable in cases when when our binary classifier is not ideal (which is often) and we are aiming to not get a general good result but specified low error. In that case we usually need to decide for some trade off, for example we need all (as many as possible) classified true positive objects, but we do not mind getting (usually as few as possible) false positive objects. What we want to achieve  When we are doing semantic segmentation, we are aiming to classify each pixel (ideally correctly) to each of our classes. But that can be hugr ammount of information, and our object might have significantly much less pixels then number of pixels belonging to background and/or other classes. Before choosing right metrics, we need to set up goal for our classification results. Idealy, we would like to have high accuracy and precission for ach class (as is on pictur above), but we might be happy getting high accuracy with good precision. Realisticaly we might need to be more specific, as to choose how big error we are prepared to accept, or decide if it is acceptable to have FN findings but no FP. Picking up a metric for highly unbalanced classification as in semantic segmentation is challenging. Most of the classic metrics wil fail (but they are stil usable object-wise). And we usually stick up with Jaccard Index/Threat score, F1 Score or anything that will tell us result for TP rate (as we expect we will have less pixels for objects then background and/or other classes). Literature recommendation How to choose the right metric for comparing segmentation results is explained in this paper: * Metrics reloaded: Pitfalls and recommendations for image analysis validation. Maier-Hein L. and Reinke A. et al. Related plugins If you aim at automatically optimizing segmentation quality, there are also napari plugins available with this capability:  napari-accelerated-pixel-and-object-classification napari-workflow-optimizer  Installation You can install the-segmentation-game via pip: pip install the-segmentation-game  Contributing Contributions - especially new image segmentation quality metrics - are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"the-segmentation-game\" is free and open source software Issues If you encounter any problems, please open a thread on image.sc along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "the-segmentation-game",
    "documentation": "https://github.com/haesleinhuepf/the-segmentation-game#README.md",
    "first_released": "2022-05-27T19:11:51.793388Z",
    "license": "BSD-3-Clause",
    "name": "the-segmentation-game",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/haesleinhuepf/the-segmentation-game",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-10-02T21:24:14.068301Z",
    "report_issues": "https://github.com/haesleinhuepf/the-segmentation-game/issues",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "napari-tools-menu",
      "napari-skimage-regionprops",
      "scikit-learn"
    ],
    "summary": "Gamified image segmentation quality estimation",
    "support": "https://github.com/haesleinhuepf/the-segmentation-game/issues",
    "twitter": "",
    "version": "0.2.0",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      { "email": "robert.haase@tu-dresden.de", "name": "Robert Haase" }
    ],
    "code_repository": "https://github.com/haesleinhuepf/napari-workflow-optimizer",
    "description": "# napari-workflow-optimizer  [![License](https://img.shields.io/pypi/l/napari-workflow-optimizer.svg?color=green)](https://github.com/haesleinhuepf/napari-workflow-optimizer/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-workflow-optimizer.svg?color=green)](https://pypi.org/project/napari-workflow-optimizer) [![Python Version](https://img.shields.io/pypi/pyversions/napari-workflow-optimizer.svg?color=green)](https://python.org) [![tests](https://github.com/haesleinhuepf/napari-workflow-optimizer/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-workflow-optimizer/actions) [![codecov](https://codecov.io/gh/haesleinhuepf/napari-workflow-optimizer/branch/main/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-workflow-optimizer) [![Development Status](https://img.shields.io/pypi/status/napari-workflow-optimizer.svg)](https://en.wikipedia.org/wiki/Software_release_life_cycle#Alpha) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-workflow-optimizer)](https://napari-hub.org/plugins/napari-workflow-optimizer)  Optimize image processing workflows in napari for segmentation quality  ![img.png](https://github.com/haesleinhuepf/napari-workflow-optimizer/raw/main/docs/napari-workflow-optimizer.gif)  ## Usage  The starting point for workflow optimization is a workflow and some reference (\"ground truth\") labels image.  The label image can be a sparse annotation, which means only some objects and also parts of objets are annotated (see [hints](https://github.com/haesleinhuepf/napari-workflow-optimizer#optimization-hints)).  These datasets should be ready. You can reproduce the following procedure by downloading an  [examle raw image](https://github.com/haesleinhuepf/napari-workflow-optimizer/blob/main/demo/membranes_2d.tif) (derived from the  [scikit-image cells3d example data set](https://scikit-image.org/docs/dev/api/skimage.data.html#skimage.data.cells3d)) and a corresponding  [sparse annotation label image](https://github.com/haesleinhuepf/napari-workflow-optimizer/blob/main/demo/membranes_2d_sparse_labels.tif). For reproducing the following procedure, also follow the [installation instructions](https://github.com/haesleinhuepf/napari-workflow-optimizer#optimization-hints) below. The whole procedure is [also shown in this video](https://github.com/haesleinhuepf/napari-workflow-optimizer/raw/main/docs/napari-workflow-optimizer.mp4), an extended version of the trailer above.  ### Step 0: Loading data and setting up the workflow  Load the \"membranes_2d.tif\" data set, e.g. by drag&drop on napari and start the Assistant from the `Tools > Utilities > Assistant (clEsperanto)` menu.  ![](https://github.com/haesleinhuepf/napari-workflow-optimizer/raw/main/docs/screenshot1_start_raw.png)  Click the `Label` button and select as operation \"Seeded watershed using local minima as seeds and an intensity threshold (nsbatwm)\".  ![](https://github.com/haesleinhuepf/napari-workflow-optimizer/raw/main/docs/screenshot2_labeled_beginning.png)  Draw an annotation in a new labels layer or load the example spare annotation \"membranes_2d_sparse_labels.tif\".   ![](https://github.com/haesleinhuepf/napari-workflow-optimizer/raw/main/docs/screenshot4_loaded_manual_annotation.png)  In case the image is not displayed as label image, convert it to a label image by right-clicking on the entry in the layers list:  ![](https://github.com/haesleinhuepf/napari-workflow-optimizer/raw/main/docs/screenshot3_load_manual_annotation.png)  ### Step 1: The Workflow Optimizer  Start the Workflow Optimizer from the `Tools > Utilities > Workflow optimizer (Labels)` menu.  Configure the target layer, showing the label image that should be optimized. Select the manual annotation as reference layer for the optimization.  Consider increasing the number of iterations. This number depends on your segmenation problem.  In the present example, 100 iterations should be enough.  ![](https://github.com/haesleinhuepf/napari-workflow-optimizer/raw/main/docs/screenshot5_start_optimization.png)  The optimizer will plot quality over the number of iterations to show the progress of optimization.  To determine the quality, the optimizer will measure the maximum overlap ([Jaccard index](https://en.wikipedia.org/wiki/Jaccard_index))  of any labeled object over the manually annotated objects and calculate the mean of this value over all annotated objects. After a moment, optimization will finish and update the labeled image.  If your starting point for the optimization was already good, the result may now look better than before.  ![](https://github.com/haesleinhuepf/napari-workflow-optimizer/raw/main/docs/screenshot6_finished_optimization.png)  ### Step 2: Manual parameter space plotting  In case the result is not perfect yet (as the fringed segmentation above suggests), consider manual plotting of the  individual parameters and their relation to segmentation quality to get an idea about the surrounding parameter space. Therefore, click the `Plot` button next to one of the workflow parameters. Select the range in which the labeling quality should be determined (green arrows). In our example, the optimizer was setting the parameter to 2.34.  Thus, to demonstrate the procedure we plot the parameter space beween 0 and 10.  The quality plotted over this parameter obviously has a local maxium at 2.34, which was detected by the optimizer. However, it also has another local maxium at 8 and actually a plateau in the quality plot (orange arrows).  ![](https://github.com/haesleinhuepf/napari-workflow-optimizer/raw/main/docs/screenshot7_parameter_quality_plot.png)  For further optimization, we re-configure the algorithm and set a new starting point for optimization of the parameter to 8. Afterwards, we restart the optimization. It will then optimize the settings again from the new starting point.  ![](https://github.com/haesleinhuepf/napari-workflow-optimizer/raw/main/docs/screenshot8_start_optimization_again.png)  After another moment, optimization will finish again, potentially leading to an even better result.  ![](https://github.com/haesleinhuepf/napari-workflow-optimizer/raw/main/docs/screenshot9_finished_optimization_again.png)  ### Step 3: Visualization of results  Make sure the segmentation has high quality by inspecting the result visually. Use the `contour` setting of the labels layer and hide/show the outlines of the labeled layer:  ![](https://github.com/haesleinhuepf/napari-workflow-optimizer/raw/main/docs/screenshot10_contours_on.png)  ![](https://github.com/haesleinhuepf/napari-workflow-optimizer/raw/main/docs/screenshot11_contours_off.jpg)  ### Optimization Hints  The Workflow Optimizer uses the [Nelder-Mead simplex method](https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method) for optimizing parameters. This algorithm varies individual parameters and makes steps in the parameter space ideally following a gradient  to a local optimum. Hence, this algorithm may not be capable of determining a global optimum in parameter space.  Parameter optimization is no magic. If it does not immediately work on your data, plot the parameters as introduced in Step 2  and identify parameters with a clear gradient and those with many local maxima.  Consider optimizing the parameters with many local maxima manually and de-selecting their checkboxes for the optimization. The optimizer will then only optimize the parameters showing the clear gradient.  Repeat these steps a couple of times to get a feeling for your parameter space.   Furthermore, parameter optimization works well if * the initial settings are close to a good segmentation, * a small number of parameters (a short workflow) are optimized, * the reference annotation is prepared carefully and * the dataset is small. Consider using a small representative crop in case of bigger datasets.  ### Workflow optimization scripting  For optimizing workflows from within a jupyter notebook, check out our [example notebook for optimization using spare labels](https://github.com/haesleinhuepf/napari-workflow-optimizer/blob/main/demo/sparse_label_image_optimizer.ipynb).  The examples are more flexible than the graphical user interface and allow for example [optimizing intensity images](https://github.com/haesleinhuepf/napari-workflow-optimizer/blob/main/demo/intensity_image_optimizer.ipynb) and [binary images](https://github.com/haesleinhuepf/napari-workflow-optimizer/blob/main/demo/binary_image_optimizer.ipynb). The membrane segmentation workflow optimization similar to the one shown above is also available as [jupyter notebook](https://github.com/haesleinhuepf/napari-workflow-optimizer/blob/main/demo/membrane_segmentation.ipynb).  ### Known issues  If you change the workflow architecture after the optimizer window was opened, please re-open it to select the parameters that should be optimized. Changing parameters is ok and re-opening is not necessary.  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  ## Installation  Furthermore, to reproduce the procedure above, please download and install  [napari](https://napari.org/), [pyopencl](https://documen.tician.de/pyopencl/), the [napari-pyclesperanto-assistant](https://www.napari-hub.org/plugins/napari-pyclesperanto-assistant) and the [napari-segment-blobs-and-things-with-membranes](https://www.napari-hub.org/plugins/napari-segment-blobs-and-things-with-membranes) plugin. E.g. using  [conda](https://docs.conda.io/en/latest/) and [pip](https://pypi.org/project/pip/):  ``` conda create --name napari-opti python=3.8 conda activate napari-opti  conda install pyopencl napari pip install napari-pyclesperanto-assistant napari-segment-blobs-and-things-with-membranes pip install napari-workflow-optimizer ```  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-workflow-optimizer\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/haesleinhuepf/napari-workflow-optimizer/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-workflow-optimizer        Optimize image processing workflows in napari for segmentation quality  Usage The starting point for workflow optimization is a workflow and some reference (\"ground truth\") labels image.  The label image can be a sparse annotation, which means only some objects and also parts of objets are annotated (see hints).  These datasets should be ready. You can reproduce the following procedure by downloading an  examle raw image (derived from the  scikit-image cells3d example data set) and a corresponding  sparse annotation label image. For reproducing the following procedure, also follow the installation instructions below. The whole procedure is also shown in this video, an extended version of the trailer above. Step 0: Loading data and setting up the workflow Load the \"membranes_2d.tif\" data set, e.g. by drag&drop on napari and start the Assistant from the Tools > Utilities > Assistant (clEsperanto) menu.  Click the Label button and select as operation \"Seeded watershed using local minima as seeds and an intensity threshold (nsbatwm)\".  Draw an annotation in a new labels layer or load the example spare annotation \"membranes_2d_sparse_labels.tif\".   In case the image is not displayed as label image, convert it to a label image by right-clicking on the entry in the layers list:  Step 1: The Workflow Optimizer Start the Workflow Optimizer from the Tools > Utilities > Workflow optimizer (Labels) menu.  Configure the target layer, showing the label image that should be optimized. Select the manual annotation as reference layer for the optimization.  Consider increasing the number of iterations. This number depends on your segmenation problem.  In the present example, 100 iterations should be enough.  The optimizer will plot quality over the number of iterations to show the progress of optimization.  To determine the quality, the optimizer will measure the maximum overlap (Jaccard index)  of any labeled object over the manually annotated objects and calculate the mean of this value over all annotated objects. After a moment, optimization will finish and update the labeled image.  If your starting point for the optimization was already good, the result may now look better than before.  Step 2: Manual parameter space plotting In case the result is not perfect yet (as the fringed segmentation above suggests), consider manual plotting of the  individual parameters and their relation to segmentation quality to get an idea about the surrounding parameter space. Therefore, click the Plot button next to one of the workflow parameters. Select the range in which the labeling quality should be determined (green arrows). In our example, the optimizer was setting the parameter to 2.34.  Thus, to demonstrate the procedure we plot the parameter space beween 0 and 10.  The quality plotted over this parameter obviously has a local maxium at 2.34, which was detected by the optimizer. However, it also has another local maxium at 8 and actually a plateau in the quality plot (orange arrows).  For further optimization, we re-configure the algorithm and set a new starting point for optimization of the parameter to 8. Afterwards, we restart the optimization. It will then optimize the settings again from the new starting point.  After another moment, optimization will finish again, potentially leading to an even better result.  Step 3: Visualization of results Make sure the segmentation has high quality by inspecting the result visually. Use the contour setting of the labels layer and hide/show the outlines of the labeled layer:   Optimization Hints The Workflow Optimizer uses the Nelder-Mead simplex method for optimizing parameters. This algorithm varies individual parameters and makes steps in the parameter space ideally following a gradient  to a local optimum. Hence, this algorithm may not be capable of determining a global optimum in parameter space.  Parameter optimization is no magic. If it does not immediately work on your data, plot the parameters as introduced in Step 2  and identify parameters with a clear gradient and those with many local maxima.  Consider optimizing the parameters with many local maxima manually and de-selecting their checkboxes for the optimization. The optimizer will then only optimize the parameters showing the clear gradient.  Repeat these steps a couple of times to get a feeling for your parameter space.  Furthermore, parameter optimization works well if * the initial settings are close to a good segmentation, * a small number of parameters (a short workflow) are optimized, * the reference annotation is prepared carefully and * the dataset is small. Consider using a small representative crop in case of bigger datasets. Workflow optimization scripting For optimizing workflows from within a jupyter notebook, check out our example notebook for optimization using spare labels.  The examples are more flexible than the graphical user interface and allow for example optimizing intensity images and binary images. The membrane segmentation workflow optimization similar to the one shown above is also available as jupyter notebook. Known issues If you change the workflow architecture after the optimizer window was opened, please re-open it to select the parameters that should be optimized. Changing parameters is ok and re-opening is not necessary.  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template. Installation Furthermore, to reproduce the procedure above, please download and install  napari, pyopencl, the napari-pyclesperanto-assistant and the napari-segment-blobs-and-things-with-membranes plugin. E.g. using  conda and pip: ``` conda create --name napari-opti python=3.8 conda activate napari-opti conda install pyopencl napari pip install napari-pyclesperanto-assistant napari-segment-blobs-and-things-with-membranes pip install napari-workflow-optimizer ``` Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-workflow-optimizer\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 3 - Alpha"],
    "display_name": "napari-workflow-optimizer",
    "documentation": "https://github.com/haesleinhuepf/napari-workflow-optimizer#README.md",
    "first_released": "2021-12-24T17:11:46.811500Z",
    "license": "BSD-3-Clause",
    "name": "napari-workflow-optimizer",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/haesleinhuepf/napari-workflow-optimizer",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-04-15T16:35:08.144452Z",
    "report_issues": "https://github.com/haesleinhuepf/napari-workflow-optimizer/issues",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "pyclesperanto-prototype",
      "scikit-learn",
      "napari-time-slicer",
      "matplotlib",
      "scipy",
      "napari-workflows",
      "napari-assistant (>=0.1.9)"
    ],
    "summary": "Optimize image processing workflows in napari for segmentation quality",
    "support": "https://github.com/haesleinhuepf/napari-workflow-optimizer/issues",
    "twitter": "",
    "version": "0.1.4",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "email": "hectormz.git@gmail.com", "name": "Hector Munoz" }],
    "code_repository": "https://github.com/hectormz/napari-mat-images",
    "description": "# napari-mat-images  [![PyPI version](https://img.shields.io/pypi/v/napari-mat-images.svg)](https://pypi.org/project/napari-mat-images)  [![Python versions](https://img.shields.io/pypi/pyversions/napari-mat-images.svg)](https://pypi.org/project/napari-mat-images)  [![See Build Status on Azure Pipelines](https://dev.azure.com/hectormz-1/napari-mat-images/_apis/build/status/hectormz.napari-mat-images?branchName=main)](https://dev.azure.com/hectormz-1/napari-mat-images/_build/latest?definitionId=1&branchName=main)  ## Features  This plugin loads image variables stored in `MATLAB` `.mat` files into [napari](https://github.com/napari/napari).  It loads any variable that looks like an image. Presently, that includes any array with more than two dimensions with size greater than 20 pixels (determined by `shape_is_image()`).  If loading a variable with 3 or more dimensions, the plugin assumes that it is a stack of images, and the dimension with greatest size is the axis of the stack.  ### Loading Large Files  If loading a large `.mat` file saved in `HDF5`/`v7.3` format, chunks of the images are loaded as needed, resulting in fast initial load, but potentially slower scrolling.  Slices of the image stacks are randomly sampled to determine min/max contrast values.  ## Requirements  This plugin relies on `scipy` to load small `.mat` files and `h5py` (with `dask`) to load larger `HDF5`/`v7.3` `.mat` files.  It implicitly requires `napari` for use.  ## Installation  `napari-mat-images` requires [napari](https://github.com/napari/napari) to be installed, although it is not listed as a requirement for installation. This plugin relies on plugin functionality found in `napari` version \\\\> `0.2.12`. This can be installed via [pip](https://pypi.org/project/pip/) from [PyPI](https://pypi.org/project):      $ pip install napari>0.2.12  You can install `napari-mat-images` via [pip](https://pypi.org/project/pip/) from [PyPI](https://pypi.org/project):      $ pip install napari-mat-images  ## Usage  Once installed, the plugin will be used whenever trying to load a `.mat` file. This can be done from the `napari` GUI or commandline:      $ napari my_file.mat  ## Contributing  Contributions are very welcome. Tests can be run with [pytest](https://docs.pytest.org/en/latest/), please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3](http://opensource.org/licenses/BSD-3-Clause) license, `napari-mat-images` is free and open source software  ## Issues  If you encounter any problems, please [file an issue](https://github.com/hectormz/napari-mat-images/issues) along with a detailed description.  ---  This [napari](https://github.com/napari/napari) plugin was generated with [Cookiecutter](https://github.com/audreyr/cookiecutter) along with [napari](https://github.com/napari/napari)\\\\'s [cookiecutter-napari-plugin](https://github.com/napari/cookiecutter-napari-plugin) template.   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-mat-images    Features This plugin loads image variables stored in MATLAB .mat files into napari. It loads any variable that looks like an image. Presently, that includes any array with more than two dimensions with size greater than 20 pixels (determined by shape_is_image()). If loading a variable with 3 or more dimensions, the plugin assumes that it is a stack of images, and the dimension with greatest size is the axis of the stack. Loading Large Files If loading a large .mat file saved in HDF5/v7.3 format, chunks of the images are loaded as needed, resulting in fast initial load, but potentially slower scrolling. Slices of the image stacks are randomly sampled to determine min/max contrast values. Requirements This plugin relies on scipy to load small .mat files and h5py (with dask) to load larger HDF5/v7.3 .mat files. It implicitly requires napari for use. Installation napari-mat-images requires napari to be installed, although it is not listed as a requirement for installation. This plugin relies on plugin functionality found in napari version > 0.2.12. This can be installed via pip from PyPI: $ pip install napari>0.2.12  You can install napari-mat-images via pip from PyPI: $ pip install napari-mat-images  Usage Once installed, the plugin will be used whenever trying to load a .mat file. This can be done from the napari GUI or commandline: $ napari my_file.mat  Contributing Contributions are very welcome. Tests can be run with pytest, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, napari-mat-images is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.  This napari plugin was generated with Cookiecutter along with napari\\\\'s cookiecutter-napari-plugin template.",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "napari-mat-images",
    "documentation": "",
    "first_released": "2020-03-29T00:42:46.621697Z",
    "license": "BSD-3-Clause",
    "name": "napari-mat-images",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": [],
    "project_site": "https://github.com/hectormz/napari-mat-images",
    "python_version": ">=3.6",
    "reader_file_extensions": [],
    "release_date": "2021-06-30T14:03:05.689061Z",
    "report_issues": "",
    "requirements": ["dask[delayed]", "h5py", "numpy", "pluggy", "scipy"],
    "summary": "A plugin to load images stored in MATLAB .mat files with napari",
    "support": "",
    "twitter": "",
    "version": "0.1.3",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Robert Cudmore" }],
    "code_repository": "https://github.com/mapmanager/napari-layer-table",
    "conda": [{ "channel": "conda-forge", "package": "napari-layer-table" }],
    "description": "# napari-layer-table  [![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0) [![PyPI version](https://badge.fury.io/py/napari-layer-table.svg)](https://badge.fury.io/py/napari-layer-table) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-layer-table)](https://napari-hub.org/plugins/napari-layer-table) [![Python](https://img.shields.io/badge/python-3.7|3.8|3.9|3.10-blue.svg)](https://www.python.org/downloads/release/python-3100/) [![OS](https://img.shields.io/badge/OS-Linux|Windows|macOS-blue.svg)]() [![tests](https://github.com/mapmanager/napari-layer-table/workflows/Tests/badge.svg)](https://github.com/mapmanager/napari-layer-table/actions) [![codecov](https://codecov.io/gh/mapmanager/napari-layer-table/branch/main/graph/badge.svg?token=8S8EFI8NBC)](https://codecov.io/gh/mapmanager/napari-layer-table) <!-- [![PyPI](https://img.shields.io/pypi/v/napari-layer-table.svg?color=green)](https://pypi.org/project/napari-layer-table) --> <!-- [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-layer-table)](https://napari-hub.org/plugins/napari-layer-table) -->  A plugin to display a layer as a table.  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/plugins/stable/index.html -->  ## Installation  You can install `napari-layer-table` via [pip]:      pip install napari-layer-table    To install latest development version :      pip install git+https://github.com/mapmanager/napari-layer-table.git  ## Using the Plugin  You can use the napari-layer-table plugin to display points layer as a table.  - Open a napari viewer with a Points layer - Add the plugin to the napari viewer from Plugins menu -> Add dock widget -> napari-layer-table: Points Table - The selected layer is displayed in the table. - The table has columns for:     - Point symbol with face color     - Point coordinates (x,y,z)     - If the layer has properties, they are also shown as columns  ![](plugin-2.gif)  ## Plugin Features  - Bi-directional selection between layer and table. - Bi-directional deletion between layer and table. - Points added to the layer are added to the table. - Points moved in the layer are updated in the table. - Multiple points selected in the layer are also selected in the table - Changes to face color and symbol in the layer are updated in the table. - Ability to sort individual columns from low to high or high to low - `Refresh` button to manually refresh the table data - `btf` button to manually bring the layer whose table data is being shown to front  Right-click for context menu to:  - Toggle table columns on/off. - Toggle shift+click to add a point to the layer (no need to switch viewer mode) - Copy table to clipboard  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [GNU GPL v3.0] license, \"napari-layer-table\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/mapmanager/napari-layer-table/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-layer-table          A plugin to display a layer as a table.  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install napari-layer-table via pip: pip install napari-layer-table  To install latest development version : pip install git+https://github.com/mapmanager/napari-layer-table.git  Using the Plugin You can use the napari-layer-table plugin to display points layer as a table.  Open a napari viewer with a Points layer Add the plugin to the napari viewer from Plugins menu -> Add dock widget -> napari-layer-table: Points Table The selected layer is displayed in the table. The table has columns for: Point symbol with face color Point coordinates (x,y,z) If the layer has properties, they are also shown as columns     Plugin Features  Bi-directional selection between layer and table. Bi-directional deletion between layer and table. Points added to the layer are added to the table. Points moved in the layer are updated in the table. Multiple points selected in the layer are also selected in the table Changes to face color and symbol in the layer are updated in the table. Ability to sort individual columns from low to high or high to low Refresh button to manually refresh the table data btf button to manually bring the layer whose table data is being shown to front  Right-click for context menu to:  Toggle table columns on/off. Toggle shift+click to add a point to the layer (no need to switch viewer mode) Copy table to clipboard  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the GNU GPL v3.0 license, \"napari-layer-table\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "Layer Table",
    "documentation": "https://github.com/mapmanager/napari-layer-table#README.md",
    "first_released": "2022-04-20T19:31:44.330744Z",
    "license": "GPL-3.0",
    "name": "napari-layer-table",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/mapmanager/napari-layer-table",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-10-23T16:38:13.083746Z",
    "report_issues": "https://github.com/mapmanager/napari-layer-table/issues",
    "requirements": ["numpy"],
    "summary": "A plugin to display a layer as a table.",
    "support": "https://github.com/mapmanager/napari-layer-table/issues",
    "twitter": "",
    "version": "0.0.10",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      { "name": "Gursharan Ahir" },
      { "name": "Michael Sandler" },
      { "name": "Ryan Thiermann" }
    ],
    "code_repository": "https://github.com/ahirsharan/napari-mm3",
    "conda": [],
    "description": "# napari-mm3  [![License](https://img.shields.io/pypi/l/napari-mm3.svg?color=green)](https://github.com/ahirsharan/napari-mm3/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-mm3.svg?color=green)](https://pypi.org/project/napari-mm3) [![Python Version](https://img.shields.io/pypi/pyversions/napari-mm3.svg?color=green)](https://python.org) [![tests](https://github.com/ahirsharan/napari-mm3/workflows/tests/badge.svg)](https://github.com/ahirsharan/napari-mm3/actions) [![codecov](https://codecov.io/gh/ahirsharan/napari-mm3/branch/main/graph/badge.svg)](https://codecov.io/gh/ahirsharan/napari-mm3) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-mm3)](https://napari-hub.org/plugins/napari-mm3)  A plugin for Mother Machine Image Analysis by [Jun Lab](https://jun.ucsd.edu/).  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/plugins/stable/index.html -->  ## Installation  Load up a new environment. We run the following command, replacing `environment-name-here` with a name of your choosing:  `conda create -y -n environment-name-here python=3.9 napari tensorflow`   Now, to install our code: if you would like to have the latest version, do the following.  1. You can clone the repository with `git clone git@github.com:junlabucsd/napari-mm3.git` (SSH) or `git clone https://github.com/junlabucsd/napari-mm3.git` (https) 2. With your environment active, run `pip install -e .` from inside your cloned repo.  If you would like to have a more stable verison, simply run `pip install napari-mm3`.  NOTE: Not running the conda command and trying to install things in a different way may lead to difficult issues with PyQt5.  We recommend following the above commands to simplify the situation.  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## Usage guide  **Brief video introduction:** available [here](https://youtu.be/7MCiGTg6mq4)  ### a. Preprocessing  * [nd2ToTIFF](https://github.com/junlabucsd/napari-mm3/blob/main/docs/nd2totiff-widget.md) -- Turn your nd2 microscopy data into TIFFs. If your data is not in the nd2 format, follow the [input image guidelines](/docs/input-images-guidelines.md). Make sure to set 'image source' in Compile to 'Other'.  * [Compile](https://github.com/junlabucsd/napari-mm3/blob/main/docs/compile-widget.md) -- Locate traps, separate their timelapses into their own TIFFs, and return metadata.  ### b. Segmentation  ___With Otsu:___  * [PickChannels](https://github.com/junlabucsd/napari-mm3/blob/main/docs/pickchannels-widget.md) -- User guided selection of empty and full traps.  * [Subtract](https://github.com/junlabucsd/napari-mm3/blob/main/docs/subtract-widget.md) -- Remove (via subtraction) empty traps from the background of traps that contain cells; run this on the phase contrast channel.  * [SegmentOtsu](https://github.com/junlabucsd/napari-mm3/blob/main/docs/segmentotsu-widget.md) -- Use Otsu segmentation to segment cells.  ___With UNet:___  * Annotate -- annotate images for ML (U-Net or similar) training purposes; you can generate a model via TODO.  * [SegmentUnet](https://github.com/junlabucsd/napari-mm3/blob/main/docs/segmentunet-widget.md) -- Run U-Net segmentation (you will need to supply your own model)  ### c. Tracking  * [Track](https://github.com/junlabucsd/napari-mm3/blob/main/docs/track-widget.md) -- Acquire individual cell properties and track lineages.  ### d. Fluorescence data analysis  * [PickChannels](https://github.com/junlabucsd/napari-mm3/blob/main/docs/pickchannels-widget.md) -- If you've already done this (e.g. for otsu segmentation), no need to do it again. User guided selection of empty and full traps.   * [Subtract](https://github.com/junlabucsd/napari-mm3/blob/main/docs/subtract-widget.md) -- Remove (via subtraction) empty traps from the background of traps that contain cells. This time, run this on your fluorescence channels.  * [Colors](https://github.com/junlabucsd/napari-mm3/blob/main/docs/colors-widget.md) -- Calculate fluorescence information.  ### e. (Uncommon) Foci tracking  * [Foci](https://github.com/junlabucsd/napari-mm3/blob/main/docs/foci-widget.md) -- We use this to track `foci' (bright fluorescent spots) inside of cells.   ### f. Outputs, inputs, and file structure Finally, to better understand the data formats, you may wish to refer to the following documents:  * [Input image guidelines](https://github.com/junlabucsd/napari-mm3/blob/main/docs/input-images-guidelines.md)  * [File structure](https://github.com/junlabucsd/napari-mm3/blob/main/docs/file-structure.md)  * [Output file structure](https://github.com/junlabucsd/napari-mm3/blob/main/docs/Cell-class-docs.md)  ## License  Distributed under the terms of the [BSD-3] license, \"napari-mm3\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/ahirsharan/napari-mm3/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-mm3       A plugin for Mother Machine Image Analysis by Jun Lab.  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation Load up a new environment. We run the following command, replacing environment-name-here with a name of your choosing: conda create -y -n environment-name-here python=3.9 napari tensorflow  Now, to install our code: if you would like to have the latest version, do the following.  You can clone the repository with git clone git@github.com:junlabucsd/napari-mm3.git (SSH) or git clone https://github.com/junlabucsd/napari-mm3.git (https) With your environment active, run pip install -e . from inside your cloned repo.  If you would like to have a more stable verison, simply run pip install napari-mm3. NOTE: Not running the conda command and trying to install things in a different way may lead to difficult issues with PyQt5.  We recommend following the above commands to simplify the situation. Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. Usage guide Brief video introduction: available here a. Preprocessing   nd2ToTIFF -- Turn your nd2 microscopy data into TIFFs. If your data is not in the nd2 format, follow the input image guidelines. Make sure to set 'image source' in Compile to 'Other'.   Compile -- Locate traps, separate their timelapses into their own TIFFs, and return metadata.   b. Segmentation With Otsu:   PickChannels -- User guided selection of empty and full traps.   Subtract -- Remove (via subtraction) empty traps from the background of traps that contain cells; run this on the phase contrast channel.   SegmentOtsu -- Use Otsu segmentation to segment cells.   With UNet:   Annotate -- annotate images for ML (U-Net or similar) training purposes; you can generate a model via TODO.   SegmentUnet -- Run U-Net segmentation (you will need to supply your own model)   c. Tracking  Track -- Acquire individual cell properties and track lineages.  d. Fluorescence data analysis   PickChannels -- If you've already done this (e.g. for otsu segmentation), no need to do it again. User guided selection of empty and full traps.    Subtract -- Remove (via subtraction) empty traps from the background of traps that contain cells. This time, run this on your fluorescence channels.   Colors -- Calculate fluorescence information.   e. (Uncommon) Foci tracking  Foci -- We use this to track `foci' (bright fluorescent spots) inside of cells.  f. Outputs, inputs, and file structure Finally, to better understand the data formats, you may wish to refer to the following documents:   Input image guidelines   File structure   Output file structure   License Distributed under the terms of the BSD-3 license, \"napari-mm3\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari-mm3",
    "documentation": "https://github.com/ahirsharan/napari-mm3#README.md",
    "first_released": "2022-06-02T19:59:32.471036Z",
    "license": "BSD-3-Clause",
    "name": "napari-mm3",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/junlabucsd/napari-mm3",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-10-06T20:52:51.778388Z",
    "report_issues": "https://github.com/ahirsharan/napari-mm3/issues",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "h5py",
      "tifffile (==2021.11.2)",
      "scikit-learn",
      "scikit-image",
      "tensorflow",
      "pims-nd2",
      "seaborn"
    ],
    "summary": "a plugin for mother machine image analysis",
    "support": "https://github.com/ahirsharan/napari-mm3/issues",
    "twitter": "",
    "version": "0.0.10",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      { "email": "robert.haase@tu-dresden.de", "name": "Robert Haase" }
    ],
    "code_repository": "https://github.com/haesleinhuepf/napari-mouse-controls",
    "description": "# napari-mouse-controls  [![License](https://img.shields.io/pypi/l/napari-mouse-controls.svg?color=green)](https://github.com/haesleinhuepf/napari-mouse-controls/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-mouse-controls.svg?color=green)](https://pypi.org/project/napari-mouse-controls) [![Python Version](https://img.shields.io/pypi/pyversions/napari-mouse-controls.svg?color=green)](https://python.org) [![tests](https://github.com/haesleinhuepf/napari-mouse-controls/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-mouse-controls/actions) [![codecov](https://codecov.io/gh/haesleinhuepf/napari-mouse-controls/branch/main/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-mouse-controls) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-mouse-controls)](https://napari-hub.org/plugins/napari-mouse-controls)  Control zoom, slicing and contrast windowing with mouse and touch screen  ----------------------------------  ## Usage  You find the mouse control panel in the menu `Tools > Utilities > Mouse controls`  ### Zoom  After clicking the Zoom button ![](https://github.com/haesleinhuepf/napari-mouse-controls/raw/main/src/napari_mouse_controls/icons/Zoom.png), you can click in the napari canvas and move the mouse up and down to zoom in and out.  ![](https://github.com/haesleinhuepf/napari-mouse-controls/raw/main/docs/zoom.gif)  ### Slicing  After clicking the Slicing button ![](https://github.com/haesleinhuepf/napari-mouse-controls/raw/main/src/napari_mouse_controls/icons/Slicing.png), you can control the currently displayed slice by moving the mouse. By moving the mouse up and down, you control the currently selected Z-plane. By moving the mouse left and right, you control the currently seleted time point.  ![](https://github.com/haesleinhuepf/napari-mouse-controls/raw/main/docs/slicing.gif)  ### Windowing  After clicking the Windowing button ![](https://github.com/haesleinhuepf/napari-mouse-controls/raw/main/src/napari_mouse_controls/icons/Windowing.png), you can modify the brightness and contrast by moving the mouse.  By moving the mouse up and down, you control window width of the range of displayed grey values (max - min). By moving the mouse left and right, you control the center of the grey value window.   ![](https://github.com/haesleinhuepf/napari-mouse-controls/raw/main/docs/windowing.gif)  ### Normal / default mode  Click the Default button ![](https://github.com/haesleinhuepf/napari-mouse-controls/raw/main/src/napari_mouse_controls/icons/Default.png) to return to napari's normal mode.   This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.   ## Installation  You can install `napari-mouse-controls` via [pip]:      pip install napari-mouse-controls  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-mouse-controls\" is free and open source software  ## Issues  If you encounter any problems, please create a thread on [image.sc] along with a detailed description and tag [@haesleinhuepf].  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/haesleinhuepf/napari-mouse-controls/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ [image.sc]: https://image.sc  [@haesleinhuepf]: https://twitter.com/haesleinhuepf   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-mouse-controls       Control zoom, slicing and contrast windowing with mouse and touch screen  Usage You find the mouse control panel in the menu Tools > Utilities > Mouse controls Zoom After clicking the Zoom button , you can click in the napari canvas and move the mouse up and down to zoom in and out.  Slicing After clicking the Slicing button , you can control the currently displayed slice by moving the mouse. By moving the mouse up and down, you control the currently selected Z-plane. By moving the mouse left and right, you control the currently seleted time point.  Windowing After clicking the Windowing button , you can modify the brightness and contrast by moving the mouse.  By moving the mouse up and down, you control window width of the range of displayed grey values (max - min). By moving the mouse left and right, you control the center of the grey value window.   Normal / default mode Click the Default button  to return to napari's normal mode. This napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template. Installation You can install napari-mouse-controls via pip: pip install napari-mouse-controls  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-mouse-controls\" is free and open source software Issues If you encounter any problems, please create a thread on image.sc along with a detailed description and tag @haesleinhuepf.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari-mouse-controls",
    "documentation": "https://github.com/haesleinhuepf/napari-mouse-controls#README.md",
    "first_released": "2021-10-30T19:11:36.638852Z",
    "license": "BSD-3-Clause",
    "name": "napari-mouse-controls",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/haesleinhuepf/napari-mouse-controls",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-01-01T16:28:08.331706Z",
    "report_issues": "https://github.com/haesleinhuepf/napari-mouse-control/issues",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "napari",
      "napari-tools-menu"
    ],
    "summary": "Control napari using a touch screen",
    "support": "https://github.com/haesleinhuepf/napari-mouse-controls/issues",
    "twitter": "",
    "version": "0.1.3",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      {
        "email": "p.schoennenbeck@fz-juelich.de",
        "name": "Philipp Schoennenbeck"
      }
    ],
    "code_repository": "https://github.com/Croxa/napari-mrcfile_handler",
    "description": "# napari-mrcfile_handler  [![License](https://img.shields.io/pypi/l/napari-mrcfile_handler.svg?color=green)](https://github.com/Croxa/napari-mrcfile_handler/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-mrcfile_handler.svg?color=green)](https://pypi.org/project/napari-mrcfile_handler) [![Python Version](https://img.shields.io/pypi/pyversions/napari-mrcfile_handler.svg?color=green)](https://python.org) [![tests](https://github.com/Croxa/napari-mrcfile_handler/workflows/tests/badge.svg)](https://github.com/Croxa/napari-mrcfile_handler/actions) [![codecov](https://codecov.io/gh/Croxa/napari-mrcfile_handler/branch/master/graph/badge.svg)](https://codecov.io/gh/Croxa/napari-mrcfile_handler)  A simple plugin to read, write and adjust mrcfiles in napari.  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/docs/plugins/index.html -->  ## Installation  You can install `napari-mrcfile_handler` via [pip]:      pip install napari-mrcfile_handler  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-mrcfile_handler\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin [file an issue]: https://github.com/Croxa/napari-mrcfile_handler/issues [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-mrcfile_handler      A simple plugin to read, write and adjust mrcfiles in napari.  This napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.  Installation You can install napari-mrcfile_handler via pip: pip install napari-mrcfile_handler  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-mrcfile_handler\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari-mrcfile-handler",
    "documentation": "https://github.com/Croxa/napari-mrcfile_handler#README.md",
    "first_released": "2021-08-30T10:31:32.458679Z",
    "license": "BSD-3-Clause",
    "name": "napari-mrcfile-handler",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader", "writer", "widget"],
    "project_site": "https://github.com/Croxa/napari-mrcfile_handler",
    "python_version": ">=3.7",
    "reader_file_extensions": ["*"],
    "release_date": "2021-09-01T14:32:19.283708Z",
    "report_issues": "https://github.com/Croxa/napari-mrcfile_handler/issues",
    "requirements": ["napari-plugin-engine (>=0.1.4)", "numpy", "mrcfile"],
    "summary": "A simple plugin to read, write and adjust mrcfiles in napari.",
    "support": "https://github.com/Croxa/napari-mrcfile_handler/issues",
    "twitter": "",
    "version": "0.0.6",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": ["labels", "image"]
  },
  {
    "authors": [
      {
        "email": "david.pinto@bioch.ox.ac.uk",
        "name": "David Miguel Susano Pinto"
      }
    ],
    "code_repository": null,
    "description": "Microscope control plugin for Napari via Python Microscope.  Current development stage is whatever comes before alpha and \"proof of concept\".  To test -------  I haven't had access to real hardware yet, so this has all been developed with simulated devices.  1. Start the device server with simulated devices.      a. Create a device server configuration file like so::          import microscope         from microscope.device_server import device         from microscope.simulators import (             SimulatedCamera,             SimulatedFilterWheel,             SimulatedLightSource,             SimulatedStage,         )          DEVICES = [             device(SimulatedCamera, \"localhost\", 8000,),             device(SimulatedLightSource, \"localhost\", 8001),             device(SimulatedFilterWheel, \"localhost\", 8002,                    {\"positions\": 6}),             device(SimulatedStage, \"localhost\", 8003,                    {\"limits\": {\"X\": microscope.AxisLimits(0, 25000),                                \"Y\": microscope.AxisLimits(0, 12000)}}),         ]      b. Start the device server (ensure port 8000-8003 are unused)::          $ device-server path-to-microscope-config.py  2. Start napari  3. Plugins > Add Dock Widget > microscope: MicroscopeWidget  4. Connect to the camera:      a. On the new widget, click on the \"Add device\" button.      b. Enter the camera URI `PYRO:SimulatedCamera@localhost:8000`  5. Tick the `Enabled` box to enable the camera and then press the \"Snap\" button.  6. A random values image will appear displayed on the napari viewer. Keep pressing the \"Snap\" button to get new images.  The top left corner of the image is the simulated image number.  7. Connect to the other simulated devices.  Their URIs are:      a. PYRO:SimulatedLightSource@localhost:8001     b. PYRO:SimulatedFilterWheel@localhost:8002     c. PYRO:SimulatedStage@localhost:8003  8. Changing the other simulated devices, doesn't really do much (but does change state of the devices, as can be seen in the logs)   Test with stage aware camera ----------------------------  This is pretty much the same as before but one can use a large RGB TIFF (histology samples are perfect) to simulate a camera that returns subsections of the image file based on the simulated stage position.  For quick example, try::      wget https://zenodo.org/record/1445489/files/B0002.tif  And use the following device server configuration file::      from microscope.device_server import device     from microscope.simulators.stage_aware_camera import simulated_setup_from_image      DEVICES = [         device(simulated_setup_from_image, \"localhost\", 8000,                conf={\"filepath\": \"B0002.tif\"}),     ]  The URI for the devices will be::      PYRO:camera@localhost:8000     PYRO:filterwheel@localhost:8000     PYRO:stage@localhost:8000  Changing the filterwheel changes which channel from the image is returned.  Changing the stage coordinates changes the image that is returned (but beware of the corners, pixels outside the image size are not handled yet and will give an error).",
    "description_content_type": "",
    "description_text": "Microscope control plugin for Napari via Python Microscope. Current development stage is whatever comes before alpha and \"proof of concept\". To test I haven't had access to real hardware yet, so this has all been developed with simulated devices.   Start the device server with simulated devices. a. Create a device server configuration file like so:: import microscope from microscope.device_server import device from microscope.simulators import (     SimulatedCamera,     SimulatedFilterWheel,     SimulatedLightSource,     SimulatedStage, )  DEVICES = [     device(SimulatedCamera, \"localhost\", 8000,),     device(SimulatedLightSource, \"localhost\", 8001),     device(SimulatedFilterWheel, \"localhost\", 8002,            {\"positions\": 6}),     device(SimulatedStage, \"localhost\", 8003,            {\"limits\": {\"X\": microscope.AxisLimits(0, 25000),                        \"Y\": microscope.AxisLimits(0, 12000)}}), ]  b. Start the device server (ensure port 8000-8003 are unused):: $ device-server path-to-microscope-config.py    Start napari   Plugins > Add Dock Widget > microscope: MicroscopeWidget   Connect to the camera: a. On the new widget, click on the \"Add device\" button. b. Enter the camera URI PYRO:SimulatedCamera@localhost:8000   Tick the Enabled box to enable the camera and then press the \"Snap\" button.   A random values image will appear displayed on the napari viewer. Keep pressing the \"Snap\" button to get new images.  The top left corner of the image is the simulated image number.   Connect to the other simulated devices.  Their URIs are: a. PYRO:SimulatedLightSource@localhost:8001 b. PYRO:SimulatedFilterWheel@localhost:8002 c. PYRO:SimulatedStage@localhost:8003   Changing the other simulated devices, doesn't really do much (but does change state of the devices, as can be seen in the logs)   Test with stage aware camera This is pretty much the same as before but one can use a large RGB TIFF (histology samples are perfect) to simulate a camera that returns subsections of the image file based on the simulated stage position. For quick example, try:: wget https://zenodo.org/record/1445489/files/B0002.tif  And use the following device server configuration file:: from microscope.device_server import device from microscope.simulators.stage_aware_camera import simulated_setup_from_image  DEVICES = [     device(simulated_setup_from_image, \"localhost\", 8000,            conf={\"filepath\": \"B0002.tif\"}), ]  The URI for the devices will be:: PYRO:camera@localhost:8000 PYRO:filterwheel@localhost:8000 PYRO:stage@localhost:8000  Changing the filterwheel changes which channel from the image is returned.  Changing the stage coordinates changes the image that is returned (but beware of the corners, pixels outside the image size are not handled yet and will give an error).",
    "development_status": ["Development Status :: 3 - Alpha"],
    "display_name": "napari-microscope",
    "documentation": "",
    "first_released": "2021-01-22T19:50:42.567472Z",
    "license": "GPL-3.0-or-later",
    "name": "napari-microscope",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "",
    "python_version": ">=3.6",
    "reader_file_extensions": [],
    "release_date": "2021-11-23T12:10:33.763843Z",
    "report_issues": "",
    "requirements": null,
    "summary": "Napari plugin for Microscope.",
    "support": "",
    "twitter": "",
    "version": "0.0.3",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "email": "brisvag@gmail.com", "name": "Lorenzo Gaifas" }],
    "code_repository": "https://github.com/brisvag/napari-molecule-reader",
    "description": "# napari-molecule-reader  [![License](https://img.shields.io/pypi/l/napari-molecule-reader.svg?color=green)](https://github.com/brisvag/napari-molecule-reader/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-molecule-reader.svg?color=green)](https://pypi.org/project/napari-molecule-reader) [![Python Version](https://img.shields.io/pypi/pyversions/napari-molecule-reader.svg?color=green)](https://python.org) [![tests](https://github.com/brisvag/napari-molecule-reader/workflows/tests/badge.svg)](https://github.com/brisvag/napari-molecule-reader/actions) [![codecov](https://codecov.io/gh/brisvag/napari-molecule-reader/branch/main/graph/badge.svg)](https://codecov.io/gh/brisvag/napari-molecule-reader) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-molecule-reader)](https://napari-hub.org/plugins/napari-molecule-reader)  A napari plugin that read molecular structure files. It reads PDB and MMCIF files using [`atomium`](https://github.com/samirelanduk/atomium), expanding molecular assemblies to a full visualization. Data is loaded into napari as `Points` for ball representation and `Vectors` for stick representation. If multiple models or assemblies are detected, they will be loaded as separate objects.  https://user-images.githubusercontent.com/23482191/150109390-bd7fb3b4-79b4-43da-aafc-20921714df25.mp4  TODO list: - [] handle alternate locations (i.e: different conformations in the same pdb model)  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/plugins/stable/index.html -->  ## Installation  You can install `napari-molecule-reader` via [pip]:      pip install napari-molecule-reader    To install latest development version :      pip install git+https://github.com/brisvag/napari-molecule-reader.git   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-molecule-reader\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/brisvag/napari-molecule-reader/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-molecule-reader       A napari plugin that read molecular structure files. It reads PDB and MMCIF files using atomium, expanding molecular assemblies to a full visualization. Data is loaded into napari as Points for ball representation and Vectors for stick representation. If multiple models or assemblies are detected, they will be loaded as separate objects. https://user-images.githubusercontent.com/23482191/150109390-bd7fb3b4-79b4-43da-aafc-20921714df25.mp4 TODO list: - [] handle alternate locations (i.e: different conformations in the same pdb model)  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install napari-molecule-reader via pip: pip install napari-molecule-reader  To install latest development version : pip install git+https://github.com/brisvag/napari-molecule-reader.git  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-molecule-reader\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari molecule reader",
    "documentation": "https://github.com/brisvag/napari-molecule-reader#README.md",
    "first_released": "2021-12-21T11:39:40.715897Z",
    "license": "BSD-3-Clause",
    "name": "napari-molecule-reader",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader"],
    "project_site": "https://github.com/brisvag/napari-molecule-reader",
    "python_version": ">=3.7",
    "reader_file_extensions": ["*.pdb", "*.cif"],
    "release_date": "2022-01-31T17:59:24.494345Z",
    "report_issues": "https://github.com/brisvag/napari-molecule-reader/issues",
    "requirements": ["numpy", "pandas", "scipy", "atomium"],
    "summary": "A napari plugin that read molecular structure files.",
    "support": "https://github.com/brisvag/napari-molecule-reader/issues",
    "twitter": "",
    "version": "0.1.2",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Kevin Tan" }],
    "code_repository": null,
    "description": "# napari-live-flim  [![License GNU GPL v3.0](https://img.shields.io/pypi/l/napari-live-flim.svg?color=green)](https://github.com/facetorched/napari-live-flim/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-live-flim.svg?color=green)](https://pypi.org/project/napari-live-flim) [![Python Version](https://img.shields.io/pypi/pyversions/napari-live-flim.svg?color=green)](https://python.org) [![tests](https://github.com/facetorched/napari-live-flim/workflows/tests/badge.svg)](https://github.com/facetorched/napari-live-flim/actions) [![codecov](https://codecov.io/gh/facetorched/napari-live-flim/branch/main/graph/badge.svg)](https://codecov.io/gh/facetorched/napari-live-flim) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-live-flim)](https://napari-hub.org/plugins/napari-live-flim)  A plugin for real-time FLIM analysis  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/plugins/index.html -->  ## Installation  You can install `napari-live-flim` via [pip]:      pip install napari-live-flim     ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [GNU GPL v3.0] license, \"napari-live-flim\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-live-flim       A plugin for real-time FLIM analysis  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install napari-live-flim via pip: pip install napari-live-flim  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the GNU GPL v3.0 license, \"napari-live-flim\" is free and open source software Issues If you encounter any problems, please [file an issue] along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "Napari Live Flim",
    "documentation": "",
    "first_released": "2022-11-09T21:14:47.769823Z",
    "license": "GPL-3.0-only",
    "name": "napari-live-flim",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-11-09T21:14:47.769823Z",
    "report_issues": "",
    "requirements": [
      "dataclasses-json",
      "flimlib",
      "magicgui",
      "matplotlib",
      "numpy",
      "qtpy",
      "scipy",
      "superqt",
      "vispy",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "summary": "A plugin for real-time FLIM analysis",
    "support": "",
    "twitter": "",
    "version": "0.1.0",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Alan R. Lowe" }],
    "category": { "Workflow step": ["Object tracking"] },
    "category_hierarchy": {
      "Workflow step": [
        ["Object tracking", "Isolated object tracking", "Cell tracking"],
        ["Object tracking", "Cell lineage extraction"],
        ["Object tracking"]
      ]
    },
    "code_repository": "https://github.com/quantumjot/arboretum",
    "conda": [{ "channel": "conda-forge", "package": "napari-arboretum" }],
    "description": " <!--[![Downloads](https://pepy.tech/badge/napari-arboretum)](https://pepy.tech/project/napari-arboretum)--> [![License](https://img.shields.io/pypi/l/napari-arboretum.svg?color=green)](https://github.com/lowe-lab-ucl/napari-arboretum/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-arboretum.svg?color=green)](https://pypi.org/project/napari-arboretum) [![Python Version](https://img.shields.io/pypi/pyversions/napari-arboretum.svg?color=green)](https://python.org) [![tests](https://github.com/lowe-lab-ucl/arboretum/workflows/tests/badge.svg)](https://github.com/quantumjot/arboretum/actions) [![codecov](https://codecov.io/gh/lowe-lab-ucl/arboretum/branch/master/graph/badge.svg?token=2M2HhM60op)](https://codecov.io/gh/lowe-lab-ucl/arboretum)  # Arboretum   ![](https://raw.githubusercontent.com/lowe-lab-ucl/arboretum/master/examples/arboretum.gif) *Automated cell tracking and lineage tree reconstruction*.  ### Overview  A dockable widget for [Napari](https://github.com/napari) for visualizing cell lineage trees.  Features: + Lineage tree plot widget + Integration with [btrack](https://github.com/quantumjot/BayesianTracker)  ---  ### Usage  Once installed, Arboretum will be visible in the `Plugins > Add Dock Widget > napari-arboretum` menu in napari.  To visualize a lineage tree, (double) click on one of the tracks in a napari `Tracks` layer.    ### Examples  You can use the example script to display some sample tracking data in napari and load the arboretum tree viewer:  ```sh python ./examples/show_sample_data.py ```  Alternatively, you can use *btrack* to generate tracks from your image data. See the example notebook here: https://github.com/quantumjot/BayesianTracker/blob/master/examples  ---  ### History  This project has changed considerably. The `Tracks` layer, originally developed for this plugin, is now an official layer type in napari. Read the napari documentation here:  https://napari.org/api/napari.layers.Tracks.html   To view the legacy version of this plugin, visit the legacy branch: https://github.com/quantumjot/arboretum/tree/v1-legacy ",
    "description_content_type": "text/markdown",
    "description_text": "      Arboretum  Automated cell tracking and lineage tree reconstruction. Overview A dockable widget for Napari for visualizing cell lineage trees. Features: + Lineage tree plot widget + Integration with btrack  Usage Once installed, Arboretum will be visible in the Plugins > Add Dock Widget > napari-arboretum menu in napari.  To visualize a lineage tree, (double) click on one of the tracks in a napari Tracks layer. Examples You can use the example script to display some sample tracking data in napari and load the arboretum tree viewer: sh python ./examples/show_sample_data.py Alternatively, you can use btrack to generate tracks from your image data. See the example notebook here: https://github.com/quantumjot/BayesianTracker/blob/master/examples  History This project has changed considerably. The Tracks layer, originally developed for this plugin, is now an official layer type in napari. Read the napari documentation here:  https://napari.org/api/napari.layers.Tracks.html To view the legacy version of this plugin, visit the legacy branch: https://github.com/quantumjot/arboretum/tree/v1-legacy",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "napari-arboretum",
    "documentation": "",
    "first_released": "2021-05-11T11:19:31.102493Z",
    "license": "MIT",
    "name": "napari-arboretum",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/quantumjot/arboretum",
    "python_version": "",
    "reader_file_extensions": [],
    "release_date": "2022-09-13T20:19:14.261861Z",
    "report_issues": "",
    "requirements": [
      "matplotlib",
      "napari (>=0.4.0)",
      "napari-matplotlib (>=0.2.1)",
      "numpy (>=1.17.3)",
      "pandas",
      "pooch (>=1)",
      "vispy"
    ],
    "summary": "Track graph and lineage tree visualization with napari",
    "support": "",
    "twitter": "",
    "version": "0.1.1",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "email": "cchiu@chanzuckerberg.com", "name": "Chi-Li Chiu" }],
    "code_repository": "https://github.com/chili-chiu/napari-labels-overlap",
    "conda": [{ "channel": "conda-forge", "package": "napari-labels-overlap" }],
    "description": "# napari-labels-overlap  [![License](https://img.shields.io/pypi/l/napari-labels-overlap.svg?color=green)](https://github.com/chili-chiu/napari-labels-overlap/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-labels-overlap.svg?color=green)](https://pypi.org/project/napari-labels-overlap) [![Python Version](https://img.shields.io/pypi/pyversions/napari-labels-overlap.svg?color=green)](https://python.org) [![tests](https://github.com/chili-chiu/napari-labels-overlap/workflows/tests/badge.svg)](https://github.com/chili-chiu/napari-labels-overlap/actions) [![codecov](https://codecov.io/gh/chili-chiu/napari-labels-overlap/branch/main/graph/badge.svg)](https://codecov.io/gh/chili-chiu/napari-labels-overlap) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-labels-overlap)](https://napari-hub.org/plugins/napari-labels-overlap)  create an overlap labels layer from two labels layers  ## Description  This plugin takes two labels layers (layerA, layerB) as inputs, and generate the overlapped regions as a binary labels layer. Three modes:<br> (1) A_OR_B: new layer = layerA OR layerB (union)<br> (2) A_AND_B: new layer = layerA AND layerB (intersection)<br> (3) A_OUTSIDE_B: new layer = layerA OUTSIDE layerB (complement)<br>  [comment]: <need to update the gif>  ![labels_overlap](https://user-images.githubusercontent.com/89602983/144129087-9a88d55f-f1a0-4825-bd01-770909bfc64f.gif)  ## Applicaions - Object colocalization - Merge separately identified objects  ## Future work - Support N labels layers - Basic coloc stats (% volume overlap) - Output Labels with distinct IDs and links to original label IDs  ## Release log - 0.0.2<br> -- Run on npe2<br> -- Add output types: binary/connected component<br> - 0.0.1<br> -- Run on npe1<br>   ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/docs/plugins/index.html -->  ## Installation  You can install `napari-labels-overlap` via [pip]:      pip install napari-labels-overlap    To install latest development version :      pip install git+https://github.com/chili-chiu/napari-labels-overlap.git   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-labels-overlap\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/chili-chiu/napari-labels-overlap/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-labels-overlap       create an overlap labels layer from two labels layers Description This plugin takes two labels layers (layerA, layerB) as inputs, and generate the overlapped regions as a binary labels layer. Three modes: (1) A_OR_B: new layer = layerA OR layerB (union) (2) A_AND_B: new layer = layerA AND layerB (intersection) (3) A_OUTSIDE_B: new layer = layerA OUTSIDE layerB (complement) [comment]:   Applicaions  Object colocalization Merge separately identified objects  Future work  Support N labels layers Basic coloc stats (% volume overlap) Output Labels with distinct IDs and links to original label IDs  Release log  0.0.2 -- Run on npe2 -- Add output types: binary/connected component 0.0.1 -- Run on npe1   This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install napari-labels-overlap via pip: pip install napari-labels-overlap  To install latest development version : pip install git+https://github.com/chili-chiu/napari-labels-overlap.git  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-labels-overlap\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari labels overlap",
    "documentation": "https://github.com/chili-chiu/napari-labels-overlap#README.md",
    "first_released": "2021-11-30T17:47:47.968332Z",
    "license": "BSD-3-Clause",
    "name": "napari-labels-overlap",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/chili-chiu/napari-labels-overlap",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-06-22T22:19:03.264147Z",
    "report_issues": "https://github.com/chili-chiu/napari-labels-overlap/issues",
    "requirements": ["scikit-image"],
    "summary": "create an overlap labels layer from two labels layers",
    "support": "https://github.com/chili-chiu/napari-labels-overlap/issues",
    "twitter": "",
    "version": "0.0.3",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "email": "alisterburt@gmail.com", "name": "Alister Burt" }],
    "code_repository": "https://github.com/alisterburt/napari-mrcfile-reader",
    "description": "# napari-mrcfile-reader  [![License](https://img.shields.io/pypi/l/napari-mrcfile-reader.svg?color=green)](https://github.com/alisterburt/napari-mrcfile-reader/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-mrcfile-reader.svg?color=green)](https://pypi.org/project/napari-mrcfile-reader) [![Python Version](https://img.shields.io/pypi/pyversions/napari-mrcfile-reader.svg?color=green)](https://python.org) [![tests](https://github.com/alisterburt/napari-mrcfile-reader/workflows/tests/badge.svg)](https://github.com/alisterburt/napari-mrcfile-reader/actions) [![codecov](https://codecov.io/gh/alisterburt/napari-mrcfile-reader/branch/master/graph/badge.svg)](https://codecov.io/gh/alisterburt/napari-mrcfile-reader)  Read MRC format image files into napari using the [mrcfile] package from [CCP-EM]  ---------------------------------- ![example usage](example.gif) ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.   ## Installation  You can install `napari-mrcfile-reader` via [pip]:      pip install napari-mrcfile-reader  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-mrcfile-reader\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.   [CCP-EM]: https://www.ccpem.ac.uk/ [mrcfile]: https://github.com/ccpem/mrcfile [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin [file an issue]: https://github.com/alisterburt/napari-mrcfile-reader/issues [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/  ",
    "description_content_type": "text/markdown",
    "description_text": "napari-mrcfile-reader      Read MRC format image files into napari using the mrcfile package from CCP-EM   This napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template. Installation You can install napari-mrcfile-reader via pip: pip install napari-mrcfile-reader  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-mrcfile-reader\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "napari-mrcfile-reader",
    "documentation": "",
    "first_released": "2020-10-17T12:56:13.263981Z",
    "license": "BSD-3-Clause",
    "name": "napari-mrcfile-reader",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader"],
    "project_site": "https://github.com/alisterburt/napari-mrcfile-reader",
    "python_version": ">=3.6",
    "reader_file_extensions": ["*"],
    "release_date": "2021-09-16T12:19:01.757212Z",
    "report_issues": "",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy (>=1.18)",
      "mrcfile (>=1.1)"
    ],
    "summary": "read MRC format image files into napari",
    "support": "",
    "twitter": "",
    "version": "0.1.3",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "email": "d.stansby@ucl.ac.uk", "name": "David Stansby" }],
    "code_repository": "https://github.com/matplotlib/napari-matplotlib",
    "conda": [],
    "description": "# napari-matplotlib  [![License](https://img.shields.io/pypi/l/napari-matplotlib.svg?color=green)](https://github.com/dstansby/napari-matplotlib/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-matplotlib.svg?color=green)](https://pypi.org/project/napari-matplotlib) [![Python Version](https://img.shields.io/pypi/pyversions/napari-matplotlib.svg?color=green)](https://python.org) [![tests](https://github.com/dstansby/napari-matplotlib/workflows/tests/badge.svg)](https://github.com/dstansby/napari-matplotlib/actions) [![codecov](https://codecov.io/gh/dstansby/napari-matplotlib/branch/main/graph/badge.svg)](https://codecov.io/gh/dstansby/napari-matplotlib) [![pre-commit.ci status](https://results.pre-commit.ci/badge/github/matplotlib/pytest-mpl/master.svg)](https://results.pre-commit.ci/latest/github/matplotlib/pytest-mpl/master) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-matplotlib)](https://napari-hub.org/plugins/napari-matplotlib)  A plugin to create Matplotlib plots from napari layers  ----------------------------------  ## Introduction `napari-matplotlib` is a bridge between `napari` and `matplotlib`, making it easy to create publication quality `Matplotlib` plots based on the data loaded in `napari` layers.  ## Available widgets  ### `Slice` Plots 1D slices of data along a specified axis. ![](https://raw.githubusercontent.com/dstansby/napari-matplotlib/main/examples/slice.png)  ### `Histogram` Plots histograms of individual image layers, or RGB histograms of an RGB image ![](https://raw.githubusercontent.com/dstansby/napari-matplotlib/main/examples/hist.png)  ### `Scatter` Scatters the values of two similarly sized images layers against each other. ![](https://raw.githubusercontent.com/dstansby/napari-matplotlib/main/examples/scatter.png)  ## Installation  You can install `napari-matplotlib` via [pip]:      pip install napari-matplotlib    To install latest development version :      pip install git+https://github.com/dstansby/napari-matplotlib.git   ## Contributing  Contributions are very welcome! Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, `napari-matplotlib` is free and open source software.  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [@napari]: https://github.com/napari [BSD-3]: http://opensource.org/licenses/BSD-3-Clause  [file an issue]: https://github.com/dstansby/napari-matplotlib/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-matplotlib        A plugin to create Matplotlib plots from napari layers  Introduction napari-matplotlib is a bridge between napari and matplotlib, making it easy to create publication quality Matplotlib plots based on the data loaded in napari layers. Available widgets Slice Plots 1D slices of data along a specified axis.  Histogram Plots histograms of individual image layers, or RGB histograms of an RGB image  Scatter Scatters the values of two similarly sized images layers against each other.  Installation You can install napari-matplotlib via pip: pip install napari-matplotlib  To install latest development version : pip install git+https://github.com/dstansby/napari-matplotlib.git  Contributing Contributions are very welcome! Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, napari-matplotlib is free and open source software. Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 3 - Alpha"],
    "display_name": "napari Matplotlib",
    "documentation": "https://github.com/matplotlib/napari-matplotlib#README.md",
    "first_released": "2022-05-02T09:05:57.008000Z",
    "license": "BSD-3-Clause",
    "name": "napari-matplotlib",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/matplotlib/napari-matplotlib",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-06-06T10:10:57.406724Z",
    "report_issues": "https://github.com/matplotlib/napari-matplotlib/issues",
    "requirements": [
      "matplotlib",
      "napari",
      "numpy",
      "numpydoc ; extra == 'docs'",
      "pydata-sphinx-theme ; extra == 'docs'",
      "qtgallery ; extra == 'docs'",
      "sphinx ; extra == 'docs'",
      "sphinx-automodapi ; extra == 'docs'",
      "sphinx-gallery ; extra == 'docs'",
      "napari[pyqt5] ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "tox ; extra == 'testing'",
      "pytest-xvfb ; (sys_platform == \"linux\") and extra == 'testing'"
    ],
    "summary": "A plugin to use Matplotlib with napari",
    "support": "https://github.com/matplotlib/napari-matplotlib/issues",
    "twitter": "",
    "version": "0.2.1",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Ruben Lopez" }],
    "code_repository": "https://github.com/rjlopez2/napari-mat-file-reader",
    "conda": [],
    "description": "# napari-mat-file-reader  [![License BSD-3](https://img.shields.io/pypi/l/napari-mat-file-reader.svg?color=green)](https://github.com/rjlopez2/napari-mat-file-reader/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-mat-file-reader.svg?color=green)](https://pypi.org/project/napari-mat-file-reader) [![Python Version](https://img.shields.io/pypi/pyversions/napari-mat-file-reader.svg?color=green)](https://python.org) [![tests](https://github.com/rjlopez2/napari-mat-file-reader/workflows/tests/badge.svg)](https://github.com/rjlopez2/napari-mat-file-reader/actions) [![codecov](https://codecov.io/gh/rjlopez2/napari-mat-file-reader/branch/main/graph/badge.svg)](https://codecov.io/gh/rjlopez2/napari-mat-file-reader) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-mat-file-reader)](https://napari-hub.org/plugins/napari-mat-file-reader)  This is a simple wraper to read .mat files from Matlab  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/stable/plugins/index.html -->  ## Installation  You can install `napari-mat-file-reader` via [pip]:      pip install napari-mat-file-reader    To install latest development version :      pip install git+https://github.com/rjlopez2/napari-mat-file-reader.git   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-mat-file-reader\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/rjlopez2/napari-mat-file-reader/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-mat-file-reader       This is a simple wraper to read .mat files from Matlab  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install napari-mat-file-reader via pip: pip install napari-mat-file-reader  To install latest development version : pip install git+https://github.com/rjlopez2/napari-mat-file-reader.git  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-mat-file-reader\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari mat file reader",
    "documentation": "https://github.com/rjlopez2/napari-mat-file-reader#README.md",
    "first_released": "2022-11-01T22:14:35.861855Z",
    "license": "BSD-3-Clause",
    "name": "napari-mat-file-reader",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader", "sample_data"],
    "project_site": "https://github.com/rjlopez2/napari-mat-file-reader",
    "python_version": ">=3.8",
    "reader_file_extensions": ["*.mat"],
    "release_date": "2022-11-01T22:14:35.861855Z",
    "report_issues": "https://github.com/rjlopez2/napari-mat-file-reader/issues",
    "requirements": [
      "numpy",
      "mat73",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'"
    ],
    "summary": "This is a simple wraper to read .mat files from Matlab",
    "support": "https://github.com/rjlopez2/napari-mat-file-reader/issues",
    "twitter": "",
    "version": "0.0.2",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Varun Kapoor" }],
    "category": {
      "Image modality": ["Medical imaging"],
      "Supported data": ["2D", "3D", "Time series", "Multi-channel"],
      "Workflow step": ["Image Segmentation"]
    },
    "category_hierarchy": {
      "Image modality": [["Medical imaging"]],
      "Supported data": [["2D"], ["3D"], ["Time series"], ["Multi-channel"]],
      "Workflow step": [
        ["Image Segmentation", "Cell segmentation"],
        ["Image Segmentation", "Model-based segmentation"]
      ]
    },
    "code_repository": "https://github.com/kapoorlab/vollseg-napari",
    "conda": [{ "channel": "conda-forge", "package": "vollseg-napari" }],
    "description": "# VollSeg Napari Plugin    [![PyPI version](https://img.shields.io/pypi/v/vollseg-napari.svg)](https://pypi.org/project/vollseg-napari) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/vollseg-napari)](https://napari-hub.org/plugins/vollseg-napari) [![License](https://img.shields.io/pypi/l/napari-metroid.svg?color=green)](https://github.com/kapoorlab/napari-vollseg/raw/main/LICENSE) [![codecov](https://codecov.io/gh/kapoorlab/napari-vollseg/branch/main/graph/badge.svg)](https://codecov.io/gh/kapoorlab/napari-vollseg) [![Twitter Badge](https://badgen.net/badge/icon/twitter?icon=twitter&label)](https://twitter.com/entracod)   This project provides the [napari](https://napari.org/) plugin for [VollSeg](https://github.com/kapoorlab/vollseg), a deep learning based 2D and 3D segmentation tool for irregular shaped cells. VollSeg has originally been developed (see [papers](http://conference.scipy.org/proceedings/scipy2021/varun_kapoor.html)) for the segmentation of densely packed membrane labelled cells in challenging images with low signal-to-noise ratios. The plugin allows to apply pretrained and custom trained models from within napari. For detailed demo of the plugin see these [videos](https://www.youtube.com/watch?v=W_gKrLWKNpQ) and a short video about the [parameter selection](https://www.youtube.com/watch?v=7tQMn_u8_7s&t=1s)    ## Installation & Usage  Install the plugin with `pip install vollseg-napari` or from within napari via `Plugins > Install/Uninstall Package(s)…`. If you want GPU-accelerated prediction, please read the more detailed [installation instructions](https://github.com/kapoorlab/vollseg-napari#gpu_installation) for VollSeg.  You can activate the plugin in napari via `Plugins > VollSeg: VollSeg`. Example images for testing are provided via `File > Open Sample > VollSeg`.  If you use this plugin for your research, please [cite us](http://conference.scipy.org/proceedings/scipy2021/varun_kapoor.html).  ## GPU_Installation  This package is compatible with Python 3.6 - 3.9.  1. Please first [install TensorFlow](https://www.tensorflow.org/install) (TensorFlow 2) by following the official instructions. For [GPU support](https://www.tensorflow.org/install/gpu), it is very important to install the specific versions of CUDA and cuDNN that are compatible with the respective version of TensorFlow. (If you need help and can use `conda`, take a look at [this](https://github.com/CSBDeep/CSBDeep/tree/master/extras#conda-environment).)  2. *VollSeg* can then be installed with `pip`:      - If you installed TensorFlow 2 (version *2.x.x*):            pip install vollseg   ## Examples  VollSeg comes with different options to combine CARE based denoising with UNET, StarDist and segmentation in a region of interest (ROI). We present some examples which are represent optimal combination of these different modes for segmenting different cell types. We summarize this in the table below:  | Example Image | Description | Training Data | Trained Model | GT image   | Optimal combination  | Notebook Code | Model Prediction | Metrics | | --- | --- |--- | --- |--- | --- |--- | --- | --- | | <img src=\"https://github.com/kapoorlab/vollseg-napari/blob/main/vollseg_napari/images/Ascadian_raw.png\"  title=\"Raw Ascadian Embryo\" width=\"200\">| Light sheet fused from four angles 3D single channel| [Training Data ~320 GB](https://figshare.com/articles/dataset/Astec-half-Pm1_Cut_at_2-cell_stage_half_Phallusia_mammillata_embryo_live_SPIM_imaging_stages_6-16_/11309570?backTo=/s/765d4361d1b073beedd5)| [UNET model](https://zenodo.org/record/6337699) |<img src=\"https://github.com/kapoorlab/vollseg-napari/blob/main/vollseg_napari/images/Ascadian_GT.png\" title=\"GT Ascadian Embryo\" width=\"200\"> | UNET model, slice_merge = False | [Colab Notebook](https://github.com/kapoorlab/VollSeg/blob/main/examples/Predict/Colab_VollSeg_Ascadian_Embryo.ipynb) |<img src=\"https://github.com/kapoorlab/vollseg-napari/blob/main/vollseg_napari/images/Ascadian_pred.png\" title=\"Prediction Ascadian Embryo\" width=\"200\" > | <img src=\"https://github.com/kapoorlab/vollseg-napari/blob/main/vollseg_napari/images/Metrics_Ascadian.png\" title=\"Metrics Ascadian Embryo\" width=\"200\" >  | |  |  | | | | | | |  | | <img src=\"https://github.com/kapoorlab/vollseg-napari/blob/main/vollseg_napari/images/Carcinoma_raw.png\"  title=\"Raw Carcinoma\" width=\"200\">| Confocal microscopy 3D single channel 8 bit| [Training Data](https://zenodo.org/record/5904082#.Yi8-BnrMJD8)| [Denoising Model](https://zenodo.org/record/5910645/) and [StarDist Model](https://zenodo.org/record/6354077/) |<img src=\"https://github.com/kapoorlab/vollseg-napari/blob/main/vollseg_napari/images/Carcinoma_GT.png\" title=\"GT Carcinoma\" width=\"200\"> | StarDist model + Denoising Model, dounet = False | [Colab Notebook](https://github.com/kapoorlab/VollSeg/blob/main/examples/Predict/Colab_VollSeg_Mamary_gland.ipynb) |<img src=\"https://github.com/kapoorlab/vollseg-napari/blob/main/vollseg_napari/images/Carcinoma_pred.png\" title=\"Prediction Carcinoma Cells\" width=\"200\" > | <img src=\"https://github.com/kapoorlab/vollseg-napari/blob/main/vollseg_napari/images/Metrics_carcinoma.png\" title=\"Metrics Carcinoma Cells\" width=\"200\" >  | |  |  | | | | | | |  | | <img src=\"https://github.com/kapoorlab/vollseg-napari/blob/main/vollseg_napari/images/Xenopus_tissue_raw.png\"  title=\"Raw Xenopus Tissue\" width=\"200\">| LaserScanningConfocalMicroscopy 2D single channel| [Dataset](https://zenodo.org/record/6076614#.YjBaNnrMJD8)| [UNET Model](https://zenodo.org/record/6060378/)  |<img src=\"https://github.com/kapoorlab/vollseg-napari/blob/main/vollseg_napari/images/Xenopus_tissue_GT.png\" title=\"GT Xenopus Tissue\" width=\"200\"> | UNET model| [Colab Notebook](https://github.com/kapoorlab/VollSeg/blob/main/examples/Predict/Colab_VollSeg_tissue_segmentation.ipynb) |<img src=\"https://github.com/kapoorlab/vollseg-napari/blob/main/vollseg_napari/images/Xenopus_tissue_pred.png\" title=\"Prediction Xenopus Tissue\" width=\"200\" > | No Metrics  | |  |  | | | | | | |  | | <img src=\"https://github.com/kapoorlab/vollseg-napari/blob/main/vollseg_napari/images/microtubule_kymo_raw.png\"  title=\"Raw Microtubule Kymograph\" width=\"200\">| TIRF + MultiKymograph Fiji tool 2D single channel| [Training Dataset](https://zenodo.org/record/6355705/files/Microtubule_edgedetector_training.zip)| [UNET Model](https://zenodo.org/record/6355705/)  |<img src=\"https://github.com/kapoorlab/vollseg-napari/blob/main/vollseg_napari/images/microtubule_kymo_GT.png\" title=\"GT Microtubule Kymograph\" width=\"200\"> | UNET model| [Colab Notebook](https://github.com/kapoorlab/VollSeg/blob/main/examples/Predict/Colab_Microtubule_kymo_segmentation.ipynb) |<img src=\"https://github.com/kapoorlab/vollseg-napari/blob/main/vollseg_napari/images/microtubule_kymo_pred.png\" title=\"Prediction Microtubule Kymographe\" width=\"200\" > | No Metrics  | |  |  | | | | | | |  | | <img src=\"https://github.com/kapoorlab/vollseg-napari/blob/main/vollseg_napari/images/lung_xray_raw.png\"  title=\"Raw Lung Xray\" width=\"200\">| XRay of Lung 2D single channel| [Training Dataset](https://www.kaggle.com/nikhilpandey360/lung-segmentation-from-chest-x-ray-dataset)| [UNET Model](https://zenodo.org/record/6060177/)  |<img src=\"https://github.com/kapoorlab/vollseg-napari/blob/main/vollseg_napari/images/lung_xray_GT.png\" title=\"GT Lung Xray\" width=\"200\"> | UNET model| [Colab Notebook](https://github.com/kapoorlab/VollSeg/blob/main/examples/Predict/Colab_Microtubule_kymo_segmentation.ipynb) |<img src=\"https://github.com/kapoorlab/vollseg-napari/blob/main/vollseg_napari/images/lung_xray_pred.png\" title=\"Prediction Lung Xray\" width=\"200\" > | <img src=\"https://github.com/kapoorlab/vollseg-napari/blob/main/vollseg_napari/images/Metrics_lung_xray.png\" title=\"Metrics Lung Xray\" width=\"200\" >   | |  |  | | | | | | |  | | <img src=\"https://github.com/kapoorlab/vollseg-napari/blob/main/vollseg_napari/images/nuclei_mask_raw.png\"  title=\"Raw Nuclei Mask\" width=\"200\">| LaserScanningConfocalMicroscopy 2D single channell| [Test Dataset](https://zenodo.org/record/6359349/)|Private |<img src=\"https://github.com/kapoorlab/vollseg-napari/blob/main/vollseg_napari/images/nuclei_mask_GT.png\" title=\"GT Nuclei Mask\" width=\"200\"> | UNET model| [Colab Notebook](https://github.com/kapoorlab/VollSeg/blob/main/examples/Predict/Colab_Microtubule_kymo_segmentation.ipynb) |<img src=\"https://github.com/kapoorlab/vollseg-napari/blob/main/vollseg_napari/images/nuclei_mask_pred.png\" title=\"Prediction Nuclei Mask\" width=\"200\" > | No metrics   | |  |  | | | | | | |  | | <img src=\"https://github.com/kapoorlab/vollseg-napari/blob/main/vollseg_napari/images/nuclei_raw.png\"  title=\"Raw Nuclei\" width=\"200\">| LaserScanningConfocalMicroscopy 3D single channell| [Test Dataset](https://zenodo.org/record/6359295/)|Private |<img src=\"https://github.com/kapoorlab/vollseg-napari/blob/main/vollseg_napari/images/nuclei_GT.png\" title=\"GT Nuclei\" width=\"200\"> | UNET model + StarDist model + ROI model| [Colab Notebook](https://github.com/kapoorlab/VollSeg/blob/main/examples/Predict/Colab_VollSeg_star_roi.ipynb) |<img src=\"https://github.com/kapoorlab/vollseg-napari/blob/main/vollseg_napari/images/nuclei_pred.png\" title=\"Prediction Nuclei\" width=\"200\" > |  <img src=\"https://github.com/kapoorlab/vollseg-napari/blob/main/vollseg_napari/images/Metrics_nuclei.png\" title=\"Metrics Nuclei\" width=\"200\" >   |   ## Troubleshooting & Support  - The [image.sc forum](https://forum.image.sc/tag/vollseg) is the best place to start getting help and support. Make sure to use the tag `vollseg`, since we are monitoring all questions with this tag. - If you have technical questions or found a bug, feel free to [open an issue](https://github.com/kapoorlab/vollseg-napari/issues).  ",
    "description_content_type": "text/markdown",
    "description_text": "VollSeg Napari Plugin      This project provides the napari plugin for VollSeg, a deep learning based 2D and 3D segmentation tool for irregular shaped cells. VollSeg has originally been developed (see papers) for the segmentation of densely packed membrane labelled cells in challenging images with low signal-to-noise ratios. The plugin allows to apply pretrained and custom trained models from within napari. For detailed demo of the plugin see these videos and a short video about the parameter selection  Installation & Usage Install the plugin with pip install vollseg-napari or from within napari via Plugins > Install/Uninstall Package(s)…. If you want GPU-accelerated prediction, please read the more detailed installation instructions for VollSeg. You can activate the plugin in napari via Plugins > VollSeg: VollSeg. Example images for testing are provided via File > Open Sample > VollSeg. If you use this plugin for your research, please cite us. GPU_Installation This package is compatible with Python 3.6 - 3.9.   Please first install TensorFlow (TensorFlow 2) by following the official instructions. For GPU support, it is very important to install the specific versions of CUDA and cuDNN that are compatible with the respective version of TensorFlow. (If you need help and can use conda, take a look at this.)   VollSeg can then be installed with pip:   If you installed TensorFlow 2 (version 2.x.x): pip install vollseg     Examples VollSeg comes with different options to combine CARE based denoising with UNET, StarDist and segmentation in a region of interest (ROI). We present some examples which are represent optimal combination of these different modes for segmenting different cell types. We summarize this in the table below: | Example Image | Description | Training Data | Trained Model | GT image   | Optimal combination  | Notebook Code | Model Prediction | Metrics | | --- | --- |--- | --- |--- | --- |--- | --- | --- | | | Light sheet fused from four angles 3D single channel| Training Data ~320 GB| UNET model | | UNET model, slice_merge = False | Colab Notebook | |   | |  |  | | | | | | |  | | | Confocal microscopy 3D single channel 8 bit| Training Data| Denoising Model and StarDist Model | | StarDist model + Denoising Model, dounet = False | Colab Notebook | |   | |  |  | | | | | | |  | | | LaserScanningConfocalMicroscopy 2D single channel| Dataset| UNET Model  | | UNET model| Colab Notebook | | No Metrics  | |  |  | | | | | | |  | | | TIRF + MultiKymograph Fiji tool 2D single channel| Training Dataset| UNET Model  | | UNET model| Colab Notebook | | No Metrics  | |  |  | | | | | | |  | | | XRay of Lung 2D single channel| Training Dataset| UNET Model  | | UNET model| Colab Notebook | |    | |  |  | | | | | | |  | | | LaserScanningConfocalMicroscopy 2D single channell| Test Dataset|Private | | UNET model| Colab Notebook | | No metrics   | |  |  | | | | | | |  | | | LaserScanningConfocalMicroscopy 3D single channell| Test Dataset|Private | | UNET model + StarDist model + ROI model| Colab Notebook | |     | Troubleshooting & Support  The image.sc forum is the best place to start getting help and support. Make sure to use the tag vollseg, since we are monitoring all questions with this tag. If you have technical questions or found a bug, feel free to open an issue. ",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "vollseg-napari",
    "documentation": "https://github.com/kapoorlab/vollseg-napari",
    "first_released": "2021-12-10T16:26:44.646909Z",
    "license": "BSD-3-Clause",
    "name": "vollseg-napari",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget", "sample_data"],
    "project_site": "https://github.com/kapoorlab/vollseg-napari",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-10-03T17:46:52.271244Z",
    "report_issues": "https://github.com/kapoorlab/vollseg-napari/issues",
    "requirements": [
      "vollseg",
      "napari (>=0.4.13)",
      "magicgui (>=0.4.0)",
      "tensorflow ; platform_system != \"Darwin\" or platform_machine != \"arm64\"",
      "tensorflow-macos ; platform_system == \"Darwin\" and platform_machine == \"arm64\"",
      "pytest ; extra == 'test'",
      "pytest-qt ; extra == 'test'",
      "napari[pyqt] (>=0.4.13) ; extra == 'test'"
    ],
    "summary": "Irregular cell shape segmentation using VollSeg",
    "support": "https://forum.image.sc/tag/vollseg-napari",
    "twitter": "https://twitter.com/entracod",
    "version": "2.3.7",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Zach Marin" }, { "name": "Talley Lambert" }],
    "code_repository": "https://github.com/zacsimile/napari-math",
    "conda": [{ "channel": "conda-forge", "package": "napari-math" }],
    "description": "# napari-math  [![License](https://img.shields.io/pypi/l/napari-math.svg?color=green)](https://github.com/zacsimile/napari-math/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-math.svg?color=green)](https://pypi.org/project/napari-math) [![Python Version](https://img.shields.io/pypi/pyversions/napari-math.svg?color=green)](https://python.org) [![tests](https://github.com/zacsimile/napari-math/workflows/tests/badge.svg)](https://github.com/zacsimile/napari-math/actions) [![codecov](https://codecov.io/gh/zacsimile/napari-math/branch/main/graph/badge.svg)](https://codecov.io/gh/zacsimile/napari-math) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-math)](https://napari-hub.org/plugins/napari-math)  This package provides a GUI interfrace for simple mathematical operations on image, point and surface layers.  - addition - subtraction - multiplication - division - logical and, or, xor - z-projection (mean and sum)  Operations can be peformed on a single layer or between Image layers (functionaly pending for Surface and Point layers),  for example adding one layer to another.  When performing operations on two images of different sizes, the result will be the size of the smallest of the two images.  ----------------------------------  <!-- This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template. -->  ## Installation  You can install `napari-math` via [pip]:      pip install napari-math     ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [MIT] license, \"napari-math\" is free and open source software  ## Issues  If you encounter any problems, please file an [issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-math       This package provides a GUI interfrace for simple mathematical operations on image, point and surface layers.  addition subtraction multiplication division logical and, or, xor z-projection (mean and sum)  Operations can be peformed on a single layer or between Image layers (functionaly pending for Surface and Point layers),  for example adding one layer to another. When performing operations on two images of different sizes, the result will be the size of the smallest of the two images.   Installation You can install napari-math via pip: pip install napari-math  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the MIT license, \"napari-math\" is free and open source software Issues If you encounter any problems, please file an [issue] along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari math",
    "documentation": "https://github.com/zacsimile/napari-math#README.md",
    "first_released": "2022-01-18T17:06:10.593793Z",
    "license": "MIT",
    "name": "napari-math",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/zacsimile/napari-math",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-03-29T01:40:25.735077Z",
    "report_issues": "https://github.com/zacsimile/napari-math/issues",
    "requirements": ["numpy"],
    "summary": "Simple mathematical operations on image, point and surface layers.",
    "support": "https://github.com/zacsimile/napari-math/issues",
    "twitter": "",
    "version": "0.0.1b0",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Allen Institute for Cell Science" }],
    "category": {
      "Image modality": ["Fluorescence microscopy"],
      "Supported data": ["3D"],
      "Workflow step": ["Image Segmentation", "Visualization"]
    },
    "category_hierarchy": {
      "Image modality": [["Fluorescence microscopy"]],
      "Supported data": [["3D"]],
      "Workflow step": [
        ["Image Segmentation", "Cell segmentation"],
        ["Image Segmentation", "Model-based segmentation"],
        ["Visualization", "Image visualisation"]
      ]
    },
    "code_repository": "https://github.com/AllenCell/napari-allencell-segmenter",
    "conda": [
      { "channel": "conda-forge", "package": "napari-allencell-segmenter" }
    ],
    "description": "# napari-allencell-segmenter  [![License](https://img.shields.io/pypi/l/napari-allencell-segmenter.svg?color=green)](https://github.com/AllenCell/napari-allencell-segmenter/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-allencell-segmenter.svg?color=green)](https://pypi.org/project/napari-allencell-segmenter) [![Python Version](https://img.shields.io/pypi/pyversions/napari-allencell-segmenter.svg?color=green)](https://python.org) [![tests](https://github.com/AllenCell/napari-allencell-segmenter/workflows/tests/badge.svg)](https://github.com/AllenCell/napari-allencell-segmenter/actions) [![codecov](https://codecov.io/gh/AllenCell/napari-allencell-segmenter/branch/main/graph/badge.svg)](https://codecov.io/gh/AllenCell/napari-allencell-segmenter)   A plugin that enables 3D image segmentation provided by Allen Institute for Cell Science  The Allen Cell & Structure Segmenter plugin for napari provides an intuitive graphical user interface to access the powerful segmentation capabilities of an open source 3D segmentation software package developed and maintained by the Allen Institute for Cell Science (classic workflows only with v1.0). ​[The Allen Cell & Structure Segmenter](https://allencell.org/segmenter) is a Python-based open source toolkit developed at the Allen Institute for Cell Science for 3D segmentation of intracellular structures in fluorescence microscope images. This toolkit brings together classic image segmentation and iterative deep learning workflows first to generate initial high-quality 3D intracellular structure segmentations and then to easily curate these results to generate the ground truths for building robust and accurate deep learning models. The toolkit takes advantage of the high replicate 3D live cell image data collected at the Allen Institute for Cell Science of over 30 endogenous fluorescently tagged human induced pluripotent stem cell (hiPSC) lines. Each cell line represents a different intracellular structure with one or more distinct localization patterns within undifferentiated hiPS cells and hiPSC-derived cardiomyocytes.  More details about Segmenter can be found at https://allencell.org/segmenter  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/docs/plugins/index.html -->  ## Installation  ### Option 1 (recommended):  After you installed the lastest version of napari, you can go to \"Plugins\" --> \"Install/Uninstall Package(s)\". Then, you will be able to see all available napari plugins and you can find us by name `napari-allencell-segmenter`. Just click the \"install\" button to install the Segmenter plugin.  ### Option 2:  You can also install `napari-allencell-segmenter` via [pip]:      pip install napari-allencell-segmenter  ## Quick Start  In the current version, there are two parts in the plugin: **workflow editor** and **batch processing**. The **workflow editor** allows users adjusting parameters in all the existing workflows in the lookup table, so that the workflow can be optimized on users' data. The adjusted workflow can be saved and then applied to a large batch of files using the **batch processing** part of the plugin.   1. Open a file in napari (the plugin is able to support multi-dimensional data in .tiff, .tif. ome.tif, .ome.tiff, .czi) 2. Start the plugin (open napari, go to \"Plugins\" --> \"napari-allencell-segmenter\" --> \"workflow editor\") 3. Select the image and channel to work on 4. Select a workflow based on the example image and target segmentation based on user's data. Ideally, it is recommend to start with the example with very similar morphology as user's data. 5. Click \"Run All\" to execute the whole workflow on the sample data. 6. Adjust the parameters of steps, based on the intermediate results. For instruction on the details on each function and the effect of each parameter, click the tooltip button. A complete list of all functions can be found [here](https://github.com/AllenCell/aics-segmentation/blob/main/aicssegmentation/structure_wrapper_config/function_params.md) 7. Click \"Run All\" again after adjusting the parameters and repeat step 6 and 7 until the result is satisfactory. 8. Save the workflow 9. Close the plugin and open the **batch processing** part by (go to \"Plugins\" --> \"napari-allencell-segmenter\" --> \"batch processing\") 10. Load the customized workflow (or an off-the-shelf workflow) json file 11. Load the folder with all the images to process 12. Click \"Run\"  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-allencell-segmenter\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin [file an issue]: https://github.com/AllenCell/napari-allencell-segmenter/issues [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-allencell-segmenter      A plugin that enables 3D image segmentation provided by Allen Institute for Cell Science The Allen Cell & Structure Segmenter plugin for napari provides an intuitive graphical user interface to access the powerful segmentation capabilities of an open source 3D segmentation software package developed and maintained by the Allen Institute for Cell Science (classic workflows only with v1.0). ​The Allen Cell & Structure Segmenter is a Python-based open source toolkit developed at the Allen Institute for Cell Science for 3D segmentation of intracellular structures in fluorescence microscope images. This toolkit brings together classic image segmentation and iterative deep learning workflows first to generate initial high-quality 3D intracellular structure segmentations and then to easily curate these results to generate the ground truths for building robust and accurate deep learning models. The toolkit takes advantage of the high replicate 3D live cell image data collected at the Allen Institute for Cell Science of over 30 endogenous fluorescently tagged human induced pluripotent stem cell (hiPSC) lines. Each cell line represents a different intracellular structure with one or more distinct localization patterns within undifferentiated hiPS cells and hiPSC-derived cardiomyocytes. More details about Segmenter can be found at https://allencell.org/segmenter  This napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.  Installation Option 1 (recommended): After you installed the lastest version of napari, you can go to \"Plugins\" --> \"Install/Uninstall Package(s)\". Then, you will be able to see all available napari plugins and you can find us by name napari-allencell-segmenter. Just click the \"install\" button to install the Segmenter plugin. Option 2: You can also install napari-allencell-segmenter via pip: pip install napari-allencell-segmenter  Quick Start In the current version, there are two parts in the plugin: workflow editor and batch processing. The workflow editor allows users adjusting parameters in all the existing workflows in the lookup table, so that the workflow can be optimized on users' data. The adjusted workflow can be saved and then applied to a large batch of files using the batch processing part of the plugin.   Open a file in napari (the plugin is able to support multi-dimensional data in .tiff, .tif. ome.tif, .ome.tiff, .czi) Start the plugin (open napari, go to \"Plugins\" --> \"napari-allencell-segmenter\" --> \"workflow editor\") Select the image and channel to work on Select a workflow based on the example image and target segmentation based on user's data. Ideally, it is recommend to start with the example with very similar morphology as user's data. Click \"Run All\" to execute the whole workflow on the sample data. Adjust the parameters of steps, based on the intermediate results. For instruction on the details on each function and the effect of each parameter, click the tooltip button. A complete list of all functions can be found here Click \"Run All\" again after adjusting the parameters and repeat step 6 and 7 until the result is satisfactory. Save the workflow Close the plugin and open the batch processing part by (go to \"Plugins\" --> \"napari-allencell-segmenter\" --> \"batch processing\") Load the customized workflow (or an off-the-shelf workflow) json file Load the folder with all the images to process Click \"Run\"  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-allencell-segmenter\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 5 - Production/Stable"],
    "display_name": "napari-allencell-segmenter",
    "documentation": "https://github.com/AllenCell/napari-allencell-segmenter#README.md",
    "first_released": "2021-06-24T22:44:23.592978Z",
    "license": "BSD-3-Clause",
    "name": "napari-allencell-segmenter",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/AllenCell/napari-allencell-segmenter",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-09-07T18:30:25.283847Z",
    "report_issues": "",
    "requirements": [
      "napari (>=0.4.9)",
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "aicssegmentation (>=0.5.2)",
      "magicgui (>=0.2.9)",
      "aicsimageio (>=4.7.0)",
      "xarray (==2022.6.0)",
      "opencv-python-headless (>=4.5.1)",
      "importlib-metadata (==4.11.4)",
      "napari (>=0.4.9) ; extra == 'all'",
      "napari-plugin-engine (>=0.1.4) ; extra == 'all'",
      "numpy ; extra == 'all'",
      "aicssegmentation (>=0.5.2) ; extra == 'all'",
      "magicgui (>=0.2.9) ; extra == 'all'",
      "aicsimageio (>=4.7.0) ; extra == 'all'",
      "xarray (==2022.6.0) ; extra == 'all'",
      "opencv-python-headless (>=4.5.1) ; extra == 'all'",
      "importlib-metadata (==4.11.4) ; extra == 'all'",
      "black (>=19.10b0) ; extra == 'all'",
      "codecov (>=2.0.22) ; extra == 'all'",
      "docutils (<0.16,>=0.10) ; extra == 'all'",
      "flake8 (>=3.7.7) ; extra == 'all'",
      "psutil (>=5.7.0) ; extra == 'all'",
      "pytest (>=4.3.0) ; extra == 'all'",
      "pytest-cov (==2.6.1) ; extra == 'all'",
      "pytest-raises (>=0.10) ; extra == 'all'",
      "pytest-qt (>=3.3.0) ; extra == 'all'",
      "quilt3 (>=3.1.12) ; extra == 'all'",
      "pytest-runner ; extra == 'all'",
      "bumpversion (>=0.5.3) ; extra == 'all'",
      "coverage (>=5.0a4) ; extra == 'all'",
      "gitchangelog (>=3.0.4) ; extra == 'all'",
      "ipython (>=7.5.0) ; extra == 'all'",
      "m2r (>=0.2.1) ; extra == 'all'",
      "pytest-runner (>=4.4) ; extra == 'all'",
      "Sphinx (<3,>=2.0.0b1) ; extra == 'all'",
      "sphinx-rtd-theme (>=0.1.2) ; extra == 'all'",
      "tox (==3.25.0) ; extra == 'all'",
      "twine (>=1.13.0) ; extra == 'all'",
      "wheel (>=0.33.1) ; extra == 'all'",
      "black (>=19.10b0) ; extra == 'dev'",
      "bumpversion (>=0.5.3) ; extra == 'dev'",
      "coverage (>=5.0a4) ; extra == 'dev'",
      "docutils (<0.16,>=0.10) ; extra == 'dev'",
      "flake8 (>=3.7.7) ; extra == 'dev'",
      "gitchangelog (>=3.0.4) ; extra == 'dev'",
      "ipython (>=7.5.0) ; extra == 'dev'",
      "m2r (>=0.2.1) ; extra == 'dev'",
      "pytest (>=4.3.0) ; extra == 'dev'",
      "pytest-cov (==2.6.1) ; extra == 'dev'",
      "pytest-raises (>=0.10) ; extra == 'dev'",
      "pytest-runner (>=4.4) ; extra == 'dev'",
      "pytest-qt (>=3.3.0) ; extra == 'dev'",
      "quilt3 (>=3.1.12) ; extra == 'dev'",
      "Sphinx (<3,>=2.0.0b1) ; extra == 'dev'",
      "sphinx-rtd-theme (>=0.1.2) ; extra == 'dev'",
      "tox (==3.25.0) ; extra == 'dev'",
      "twine (>=1.13.0) ; extra == 'dev'",
      "wheel (>=0.33.1) ; extra == 'dev'",
      "pytest-runner ; extra == 'setup'",
      "black (>=19.10b0) ; extra == 'test'",
      "codecov (>=2.0.22) ; extra == 'test'",
      "docutils (<0.16,>=0.10) ; extra == 'test'",
      "flake8 (>=3.7.7) ; extra == 'test'",
      "psutil (>=5.7.0) ; extra == 'test'",
      "pytest (>=4.3.0) ; extra == 'test'",
      "pytest-cov (==2.6.1) ; extra == 'test'",
      "pytest-raises (>=0.10) ; extra == 'test'",
      "pytest-qt (>=3.3.0) ; extra == 'test'",
      "quilt3 (>=3.1.12) ; extra == 'test'"
    ],
    "summary": "A plugin that enables 3D image segmentation provided by Allen Institute for Cell Science",
    "support": "https://github.com/AllenCell/napari-allencell-segmenter/issues",
    "twitter": "",
    "version": "2.1.3",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      { "email": "sofroniewn@gmail.com", "name": "Nicholas Sofroniew" }
    ],
    "code_repository": "https://github.com/sofroniewn/waver",
    "description": "# waver  [![License](https://img.shields.io/pypi/l/waver.svg?color=green)](https://github.com/sofroniewn/waver/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/waver.svg?color=green)](https://pypi.org/project/waver) [![Python Version](https://img.shields.io/pypi/pyversions/waver.svg?color=green)](https://python.org) [![tests](https://github.com/sofroniewn/waver/workflows/tests/badge.svg)](https://github.com/sofroniewn/waver/actions) [![codecov](https://codecov.io/gh/sofroniewn/waver/branch/main/graph/badge.svg?token=QBP7K6YUT7)](https://codecov.io/gh/sofroniewn/waver)  Run simulations of the [wave equation](https://en.wikipedia.org/wiki/Wave_equation) in nD on grids of variable speed in Python. This library owes a lot of its design and approach to the [fdtd](https://github.com/flaport/fdtd) library, a Python 3D electromagnetic FDTD simulator.  This package allows for a fair amount of customization over your wave simulation. You can  - specify the size and spacing of the grid  - specify the time step for the simulation, which will be checked to ensure stability of the simulation  - specify the duration of the simulation  - setting a variable speed array (one value per grid point) to allow for \"objects\" in your environment  - set the source of the wave, which can be a point, line, or any (n-1)D subarray  - record the wave with a detector, which can be the full grid, the full boundary, or a particular boundary  - use convenience methods to run many simulations with different sources on the same grid and detector combination  You can use [napari](https://napari.org/), a multi-dimensional image viewer for Python, to allow for easy visualization of the detected wave. Some functionality is also available as a napari plugin to allow for running simulations from a graphical user interface.  Results can look like  https://user-images.githubusercontent.com/6531703/128283012-a784ec06-4df9-4ddf-bf4f-e21b927fe4a3.mov  ----------------------------------  ## Installation  You can install `waver` via [pip]:      pip install waver  ## Usage  ### Convenience Methods  The most convenient way to use waver is to use one of two convenience methods that will create and run a simulation for you and return the results.  The first method `run_single_source` allows you to run a single simulation with a single source on one grid and  record the results using a detector. For example  ```python from waver.simulation import run_single_source  single_sim_params = {     'size': (12.8e-3, 12.8e-3),     'spacing': 100e-6,     'duration': 80e-6,     'min_speed': 343,     'max_speed': 686,     'speed': 686,     'time_step': 50e-9,     'temporal_downsample': 2,     'location': (6.4e-3, 6.4e-3),     'period': 5e-6,     'ncycles':1, }  detected_wave, speed_grid = run_single_source(**single_sim_params) ```  The second method `run_multiple_sources` allows you to run multiple simulations with multiple sources on the same grid and with the same detector and return the results. For example  ```python from waver.simulation import run_multiple_sources  multi_sim_params = {     'size': (12.8e-3, 12.8e-3),     'spacing': 100e-6,     'duration': 80e-6,     'min_speed': 343,     'max_speed': 686,     'speed': 686,     'time_step': 50e-9,     'temporal_downsample': 2,     'sources': [{         'location': (6.4e-3, 6.4e-3),         'period': 5e-6,         'ncycles':1,     }] }  detected_wave, speed_grid = run_multiple_sources(**multi_sim_params) ```  The main difference between these two methods is that `run_multiple_sources` takes a `sources` parameter which takes a list  of dictionaries with keys corresponding to source related keyword arguments found in `run_single_source`.  ### Visualization  If you want to quickly visualize the results of `run_multiple_sources`, you can use the `run_and_visualize` command which will  run the simulation and then launch napari with the results, as seen in [examples/2D/point_source.py](./examples/2D/point_source.py)  ```python from waver.datasets import run_and_visualize  run_and_visualize(**multi_sim_params) ```  ### Datasets  If you want to run simulations with on many different speed grids you can use the `generate_simulation_dataset` method as a convenience. The results will be saved to a [zarr](https://zarr.readthedocs.io/en/stable/) file of your chosing. You can then use the `load_simulation_dataset` to load the dataset.  ```python from waver.datasets import generate_simulation_dataset  # Define root path for simulation path = './simulation_dataset.zarr' runs = 5  # Define a simulation, 12.8mm, 100um spacing dataset_sim_params = {     'size': (12.8e-3, 12.8e-3),     'spacing': 100e-6,     'duration': 80e-6,     'min_speed': 343,     'max_speed': 686,     'speed': 'mixed_random_ifft',     'time_step': 50e-9,     'sources': [{         'location': (None, 0),         'period': 5e-6,         'ncycles':1,     }],     'temporal_downsample': 2,     'boundary': 1,     'edge': 1, }  # Run and save simulation generate_simulation_dataset(path, runs, **dataset_sim_params) ```  The `generate_simulation_dataset` allows the `speed` to be a string that will specify a particular method of randomly generating speed values for the simulation grid.  ### The Simulation Object  If you'd like to understand in a little bit more detail how a simulation is defined then you might want to use the unerlying simulation object `Simulation` and manually set key objects like the `Source` and `Detector`. A full example of this is as follows  ```python # Create a simulation sim = Simulation(size=size, spacing=spacing, max_speed=max_speed, time_step=time_step)  # Set speed array sim.set_speed(speed=speed, min_speed=min_speed, max_speed=max_speed)  # Add source sim.add_source(location=location, period=period, ncycles=ncycles, phase=phase)  # Add detector grid sim.add_detector(spatial_downsample=spatial_downsample,                     boundary=boundary, edge=edge)  # Run simulation sim.run(duration=duration, temporal_downsample=temporal_downsample, progress=progress, leave=leave)  # Print simulation wave and speed data print('wave: ', sim.detected_wave) print('speed: ', sim.grid_speed) ```  Note these steps are done inside the `run_single_source` method for you as a convenience.  ## Known Limitations  A [perfectly matched layer](https://en.wikipedia.org/wiki/Perfectly_matched_layer) boundary has recently been added, but might not perform well under all conditions. Additional contributions would be welcome here.  Right now the simulations are quite slow. I'd like to add a [JAX](https://github.com/google/jax) backend, but  havn't done so yet. Contributions would be welcome.  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"waver\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin [file an issue]: https://github.com/sofroniewn/waver/issues [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "waver      Run simulations of the wave equation in nD on grids of variable speed in Python. This library owes a lot of its design and approach to the fdtd library, a Python 3D electromagnetic FDTD simulator. This package allows for a fair amount of customization over your wave simulation. You can  - specify the size and spacing of the grid  - specify the time step for the simulation, which will be checked to ensure stability of the simulation  - specify the duration of the simulation  - setting a variable speed array (one value per grid point) to allow for \"objects\" in your environment  - set the source of the wave, which can be a point, line, or any (n-1)D subarray  - record the wave with a detector, which can be the full grid, the full boundary, or a particular boundary  - use convenience methods to run many simulations with different sources on the same grid and detector combination You can use napari, a multi-dimensional image viewer for Python, to allow for easy visualization of the detected wave. Some functionality is also available as a napari plugin to allow for running simulations from a graphical user interface. Results can look like https://user-images.githubusercontent.com/6531703/128283012-a784ec06-4df9-4ddf-bf4f-e21b927fe4a3.mov  Installation You can install waver via pip: pip install waver  Usage Convenience Methods The most convenient way to use waver is to use one of two convenience methods that will create and run a simulation for you and return the results. The first method run_single_source allows you to run a single simulation with a single source on one grid and  record the results using a detector. For example ```python from waver.simulation import run_single_source single_sim_params = {     'size': (12.8e-3, 12.8e-3),     'spacing': 100e-6,     'duration': 80e-6,     'min_speed': 343,     'max_speed': 686,     'speed': 686,     'time_step': 50e-9,     'temporal_downsample': 2,     'location': (6.4e-3, 6.4e-3),     'period': 5e-6,     'ncycles':1, } detected_wave, speed_grid = run_single_source(**single_sim_params) ``` The second method run_multiple_sources allows you to run multiple simulations with multiple sources on the same grid and with the same detector and return the results. For example ```python from waver.simulation import run_multiple_sources multi_sim_params = {     'size': (12.8e-3, 12.8e-3),     'spacing': 100e-6,     'duration': 80e-6,     'min_speed': 343,     'max_speed': 686,     'speed': 686,     'time_step': 50e-9,     'temporal_downsample': 2,     'sources': [{         'location': (6.4e-3, 6.4e-3),         'period': 5e-6,         'ncycles':1,     }] } detected_wave, speed_grid = run_multiple_sources(**multi_sim_params) ``` The main difference between these two methods is that run_multiple_sources takes a sources parameter which takes a list  of dictionaries with keys corresponding to source related keyword arguments found in run_single_source. Visualization If you want to quickly visualize the results of run_multiple_sources, you can use the run_and_visualize command which will  run the simulation and then launch napari with the results, as seen in examples/2D/point_source.py ```python from waver.datasets import run_and_visualize run_and_visualize(**multi_sim_params) ``` Datasets If you want to run simulations with on many different speed grids you can use the generate_simulation_dataset method as a convenience. The results will be saved to a zarr file of your chosing. You can then use the load_simulation_dataset to load the dataset. ```python from waver.datasets import generate_simulation_dataset Define root path for simulation path = './simulation_dataset.zarr' runs = 5 Define a simulation, 12.8mm, 100um spacing dataset_sim_params = {     'size': (12.8e-3, 12.8e-3),     'spacing': 100e-6,     'duration': 80e-6,     'min_speed': 343,     'max_speed': 686,     'speed': 'mixed_random_ifft',     'time_step': 50e-9,     'sources': [{         'location': (None, 0),         'period': 5e-6,         'ncycles':1,     }],     'temporal_downsample': 2,     'boundary': 1,     'edge': 1, } Run and save simulation generate_simulation_dataset(path, runs, **dataset_sim_params) ``` The generate_simulation_dataset allows the speed to be a string that will specify a particular method of randomly generating speed values for the simulation grid. The Simulation Object If you'd like to understand in a little bit more detail how a simulation is defined then you might want to use the unerlying simulation object Simulation and manually set key objects like the Source and Detector. A full example of this is as follows ```python Create a simulation sim = Simulation(size=size, spacing=spacing, max_speed=max_speed, time_step=time_step) Set speed array sim.set_speed(speed=speed, min_speed=min_speed, max_speed=max_speed) Add source sim.add_source(location=location, period=period, ncycles=ncycles, phase=phase) Add detector grid sim.add_detector(spatial_downsample=spatial_downsample,                     boundary=boundary, edge=edge) Run simulation sim.run(duration=duration, temporal_downsample=temporal_downsample, progress=progress, leave=leave) Print simulation wave and speed data print('wave: ', sim.detected_wave) print('speed: ', sim.grid_speed) ``` Note these steps are done inside the run_single_source method for you as a convenience. Known Limitations A perfectly matched layer boundary has recently been added, but might not perform well under all conditions. Additional contributions would be welcome here. Right now the simulations are quite slow. I'd like to add a JAX backend, but  havn't done so yet. Contributions would be welcome. Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"waver\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "waver",
    "documentation": "",
    "first_released": "2021-05-15T19:23:11.731841Z",
    "license": "BSD-3-Clause",
    "name": "waver",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader", "widget"],
    "project_site": "https://github.com/sofroniewn/waver",
    "python_version": ">=3.7",
    "reader_file_extensions": ["*"],
    "release_date": "2021-08-15T23:04:51.974505Z",
    "report_issues": "",
    "requirements": [
      "magicgui (>=0.2.10)",
      "napari (>=0.4.10)",
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "zarr"
    ],
    "summary": "Wave simulations",
    "support": "",
    "twitter": "",
    "version": "0.0.4",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Robert Haase" }],
    "category": {
      "Workflow step": [
        "Pixel classification",
        "Object classification",
        "Image annotation",
        "Image feature detection"
      ]
    },
    "category_hierarchy": {
      "Workflow step": [
        ["Pixel classification"],
        ["Object classification"],
        ["Image annotation"],
        ["Image feature detection"]
      ]
    },
    "code_repository": "https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification",
    "description": "# napari-accelerated-pixel-and-object-classification (APOC)\\r \\r [![License](https://img.shields.io/pypi/l/napari-accelerated-pixel-and-object-classification.svg?color=green)](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/LICENSE)\\r [![PyPI](https://img.shields.io/pypi/v/napari-accelerated-pixel-and-object-classification.svg?color=green)](https://pypi.org/project/napari-accelerated-pixel-and-object-classification)\\r [![Python Version](https://img.shields.io/pypi/pyversions/napari-accelerated-pixel-and-object-classification.svg?color=green)](https://python.org)\\r [![tests](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/actions)\\r [![codecov](https://codecov.io/gh/haesleinhuepf/napari-accelerated-pixel-and-object-classification/branch/main/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-accelerated-pixel-and-object-classification)\\r [![Development Status](https://img.shields.io/pypi/status/napari-accelerated-pixel-and-object-classification.svg)](https://en.wikipedia.org/wiki/Software_release_life_cycle#Alpha)\\r [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-accelerated-pixel-and-object-classification)](https://napari-hub.org/plugins/napari-accelerated-pixel-and-object-classification)\\r [![DOI](https://zenodo.org/badge/412525441.svg)](https://zenodo.org/badge/latestdoi/412525441)\\r \\r [clesperanto](https://github.com/clEsperanto/pyclesperanto_prototype) meets [scikit-learn](https://scikit-learn.org/stable/) to classify pixels and objects in images, on a [GPU](https://en.wikipedia.org/wiki/Graphics_processing_unit) using [OpenCL](https://www.khronos.org/opencl/) in [napari].\\r \\r ![](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/screencast.gif)\\r The processed example image was kindly acquired by Daniela Vorkel, Myers lab, MPI-CBG / CSBD ([Download full video](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/demo_lund.mp4))\\r \\r For using the accelerated pixel and object classifiers in python, check out [apoc](https://github.com/haesleinhuepf/apoc).\\r Training classifiers from pairs of image and label-mask folders is explained in \\r [this notebook](https://github.com/haesleinhuepf/apoc/blob/main/demo/train_on_folders.ipynb).\\r For executing APOC classifiers in [Fiji](https://fiji.sc) using [clij2](https://clij.github.io) please read the documentation of the [corresponding Fiji plugin](https://github.com/clij/clijx-accelerated-pixel-and-object-classification).\\r \\r ![](https://github.com/clij/clijx-accelerated-pixel-and-object-classification/raw/main/docs/screenshot.png)\\r \\r \\r \\r ## Usage\\r \\r ### Object and Semantic Segmentation\\r \\r Starting point is napari with at least one image layer and one labels layer (your annotation).\\r \\r ![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/object_segmentation_starting_point.png)\\r \\r You find Object and Semantic Segmentation in the `Tools > Segmentation / labeling`. When starting those, the following graphical user interface will show up.\\r \\r ![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/object_and_semantic_segmentation.png)\\r \\r 1. Choose one or multiple images to train on. These images will be considered as multiple channels. Thus, they need to be spatially correlated. \\r    Training from multiple images showing different scenes is not (yet) supported from the graphical user interface. Check out [this notebook](https://github.com/haesleinhuepf/apoc/blob/main/demo/demp_pixel_classifier_continue_training.ipynb) if you want to train from multiple image-annotation pairs.\\r 2. Select a file where the classifier should be saved. If the file exists already, it will be overwritten.\\r 3. Select the ground-truth annotation labels layer. \\r 4. Select which label corresponds to foreground (not available in Semantic Segmentation)\\r 5. Select the feature images that should be considered for segmentation. If segmentation appears pixelated, try increasing the selected sigma values and untick `Consider original image`.\\r 6. Tree depth and number of trees allow you to fine-tune how to deal with manifold regions of different characteristics. The higher these numbers, the longer segmentation will take. In case you use many images and many features, high depth and number of trees might be necessary. (See also `max_depth` and `n_estimators` in the [scikit-learn documentation of the Random Forest Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).\\r 7. The estimation of memory consumption allows you to tune the configuration to your GPU-hardware. Also consider the GPU-hardware of others who want to use your classifier.\\r 8. Click on Run when you're done with configuring. If the segmentation doesn't fit after the first execution, consider fine-tuning the ground-truth annotation and try again.\\r \\r A successful segmentation can for example look like this:\\r \\r ![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/object_segmentation_result.png)\\r \\r After your classifier has been trained successfully, click on the \"Application / Prediction\" tab. If you apply the classifier again, python code will be generated. \\r You can use this code for example to apply the same classifier to a folder of images. If you're new to this, check out [this notebook](https://github.com/BiAPoL/Bio-image_Analysis_with_Python/blob/main/image_processing/12_process_folders.ipynb).\\r \\r ![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/code_generation.png)\\r \\r A pre-trained classifier can be [applied from scripts as shown in the example notebook](https://github.com/haesleinhuepf/apoc/blob/main/demo/demo_object_segmenter.ipynb) or from the `Tools > Segmentation / labeling > Object segmentation (apply pretrained, APOC)`.\\r \\r ### Integration with the napari-assistant\\r \\r Pre-trained models can also be assembled to workflows using the [napari-assistant](https://www.napari-hub.org/plugins/napari-assistant). You find APOC-operations in the categories `Filter`, `Label` and `Label Filters`:\\r \\r ![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/assistant.png)\\r \\r ### Semantic segmentation\\r \\r Users can also generate semantic segmentation label images where the label identifier corresponds to a class the pixel has been allocated to. \\r The tool can be found in the menu `Tools > Segmentation / labeling > Semantic segmentation (APOC)`.\\r It works analogously like the Object Segmenter, just without the need to specify the class identifier that objects correspond to.\\r \\r ![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/semantic_segmentation.png)\\r \\r ### Probability maps\\r \\r The tool for generating probability maps (`Tools > Filtering > Probability Mapper (APOC)` menu) works analogously to the Object Segmenter as well. \\r The only difference is that the result image is not a label image but an intensity image where the intensity represents the probability (between 0 and 1)\\r that a pixel belongs to a given class. In this example: The raw image (grey) has been annotated with three classes: background (black, label 1), foreground (white, label 2) and edges (grey, label 3).\\r The probability mapper was configured to create probability image (shown in green) for edges (label 3):\\r \\r ![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/probability_mapper.png)\\r \\r ### Classifier statistics\\r \\r While training, you can also activate the `Show classifier statistics` checkbox. \\r When doing so, it is recommended to increase the number of trees so that the measurements are more reliable, especially when selecting many features.\\r This will open a small table after training where you can see how large the share of decision trees are for each analysed feature image.\\r \\r ![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/classifier_statistics.png)\\r \\r It is recommended to turn on/off the features that hold a very large share (green) or a very small share (magenta) of trees in the random forest. \\r Retrain the classifier to see how the features influence the decision making.\\r \\r Note: Multiple of these parameters may be correlated. \\r If you select 11 feature images, which all allow to make the pixel classification similarly, but 10 of those are correlated, these 10 may appear with a share of about 0.05 while the 11th parameter has a share of 0.5. \\r Thus, study these values with care.\\r \\r ### Merging objects\\r \\r After segmentation, you can merge labeled objects using the `Tools > Segmentation post-processing > Merge objects (APOC)` menu. \\r Annotate label edges that should be merged with intensity 1 and those which should be kept with intensity 2 in a blank label image.\\r Select which features should be considered for merging:\\r * `touch_portion`: The relative amount an object touches another. E.g. in a symmetric, honey-comb like tissue, neighboring cells have a touch-portion of `1/6` to each other.\\r * `touch_count`: The number of pixels where object touch. When using this parameter, make sure that images used for training and prediction have the same voxel size.\\r * `mean_touch_intensity`: The mean average intensity between touching objects. When using this parameter, make sure images used for training and prediction are normalized the same way.\\r * `centroid_distance`: The distance (in pixels or voxels) between centroids of labeled objects. \\r \\r Note: most features are recommended to be used in isotropic images only.\\r \\r ![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/images/merge_objects1.png)\\r \\r For training, use an image with equivalized intensity (1), an over-segmented label image (2) and annotations (3). When drawing annotations in a new labels layer, make sure to misguide the algorithm draw on edges of touching objects a 1 if those should be merged and a 2 if they should be kept. Make sure there are no 1/2 annotation circles on both: labels which should be merged and kept.\\r \\r ![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/images/merge_objects2.png)\\r \\r ### Object classification\\r \\r Click the menu `Tools > Segmentation post-processing > Object classification (APOC)`. \\r \\r ![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/menu.png)\\r \\r This user interface will be shown:\\r \\r ![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/object_classifier_gui.png)\\r \\r 1. The image layer will be used for intensity based feature extraction (see below).\\r 2. The labels layer should be contain the segmentation of objects that should be classified. \\r    You can use the Object Segmenter explained above to create this layer.\\r 3. The annotation layer should contain manual annotations of object classes. \\r    You can draw lines crossing single and multiple objects of the same kind. \\r    For example draw a line through some elongated objects with label \"1\" and another line through some rather roundish objects with label \"2\".\\r    If these lines touch the background, that will be ignored.\\r 4. Tree depth and number of trees allow you to fine-tune how to deal with manifold objects of different characteristics. The higher these numbers, the longer classification will take. In case you use many features, high depth and number of trees might be necessary. (See also `max_depth` and `n_estimators` in the [scikit-learn documentation of the Random Forest Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).\\r 5. Select the right features for training. For example, for differentiating objects according to their shape as suggested above, select \"shape\".\\r    The features are extracted using clEsperanto and are shown by example in [this notebook](https://github.com/clEsperanto/pyclesperanto_prototype/blob/master/demo/tissues/parametric_maps.ipynb).\\r 6. Click on the `Run` button. If classification doesn't perform well in the first attempt, try changing selected features.  \\r \\r If classification worked well, it may for example look like this. Note the two thick lines which were drawn to annotate elongated and roundish objects with brown and cyan:\\r \\r ![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/object_classification_result.png)\\r \\r A pre-trained model can later be applied [from scripts as shown in the example notebook](https://github.com/haesleinhuepf/apoc/blob/main/demo/cell_classification.ipynb) or using the menu `Tools > Segmentation post-processing > Object classification (apply pretrained, APOC)`.\\r \\r ### Feature correlation matrix\\r \\r When training object classifiers it is crucial to investigate to which degree features are correlated and select the right, ideally uncorrelated features to classify objects robustly.\\r After measuring features with any compatible napari plugin listed below, you can visualize the feature correlation matrix using the menu `Tools > Measurement > Show feature correlation matrix (pandas, APOC)` and by selecting the labels layer which has been analyzed.\\r Before computing the correlation matrix, all rows containing [NaN](https://en.wikipedia.org/wiki/NaN) values are removed.\\r For further details, please refer to the [documentation of the underlying function in pandas](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html).\\r \\r ![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/feature_correlation_matrix.png)\\r \\r ### Surface Vertex Classification (SVeC)\\r \\r When using napari-APOC in combination with [napari-process-points-and-surfaces>=0.3.3](https://github.com/haesleinhuepf/napari-process-points-and-surfaces), \\r one can also classify vertices. Therefore, use for example the menu `Measurement > Surface quality table (vedo, nppas)` to determine quantitative measurements\\r and the menu `Surfaces > Annotate surface manually (nppas)` for manual annotations. It is recommended to annotate the entire surface with value 1 as background, and specific regions of interest with integer numbers > 1.\\r After measurements have been extracted and annotations were made, start SVeC from the `Surfaces > Surface vertex classification (custom properties, APOC)` menu. It can be used like the Object Classifier explained above.\\r \\r ![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/demo_vertex_classification.gif)\\r \\r [Download full video](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/demo_vertex_classification.mp4)\\r \\r ### Classifier statistics\\r After classifier training, you can study the share of the individual features/measurements and how they are correlated by activating the checkboxes `Show classifier statistics` and `Show feature correlation matrix`.\\r \\r ![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/correlation_matrix2.png)\\r \\r This can help understanding how the classifier works. Furthermore, you can accelerate the classifier by reducing the number of correlated features.\\r \\r ### Object classification from custom measurements\\r \\r You can also classify labeled objects according to custom measurements. For deriving those measurements, you can use these napari plugins:\\r \\r * [morphometrics](https://www.napari-hub.org/plugins/morphometrics)\\r * [PartSeg](https://www.napari-hub.org/plugins/PartSeg)\\r * [napari-simpleitk-image-processing](https://www.napari-hub.org/plugins/napari-simpleitk-image-processing)\\r * [napari-cupy-image-processing](https://www.napari-hub.org/plugins/napari-cupy-image-processing)\\r * [napari-pyclesperanto-assistant](https://www.napari-hub.org/plugins/napari-pyclesperanto-assistant)\\r * [napari-skimage-regionprops](https://www.napari-hub.org/plugins/napari-skimage-regionprops)\\r \\r Furthermore, if you use napari from Python, you can also create a dictionary or pandas DataFrame with measurements and store it in the `labels_layer.features` to make them available in the object classifier.\\r \\r After labels have been measured, you can start the `Object Classifier (custom properties, APOC)` from the `Tools > Segmentation post-processing` menu:\\r \\r ![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/table_row_classifier_gui.png)\\r \\r 1. Select the labels layers that has been measured.\\r 2. The annotation layer should contain manual annotations of object classes. \\r    You can draw lines crossing single and multiple objects of the same kind. \\r    For example draw a line through some elongated objects with label \"1\" and another line through some rather roundish objects with label \"2\".\\r    If these lines touch the background, that will be ignored.\\r 3. Select the measurements / features that should be used for object classification.\\r 4. Use the `Update Measurements` button in case you did new measurements after Object classifier dialog was opened.\\r 5. Enter the filename of the classifier to be trained here. This file will be overwritten in case it existed already.\\r 6. Tree depth and number of trees allow you to fine-tune how to deal with manifold objects of different characteristics. The higher these numbers, the longer classification will take. In case you use many features, high depth and number of trees might be necessary. (See also `max_depth` and `n_estimators` in the [scikit-learn documentation of the Random Forest Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).\\r 7. The classification result will be stored under this name in the labels-layer's properties.\\r 8. Choose if the results table should be shown. Choose if classifier statistics should be shown. [Read more about classifier statistics](https://haesleinhuepf.github.io/BioImageAnalysisNotebooks/27_cell_classification/forest_statistics.html).\\r 9. Click on `Run` to start training and prediction.\\r \\r You can also train those classifiers from Python and reuse them: [Read more about using the TableRowClassifier from python](https://haesleinhuepf.github.io/BioImageAnalysisNotebooks/27_cell_classification/apoc_simpleitk_object_classification.html)\\r \\r ### Classifier statistics and correlation matrix\\r After classifier training, you can study the share of the individual features/measurements and how they are correlated by activating the checkboxes `Show classifier statistics` and `Show correlation matrix`.\\r ![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/correlation_matrix.png)\\r \\r This can help understanding how the classifier works. Furthermore, you can accelerate the classifier by reducing the number of correlated features.\\r \\r ----------------------------------\\r \\r This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\\r \\r ## Installation\\r \\r It is recommended to install the plugin in a conda environment. Therefore install conda first, e.g. [mini-conda](https://docs.conda.io/en/latest/miniconda.html).\\r If you never worked with conda before, reading this [short introduction](https://github.com/BiAPoL/Bio-image_Analysis_with_Python/blob/main/conda_basics/01_conda_environments.md) might be helpful.\\r \\r Optional: Setup a fresh conda environment, activate it and install napari:\\r \\r ```\\r conda create --name napari_apoc python=3.9\\r conda activate napari_apoc\\r conda install napari\\r ```\\r \\r If your conda environment is set up, you can install `napari-accelerated-pixel-and-object-classification` using [pip]. Note: you need [pyopencl](https://documen.tician.de/pyopencl/) first.\\r \\r ```\\r conda install -c conda-forge pyopencl\\r pip install napari-accelerated-pixel-and-object-classification\\r ```\\r \\r Mac-users please also install this:\\r \\r     conda install -c conda-forge ocl_icd_wrapper_apple\\r     \\r Linux users please also install this:\\r     \\r     conda install -c conda-forge ocl-icd-system\\r \\r \\r ## Contributing\\r  \\r Contributions, feedback and suggestions are very welcome. Tests can be run with [tox], please ensure\\r the coverage at least stays the same before you submit a pull request.\\r \\r ## Similar napari plugins\\r There are other plugins with similar functionality for interactive classification of pixels and objects.\\r \\r * [napari-feature-classifier](https://github.com/fractal-napari-plugins-collection/napari-feature-classifier)\\r \\r ## License\\r \\r Distributed under the terms of the [BSD-3] license,\\r \"napari-accelerated-pixel-and-object-classification\" is free and open source software\\r \\r ## Issues\\r \\r If you encounter any problems, please [open a thread on image.sc](https://image.sc) along with a detailed description and tag [@haesleinhuepf](https://github.com/haesleinhuepf).\\r \\r [napari]: https://github.com/napari/napari\\r [Cookiecutter]: https://github.com/audreyr/cookiecutter\\r [@napari]: https://github.com/napari\\r [MIT]: http://opensource.org/licenses/MIT\\r [BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\r [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\r [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\r [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\r [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\r [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\r [file an issue]: https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/issues\\r [napari]: https://github.com/napari/napari\\r [tox]: https://tox.readthedocs.io/en/latest/\\r [pip]: https://pypi.org/project/pip/\\r [PyPI]: https://pypi.org/\\r ",
    "description_content_type": "text/markdown",
    "description_text": "napari-accelerated-pixel-and-object-classification (APOC)         clesperanto meets scikit-learn to classify pixels and objects in images, on a GPU using OpenCL in napari.  The processed example image was kindly acquired by Daniela Vorkel, Myers lab, MPI-CBG / CSBD (Download full video) For using the accelerated pixel and object classifiers in python, check out apoc. Training classifiers from pairs of image and label-mask folders is explained in  this notebook. For executing APOC classifiers in Fiji using clij2 please read the documentation of the corresponding Fiji plugin.  Usage Object and Semantic Segmentation Starting point is napari with at least one image layer and one labels layer (your annotation).  You find Object and Semantic Segmentation in the Tools > Segmentation / labeling. When starting those, the following graphical user interface will show up.   Choose one or multiple images to train on. These images will be considered as multiple channels. Thus, they need to be spatially correlated.     Training from multiple images showing different scenes is not (yet) supported from the graphical user interface. Check out this notebook if you want to train from multiple image-annotation pairs. Select a file where the classifier should be saved. If the file exists already, it will be overwritten. Select the ground-truth annotation labels layer.  Select which label corresponds to foreground (not available in Semantic Segmentation) Select the feature images that should be considered for segmentation. If segmentation appears pixelated, try increasing the selected sigma values and untick Consider original image. Tree depth and number of trees allow you to fine-tune how to deal with manifold regions of different characteristics. The higher these numbers, the longer segmentation will take. In case you use many images and many features, high depth and number of trees might be necessary. (See also max_depth and n_estimators in the scikit-learn documentation of the Random Forest Classifier. The estimation of memory consumption allows you to tune the configuration to your GPU-hardware. Also consider the GPU-hardware of others who want to use your classifier. Click on Run when you're done with configuring. If the segmentation doesn't fit after the first execution, consider fine-tuning the ground-truth annotation and try again.  A successful segmentation can for example look like this:  After your classifier has been trained successfully, click on the \"Application / Prediction\" tab. If you apply the classifier again, python code will be generated.  You can use this code for example to apply the same classifier to a folder of images. If you're new to this, check out this notebook.  A pre-trained classifier can be applied from scripts as shown in the example notebook or from the Tools > Segmentation / labeling > Object segmentation (apply pretrained, APOC). Integration with the napari-assistant Pre-trained models can also be assembled to workflows using the napari-assistant. You find APOC-operations in the categories Filter, Label and Label Filters:  Semantic segmentation Users can also generate semantic segmentation label images where the label identifier corresponds to a class the pixel has been allocated to.  The tool can be found in the menu Tools > Segmentation / labeling > Semantic segmentation (APOC). It works analogously like the Object Segmenter, just without the need to specify the class identifier that objects correspond to.  Probability maps The tool for generating probability maps (Tools > Filtering > Probability Mapper (APOC) menu) works analogously to the Object Segmenter as well.  The only difference is that the result image is not a label image but an intensity image where the intensity represents the probability (between 0 and 1) that a pixel belongs to a given class. In this example: The raw image (grey) has been annotated with three classes: background (black, label 1), foreground (white, label 2) and edges (grey, label 3). The probability mapper was configured to create probability image (shown in green) for edges (label 3):  Classifier statistics While training, you can also activate the Show classifier statistics checkbox.  When doing so, it is recommended to increase the number of trees so that the measurements are more reliable, especially when selecting many features. This will open a small table after training where you can see how large the share of decision trees are for each analysed feature image.  It is recommended to turn on/off the features that hold a very large share (green) or a very small share (magenta) of trees in the random forest.  Retrain the classifier to see how the features influence the decision making. Note: Multiple of these parameters may be correlated.  If you select 11 feature images, which all allow to make the pixel classification similarly, but 10 of those are correlated, these 10 may appear with a share of about 0.05 while the 11th parameter has a share of 0.5.  Thus, study these values with care. Merging objects After segmentation, you can merge labeled objects using the Tools > Segmentation post-processing > Merge objects (APOC) menu.  Annotate label edges that should be merged with intensity 1 and those which should be kept with intensity 2 in a blank label image. Select which features should be considered for merging: * touch_portion: The relative amount an object touches another. E.g. in a symmetric, honey-comb like tissue, neighboring cells have a touch-portion of 1/6 to each other. * touch_count: The number of pixels where object touch. When using this parameter, make sure that images used for training and prediction have the same voxel size. * mean_touch_intensity: The mean average intensity between touching objects. When using this parameter, make sure images used for training and prediction are normalized the same way. * centroid_distance: The distance (in pixels or voxels) between centroids of labeled objects.  Note: most features are recommended to be used in isotropic images only.  For training, use an image with equivalized intensity (1), an over-segmented label image (2) and annotations (3). When drawing annotations in a new labels layer, make sure to misguide the algorithm draw on edges of touching objects a 1 if those should be merged and a 2 if they should be kept. Make sure there are no 1/2 annotation circles on both: labels which should be merged and kept.  Object classification Click the menu Tools > Segmentation post-processing > Object classification (APOC).   This user interface will be shown:   The image layer will be used for intensity based feature extraction (see below). The labels layer should be contain the segmentation of objects that should be classified.     You can use the Object Segmenter explained above to create this layer. The annotation layer should contain manual annotations of object classes.     You can draw lines crossing single and multiple objects of the same kind.     For example draw a line through some elongated objects with label \"1\" and another line through some rather roundish objects with label \"2\".    If these lines touch the background, that will be ignored. Tree depth and number of trees allow you to fine-tune how to deal with manifold objects of different characteristics. The higher these numbers, the longer classification will take. In case you use many features, high depth and number of trees might be necessary. (See also max_depth and n_estimators in the scikit-learn documentation of the Random Forest Classifier. Select the right features for training. For example, for differentiating objects according to their shape as suggested above, select \"shape\".    The features are extracted using clEsperanto and are shown by example in this notebook. Click on the Run button. If classification doesn't perform well in the first attempt, try changing selected features.    If classification worked well, it may for example look like this. Note the two thick lines which were drawn to annotate elongated and roundish objects with brown and cyan:  A pre-trained model can later be applied from scripts as shown in the example notebook or using the menu Tools > Segmentation post-processing > Object classification (apply pretrained, APOC). Feature correlation matrix When training object classifiers it is crucial to investigate to which degree features are correlated and select the right, ideally uncorrelated features to classify objects robustly. After measuring features with any compatible napari plugin listed below, you can visualize the feature correlation matrix using the menu Tools > Measurement > Show feature correlation matrix (pandas, APOC) and by selecting the labels layer which has been analyzed. Before computing the correlation matrix, all rows containing NaN values are removed. For further details, please refer to the documentation of the underlying function in pandas.  Surface Vertex Classification (SVeC) When using napari-APOC in combination with napari-process-points-and-surfaces>=0.3.3,  one can also classify vertices. Therefore, use for example the menu Measurement > Surface quality table (vedo, nppas) to determine quantitative measurements and the menu Surfaces > Annotate surface manually (nppas) for manual annotations. It is recommended to annotate the entire surface with value 1 as background, and specific regions of interest with integer numbers > 1. After measurements have been extracted and annotations were made, start SVeC from the Surfaces > Surface vertex classification (custom properties, APOC) menu. It can be used like the Object Classifier explained above.  Download full video Classifier statistics After classifier training, you can study the share of the individual features/measurements and how they are correlated by activating the checkboxes Show classifier statistics and Show feature correlation matrix.  This can help understanding how the classifier works. Furthermore, you can accelerate the classifier by reducing the number of correlated features. Object classification from custom measurements You can also classify labeled objects according to custom measurements. For deriving those measurements, you can use these napari plugins:  morphometrics PartSeg napari-simpleitk-image-processing napari-cupy-image-processing napari-pyclesperanto-assistant napari-skimage-regionprops  Furthermore, if you use napari from Python, you can also create a dictionary or pandas DataFrame with measurements and store it in the labels_layer.features to make them available in the object classifier. After labels have been measured, you can start the Object Classifier (custom properties, APOC) from the Tools > Segmentation post-processing menu:   Select the labels layers that has been measured. The annotation layer should contain manual annotations of object classes.     You can draw lines crossing single and multiple objects of the same kind.     For example draw a line through some elongated objects with label \"1\" and another line through some rather roundish objects with label \"2\".    If these lines touch the background, that will be ignored. Select the measurements / features that should be used for object classification. Use the Update Measurements button in case you did new measurements after Object classifier dialog was opened. Enter the filename of the classifier to be trained here. This file will be overwritten in case it existed already. Tree depth and number of trees allow you to fine-tune how to deal with manifold objects of different characteristics. The higher these numbers, the longer classification will take. In case you use many features, high depth and number of trees might be necessary. (See also max_depth and n_estimators in the scikit-learn documentation of the Random Forest Classifier. The classification result will be stored under this name in the labels-layer's properties. Choose if the results table should be shown. Choose if classifier statistics should be shown. Read more about classifier statistics. Click on Run to start training and prediction.  You can also train those classifiers from Python and reuse them: Read more about using the TableRowClassifier from python Classifier statistics and correlation matrix After classifier training, you can study the share of the individual features/measurements and how they are correlated by activating the checkboxes Show classifier statistics and Show correlation matrix.  This can help understanding how the classifier works. Furthermore, you can accelerate the classifier by reducing the number of correlated features.  This napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template. Installation It is recommended to install the plugin in a conda environment. Therefore install conda first, e.g. mini-conda. If you never worked with conda before, reading this short introduction might be helpful. Optional: Setup a fresh conda environment, activate it and install napari: conda create --name napari_apoc python=3.9 conda activate napari_apoc conda install napari If your conda environment is set up, you can install napari-accelerated-pixel-and-object-classification using pip. Note: you need pyopencl first. conda install -c conda-forge pyopencl pip install napari-accelerated-pixel-and-object-classification Mac-users please also install this: conda install -c conda-forge ocl_icd_wrapper_apple  Linux users please also install this: conda install -c conda-forge ocl-icd-system  Contributing Contributions, feedback and suggestions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. Similar napari plugins There are other plugins with similar functionality for interactive classification of pixels and objects.  napari-feature-classifier  License Distributed under the terms of the BSD-3 license, \"napari-accelerated-pixel-and-object-classification\" is free and open source software Issues If you encounter any problems, please open a thread on image.sc along with a detailed description and tag @haesleinhuepf.",
    "development_status": ["Development Status :: 3 - Alpha"],
    "display_name": "napari-accelerated-pixel-and-object-classification",
    "documentation": "https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification",
    "first_released": "2021-10-02T19:44:17.676429Z",
    "license": "BSD-3-Clause",
    "name": "napari-accelerated-pixel-and-object-classification",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-12-26T23:39:35.685852Z",
    "report_issues": "https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/issues",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "apoc (>=0.11.0)",
      "napari-tools-menu (>=0.1.17)",
      "napari-time-slicer",
      "superqt",
      "imageio (!=2.22.1)"
    ],
    "summary": "Pixel and label classification using OpenCL-based Random Forest Classifiers",
    "support": "https://forum.image.sc/tag/clij",
    "twitter": "",
    "version": "0.11.1",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "email": "h.kawai888@gmail.com", "name": "Hiroki Kawai" }],
    "code_repository": "https://github.com/neurobiology-ut/PHILOW",
    "description": "# napari-PHILOW  [![License](https://img.shields.io/pypi/l/napari-PHILOW.svg?color=green)](https://github.com/neurobiology-ut/PHILOW/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-PHILOW.svg?color=green)](https://pypi.org/project/napari-PHILOW) [![Python Version](https://img.shields.io/pypi/pyversions/napari-PHILOW.svg?color=green)](https://python.org) [![tests](https://github.com/neurobiology-ut/napari-PHILOW/workflows/tests/badge.svg)](https://github.com/neurobiology-ut/PHILOW/actions) [![codecov](https://codecov.io/gh/neurobiology-ut/napari-PHILOW/branch/main/graph/badge.svg)](https://codecov.io/gh/neurobiology-ut/PHILOW) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-PHILOW)](https://napari-hub.org/plugins/napari-PHILOW)  # PHILOW <br> ***P***ython-based platform for ***h***uman-***i***n-the-***lo***op (HITL)  ***w***orkflow (PHILOW) <br>  PHILOW is an interactive deep learning-based platform for 3D datasets implemented on top of [napari](https://github.com/napari/napari)  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/plugins/stable/index.html -->  ## Installation  You can install `napari-PHILOW` via [pip]:      pip install napari-PHILOW      or clone this repository    then ```angular2 cd PHILOW pip install -e . ```       ## Usage  Launch napari   ```angular2 napari ```   #### load dataset   1) Plugins > napari-PHILOW > Annotation Mode  2) Select original dir : all slices must be in separate PNG and must be sequentially numbered (e.g. 000.png, 001.png ...)  3) Select mask dir : To resume from the middle of the annotation, specify here the name of the directory containing the mask image. The directory must contain the same number of files with the same name as the original image.     If you are starting a completely new annotation, you do not need to specify a directory. The directory for mask is automatically created and blank images are generated and stored.  4) Enter a name for the label or model you want to create (e.g. mito, cristae, ...)    This name will be used as the directory name of the newly created mask dir if no mask dir is specified,  and as the name of the csv file for training dataset management.  5) Check if you want to create new dataset (new model) When checked, if there is already a csv file for training dataset management, a new csv file with one sequential number will be generated.  6) Start tracing   #### create labels Create a label with the brush function. more information → https://napari.org/tutorials/fundamentals/labels.html  #### Orthogonal view If you want to see orthogonal view, click on the location you want to see while holding down the Shift button.     The image from xy, yz, and zx will be displayed on the right side of the screen.  #### Low confident layer If you are in the second iteration and you are loading the prediction results, you will see a low confidence layer.     This shows the area where the confidence of the prediction result is low.     Use this as a reference for correction.     #### Small object layer We provide a small object layer to find small painted areas.    This is a layer for displaying small objects.    The slider widget on the left allows you to change the maximum object size to be displayed.     #### save labels If you want to save your label, click the \"save\" button on the bottom right.  #### select training dataset We are providing a way to manage the dataset for use in training.    If you want to use the currently displayed slice as your training data, click the 'Not Checked' button near the center left to display 'Checked'.   ### Train and pred with your gpu machine #### Train To train on your GPU machine (or with CPU),   1) Plugins > napari-PHILOW > Trainer     2) Select original dir : all slices must be in separate PNG and must be sequentially numbered (e.g. 000.png, 001.png ...)        3) Select labels dir : all label images should be named same as original images and contains data management csv file        4) Select dir for save trained model        5) Click on the \"start training\" button     6) Dice score and dice loss are displayed. For more detail, check the command line for the progress of training. If you want to stop in the middle, click stop button.        #### Predict To predict labels on your machine,    1) Plugins > napari-PHILOW > Predicter     2) Select original dir : all slices must be in separate PNG and must be sequentially numbered (e.g. 000.png, 001.png ...)        3) (Optional) Select labels dir if you want to keep labels witch were used on training, and data management csv file        4) Select model dir contains hdf5 file        5) Select output dir for predicted labels     6) Uncheck the box if you DO NOT want to use TAP (Three-Axis-Prediction)        7) Click on the \"predict\" button    8) Check the command line for the progress of prediction. If you want to stop in the middle, use ctrl+C.     9) You can start the next round of annotation by selecting the merged_prediction directory as the mask dir in Annotation mode.  ### Train and predict with Google Colab    If you don't have a GPU machine, you can use Google Colab to perform GPU-based training and prediction for free.      1) Open [train and predict notebook](https://github.com/neurobiology-ut/PHILOW/blob/develop/notebooks/train_and_pred_using_PHILOW.ipynb) and click \"Open in Colab\" button  2) You can upload your own dataset to train and predict, or try it on demo data      ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [GNU GPL v3.0] license, \"napari-PHILOW\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  # Authors <br>  Shogo Suga <br> Hiroki Kawai <br> <a href=\"http://park.itc.u-tokyo.ac.jp/Hirabayashi/WordPress/\">Yusuke Hirabayashi</a>    # How to Cite <br> Shogo Suga, Koki Nakamura, Bruno M Humbel, Hiroki Kawai, Yusuke Hirabayashi, An interactive deep learning-based approach reveals mitochondrial cristae topologies <a href=\"https://doi.org/10.1101/2021.06.11.448083\">https://doi.org/10.1101/2021.06.11.448083</a>   ``` @article {Suga2021.06.11.448083, \\tauthor = {Suga, Shogo and Nakamura, Koki and Humbel, Bruno M and Kawai, Hiroki and Hirabayashi, Yusuke}, \\ttitle = {An interactive deep learning-based approach reveals mitochondrial cristae topologies}, \\telocation-id = {2021.06.11.448083}, \\tyear = {2021}, \\tdoi = {10.1101/2021.06.11.448083}, \\tpublisher = {Cold Spring Harbor Laboratory}, \\tURL = {https://www.biorxiv.org/content/early/2021/06/11/2021.06.11.448083}, \\teprint = {https://www.biorxiv.org/content/early/2021/06/11/2021.06.11.448083.full.pdf}, \\tjournal = {bioRxiv} } ```  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-PHILOW       PHILOW  Python-based platform for human-in-the-loop (HITL)  workflow (PHILOW)  PHILOW is an interactive deep learning-based platform for 3D datasets implemented on top of napari  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install napari-PHILOW via pip: pip install napari-PHILOW  or clone this repository  then angular2 cd PHILOW pip install -e . Usage Launch napari  angular2 napari load dataset 1) Plugins > napari-PHILOW > Annotation Mode 2) Select original dir : all slices must be in separate PNG and must be sequentially numbered (e.g. 000.png, 001.png ...) 3) Select mask dir : To resume from the middle of the annotation, specify here the name of the directory containing the mask image. The directory must contain the same number of files with the same name as the original image.   If you are starting a completely new annotation, you do not need to specify a directory. The directory for mask is automatically created and blank images are generated and stored. 4) Enter a name for the label or model you want to create (e.g. mito, cristae, ...)  This name will be used as the directory name of the newly created mask dir if no mask dir is specified,  and as the name of the csv file for training dataset management. 5) Check if you want to create new dataset (new model) When checked, if there is already a csv file for training dataset management, a new csv file with one sequential number will be generated. 6) Start tracing create labels Create a label with the brush function. more information → https://napari.org/tutorials/fundamentals/labels.html Orthogonal view If you want to see orthogonal view, click on the location you want to see while holding down the Shift button.   The image from xy, yz, and zx will be displayed on the right side of the screen. Low confident layer If you are in the second iteration and you are loading the prediction results, you will see a low confidence layer.   This shows the area where the confidence of the prediction result is low.   Use this as a reference for correction.    Small object layer We provide a small object layer to find small painted areas.  This is a layer for displaying small objects.  The slider widget on the left allows you to change the maximum object size to be displayed.    save labels If you want to save your label, click the \"save\" button on the bottom right. select training dataset We are providing a way to manage the dataset for use in training.  If you want to use the currently displayed slice as your training data, click the 'Not Checked' button near the center left to display 'Checked'. Train and pred with your gpu machine Train To train on your GPU machine (or with CPU),  1) Plugins > napari-PHILOW > Trainer 2) Select original dir : all slices must be in separate PNG and must be sequentially numbered (e.g. 000.png, 001.png ...)    3) Select labels dir : all label images should be named same as original images and contains data management csv file    4) Select dir for save trained model    5) Click on the \"start training\" button    6) Dice score and dice loss are displayed. For more detail, check the command line for the progress of training. If you want to stop in the middle, click stop button.    Predict To predict labels on your machine,   1) Plugins > napari-PHILOW > Predicter 2) Select original dir : all slices must be in separate PNG and must be sequentially numbered (e.g. 000.png, 001.png ...)    3) (Optional) Select labels dir if you want to keep labels witch were used on training, and data management csv file    4) Select model dir contains hdf5 file    5) Select output dir for predicted labels    6) Uncheck the box if you DO NOT want to use TAP (Three-Axis-Prediction)    7) Click on the \"predict\" button   8) Check the command line for the progress of prediction. If you want to stop in the middle, use ctrl+C.    9) You can start the next round of annotation by selecting the merged_prediction directory as the mask dir in Annotation mode. Train and predict with Google Colab If you don't have a GPU machine, you can use Google Colab to perform GPU-based training and prediction for free.     1) Open train and predict notebook and click \"Open in Colab\" button 2) You can upload your own dataset to train and predict, or try it on demo data    Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the GNU GPL v3.0 license, \"napari-PHILOW\" is free and open source software Issues If you encounter any problems, please [file an issue] along with a detailed description. Authors  Shogo Suga  Hiroki Kawai  Yusuke Hirabayashi  How to Cite  Shogo Suga, Koki Nakamura, Bruno M Humbel, Hiroki Kawai, Yusuke Hirabayashi, An interactive deep learning-based approach reveals mitochondrial cristae topologies https://doi.org/10.1101/2021.06.11.448083 @article {Suga2021.06.11.448083,     author = {Suga, Shogo and Nakamura, Koki and Humbel, Bruno M and Kawai, Hiroki and Hirabayashi, Yusuke},     title = {An interactive deep learning-based approach reveals mitochondrial cristae topologies},     elocation-id = {2021.06.11.448083},     year = {2021},     doi = {10.1101/2021.06.11.448083},     publisher = {Cold Spring Harbor Laboratory},     URL = {https://www.biorxiv.org/content/early/2021/06/11/2021.06.11.448083},     eprint = {https://www.biorxiv.org/content/early/2021/06/11/2021.06.11.448083.full.pdf},     journal = {bioRxiv} }",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari-PHILOW",
    "documentation": "",
    "first_released": "2022-05-02T06:05:33.653275Z",
    "license": "GPL-3.0",
    "name": "napari-PHILOW",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/neurobiology-ut/PHILOW",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-05-02T06:05:33.653275Z",
    "report_issues": "",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "tensorflow",
      "numpy",
      "scikit-image",
      "dask-image",
      "opencv-python",
      "matplotlib",
      "napari-tools-menu",
      "pandas"
    ],
    "summary": "PHILOW is an interactive deep learning-based platform for 3D datasets",
    "support": "",
    "twitter": "",
    "version": "0.0.1",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      { "name": "Laura Zigutyte" },
      { "name": "Ryan Savill" },
      { "name": "Johannes Müller" },
      { "name": "Marcelo Zoccoler" },
      { "name": "Robert Haase" }
    ],
    "category": {
      "Supported data": ["2D", "3D"],
      "Workflow step": [
        "Clustering",
        "Visualization",
        "Object feature extraction"
      ]
    },
    "category_hierarchy": {
      "Supported data": [["2D"], ["3D"]],
      "Workflow step": [
        ["Clustering"],
        ["Clustering", "Centroid-based clustering"],
        ["Clustering", "Hierarchical clustering"],
        ["Visualization"],
        ["Visualization", "Plotting"],
        ["Object feature extraction"],
        ["Object feature extraction", "Shape features extraction"]
      ]
    },
    "citations": {
      "APA": "Zigutyte L., Savill R., Müller J., Zoccoler M., Haase R. (2021). napari-clusters-plotter (version 0.5.1). DOI: 10.5281/zenodo.5884657 URL: https://github.com/BiAPoL/napari-clusters-plotter ",
      "BibTex": "@misc{YourReferenceHere, author = {Zigutyte, Laura and Savill, Ryan and Müller, Johannes and Zoccoler, Marcelo and Haase, Robert}, doi = {10.5281/zenodo.5884657}, month = {11}, title = {napari-clusters-plotter}, url = {https://github.com/BiAPoL/napari-clusters-plotter}, year = {2021} } ",
      "RIS": "TY  - GEN AB  - A plugin to use with napari for clustering objects according to their properties. AU  - Zigutyte, Laura AU  - Savill, Ryan AU  - Müller, Johannes AU  - Zoccoler, Marcelo AU  - Haase, Robert DA  - 2021-11-15 DO  - 10.5281/zenodo.5884657 PY  - 2021 TI  - napari-clusters-plotter UR  - https://github.com/BiAPoL/napari-clusters-plotter ER ",
      "citation": "cff-version: 1.2.0 title: napari-clusters-plotter message: \"If you use this software, please cite it using the metadata from this file.\" abstract: \"A plugin to use with napari for clustering objects according to their properties.\" type: software authors:   - given-names: Laura     family-names: Zigutyte     email: zigutyte@gmail.com   - given-names: Ryan     family-names: Savill   - given-names: Johannes     family-names: Müller   - given-names: Marcelo     family-names: Zoccoler   - given-names: Robert     family-names: Haase     email: robert.haase@tu-dresden.de version: 0.5.1 date-released: 2021-11-15 identifiers:   - description: This is the collection of archived snapshots of all versions of napari-clusters-plotter     type: doi     value: \"10.5281/zenodo.5884657\"   - description: This is the archived snapshot of version 0.5.1 of napari-clusters-plotter     type: doi     value: \"10.5281/zenodo.6620442\"   - description: This is the archived snapshot of version 0.5.0 of napari-clusters-plotter     type: doi     value: \"10.5281/zenodo.6520216\"   - description: This is the archived snapshot of version 0.4.0 of napari-clusters-plotter     type: doi     value: \"10.5281/zenodo.6483425\"   - description: This is the archived snapshot of version 0.3.0 of napari-clusters-plotter     type: doi     value: \"10.5281/zenodo.6412049\"   - description: This is the archived snapshot of version 0.2.2 of napari-clusters-plotter     type: doi     value: \"10.5281/zenodo.5884658\" license: BSD-3-Clause repository-code: https://github.com/BiAPoL/napari-clusters-plotter "
    },
    "code_repository": "https://github.com/BiAPoL/napari-clusters-plotter",
    "description": "# napari-clusters-plotter\\r \\r [![License](https://img.shields.io/pypi/l/napari-clusters-plotter.svg?color=green)](https://github.com/lazigu/napari-clusters-plotter/raw/master/LICENSE)\\r [![PyPI](https://img.shields.io/pypi/v/napari-clusters-plotter.svg?color=green)](https://pypi.org/project/napari-clusters-plotter)\\r [![Python Version](https://img.shields.io/pypi/pyversions/napari-clusters-plotter.svg?color=green)](https://python.org)\\r [![Anaconda-Server Badge](https://anaconda.org/conda-forge/napari-clusters-plotter/badges/version.svg)](https://anaconda.org/conda-forge/napari-clusters-plotter)\\r [![tests](https://github.com/lazigu/napari-clusters-plotter/workflows/tests/badge.svg)](https://github.com/lazigu/napari-clusters-plotter/actions)\\r [![codecov](https://codecov.io/gh/BiAPoL/napari-clusters-plotter/branch/main/graph/badge.svg?token=R6W2KO1NJ8)](https://codecov.io/gh/BiAPoL/napari-clusters-plotter)\\r [![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&logoColor=white)](https://github.com/pre-commit/pre-commit)\\r [![Anaconda-Server Badge](https://anaconda.org/conda-forge/napari-clusters-plotter/badges/downloads.svg)](https://anaconda.org/conda-forge/napari-clusters-plotter)\\r [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-clusters-plotter)](https://www.napari-hub.org/plugins/napari-clusters-plotter)\\r [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.7011471.svg)](https://doi.org/10.5281/zenodo.7011471)\\r \\r A plugin to use with napari for clustering objects according to their properties.\\r \\r ----------------------------------\\r \\r This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\\r \\r <img src=\"https://github.com/BiAPoL/napari-clusters-plotter/raw/main/images/screencast.gif\" width=\"700\"/>\\r \\r Demonstration of handling 3D time-lapse data:\\r \\r <img src=\"https://github.com/BiAPoL/napari-clusters-plotter/raw/main/images/screencast2_timelapse.gif\" width=\"700\"/>\\r \\r ----------------------------------\\r \\r Jump to:\\r - [Usage](#usage)\\r   - [Starting point](#starting-point)\\r   - [Measurements](#measurements)\\r   - [Time-Lapse Measurements](#time-lapse-measurements)\\r   - [Plotting](#plotting)\\r   - [Time-Lapse Plotting](#time-lapse-plotting)\\r   - [Dimensionality reduction: UMAP, t-SNE or PCA](#dimensionality-reduction-umap-t-sne-or-pca)\\r   - [Clustering](#clustering)\\r   - [Plotting clustering results](#plotting-clustering-results)\\r - [Installation](#installation)\\r - [Troubleshooting installation](#troubleshooting-installation)\\r - [Contributing](#contributing)\\r - [License](#license)\\r - [Acknowledgements](#acknowledgements)\\r \\r \\r ## Usage\\r \\r ### Starting point\\r For clustering objects according to their properties, the starting point is a [grey-value image](example_data/blobs.tif) and a label image\\r representing a segmentation of objects. For segmenting objects, you can for example use the\\r [Voronoi-Otsu-labelling approach](https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes#voronoi-otsu-labelling)\\r in the napari plugin [napari-segment-blobs-and-things-with-membranes](https://www.napari-hub.org/plugins/napari-segment-blobs-and-things-with-membranes).\\r \\r ![](https://github.com/BiAPoL/napari-clusters-plotter/raw/main/images/starting_point.png)\\r \\r ### Measurements\\r The first step is deriving measurements from the labelled image and the corresponding pixels in the grey-value image.\\r Since the 0.6.0 release measurements widget is no longer part of this plugin and you will have to use other napari plugins to measure your data.\\r One way is to use the measurement functions in [napari-skimage-regionprops](https://www.napari-hub.org/plugins/napari-skimage-regionprops), which comes pre-installed with the napari cluster plotter.\\r Use the menu `Tools > Measurement > Regionprops (scikit-image, nsr)` to get to the measurement widget.\\r Just select the image, the corresponding label image and the measurements to analyse and click on `Run`.\\r \\r In the previous napari-cluster-plotter release a GPU dependant measurement function was implemented which you can find in the [napari-pyclesperanto-assistant](https://www.napari-hub.org/plugins/napari-pyclesperanto-assistant).\\r To use this function you will need to install this library (see optional installation steps) and you can find the widget under the menu `Tools > Measurement > Label statistics (clEsperanto)`. As before, select the image, the corresponding label image and the measurements to analyse and click on `Run`.\\r \\r A table with the measurements will open and afterwards, you can save and/or close the measurement table. Also, close the Measure widget.\\r \\r If you want to upload your own measurements you can do this using [napari-skimage-regionprops](https://www.napari-hub.org/plugins/napari-skimage-regionprops).\\r Under the menu `Tools > Measurement > Load from CSV (nsr)` you can find a widget to upload your own csv file.\\r Make sure that there is a column that specifies the which measurement belongs to which label by adding a column with the name \"label\".\\r If you don't specify this column it will be assumed that measurements start at 1 and each\\r column describes the next label.\\r \\r Note that tables for time-lapse data need to include an **additional column named \"frame\"**, which indicates which slice in\\r time the given row refers to.\\r \\r **For the correct visualisation of clusters IDs in space**, it is **important** that label images/time-points of the time-lapse\\r are either **labelled sequentially** or missing labels still exist in the loaded csv file (i.e., missing label exists in the\\r \"label\" column with `NaN` values for other measurements in the same row). If you perform measurements using before mentioned\\r plugins, the obtained dataframe is already in the correct form.\\r \\r #### Time-Lapse Measurements\\r In case you have 2D time-lapse data you need to convert it into a suitable shape using the function: `Tools > Utilities > Convert 3D stack to 2D time-lapse (time-slicer)`,\\r which can be found in the [napari time slicer](https://www.napari-hub.org/plugins/napari-time-slicer).\\r \\r Note that tables for time-lapse data will include an additional column named \"frame\", which indicates which slice in\\r time the given row refers to. If you want to import your own csv files for time-lapse data make sure to include this column!\\r If you have tracking data where each column specifies measurements for a track instead of a label at a specific time point,\\r this column must not be added.\\r \\r Both [napari-skimage-regionprops](https://www.napari-hub.org/plugins/napari-skimage-regionprops) and [napari-pyclesperanto-assistant](https://www.napari-hub.org/plugins/napari-pyclesperanto-assistant) include measuring widgets for timelapse data.\\r \\r ### Plotting\\r \\r Once measurements were made or uploaded, these measurements were saved in the `properties/features` of the labels layer which was\\r analysed. You can then plot these measurements using the menu `Tools > Measurement > Plot measurement (ncp)`.\\r \\r In this widget, you can select the labels layer which was analysed and the measurements which should be plotted\\r on the X- and Y-axis. If you cannot see any options in axes selection boxes, but you have performed measurements, click\\r on `Update Axes/Clustering Selection Boxes` to refresh them. Click on `Run` to draw the data points in the plot area.\\r \\r ![](https://github.com/BiAPoL/napari-clusters-plotter/raw/main/images/plot_plain.png)\\r \\r You can also manually select a region in the plot. To use lasso (freehand) tool use left mouse click, and to use a\\r rectangle - right click. The resulting manual clustering will also be visualized in the original image. To optimize\\r visualization in the image, turn off the visibility of the analysed labels layer.\\r \\r ![](https://github.com/BiAPoL/napari-clusters-plotter/raw/main/images/plot_interactive.png)\\r \\r Hold down the SHIFT key while annotating regions in the plot to manually select multiple clusters.\\r \\r ![](https://github.com/BiAPoL/napari-clusters-plotter/raw/main/images/multi-select-manual-clustering.gif)\\r \\r #### Time-Lapse Plotting\\r When you plot your time-lapse datasets you will notice that the plots look slightly different.\\r Datapoints of the current time frame are highlighted in white and you can see the datapoints move through the plot if you press play:\\r \\r ![](https://github.com/BiAPoL/napari-clusters-plotter/raw/main/images/plotting_time-lapse_data_as_movie.gif)\\r \\r You can also manually select groups using the lasso tool and plot a measurement per frame and see how the group behaves in time.\\r Furthermore, you could also select a group in time and see where the datapoints lie in a different feature space:\\r \\r ![](https://github.com/BiAPoL/napari-clusters-plotter/raw/main/images/timelapse_manual_clustering_tips.gif)\\r \\r ### Dimensionality reduction: UMAP, t-SNE or PCA\\r \\r For getting more insights into your data, you can reduce the dimensionality of the measurements, e.g.\\r using the [UMAP algorithm](https://umap-learn.readthedocs.io/en/latest/), [t-SNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)\\r or [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html).\\r To apply it to your data use the menu `Tools > Measurement > Dimensionality reduction (ncp)`.\\r Select the label image that was analysed and in the list below, select all measurements that should be\\r dimensionality reduced. By default, all measurements are selected in the box. If you cannot see any measurements, but\\r you have performed them, click on `Update Measurements` to refresh the box. You can read more about parameters of both\\r algorithms by hovering over question marks or by clicking on them. When you are done with the selection, click on `Run`\\r and after a moment, the table of measurements will re-appear with two additional columns representing the reduced\\r dimensions of the dataset. These columns are automatically saved in the `properties` of the labels layer so there is no\\r need to save them for usage in other widgets unless you wish to do so.\\r \\r ![](https://github.com/BiAPoL/napari-clusters-plotter/raw/main/images/umap.png)\\r \\r Afterwards, you can again save and/or close the table. Also, close the Dimensionality Reduction widget.\\r \\r ### Clustering\\r If manual clustering, as shown above, is not an option, you can automatically cluster your data, using these implemented algorithms:\\r * [k-means clustering (KMEANS)](https://towardsdatascience.com/k-means-clustering-algorithm-applications-evaluation-methods-and-drawbacks-aa03e644b48a)\\r * [Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN)](https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html)\\r * [Gaussian Mixture Model (GMM)](https://scikit-learn.org/stable/modules/mixture.html)\\r * [Mean Shift (MS)](https://scikit-learn.org/stable/auto_examples/cluster/plot_mean_shift.html#sphx-glr-auto-examples-cluster-plot-mean-shift-py)\\r * [Agglomerative clustering (AC)](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html)\\r \\r Therefore, click the menu `Tools > Measurement > Clustering (ncp)`,\\r again, select the analysed labels layer.\\r This time select the measurements for clustering, e.g. select _only_ the `UMAP` measurements.\\r Select the clustering method `KMeans` and click on `Run`.\\r The table of measurements will reappear with an additional column `ALGORITHM_NAME_CLUSTERING_ID` containing the cluster\\r ID of each datapoint.\\r \\r ![](https://github.com/BiAPoL/napari-clusters-plotter/raw/main/images/clustering.png)\\r \\r Afterwards, you can again save and/or close the table. Also, close the clustering widget.\\r \\r ### Plotting clustering results\\r Return to the Plotter widget using the menu `Tools > Measurement > Plot measurement (ncp)`.\\r Select `UMAP_0` and `UMAP_1` as X- and Y-axis and the `ALGORITHM_NAME_CLUSTERING_ID` as `Clustering`, and click on `Run`.\\r \\r ![](https://github.com/BiAPoL/napari-clusters-plotter/raw/main/images/hdbscan_clusters_plot.png)\\r \\r Example of k-means clustering results:\\r \\r ![](https://github.com/BiAPoL/napari-clusters-plotter/raw/main/images/kmeans_clusters_plot.png)\\r \\r ## Installation\\r ### Devbio-napari installation\\r The easiest way to install this plugin is to install the [devbio-napari](https://github.com/haesleinhuepf/devbio-napari) library.\\r This library installs napari alongside many other useful plugins, including the napari-clusters-plotter.\\r We recommend this library as it is not only the easiest way to install the napari-cluster-plotter, but it includes plugins for segmentation and measurement, which we don't provide.\\r There are detailed installation instructions on the [napari-hub-page](https://www.napari-hub.org/plugins/devbio-napari) if you have any problems installing it.\\r In case you want to have a minimal installation of our plugin you can find other installation options below.\\r \\r ### Minimal installation\\r * Get a python environment, e.g. via [mini-conda](https://docs.conda.io/en/latest/miniconda.html).\\r   If you never used python/conda environments before, please follow the instructions\\r   [here](https://mpicbg-scicomp.github.io/ipf_howtoguides/guides/Python_Conda_Environments) first. It is recommended to\\r   install python 3.9 to your new conda environment from the start. The plugin is not yet supported with Python 3.10.\\r   Create a new environment, for example, like this:\\r \\r ```\\r conda create --name ncp-env python=3.9\\r ```\\r \\r * Activate the new environment via conda:\\r \\r ```\\r conda activate ncp-env\\r ```\\r \\r * Install [napari], e.g. via [pip]:\\r \\r ```\\r python -m pip install \"napari[all]\"\\r ```\\r \\r Afterwards, you can install `napari-clusters-plotter` via [pip]:\\r \\r ```\\r pip install napari-clusters-plotter\\r ```\\r \\r ### Optional installation\\r Follow these steps instead of the regular installation to include the [napari-pyclesperanto-assistant](https://www.napari-hub.org/plugins/napari-pyclesperanto-assistant).\\r Creating the environment like this will allow you to use your GPU to render your cluster results.\\r Furthermore, you can access the deprecated measurement functions of the napari-cluster-plotter in releases < 0.6.0.\\r If you have trouble with this library you can use the regular installation above.\\r \\r ```\\r conda create --name ncp-env python==3.9\\r ```\\r \\r \\r * Activate the new environment via conda:\\r \\r ```\\r conda activate ncp-env\\r ```\\r \\r * Install napari-pyclesperanto-assistant, e.g. with pip:\\r \\r ´´´\\r pip install napari-pyclesperanto-assistant\\r ´´´\\r \\r * Mac-users please also install this:\\r \\r ´´´\\r conda install -c conda-forge ocl_icd_wrapper_apple\\r ´´´\\r \\r * Linux users please also install this:\\r \\r ´´´\\r conda install -c conda-forge ocl-icd-system\\r ´´´\\r \\r * Install [napari], e.g. via [pip]:\\r \\r ```\\r python -m pip install \"napari[all]\"\\r ```\\r \\r Afterwards, you can install `napari-clusters-plotter` via [pip]:\\r \\r ```\\r pip install napari-clusters-plotter\\r ```\\r \\r ## Troubleshooting installation\\r \\r - If the plugin does not appear in napari 'Plugins' menu, and in 'Plugin errors...' you can see such an error:\\r \\r ```\\r ImportError: DLL load failed while importing _cl\\r ```\\r \\r Try downloading and installing a pyopencl with a lower cl version, e.g. cl12 : pyopencl=2020.1. However, in this case,\\r you will need an environment with a lower python version (python=3.8).\\r \\r - `Error: Could not build wheels for hdbscan which use PEP 517 and cannot be installed directly`\\r \\r Install hdbscan via conda before installing the plugin:\\r \\r ```\\r conda install -c conda-forge hdbscan\\r ```\\r \\r - `ValueError: numpy.ndarray size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject`\\r \\r Similar to the above-described error, this error can occur when importing hdbscan through pip or in the wrong order. This can be fixed by installing packages separately through conda and in the following order:\\r ```bash\\r conda install -c conda-forge napari pyopencl hdbscan\\r pip install napari-clusters-plotter\\r ```\\r \\r - `WARNING: No ICDs were found` or `LogicError: clGetPlatformIDs failed: PLATFORM_NOT_FOUND_KHR`\\r \\r Make your system-wide implementation visible by installing either of the following conda packages:\\r \\r ```\\r conda install -c conda-forge ocl-icd-system\\r conda install -c conda-forge ocl_icd_wrapper_apple\\r ```\\r \\r ## Contributing\\r \\r Contributions are very welcome. Tests can be run with [pytest], please ensure\\r the coverage at least stays the same before you submit a pull request.\\r \\r ## License\\r \\r Distributed under the terms of the [BSD-3] license,\\r \"napari-clusters-plotter\" is free and open source software\\r \\r ## Acknowledgements\\r This project was supported by the Deutsche Forschungsgemeinschaft under Germany’s Excellence Strategy – EXC2068 - Cluster of Excellence \"Physics of Life\" of TU Dresden.\\r This project has been made possible in part by grant number [2021-240341 (Napari plugin accelerator grant)](https://chanzuckerberg.com/science/programs-resources/imaging/napari/improving-image-processing/) from the Chan Zuckerberg Initiative DAF, an advised fund of the Silicon Valley Community Foundation.\\r \\r ## Issues\\r \\r If you encounter any problems, please [file an issue](https://github.com/BiAPoL/napari-clusters-plotter/issues) along\\r with a detailed description.\\r \\r [napari]: https://github.com/napari/napari\\r [Cookiecutter]: https://github.com/audreyr/cookiecutter\\r [@napari]: https://github.com/napari\\r [MIT]: http://opensource.org/licenses/MIT\\r [BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\r [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\r [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\r [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\r [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\r [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\r \\r [napari]: https://github.com/napari/napari\\r [pytest]: https://docs.pytest.org/en/7.0.x/\\r [pip]: https://pypi.org/project/pip/\\r [PyPI]: https://pypi.org/\\r ",
    "description_content_type": "text/markdown",
    "description_text": "napari-clusters-plotter           A plugin to use with napari for clustering objects according to their properties.  This napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.  Demonstration of handling 3D time-lapse data:   Jump to: - Usage   - Starting point   - Measurements   - Time-Lapse Measurements   - Plotting   - Time-Lapse Plotting   - Dimensionality reduction: UMAP, t-SNE or PCA   - Clustering   - Plotting clustering results - Installation - Troubleshooting installation - Contributing - License - Acknowledgements Usage Starting point For clustering objects according to their properties, the starting point is a grey-value image and a label image representing a segmentation of objects. For segmenting objects, you can for example use the Voronoi-Otsu-labelling approach in the napari plugin napari-segment-blobs-and-things-with-membranes.  Measurements The first step is deriving measurements from the labelled image and the corresponding pixels in the grey-value image. Since the 0.6.0 release measurements widget is no longer part of this plugin and you will have to use other napari plugins to measure your data. One way is to use the measurement functions in napari-skimage-regionprops, which comes pre-installed with the napari cluster plotter. Use the menu Tools > Measurement > Regionprops (scikit-image, nsr) to get to the measurement widget. Just select the image, the corresponding label image and the measurements to analyse and click on Run. In the previous napari-cluster-plotter release a GPU dependant measurement function was implemented which you can find in the napari-pyclesperanto-assistant. To use this function you will need to install this library (see optional installation steps) and you can find the widget under the menu Tools > Measurement > Label statistics (clEsperanto). As before, select the image, the corresponding label image and the measurements to analyse and click on Run. A table with the measurements will open and afterwards, you can save and/or close the measurement table. Also, close the Measure widget. If you want to upload your own measurements you can do this using napari-skimage-regionprops. Under the menu Tools > Measurement > Load from CSV (nsr) you can find a widget to upload your own csv file. Make sure that there is a column that specifies the which measurement belongs to which label by adding a column with the name \"label\". If you don't specify this column it will be assumed that measurements start at 1 and each column describes the next label. Note that tables for time-lapse data need to include an additional column named \"frame\", which indicates which slice in time the given row refers to. For the correct visualisation of clusters IDs in space, it is important that label images/time-points of the time-lapse are either labelled sequentially or missing labels still exist in the loaded csv file (i.e., missing label exists in the \"label\" column with NaN values for other measurements in the same row). If you perform measurements using before mentioned plugins, the obtained dataframe is already in the correct form. Time-Lapse Measurements In case you have 2D time-lapse data you need to convert it into a suitable shape using the function: Tools > Utilities > Convert 3D stack to 2D time-lapse (time-slicer), which can be found in the napari time slicer. Note that tables for time-lapse data will include an additional column named \"frame\", which indicates which slice in time the given row refers to. If you want to import your own csv files for time-lapse data make sure to include this column! If you have tracking data where each column specifies measurements for a track instead of a label at a specific time point, this column must not be added. Both napari-skimage-regionprops and napari-pyclesperanto-assistant include measuring widgets for timelapse data. Plotting Once measurements were made or uploaded, these measurements were saved in the properties/features of the labels layer which was analysed. You can then plot these measurements using the menu Tools > Measurement > Plot measurement (ncp). In this widget, you can select the labels layer which was analysed and the measurements which should be plotted on the X- and Y-axis. If you cannot see any options in axes selection boxes, but you have performed measurements, click on Update Axes/Clustering Selection Boxes to refresh them. Click on Run to draw the data points in the plot area.  You can also manually select a region in the plot. To use lasso (freehand) tool use left mouse click, and to use a rectangle - right click. The resulting manual clustering will also be visualized in the original image. To optimize visualization in the image, turn off the visibility of the analysed labels layer.  Hold down the SHIFT key while annotating regions in the plot to manually select multiple clusters.  Time-Lapse Plotting When you plot your time-lapse datasets you will notice that the plots look slightly different. Datapoints of the current time frame are highlighted in white and you can see the datapoints move through the plot if you press play:  You can also manually select groups using the lasso tool and plot a measurement per frame and see how the group behaves in time. Furthermore, you could also select a group in time and see where the datapoints lie in a different feature space:  Dimensionality reduction: UMAP, t-SNE or PCA For getting more insights into your data, you can reduce the dimensionality of the measurements, e.g. using the UMAP algorithm, t-SNE or PCA. To apply it to your data use the menu Tools > Measurement > Dimensionality reduction (ncp). Select the label image that was analysed and in the list below, select all measurements that should be dimensionality reduced. By default, all measurements are selected in the box. If you cannot see any measurements, but you have performed them, click on Update Measurements to refresh the box. You can read more about parameters of both algorithms by hovering over question marks or by clicking on them. When you are done with the selection, click on Run and after a moment, the table of measurements will re-appear with two additional columns representing the reduced dimensions of the dataset. These columns are automatically saved in the properties of the labels layer so there is no need to save them for usage in other widgets unless you wish to do so.  Afterwards, you can again save and/or close the table. Also, close the Dimensionality Reduction widget. Clustering If manual clustering, as shown above, is not an option, you can automatically cluster your data, using these implemented algorithms: * k-means clustering (KMEANS) * Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN) * Gaussian Mixture Model (GMM) * Mean Shift (MS) * Agglomerative clustering (AC) Therefore, click the menu Tools > Measurement > Clustering (ncp), again, select the analysed labels layer. This time select the measurements for clustering, e.g. select only the UMAP measurements. Select the clustering method KMeans and click on Run. The table of measurements will reappear with an additional column ALGORITHM_NAME_CLUSTERING_ID containing the cluster ID of each datapoint.  Afterwards, you can again save and/or close the table. Also, close the clustering widget. Plotting clustering results Return to the Plotter widget using the menu Tools > Measurement > Plot measurement (ncp). Select UMAP_0 and UMAP_1 as X- and Y-axis and the ALGORITHM_NAME_CLUSTERING_ID as Clustering, and click on Run.  Example of k-means clustering results:  Installation Devbio-napari installation The easiest way to install this plugin is to install the devbio-napari library. This library installs napari alongside many other useful plugins, including the napari-clusters-plotter. We recommend this library as it is not only the easiest way to install the napari-cluster-plotter, but it includes plugins for segmentation and measurement, which we don't provide. There are detailed installation instructions on the napari-hub-page if you have any problems installing it. In case you want to have a minimal installation of our plugin you can find other installation options below. Minimal installation  Get a python environment, e.g. via mini-conda.   If you never used python/conda environments before, please follow the instructions   here first. It is recommended to   install python 3.9 to your new conda environment from the start. The plugin is not yet supported with Python 3.10.   Create a new environment, for example, like this:  conda create --name ncp-env python=3.9  Activate the new environment via conda:  conda activate ncp-env  Install napari, e.g. via pip:  python -m pip install \"napari[all]\" Afterwards, you can install napari-clusters-plotter via pip: pip install napari-clusters-plotter Optional installation Follow these steps instead of the regular installation to include the napari-pyclesperanto-assistant. Creating the environment like this will allow you to use your GPU to render your cluster results. Furthermore, you can access the deprecated measurement functions of the napari-cluster-plotter in releases < 0.6.0. If you have trouble with this library you can use the regular installation above. conda create --name ncp-env python==3.9  Activate the new environment via conda:  conda activate ncp-env  Install napari-pyclesperanto-assistant, e.g. with pip:  ´´´ pip install napari-pyclesperanto-assistant ´´´  Mac-users please also install this:  ´´´ conda install -c conda-forge ocl_icd_wrapper_apple ´´´  Linux users please also install this:  ´´´ conda install -c conda-forge ocl-icd-system ´´´  Install napari, e.g. via pip:  python -m pip install \"napari[all]\" Afterwards, you can install napari-clusters-plotter via pip: pip install napari-clusters-plotter Troubleshooting installation  If the plugin does not appear in napari 'Plugins' menu, and in 'Plugin errors...' you can see such an error:  ImportError: DLL load failed while importing _cl Try downloading and installing a pyopencl with a lower cl version, e.g. cl12 : pyopencl=2020.1. However, in this case, you will need an environment with a lower python version (python=3.8).  Error: Could not build wheels for hdbscan which use PEP 517 and cannot be installed directly  Install hdbscan via conda before installing the plugin: conda install -c conda-forge hdbscan  ValueError: numpy.ndarray size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject  Similar to the above-described error, this error can occur when importing hdbscan through pip or in the wrong order. This can be fixed by installing packages separately through conda and in the following order: bash conda install -c conda-forge napari pyopencl hdbscan pip install napari-clusters-plotter  WARNING: No ICDs were found or LogicError: clGetPlatformIDs failed: PLATFORM_NOT_FOUND_KHR  Make your system-wide implementation visible by installing either of the following conda packages: conda install -c conda-forge ocl-icd-system conda install -c conda-forge ocl_icd_wrapper_apple Contributing Contributions are very welcome. Tests can be run with pytest, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-clusters-plotter\" is free and open source software Acknowledgements This project was supported by the Deutsche Forschungsgemeinschaft under Germany’s Excellence Strategy – EXC2068 - Cluster of Excellence \"Physics of Life\" of TU Dresden. This project has been made possible in part by grant number 2021-240341 (Napari plugin accelerator grant) from the Chan Zuckerberg Initiative DAF, an advised fund of the Silicon Valley Community Foundation. Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari-clusters-plotter",
    "documentation": "https://github.com/BiAPoL/napari-clusters-plotter",
    "first_released": "2021-11-15T13:51:45.035968Z",
    "license": "BSD-3-Clause",
    "name": "napari-clusters-plotter",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/BiAPoL/napari-clusters-plotter",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-12-13T19:14:18.849635Z",
    "report_issues": "https://github.com/BiAPoL/napari-clusters-plotter/issues",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy (>=1.21)",
      "scikit-learn",
      "matplotlib",
      "pandas",
      "umap-learn",
      "napari-tools-menu",
      "napari-skimage-regionprops (>=0.3.1)",
      "hdbscan",
      "joblib (==1.1.0)"
    ],
    "summary": "A plugin to use with napari for clustering objects according to their properties",
    "support": "https://github.com/BiAPoL/napari-clusters-plotter/issues",
    "twitter": "",
    "version": "0.6.0",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Allen Institute for Cell Science" }],
    "code_repository": "https://github.com/aics-int/napari-allencell-annotator",
    "conda": [
      { "channel": "conda-forge", "package": "napari-allencell-annotator" }
    ],
    "description": "# napari-allencell-annotator  [![License BSD-3](https://img.shields.io/pypi/l/napari-allencell-annotator.svg?color=green)](https://github.com/bbridge0200/napari-allencell-annotator/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-allencell-annotator.svg?color=green)](https://pypi.org/project/napari-allencell-annotator) [![Python Version](https://img.shields.io/pypi/pyversions/napari-allencell-annotator.svg?color=green)](https://python.org) [![tests](https://github.com/bbridge0200/napari-allencell-annotator/workflows/tests/badge.svg)](https://github.com/bbridge0200/napari-allencell-annotator/actions) [![codecov](https://codecov.io/gh/bbridge0200/napari-allencell-annotator/branch/main/graph/badge.svg)](https://codecov.io/gh/bbridge0200/napari-allencell-annotator) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-allencell-annotator)](https://napari-hub.org/plugins/napari-allencell-annotator)  A plugin that enables image annotation/scoring and writes annotations to a .csv file.  Plugin provided by the Allen Institute for Cell Science.  The Allen Cell Image Annotator plugin for napari provides an intuitive graphical user interface to create annotation templates, annotate large  image sets using these templates, and save image annotations to a csv file.  The Allen Cell Image Annotator is a Python-based open source toolkit  developed at the Allen Institute for Cell Science for both blind, unbiased and un-blind  microscope image annotating. This toolkit supports easy image set selection from a file finder and creation of annotation templates (text, checkbox, drop-down, and spinbox). With napari's multi-dimensional image viewing capabilities, the plugin seamlessly allows users to view each image and write annotations into the custom template. Annotation templates can be written to a json file for sharing or re-using. After annotating, the annotation template, image file list, and the annotation values  are conveniently saved to csv file, which can be re-opened for further annotating.   -   Supports the following image types:     - `OME-TIFF`     - `TIFF`     - `CZI`      - `PNG`      -   `JPEG`    ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to files up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/plugins/index.html -->  ## Installation ### 1. Prerequisites  The plugin requires [Conda](https://docs.anaconda.com/anaconda/install/). - [Installing on Windows ](https://docs.anaconda.com/anaconda/install/windows/)    - Follow the steps linked above except   - On step 8, check top the box to add to PATH   - ![Alt text](napari_allencell_annotator/assets/windowsstep8.png) - [Installing on Mac ](https://docs.anaconda.com/anaconda/install/mac-os/)   ### 2. Install the plugin Click the link corresponding to your OS. #### [Windows](https://alleninstitute-my.sharepoint.com/:u:/g/personal/beatrice_bridge_alleninstitute_org/EVOKZ8-PZB5AvO6z6OAjZ_YB2EHbaU9XRc_Z281oM0ctOg?e=skbKzh) - From the link above, click the three dots on the top menu bar and select download.  - Open a file explorer and go to the Downloads folder. Use **Option 1** below. A prompt window should open and start installing. If this fails use **Option 2**.    - **Option 1**: Double-click the file _install_napari.sh_   - **Option 2**: Search the file finder for Anaconda Prompt. Open version 3. Run the following commands one line at a time.      - conda create -n napari_annotator python=3.9 anaconda     - conda activate napari_annotator     - python -m pip install --upgrade pip     - python -m pip install \"napari[all]\"     - python -m pip install napari-allencell-annotator     - napari   - **Still not working?** Try using conda forge instead of pip.      - Ex: conda install -c conda-forge napari instead of python -m pip install \"napari[all]\" #### [MacOS/Unix](https://alleninstitute-my.sharepoint.com/:u:/g/personal/beatrice_bridge_alleninstitute_org/EaeV_RPXZz9DijxYy7qfoeMB3Hbq4vMpmJERqDyhL97KAg?e=HuKY2k) - From the link above, download the file.  - Open terminal.  - Run _chmod +x ./Downloads/install_napari.command_    - If you get a file not found error try adjusting the path to match where install_napari.command was downloaded. - Open finder, navigate to the file, double-click _install_napari.command_ .    - A terminal window should open and start installing.      ### 3. Launch the Plugin  Once the napari window opens, go to **Plugins**. - If **napari-allencell-annotator: Annotator** is listed click it to launch.  - If it is not listed  - **Install/Uninstall Plugins** ⇨ check the box next to **napari-allencell-annotator** ⇨ **close** ⇨ **Plugins** ⇨ **napari-allencell-annotator: Annotator** .  ### 4. Re-opening the Plugin After Installing - Windows   - Search for anaconda navigator in file finder   - Click on navigator version 3   - Once the navigator opens, click **Environments** on the left side   - Click on the annotator environment and wait for it to load   - Press the play button   - Type _napari_ in the prompt that opens   - Click **Plugins** ⇨ **napari-allencell-annotator: Annotator** - MacOS   - Open terminal   - Run these commands one line at a time     - conda activate napari_annotator     - napari   - Click **Plugins** ⇨ **napari-allencell-annotator: Annotator**  ## Quick Start  1. Open napari 2. Start the plugin     - Open napari, go to \"Plugins\" ⇨ \"napari-allencell-annotator\". 3. Create or import annotations and add images to annotate.  For more detailed usage instructions, check out this [document](napari_allencell_annotator/assets/AnnotatorInstructions.pdf)  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-allencell-annotator\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/bbridge0200/napari-allencell-annotator/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-allencell-annotator       A plugin that enables image annotation/scoring and writes annotations to a .csv file.  Plugin provided by the Allen Institute for Cell Science. The Allen Cell Image Annotator plugin for napari provides an intuitive graphical user interface to create annotation templates, annotate large  image sets using these templates, and save image annotations to a csv file.  The Allen Cell Image Annotator is a Python-based open source toolkit  developed at the Allen Institute for Cell Science for both blind, unbiased and un-blind  microscope image annotating. This toolkit supports easy image set selection from a file finder and creation of annotation templates (text, checkbox, drop-down, and spinbox). With napari's multi-dimensional image viewing capabilities, the plugin seamlessly allows users to view each image and write annotations into the custom template. Annotation templates can be written to a json file for sharing or re-using. After annotating, the annotation template, image file list, and the annotation values  are conveniently saved to csv file, which can be re-opened for further annotating.   Supports the following image types: OME-TIFF TIFF CZI  PNG  JPEG      This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation 1. Prerequisites The plugin requires Conda. - Installing on Windows     - Follow the steps linked above except   - On step 8, check top the box to add to PATH   -  - Installing on Mac   2. Install the plugin Click the link corresponding to your OS. Windows  From the link above, click the three dots on the top menu bar and select download.  Open a file explorer and go to the Downloads folder. Use Option 1 below. A prompt window should open and start installing. If this fails use Option 2.  Option 1: Double-click the file install_napari.sh Option 2: Search the file finder for Anaconda Prompt. Open version 3. Run the following commands one line at a time.  conda create -n napari_annotator python=3.9 anaconda conda activate napari_annotator python -m pip install --upgrade pip python -m pip install \"napari[all]\" python -m pip install napari-allencell-annotator napari   Still not working? Try using conda forge instead of pip.  Ex: conda install -c conda-forge napari instead of python -m pip install \"napari[all]\"    MacOS/Unix  From the link above, download the file.  Open terminal.  Run chmod +x ./Downloads/install_napari.command  If you get a file not found error try adjusting the path to match where install_napari.command was downloaded. Open finder, navigate to the file, double-click install_napari.command .  A terminal window should open and start installing.   3. Launch the Plugin Once the napari window opens, go to Plugins. - If napari-allencell-annotator: Annotator is listed click it to launch.  - If it is not listed  - Install/Uninstall Plugins ⇨ check the box next to napari-allencell-annotator ⇨ close ⇨ Plugins ⇨ napari-allencell-annotator: Annotator . 4. Re-opening the Plugin After Installing  Windows Search for anaconda navigator in file finder Click on navigator version 3 Once the navigator opens, click Environments on the left side Click on the annotator environment and wait for it to load Press the play button Type napari in the prompt that opens Click Plugins ⇨ napari-allencell-annotator: Annotator MacOS Open terminal Run these commands one line at a time conda activate napari_annotator napari   Click Plugins ⇨ napari-allencell-annotator: Annotator  Quick Start  Open napari Start the plugin  Open napari, go to \"Plugins\" ⇨ \"napari-allencell-annotator\". Create or import annotations and add images to annotate.  For more detailed usage instructions, check out this document  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-allencell-annotator\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": [],
    "display_name": "napari-allencell-annotator",
    "documentation": "",
    "first_released": "2022-07-27T20:13:22.558984Z",
    "license": "",
    "name": "napari-allencell-annotator",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/aics-int/napari-allencell-annotator/",
    "python_version": ">=3.9",
    "reader_file_extensions": [],
    "release_date": "2022-09-16T23:36:53.473442Z",
    "report_issues": "",
    "requirements": [
      "napari (>=0.4.9)",
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "aicsimageio (>=4.9.1)",
      "xarray (>=2022.6.0)",
      "magicgui (>=0.3.7)",
      "aicspylibczi (>=3.0.5)",
      "fsspec (>=2022.8.2)",
      "bioformats-jar",
      "bfio",
      "qtpy",
      "napari (>=0.4.9) ; extra == 'all'",
      "napari-plugin-engine (>=0.1.4) ; extra == 'all'",
      "numpy ; extra == 'all'",
      "aicsimageio (>=4.9.1) ; extra == 'all'",
      "xarray (>=2022.6.0) ; extra == 'all'",
      "magicgui (>=0.3.7) ; extra == 'all'",
      "aicspylibczi (>=3.0.5) ; extra == 'all'",
      "fsspec (>=2022.8.2) ; extra == 'all'",
      "bioformats-jar ; extra == 'all'",
      "bfio ; extra == 'all'",
      "qtpy ; extra == 'all'",
      "black (>=19.10b0) ; extra == 'all'",
      "codecov (>=2.0.22) ; extra == 'all'",
      "docutils (<0.16,>=0.10) ; extra == 'all'",
      "flake8 (>=3.7.7) ; extra == 'all'",
      "psutil (>=5.7.0) ; extra == 'all'",
      "pytest (>=4.3.0) ; extra == 'all'",
      "pytest-cov (==2.6.1) ; extra == 'all'",
      "pytest-raises (>=0.10) ; extra == 'all'",
      "pytest-qt (>=3.3.0) ; extra == 'all'",
      "quilt3 (>=3.1.12) ; extra == 'all'",
      "pytest-runner ; extra == 'all'",
      "bumpversion (>=0.5.3) ; extra == 'all'",
      "gitchangelog (>=3.0.4) ; extra == 'all'",
      "ipython (>=7.5.0) ; extra == 'all'",
      "m2r (>=0.2.1) ; extra == 'all'",
      "pytest-runner (>=4.4) ; extra == 'all'",
      "Sphinx (<3,>=2.0.0b1) ; extra == 'all'",
      "sphinx-rtd-theme (>=0.1.2) ; extra == 'all'",
      "tox (>=3.5.2) ; extra == 'all'",
      "twine (>=1.13.0) ; extra == 'all'",
      "wheel (>=0.33.1) ; extra == 'all'",
      "black (>=19.10b0) ; extra == 'dev'",
      "bumpversion (>=0.5.3) ; extra == 'dev'",
      "docutils (<0.16,>=0.10) ; extra == 'dev'",
      "flake8 (>=3.7.7) ; extra == 'dev'",
      "gitchangelog (>=3.0.4) ; extra == 'dev'",
      "ipython (>=7.5.0) ; extra == 'dev'",
      "m2r (>=0.2.1) ; extra == 'dev'",
      "pytest (>=4.3.0) ; extra == 'dev'",
      "pytest-cov (==2.6.1) ; extra == 'dev'",
      "pytest-raises (>=0.10) ; extra == 'dev'",
      "pytest-runner (>=4.4) ; extra == 'dev'",
      "pytest-qt (>=3.3.0) ; extra == 'dev'",
      "quilt3 (>=3.1.12) ; extra == 'dev'",
      "Sphinx (<3,>=2.0.0b1) ; extra == 'dev'",
      "sphinx-rtd-theme (>=0.1.2) ; extra == 'dev'",
      "tox (>=3.5.2) ; extra == 'dev'",
      "twine (>=1.13.0) ; extra == 'dev'",
      "wheel (>=0.33.1) ; extra == 'dev'",
      "pytest-runner ; extra == 'setup'",
      "black (>=19.10b0) ; extra == 'test'",
      "codecov (>=2.0.22) ; extra == 'test'",
      "docutils (<0.16,>=0.10) ; extra == 'test'",
      "flake8 (>=3.7.7) ; extra == 'test'",
      "psutil (>=5.7.0) ; extra == 'test'",
      "pytest (>=4.3.0) ; extra == 'test'",
      "pytest-cov (==2.6.1) ; extra == 'test'",
      "pytest-raises (>=0.10) ; extra == 'test'",
      "pytest-qt (>=3.3.0) ; extra == 'test'",
      "quilt3 (>=3.1.12) ; extra == 'test'"
    ],
    "summary": "A plugin that enables annotations provided by Allen Institute for Cell Science",
    "support": "",
    "twitter": "",
    "version": "1.0.7",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Robert Haase" }],
    "code_repository": "https://github.com/haesleinhuepf/napari-assistant-plugin-generator",
    "description": "# napari-assistant-plugin-generator\\r [![License](https://img.shields.io/pypi/l/napari-assistant-plugin-generator.svg?color=green)](https://github.com/haesleinhuepf/napari-assistant-plugin-generator/raw/master/LICENSE)\\r [![PyPI](https://img.shields.io/pypi/v/napari-assistant-plugin-generator.svg?color=green)](https://pypi.org/project/napari-assistant-plugin-generator)\\r [![Python Version](https://img.shields.io/pypi/pyversions/napari-assistant-plugin-generator.svg?color=green)](https://python.org)\\r [![tests](https://github.com/haesleinhuepf/napari-assistant-plugin-generator/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-assistant-plugin-generator/actions)\\r [![codecov](https://codecov.io/gh/haesleinhuepf/napari-assistant-plugin-generator/branch/master/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-assistant-plugin-generator)\\r [![Development Status](https://img.shields.io/pypi/status/napari-assistant-plugin-generator.svg)](https://en.wikipedia.org/wiki/Software_release_life_cycle#Alpha)\\r [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-assistant-plugin-generator)](https://napari-hub.org/plugins/napari-assistant-plugin-generator)\\r [![DOI](https://zenodo.org/badge/322312181.svg)](https://zenodo.org/badge/latestdoi/322312181)\\r \\r The napari-assistant-plugin-generator is a [napari](https://github.com/napari/napari) plugin to generate python code which can be pip-installed and serve as napari-plugin compatible with the [napari-assistant](https://www.napari-hub.org/plugins/napari-assistant).\\r \\r ## Usage\\r \\r For demonstrating how one can generate a Napari plugin from an existing workflow, we demonstrate the procedure by reusing function from [napari-segment-blobs-and-things-with-membranes](https://www.napari-hub.org/plugins/napari-segment-blobs-and-things-with-membranes) (nsbatwm) for generating a plugin.\\r \\r * After installing nsbatwm you can start the assistant from the menu `Tools > Utilities > Assistant (na)` or run `naparia` from the command line. \\r \\r * Open the blobs example image `Blobs`, e.g. after downloading it from [here](https://github.com/clEsperanto/napari_pyclesperanto_assistant/blob/master/napari_pyclesperanto_assistant/data/blobs.tif).\\r \\r * In the Assistant, click on the `Remove noise` button and select `Gaussian (scikit-image, nsbatwm)` from the operation pulldown.\\r * Click the `Binarize` button and select `Threshold (Otsu 1979, scikit-image, nsbatwm)` operation.\\r * Click the `Label` button and select 'Connected component labeling (scikit-image, nsbatwm)' from the operation pulldown.\\r \\r Afterwards, your Napari with the configured workflow should look like this:\\r \\r ![img.png](https://github.com/haesleinhuepf/napari-assistant-plugin-generator/raw/main/docs/screenshot_workflow.png)\\r \\r ### Plugin generation\\r \\r Before running the plugin-generator, make sure you are connected to the internet, \\r because a [plugin template](https://github.com/haesleinhuepf/cookiecutter-napari-assistant-plugin-generator-plugin) will be downloaded. \\r The plugin generator can be found in the menu `Tools > Utilities > Generate Napari plugin from workflow (na)` and also \\r in the `Generate code...` of the Assistant:\\r \\r ![img.png](https://github.com/haesleinhuepf/napari-assistant-plugin-generator/raw/main/docs/generate_napari_plugin1.png)\\r \\r In the plugin generator dialog, please enter this information:\\r * **output dir:** Folder where the Napari plugin code should be saved. If not specified, the plugin will be stored in the current directory Napari was started from.\\r * **plugin name:** Name of the plugin. A folder with this name will be generated in the folder specified above. Plugin names must not contain special characters or spaces. Use `_` instead.\\r * **developer name:** Your name as it will be displayed later on the [napari-hub](https://napari-hub.org) in case you decide to publish your plugin.\\r * **developer email:** Your email as it will be stored in the configuration of your plugin. This email-address is visible to the public.\\r * **github username:** Your username on [github](https://github.com). URLs in the plugin documentation will point to your profile on github.\\r * **short description:** Please write one sentence explaining what the plugin is doing.\\r * **license:** Choose the open-source license your plugin code will be licensed. If you are not sure which one to use, consult [choosealicense.com](https://choosealicense.com).\\r * **tools menu** The menu under `Tools` where your plugin will be found after installing it.\\r * **menu name** The menu entry will have this title.\\r \\r ![img.png](https://github.com/haesleinhuepf/napari-assistant-plugin-generator/raw/main/docs/generate_napari_plugin2.png)\\r \\r After the Napari plugin code has been generated, open it in the integrated development environment (IDE) of your choice. \\r Go through the files in the directory and search for `TODO` entries. Start with the `readme.md` and the `requirements.txt` files:\\r \\r ![img.png](https://github.com/haesleinhuepf/napari-assistant-plugin-generator/raw/main/docs/generate_napari_plugin4.png)\\r \\r Highly relevant might be the python file `my_napari_assistant_plugin/_function.py`. It contains a function with python code representing the workflow we designed by clicking above:\\r \\r ```python\\r from napari_plugin_engine import napari_hook_implementation\\r from napari_tools_menu import register_function\\r from napari_time_slicer import time_slicer\\r \\r @napari_hook_implementation\\r def napari_experimental_provide_function():\\r     return [process_image]\\r \\r @register_function(menu=\"Segmentation / labeling > Segment image\")\\r @time_slicer\\r def process_image(image0_b: \"napari.types.ImageData\", gaussian_blur_sigma_2: float = 1.0, connected_component_labeling_exclude_on_edges_3: bool = False) -> \"napari.types.LabelsData\":\\r     \"\"\"\\r     Short plugin description\\r     \\r     # TODO: Provide more detailed documentation here. E.g. specify the parameters and what values users should enter.\\r     \"\"\"\\r     # TODO: Check the list of parameters of the function definition above. \\r     # If there are parameters that should not be editable by the end user, move their definition and values here instead.\\r     \\r     import napari_segment_blobs_and_things_with_membranes as nsbatwm  # version 0.3.3\\r     \\r     \\r     # gaussian blur\\r     image1_G = nsbatwm.gaussian_blur(image0_b, sigma=gaussian_blur_sigma_2)\\r     \\r     # threshold otsu\\r     image2_T = nsbatwm.threshold_otsu(image1_G)\\r     \\r     # connected component labeling\\r     image3_C = nsbatwm.connected_component_labeling(\\r         image2_T, exclude_on_edges=connected_component_labeling_exclude_on_edges_3)\\r     return image3_C\\r ```\\r \\r It is recommended to inspect the generated code and rename variables to be more meaningful.\\r For renaming variables, make use of your IDE's tools. \\r For example variables can be renamed conveniently in [pycharm](https://www.jetbrains.com/pycharm/) using the right-click menu:\\r \\r ![img.png](https://github.com/haesleinhuepf/napari-assistant-plugin-generator/raw/main/docs/generate_napari_plugin3.png)\\r \\r The `readme.md` file also contains instructions for how to install and distribute your plugin. \\r TL:DR: As a plugin developer you typically execute this command from the terminal within your plugin's root directory to install your plugin in an `editable`. \\r This command allows you to modify the code and test it without the need for re-installing your plugin.\\r \\r ```\\r pip install -e .\\r ```\\r \\r If installation was successful, you will find your plugin in the menu you specified and a dialog will open requesting the parameters of the generated Python function in `<your_plugin_folder>/<your_plugin_folder>/_function.py`:\\r \\r ![img.png](https://github.com/haesleinhuepf/napari-assistant-plugin-generator/raw/main/docs/generate_napari_plugin5.png)\\r \\r ![img.png](https://github.com/haesleinhuepf/napari-assistant-plugin-generator/raw/main/docs/generate_napari_plugin6.png)\\r \\r \\r ## Installation\\r \\r It is recommended to install [devbio-napari](https://www.napari-hub.org/plugins/devbio-napari#installation) first. It comes with many image processing functions that can be combined in workflows and where it is easy to generate plugins from.\\r \\r Afterwards, the plugin generator can be installed using `pip`:\\r ```\\r pip install napari-assistant-plugin-generator\\r ```\\r \\r Also make sure you have `git` installed. E.g. using `mamba`/`conda`:\\r \\r ```\\r mamba install git\\r ```\\r \\r ## Feedback welcome!\\r \\r The napari-assistant is developed in the open because we believe in the open source community. Feel free to drop feedback as [github issue](https://github.com/haesleinhuepf/napari-assistant-plugin-generator/issues) or via [image.sc](https://image.sc)\\r \\r ## Contributing\\r \\r Contributions are very welcome. Please ensure\\r the test coverage at least stays the same before you submit a pull request.\\r \\r ## License\\r \\r Distributed under the terms of the [BSD-3] license,\\r \"napari-assistant-plugin-generator\" is free and open source software\\r \\r ## Acknowledgements\\r This project was supported by the Deutsche Forschungsgemeinschaft under Germany’s Excellence Strategy – EXC2068 - Cluster of Excellence \"Physics of Life\" of TU Dresden. \\r This project has been made possible in part by grant number [2021-240341 (Napari plugin accelerator grant)](https://chanzuckerberg.com/science/programs-resources/imaging/napari/improving-image-processing/) from the Chan Zuckerberg Initiative DAF, an advised fund of the Silicon Valley Community Foundation.\\r \\r [BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\r \\r ",
    "description_content_type": "text/markdown",
    "description_text": "napari-assistant-plugin-generator         The napari-assistant-plugin-generator is a napari plugin to generate python code which can be pip-installed and serve as napari-plugin compatible with the napari-assistant. Usage For demonstrating how one can generate a Napari plugin from an existing workflow, we demonstrate the procedure by reusing function from napari-segment-blobs-and-things-with-membranes (nsbatwm) for generating a plugin.   After installing nsbatwm you can start the assistant from the menu Tools > Utilities > Assistant (na) or run naparia from the command line.    Open the blobs example image Blobs, e.g. after downloading it from here.   In the Assistant, click on the Remove noise button and select Gaussian (scikit-image, nsbatwm) from the operation pulldown.  Click the Binarize button and select Threshold (Otsu 1979, scikit-image, nsbatwm) operation. Click the Label button and select 'Connected component labeling (scikit-image, nsbatwm)' from the operation pulldown.  Afterwards, your Napari with the configured workflow should look like this:  Plugin generation Before running the plugin-generator, make sure you are connected to the internet,  because a plugin template will be downloaded.  The plugin generator can be found in the menu Tools > Utilities > Generate Napari plugin from workflow (na) and also  in the Generate code... of the Assistant:  In the plugin generator dialog, please enter this information: * output dir: Folder where the Napari plugin code should be saved. If not specified, the plugin will be stored in the current directory Napari was started from. * plugin name: Name of the plugin. A folder with this name will be generated in the folder specified above. Plugin names must not contain special characters or spaces. Use _ instead. * developer name: Your name as it will be displayed later on the napari-hub in case you decide to publish your plugin. * developer email: Your email as it will be stored in the configuration of your plugin. This email-address is visible to the public. * github username: Your username on github. URLs in the plugin documentation will point to your profile on github. * short description: Please write one sentence explaining what the plugin is doing. * license: Choose the open-source license your plugin code will be licensed. If you are not sure which one to use, consult choosealicense.com. * tools menu The menu under Tools where your plugin will be found after installing it. * menu name The menu entry will have this title.  After the Napari plugin code has been generated, open it in the integrated development environment (IDE) of your choice.  Go through the files in the directory and search for TODO entries. Start with the readme.md and the requirements.txt files:  Highly relevant might be the python file my_napari_assistant_plugin/_function.py. It contains a function with python code representing the workflow we designed by clicking above: ```python from napari_plugin_engine import napari_hook_implementation from napari_tools_menu import register_function from napari_time_slicer import time_slicer @napari_hook_implementation def napari_experimental_provide_function():     return [process_image] @register_function(menu=\"Segmentation / labeling > Segment image\") @time_slicer def process_image(image0_b: \"napari.types.ImageData\", gaussian_blur_sigma_2: float = 1.0, connected_component_labeling_exclude_on_edges_3: bool = False) -> \"napari.types.LabelsData\":     \"\"\"     Short plugin description # TODO: Provide more detailed documentation here. E.g. specify the parameters and what values users should enter. \"\"\" # TODO: Check the list of parameters of the function definition above.  # If there are parameters that should not be editable by the end user, move their definition and values here instead.  import napari_segment_blobs_and_things_with_membranes as nsbatwm  # version 0.3.3   # gaussian blur image1_G = nsbatwm.gaussian_blur(image0_b, sigma=gaussian_blur_sigma_2)  # threshold otsu image2_T = nsbatwm.threshold_otsu(image1_G)  # connected component labeling image3_C = nsbatwm.connected_component_labeling(     image2_T, exclude_on_edges=connected_component_labeling_exclude_on_edges_3) return image3_C  ``` It is recommended to inspect the generated code and rename variables to be more meaningful. For renaming variables, make use of your IDE's tools.  For example variables can be renamed conveniently in pycharm using the right-click menu:  The readme.md file also contains instructions for how to install and distribute your plugin.  TL:DR: As a plugin developer you typically execute this command from the terminal within your plugin's root directory to install your plugin in an editable.  This command allows you to modify the code and test it without the need for re-installing your plugin. pip install -e . If installation was successful, you will find your plugin in the menu you specified and a dialog will open requesting the parameters of the generated Python function in <your_plugin_folder>/<your_plugin_folder>/_function.py:   Installation It is recommended to install devbio-napari first. It comes with many image processing functions that can be combined in workflows and where it is easy to generate plugins from. Afterwards, the plugin generator can be installed using pip: pip install napari-assistant-plugin-generator Also make sure you have git installed. E.g. using mamba/conda: mamba install git Feedback welcome! The napari-assistant is developed in the open because we believe in the open source community. Feel free to drop feedback as github issue or via image.sc Contributing Contributions are very welcome. Please ensure the test coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-assistant-plugin-generator\" is free and open source software Acknowledgements This project was supported by the Deutsche Forschungsgemeinschaft under Germany’s Excellence Strategy – EXC2068 - Cluster of Excellence \"Physics of Life\" of TU Dresden.  This project has been made possible in part by grant number 2021-240341 (Napari plugin accelerator grant) from the Chan Zuckerberg Initiative DAF, an advised fund of the Silicon Valley Community Foundation.",
    "development_status": [],
    "display_name": "",
    "documentation": "https://github.com/haesleinhuepf/napari-assistant-plugin-generator/",
    "first_released": "2022-11-09T14:50:52.822849Z",
    "license": "BSD-3-Clause",
    "name": "napari-assistant-plugin-generator",
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": [],
    "project_site": "https://github.com/haesleinhuepf/napari-assistant-plugin-generator/",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-11-09T14:50:52.822849Z",
    "report_issues": "https://github.com/haesleinhuepf/napari-assistant-plugin-generator/issues",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "magicgui",
      "numpy (!=1.19.4)",
      "jupytext",
      "jupyter",
      "dask",
      "autopep8",
      "cookiecutter",
      "napari-tools-menu",
      "napari-assistant (>=0.4.1)"
    ],
    "summary": "Auto-generate python code from within napari to make napari-assistant compatible plugins",
    "support": "https://forum.image.sc/",
    "twitter": "",
    "version": "0.1.0",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Allen Goodman", "orcid": "0000-0002-6434-2320" }],
    "code_repository": "https://github.com/0x00b1/napari-features",
    "description": "# napari-features  [![License](https://img.shields.io/pypi/l/napari-features.svg?color=green)](https://github.com/0x00b1/napari-features/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-features.svg?color=green)](https://pypi.org/project/napari-features) [![Python Version](https://img.shields.io/pypi/pyversions/napari-features.svg?color=green)](https://python.org) [![tests](https://github.com/0x00b1/napari-features/workflows/tests/badge.svg)](https://github.com/0x00b1/napari-features/actions) [![codecov](https://codecov.io/gh/0x00b1/napari-features/branch/master/graph/badge.svg)](https://codecov.io/gh/0x00b1/napari-features)  An extensible, general-purpose feature extraction plug-in for the [Napari](https://napari.org) image viewer.  ## Features  ### Color  #### Image      color_image_integrated_intensity     color_image_maximum_intensity     color_image_mean_intensity     color_image_median_absolute_deviation_intensity     color_image_median_intensity     color_image_minimum_intensity     color_image_quantile_1_intensity     color_image_quantile_3_intensity     color_image_standard_deviation_intensity  #### Object      color_object_center_mass_intensity_x     color_object_center_mass_intensity_y     color_object_integrated_intensity     color_object_integrated_intensity_edge     color_object_mass_displacement     color_object_maximum_intensity     color_object_maximum_intensity_edge     color_object_maximum_intensity_x     color_object_maximum_intensity_y     color_object_mean_intensity     color_object_mean_intensity_edge     color_object_median_absolute_deviation_intensity     color_object_median_intensity     color_object_median_intensity_edge     color_object_minimum_intensity     color_object_minimum_intensity_edge     color_object_quantile_1_intensity     color_object_quantile_1_intensity_edge        color_object_quantile_3_intensity     color_object_quantile_3_intensity_edge     color_object_standard_deviation_intensity     color_object_standard_deviation_intensity_edge     Object distribution     color_object_distribution_coefficient_of_variation_intensity     color_object_distribution_integrated_intensity     Color_object_distribution_mean_intensity  ### Location  #### Object neighborhood      location_object_neighborhood_angle     location_object_neighborhood_closest_0_distance     location_object_neighborhood_closest_0_object_index     location_object_neighborhood_closest_1_distance     location_object_neighborhood_closest_1_object_index     location_object_neighborhood_closest_2_distance     location_object_neighborhood_closest_2_object_index     location_object_neighborhood_neighbors     location_object_neighborhood_touching  ### Metadata  #### Image      metadata_image_checksum     metadata_image_filename  #### Layer      metadata_layer_name     metadata_layer_type  #### Object      metadata_object_index  ### Shape  #### Image      shape_image_area  #### Image skeleton      shape_image_skeleton_branches     shape_image_skeleton_endpoints     shape_image_skeleton_length     shape_image_skeleton_trunks  #### Object      shape_object_area     shape_object_bounding_box_area     shape_object_bounding_box_maximum_x     shape_object_bounding_box_maximum_y     shape_object_bounding_box_maximum_z     shape_object_bounding_box_minimum_x     shape_object_bounding_box_minimum_y     shape_object_bounding_box_minimum_z     shape_object_bounding_box_volume     shape_object_central_moment_0_0_0     shape_object_central_moment_0_0_1     shape_object_central_moment_0_1_2     shape_object_central_moment_0_1_3     shape_object_central_moment_1_2_0     shape_object_central_moment_1_2_1     shape_object_central_moment_1_3_2     shape_object_central_moment_1_3_3     shape_object_central_moment_2_0_0     shape_object_central_moment_2_0_1     shape_object_central_moment_2_1_2     shape_object_central_moment_2_1_3     shape_object_central_moment_3_2_0     shape_object_central_moment_3_2_1     shape_object_central_moment_3_3_2     shape_object_central_moment_3_3_3     shape_object_centroid_x     shape_object_centroid_y     shape_object_centroid_z     shape_object_compactness     shape_object_eccentricity     shape_object_equivalent_diameter     shape_object_euler_number     shape_object_extent     shape_object_form_factor     shape_object_hu_moment_0     shape_object_hu_moment_1     shape_object_hu_moment_2     shape_object_hu_moment_3     shape_object_hu_moment_4     shape_object_hu_moment_5     shape_object_hu_moment_6     shape_object_inertia_tensor_eigenvalues_x     shape_object_inertia_tensor_eigenvalues_y     shape_object_inertia_tensor_eigenvalues_z     shape_object_inertia_tensor_x_x     shape_object_inertia_tensor_x_y     Shape_object_inertia_tensor_x_z     shape_object_inertia_tensor_y_x     shape_object_inertia_tensor_y_y     shape_object_inertia_tensor_y_z     shape_object_inertia_tensor_z_x     shape_object_inertia_tensor_z_y     shape_object_inertia_tensor_z_z     shape_object_major_axis_length     shape_object_maximum_feret_diameter     shape_object_maximum_radius     shape_object_mean_radius     shape_object_median_radius     shape_object_minimum_feret_diameter     shape_object_minor_axis_length     shape_object_normalized_moment_x_y     shape_object_orientation     shape_object_perimeter     shape_object_solidity     shape_object_spatial_moment_0_0_0     shape_object_spatial_moment_0_0_1     shape_object_spatial_moment_0_1_2     shape_object_spatial_moment_0_1_3     shape_object_spatial_moment_1_2_0     shape_object_spatial_moment_1_2_1     shape_object_spatial_moment_1_3_2     shape_object_spatial_moment_1_3_3     shape_object_spatial_moment_2_0_0     shape_object_spatial_moment_2_0_1     shape_object_spatial_moment_2_1_2     shape_object_spatial_moment_2_1_3     shape_object_spatial_moment_3_2_0     shape_object_spatial_moment_3_2_1     shape_object_spatial_moment_3_3_2     shape_object_spatial_moment_3_3_3     shape_object_surface_area     shape_object_volume     shape_object_zernike shape features     Object skeleton     shape_object_skeleton_endpoints     shape_object_skeleton_branches     shape_object_skeleton_length     shape_object_skeleton_trunks  ### Texture  #### Object      texture_object_haralick_angular_second_moment     texture_object_haralick_contrast     texture_object_haralick_coorelation     texture_object_haralick_sum_of_squares_variance     texture_object_haralick_inverse_difference_moment     texture_object_haralick_sum_average     texture_object_haralick_sum_variance     texture_object_haralick_sum_entropy     texture_object_haralick_entropy     texture_object_haralick_difference_variance     texture_object_haralick_measure_of_correlation_0     texture_object_haralick_measure_of_correlation_1     texture_object_haralick_maximum_correlation_coefficient   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-features      An extensible, general-purpose feature extraction plug-in for the Napari image viewer. Features Color Image color_image_integrated_intensity color_image_maximum_intensity color_image_mean_intensity color_image_median_absolute_deviation_intensity color_image_median_intensity color_image_minimum_intensity color_image_quantile_1_intensity color_image_quantile_3_intensity color_image_standard_deviation_intensity  Object color_object_center_mass_intensity_x color_object_center_mass_intensity_y color_object_integrated_intensity color_object_integrated_intensity_edge color_object_mass_displacement color_object_maximum_intensity color_object_maximum_intensity_edge color_object_maximum_intensity_x color_object_maximum_intensity_y color_object_mean_intensity color_object_mean_intensity_edge color_object_median_absolute_deviation_intensity color_object_median_intensity color_object_median_intensity_edge color_object_minimum_intensity color_object_minimum_intensity_edge color_object_quantile_1_intensity color_object_quantile_1_intensity_edge    color_object_quantile_3_intensity color_object_quantile_3_intensity_edge color_object_standard_deviation_intensity color_object_standard_deviation_intensity_edge Object distribution color_object_distribution_coefficient_of_variation_intensity color_object_distribution_integrated_intensity Color_object_distribution_mean_intensity  Location Object neighborhood location_object_neighborhood_angle location_object_neighborhood_closest_0_distance location_object_neighborhood_closest_0_object_index location_object_neighborhood_closest_1_distance location_object_neighborhood_closest_1_object_index location_object_neighborhood_closest_2_distance location_object_neighborhood_closest_2_object_index location_object_neighborhood_neighbors location_object_neighborhood_touching  Metadata Image metadata_image_checksum metadata_image_filename  Layer metadata_layer_name metadata_layer_type  Object metadata_object_index  Shape Image shape_image_area  Image skeleton shape_image_skeleton_branches shape_image_skeleton_endpoints shape_image_skeleton_length shape_image_skeleton_trunks  Object shape_object_area shape_object_bounding_box_area shape_object_bounding_box_maximum_x shape_object_bounding_box_maximum_y shape_object_bounding_box_maximum_z shape_object_bounding_box_minimum_x shape_object_bounding_box_minimum_y shape_object_bounding_box_minimum_z shape_object_bounding_box_volume shape_object_central_moment_0_0_0 shape_object_central_moment_0_0_1 shape_object_central_moment_0_1_2 shape_object_central_moment_0_1_3 shape_object_central_moment_1_2_0 shape_object_central_moment_1_2_1 shape_object_central_moment_1_3_2 shape_object_central_moment_1_3_3 shape_object_central_moment_2_0_0 shape_object_central_moment_2_0_1 shape_object_central_moment_2_1_2 shape_object_central_moment_2_1_3 shape_object_central_moment_3_2_0 shape_object_central_moment_3_2_1 shape_object_central_moment_3_3_2 shape_object_central_moment_3_3_3 shape_object_centroid_x shape_object_centroid_y shape_object_centroid_z shape_object_compactness shape_object_eccentricity shape_object_equivalent_diameter shape_object_euler_number shape_object_extent shape_object_form_factor shape_object_hu_moment_0 shape_object_hu_moment_1 shape_object_hu_moment_2 shape_object_hu_moment_3 shape_object_hu_moment_4 shape_object_hu_moment_5 shape_object_hu_moment_6 shape_object_inertia_tensor_eigenvalues_x shape_object_inertia_tensor_eigenvalues_y shape_object_inertia_tensor_eigenvalues_z shape_object_inertia_tensor_x_x shape_object_inertia_tensor_x_y Shape_object_inertia_tensor_x_z shape_object_inertia_tensor_y_x shape_object_inertia_tensor_y_y shape_object_inertia_tensor_y_z shape_object_inertia_tensor_z_x shape_object_inertia_tensor_z_y shape_object_inertia_tensor_z_z shape_object_major_axis_length shape_object_maximum_feret_diameter shape_object_maximum_radius shape_object_mean_radius shape_object_median_radius shape_object_minimum_feret_diameter shape_object_minor_axis_length shape_object_normalized_moment_x_y shape_object_orientation shape_object_perimeter shape_object_solidity shape_object_spatial_moment_0_0_0 shape_object_spatial_moment_0_0_1 shape_object_spatial_moment_0_1_2 shape_object_spatial_moment_0_1_3 shape_object_spatial_moment_1_2_0 shape_object_spatial_moment_1_2_1 shape_object_spatial_moment_1_3_2 shape_object_spatial_moment_1_3_3 shape_object_spatial_moment_2_0_0 shape_object_spatial_moment_2_0_1 shape_object_spatial_moment_2_1_2 shape_object_spatial_moment_2_1_3 shape_object_spatial_moment_3_2_0 shape_object_spatial_moment_3_2_1 shape_object_spatial_moment_3_3_2 shape_object_spatial_moment_3_3_3 shape_object_surface_area shape_object_volume shape_object_zernike shape features Object skeleton shape_object_skeleton_endpoints shape_object_skeleton_branches shape_object_skeleton_length shape_object_skeleton_trunks  Texture Object texture_object_haralick_angular_second_moment texture_object_haralick_contrast texture_object_haralick_coorelation texture_object_haralick_sum_of_squares_variance texture_object_haralick_inverse_difference_moment texture_object_haralick_sum_average texture_object_haralick_sum_variance texture_object_haralick_sum_entropy texture_object_haralick_entropy texture_object_haralick_difference_variance texture_object_haralick_measure_of_correlation_0 texture_object_haralick_measure_of_correlation_1 texture_object_haralick_maximum_correlation_coefficient ",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari-features",
    "documentation": "https://github.com/0x00b1/napari-features#README.md",
    "first_released": "2021-06-17T23:20:42.709211Z",
    "license": "MIT",
    "name": "napari-features",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": [],
    "project_site": "https://github.com/0x00b1/napari-features",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2021-08-24T17:16:42.406784Z",
    "report_issues": "https://github.com/0x00b1/napari-features/issues",
    "requirements": [
      "magicgui (>=0.2.9)",
      "napari (>=0.4.10)",
      "napari-plugin-engine (>=0.1.4)",
      "numpy (>=1.19.5)",
      "pandas (>=1.2.4)",
      "qtpy (>=1.9.0)",
      "scikit-image (>=0.18.1)",
      "scipy (>=1.4.1)"
    ],
    "summary": "extensible, general-purpose feature extraction",
    "support": "https://github.com/0x00b1/napari-features/issues",
    "twitter": "",
    "version": "0.1.4",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      { "email": "talley.lambert@gmail.com", "name": "Talley Lambert" }
    ],
    "code_repository": "https://github.com/tlambert03/napari-error-reporter",
    "conda": [{ "channel": "conda-forge", "package": "napari-error-reporter" }],
    "description": "# 🐛 napari-error-reporter  [![License](https://img.shields.io/pypi/l/napari-error-reporter.svg?color=green)](https://github.com/tlambert03/napari-error-reporter/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-error-reporter.svg?color=green)](https://pypi.org/project/napari-error-reporter) [![Python Version](https://img.shields.io/pypi/pyversions/napari-error-reporter.svg?color=green)](https://python.org) [![CI](https://github.com/tlambert03/napari-error-reporter/actions/workflows/ci.yml/badge.svg)](https://github.com/tlambert03/napari-error-reporter/actions/workflows/ci.yml) [![codecov](https://codecov.io/gh/tlambert03/napari-error-reporter/branch/main/graph/badge.svg)](https://codecov.io/gh/tlambert03/napari-error-reporter)  Want to help out napari?  Install this plugin!  This plugin will automatically send error reports to napari (via [sentry.io](https://sentry.io)) whenever an exception occurs while you are using napari.  The first time you run napari after installing this plugin an opt-in notification will appear (Be sure to click \"yes\", otherwise no reports will be collected or sent).  You may opt back out at any time in napari's help menu.  Every effort is made to strip these reports of personally identifiable information.  Here is an example exception event:  <details>  <summary>Example bug report</summary>  ```python {     'breadcrumbs': {         'values': [             {                 'category': 'subprocess',                 'data': {},                 'message': 'sw_vers -productVersion',                 'timestamp': '2022-02-02T01:30:00.216738Z',                 'type': 'subprocess'             }         ]     },     'contexts': {         'runtime': {             'build': '3.9.9 | packaged by conda-forge | (main, Dec 20 2021, 02:41:37) \\ [Clang 11.1.0 ]',             'name': 'CPython',             'version': '3.9.9'         }     },     'environment': 'macOS-10.15.7-x86_64-i386-64bit',     'event_id': '02dd8ddd3a4b4743af3d7d7a09949df4',     'exception': {         'values': [             {                 'mechanism': None,                 'module': None,                 'stacktrace': {                     'frames': [                         {                             'context_line': '                x = 1 / 0',                             'filename': 'napari_error_reporter/_util.py',                             'function': 'get_sample_event',                             'in_app': True,                             'lineno': 130,                             'module': 'napari_error_reporter._util',                             'post_context': [                                 '            except Exception:',                                 '                with sentry_sdk.push_scope() as scope:',                                 '                    for k, v in _get_tags().items():',                                 '                        scope.set_tag(k, v)',                                 '                    del v, k, scope'                             ],                             'pre_context': [                                 \"            # remove locals that wouldn't really be there\",                                 '            del settings, _trans, kwargs, client, EVENT',                                 '            try:',                                 '                some_variable = 1',                                 '                another_variable = \"my_string\"'                             ]                         }                     ]                 },                 'type': 'ZeroDivisionError',                 'value': 'division by zero'             }         ]     },     'extra': {'sys.argv': ['napari']},     'level': 'error',     'modules': {         'aicsimageio': '4.5.2',         'aicspylibczi': '3.0.4',         'aiohttp': '3.8.1',         'aiosignal': '1.2.0',         'alabaster': '0.7.12',         'anyio': '3.5.0',         'appdirs': '1.4.4',         'appnope': '0.1.2',         'argon2-cffi': '21.3.0',         'argon2-cffi-bindings': '21.2.0',         'arrow': '1.2.1',         'asciitree': '0.3.3',         'asttokens': '2.0.5',         'async-timeout': '4.0.2',         'atomium': '1.0.11',         'attrs': '21.4.0',         'autopep8': '1.6.0',         'babel': '2.9.1',         'backcall': '0.2.0',         'bcrypt': '3.2.0',         'beautifulsoup4': '4.10.0',         'binaryornot': '0.4.4',         'black': '20.8b1',         'bleach': '4.1.0',         'bracex': '2.2.1',         'build': '0.7.0',         'cachey': '0.2.1',         'cellpose': '0.6.5',         'certifi': '2021.10.8',         'cffi': '1.15.0',         'cfgv': '3.3.1',         'chardet': '4.0.0',         'charset-normalizer': '2.0.10',         'check-manifest': '0.47',         'click': '7.1.2',         'click-option-group': '0.5.3',         'cloudpickle': '2.0.0',         'colorama': '0.4.4',         'commonmark': '0.9.1',         'cookiecutter': '1.7.3',         'coverage': '6.2',         'cryptography': '36.0.1',         'cycler': '0.11.0',         'dask': '2022.1.0',         'debugpy': '1.5.1',         'decorator': '5.1.1',         'defusedxml': '0.7.1',         'distlib': '0.3.4',         'dnspython': '2.2.0',         'docstring-parser': '0.13',         'docutils': '0.16',         'elementpath': '2.4.0',         'email-validator': '1.1.3',         'entrypoints': '0.3',         'executing': '0.8.2',         'fancycompleter': '0.9.1',         'fasteners': '0.17.2',         'fastremap': '1.12.2',         'filelock': '3.4.2',         'flake8': '3.8.4',         'fonttools': '4.28.5',         'freetype-py': '2.2.0',         'frozenlist': '1.3.0',         'fsspec': '2022.1.0',         'furo': '2022.1.2',         'gitdb': '4.0.9',         'gitpython': '3.1.26',         'greenlet': '1.1.2',         'heapdict': '1.0.1',         'hsluv': '5.0.2',         'hypothesis': '6.35.1',         'identify': '2.4.4',         'idna': '3.3',         'imagecodecs': '2021.11.20',         'imageio': '2.10.5',         'imageio-ffmpeg': '0.4.5',         'imagesize': '1.3.0',         'importlib-metadata': '4.10.1',         'iniconfig': '1.1.1',         'install': '1.3.5',         'intervaltree': '3.1.0',         'ipykernel': '6.7.0',         'ipython': '8.0.0',         'ipython-genutils': '0.2.0',         'ipywidgets': '7.6.5',         'jedi': '0.18.1',         'jinja2': '3.0.3',         'jinja2-time': '0.2.0',         'jsonschema': '3.2.0',         'jupyter': '1.0.0',         'jupyter-book': '0.12.1',         'jupyter-cache': '0.4.3',         'jupyter-client': '7.1.1',         'jupyter-console': '6.4.0',         'jupyter-core': '4.9.1',         'jupyter-server': '1.13.3',         'jupyter-server-mathjax': '0.2.3',         'jupyter-sphinx': '0.3.2',         'jupyterlab-pygments': '0.1.2',         'jupyterlab-widgets': '1.0.2',         'jupytext': '1.11.5',         'kiwisolver': '1.3.2',         'latexcodec': '2.0.1',         'linkify-it-py': '1.0.3',         'llvmlite': '0.38.0',         'locket': '0.2.1',         'loguru': '0.5.3',         'lxml': '4.7.1',         'magicgui': '0.3.5.dev18+g78d1687',         'markdown-it-py': '1.1.0',         'markupsafe': '2.0.1',         'matplotlib': '3.5.1',         'matplotlib-inline': '0.1.3',         'mccabe': '0.6.1',         'mdit-py-plugins': '0.2.8',         'meshzoo': '0.9.2',         'mistune': '0.8.4',         'mrc': '0.2.0',         'msgpack': '1.0.3',         'multidict': '5.2.0',         'mypy': '0.931',         'mypy-extensions': '0.4.3',         'myst-nb': '0.13.1',         'myst-parser': '0.15.2',         'napari': '0.4.14rc1.dev4+gcdf58d44b',         'napari-aicsimageio': '0.4.1',         'napari-console': '0.0.4',         'napari-dv': '0.2.7.dev0+g54e1691.d20220128',         'napari-error-reporter': '0.1.dev1+g1b388f2.d20220201',         'napari-hello': '0.0.1',         'napari-math': '0.0.1a0',         'napari-micromanager': '0.0.1rc6.dev14+g5149788.d20220128',         'napari-molecule-reader': '0.1.2.dev1+gc2ec2de',         'napari-plugin-engine': '0.2.0',         'napari-pyclesperanto-assistant': '0.12.0',         'napari-skimage-regionprops': '0.2.9',         'napari-svg': '0.1.6',         'napari-time-slicer': '0.4.2',         'napari-workflows': '0.1.2',         'natsort': '8.0.2',         'nbclient': '0.5.10',         'nbconvert': '6.4.0',         'nbdime': '3.1.1',         'nbformat': '5.1.3',         'nd2': '0.1.4',         'nest-asyncio': '1.5.4',         'networkx': '2.6.3',         'nodeenv': '1.6.0',         'notebook': '6.4.7',         'npe2': '0.1.1',         'numba': '0.55.0',         'numcodecs': '0.9.1',         'numpy': '1.20.3',         'numpydoc': '1.1.0',         'ome-types': '0.2.10',         'opencv-python-headless': '4.5.5.62',         'packaging': '21.3',         'pandas': '1.3.5',         'pandocfilters': '1.5.0',         'paramiko': '2.9.2',         'parso': '0.8.3',         'partd': '1.2.0',         'pathspec': '0.9.0',         'pdbpp': '0.10.3',         'peewee': '3.14.8',         'pep517': '0.12.0',         'pexpect': '4.8.0',         'pickleshare': '0.7.5',         'pillow': '8.4.0',         'pint': '0.18',         'pip': '21.3.1',         'platformdirs': '2.4.1',         'pluggy': '1.0.0',         'pooch': '1.5.2',         'poyo': '0.5.0',         'pre-commit': '2.16.0',         'prometheus-client': '0.12.0',         'prompt-toolkit': '3.0.24',         'psutil': '5.9.0',         'psygnal': '0.2.0',         'ptyprocess': '0.7.0',         'pure-eval': '0.2.1',         'py': '1.11.0',         'pybtex': '0.24.0',         'pybtex-docutils': '1.0.1',         'pyclesperanto-prototype': '0.12.0',         'pycodestyle': '2.8.0',         'pycparser': '2.21',         'pydantic': '1.9.0',         'pydata-sphinx-theme': '0.7.2',         'pyflakes': '2.2.0',         'pygments': '2.11.2',         'pymmcore': '10.1.1.70.5',         'pymmcore-plus': '0.1.8',         'pynacl': '1.5.0',         'pyopencl': '2021.2.13',         'pyopengl': '3.1.5',         'pyparsing': '3.0.6',         'pyperclip': '1.8.2',         'pyrepl': '0.9.0',         'pyro5': '5.13.1',         'pyrsistent': '0.18.1',         'pyside2': '5.15.2.1',         'pytest': '6.2.5',         'pytest-cookies': '0.6.1',         'pytest-cov': '3.0.0',         'pytest-faulthandler': '2.0.1',         'pytest-order': '1.0.1',         'pytest-qt': '4.0.2',         'python-dateutil': '2.8.2',         'python-dotenv': '0.19.2',         'python-slugify': '5.0.2',         'pytomlpp': '1.0.10',         'pytools': '2021.2.9',         'pytz': '2021.3',         'pywavelets': '1.2.0',         'pyyaml': '6.0',         'pyzmq': '22.3.0',         'qtconsole': '5.2.2',         'qtpy': '2.0.0',         'regex': '2021.11.10',         'requests': '2.27.1',         'rich': '11.0.0',         'rmsd': '1.4',         'ruamel.yaml': '0.17.20',         'ruamel.yaml.clib': '0.2.6',         'scikit-image': '0.19.1',         'scipy': '1.7.3',         'semgrep': '0.78.0',         'send2trash': '1.8.0',         'sentry-sdk': '1.5.4',         'serpent': '1.40',         'setuptools': '60.5.0',         'shiboken2': '5.15.2.1',         'six': '1.16.0',         'smmap': '5.0.0',         'sniffio': '1.2.0',         'snowballstemmer': '2.2.0',         'sortedcontainers': '2.4.0',         'soupsieve': '2.3.1',         'sourcery-cli': '0.10.0',         'sphinx': '4.4.0',         'sphinx-autodoc-typehints': '1.12.0',         'sphinx-book-theme': '0.1.10',         'sphinx-comments': '0.0.3',         'sphinx-copybutton': '0.4.0',         'sphinx-external-toc': '0.2.3',         'sphinx-jupyterbook-latex': '0.4.6',         'sphinx-multitoc-numbering': '0.1.3',         'sphinx-panels': '0.6.0',         'sphinx-tabs': '3.2.0',         'sphinx-thebe': '0.0.10',         'sphinx-togglebutton': '0.2.3',         'sphinxcontrib-applehelp': '1.0.2',         'sphinxcontrib-bibtex': '2.2.1',         'sphinxcontrib-devhelp': '1.0.2',         'sphinxcontrib-htmlhelp': '2.0.0',         'sphinxcontrib-jsmath': '1.0.1',         'sphinxcontrib-qthelp': '1.0.3',         'sphinxcontrib-serializinghtml': '1.1.5',         'sqlalchemy': '1.4.29',         'stack-data': '0.1.4',         'superqt': '0.2.5.post2.dev7+ga49bcd7',         'tensorstore': '0.1.16',         'terminado': '0.12.1',         'testpath': '0.5.0',         'text-unidecode': '1.3',         'tifffile': '2021.11.2',         'toml': '0.10.2',         'tomli': '2.0.0',         'toolz': '0.11.2',         'torch': '1.10.1',         'tornado': '6.1',         'tox': '3.24.5',         'tox-conda': '0.9.1',         'tqdm': '4.62.3',         'traitlets': '5.1.1',         'transforms3d': '0.3.1',         'transitions': '0.8.10',         'typed-ast': '1.5.1',         'typer': '0.4.0',         'typing-extensions': '4.0.1',         'uc-micro-py': '1.0.1',         'urllib3': '1.26.8',         'useq-schema': '0.1.1.dev13+g01d1b46.d20220120',         'valerius': '0.2.0',         'virtualenv': '20.13.0',         'vispy': '0.9.4',         'watchdog': '2.1.6',         'wcmatch': '8.3',         'wcwidth': '0.2.5',         'webencodings': '0.5.1',         'websocket-client': '1.2.3',         'wheel': '0.37.1',         'widgetsnbextension': '3.5.2',         'wmctrl': '0.4',         'wrapt': '1.13.3',         'wurlitzer': '3.0.2',         'xarray': '0.20.2',         'xmlschema': '1.9.2',         'yarl': '1.7.2',         'zarr': '2.10.3',         'zipp': '3.7.0'     },     'platform': 'python',     'release': '0.4.14rc1.dev4+gcdf58d44b',     'sdk': {         'integrations': [             'aiohttp',             'argv',             'atexit',             'dedupe',             'excepthook',             'logging',             'modules',             'sqlalchemy',             'stdlib',             'threading',             'tornado'         ],         'name': 'sentry.python',         'packages': [{'name': 'pypi:sentry-sdk', 'version': '1.5.4'}],         'version': '1.5.4'     },     'server_name': '',     'tags': {         'platform.name': 'MacOS 10.15.7',         'platform.system': 'Darwin',         'qtpy.API_NAME': 'PySide2',         'qtpy.QT_VERSION': '5.15.2'     },     'timestamp': '2022-02-02T01:30:00.229122Z' } ```  </details>  > ***NOTE**: in the opt-in dialog, there is a checkbox labeled \"include local variables\", checking this will include the value of variables in the local scope when an exception occurs.  While these can be very useful when interpreting a bug report, they may occasionally include local file path strings.  If that concerns you, please leave this box unchecked*  ## Install  This plugin requires napari version 0.4.15 or greater, or the `main` branch with PR [napari/napari#4055](https://github.com/napari/napari/pull/4055).  Install via pip with:  ```sh pip install napari-error-reporter ```  or in the built-in plugin installer (a restart will be required):  <img width=\"503\" alt=\"Untitled\" src=\"https://user-images.githubusercontent.com/1609449/153915128-09a5e3d7-8561-4c17-b543-5ea172e3e860.png\">   Thank you!!  ## Privacy FAQ  Even with the multiple layers of opt-ins, and the attempts to wipe all personal info prior to sending reports, we understand that privacy is always a concern.  ### Do you collect personal info?  We make every attempt to collect ***no*** personally identifiable information.  No name, location, IP address, etc...  We do collect your ([`uuid.getnode()`](https://docs.python.org/3.10/library/uuid.html#uuid.getnode)) to be able to track bug resolution over time. As mentioned above, allowing local variables to be collected may occasionally include a file path in the log. If that concerns you, please leave that unchecked.  ### Is this shipped with napari?  `napari-error-reporter` is **not** bundled with napari or listed as a napari dependency. In order for reports to be sent, you must first install this plugin yourself, and then opt in on the next launch.  If you uninstall the plugin, no more reports can be sent.  ### Who can access these reports?  Only the following napari core developers have access to these reports. If [this](https://raw.githubusercontent.com/tlambert03/napari-error-reporter/main/ADMINS) list changes in the future, you will be asked to opt-in again in napari:  - Juan Nunez-Iglesias ([@jni](https://github.com/jni)) - Talley Lambert ([@tlambert03](https://github.com/tlambert03))  *This plugin is **not** associated with the Chan Zuckerberg Initiative*.  ### How will these reports be used?  Commonly occuring errors will be will be manually purged of file paths and local variables and posted to https://github.com/napari/napari/issues  ### How long is data retained  Sentry retains event data for 90 days by default.  For complete details, see Sentry's page on [Security & Compliance](https://sentry.io/security/) ",
    "description_content_type": "text/markdown",
    "description_text": "🐛 napari-error-reporter      Want to help out napari?  Install this plugin! This plugin will automatically send error reports to napari (via sentry.io) whenever an exception occurs while you are using napari. The first time you run napari after installing this plugin an opt-in notification will appear (Be sure to click \"yes\", otherwise no reports will be collected or sent).  You may opt back out at any time in napari's help menu. Every effort is made to strip these reports of personally identifiable information.  Here is an example exception event:  Example bug report  ```python {     'breadcrumbs': {         'values': [             {                 'category': 'subprocess',                 'data': {},                 'message': 'sw_vers -productVersion',                 'timestamp': '2022-02-02T01:30:00.216738Z',                 'type': 'subprocess'             }         ]     },     'contexts': {         'runtime': {             'build': '3.9.9 | packaged by conda-forge | (main, Dec 20 2021, 02:41:37) \\ [Clang 11.1.0 ]',             'name': 'CPython',             'version': '3.9.9'         }     },     'environment': 'macOS-10.15.7-x86_64-i386-64bit',     'event_id': '02dd8ddd3a4b4743af3d7d7a09949df4',     'exception': {         'values': [             {                 'mechanism': None,                 'module': None,                 'stacktrace': {                     'frames': [                         {                             'context_line': '                x = 1 / 0',                             'filename': 'napari_error_reporter/_util.py',                             'function': 'get_sample_event',                             'in_app': True,                             'lineno': 130,                             'module': 'napari_error_reporter._util',                             'post_context': [                                 '            except Exception:',                                 '                with sentry_sdk.push_scope() as scope:',                                 '                    for k, v in _get_tags().items():',                                 '                        scope.set_tag(k, v)',                                 '                    del v, k, scope'                             ],                             'pre_context': [                                 \"            # remove locals that wouldn't really be there\",                                 '            del settings, _trans, kwargs, client, EVENT',                                 '            try:',                                 '                some_variable = 1',                                 '                another_variable = \"my_string\"'                             ]                         }                     ]                 },                 'type': 'ZeroDivisionError',                 'value': 'division by zero'             }         ]     },     'extra': {'sys.argv': ['napari']},     'level': 'error',     'modules': {         'aicsimageio': '4.5.2',         'aicspylibczi': '3.0.4',         'aiohttp': '3.8.1',         'aiosignal': '1.2.0',         'alabaster': '0.7.12',         'anyio': '3.5.0',         'appdirs': '1.4.4',         'appnope': '0.1.2',         'argon2-cffi': '21.3.0',         'argon2-cffi-bindings': '21.2.0',         'arrow': '1.2.1',         'asciitree': '0.3.3',         'asttokens': '2.0.5',         'async-timeout': '4.0.2',         'atomium': '1.0.11',         'attrs': '21.4.0',         'autopep8': '1.6.0',         'babel': '2.9.1',         'backcall': '0.2.0',         'bcrypt': '3.2.0',         'beautifulsoup4': '4.10.0',         'binaryornot': '0.4.4',         'black': '20.8b1',         'bleach': '4.1.0',         'bracex': '2.2.1',         'build': '0.7.0',         'cachey': '0.2.1',         'cellpose': '0.6.5',         'certifi': '2021.10.8',         'cffi': '1.15.0',         'cfgv': '3.3.1',         'chardet': '4.0.0',         'charset-normalizer': '2.0.10',         'check-manifest': '0.47',         'click': '7.1.2',         'click-option-group': '0.5.3',         'cloudpickle': '2.0.0',         'colorama': '0.4.4',         'commonmark': '0.9.1',         'cookiecutter': '1.7.3',         'coverage': '6.2',         'cryptography': '36.0.1',         'cycler': '0.11.0',         'dask': '2022.1.0',         'debugpy': '1.5.1',         'decorator': '5.1.1',         'defusedxml': '0.7.1',         'distlib': '0.3.4',         'dnspython': '2.2.0',         'docstring-parser': '0.13',         'docutils': '0.16',         'elementpath': '2.4.0',         'email-validator': '1.1.3',         'entrypoints': '0.3',         'executing': '0.8.2',         'fancycompleter': '0.9.1',         'fasteners': '0.17.2',         'fastremap': '1.12.2',         'filelock': '3.4.2',         'flake8': '3.8.4',         'fonttools': '4.28.5',         'freetype-py': '2.2.0',         'frozenlist': '1.3.0',         'fsspec': '2022.1.0',         'furo': '2022.1.2',         'gitdb': '4.0.9',         'gitpython': '3.1.26',         'greenlet': '1.1.2',         'heapdict': '1.0.1',         'hsluv': '5.0.2',         'hypothesis': '6.35.1',         'identify': '2.4.4',         'idna': '3.3',         'imagecodecs': '2021.11.20',         'imageio': '2.10.5',         'imageio-ffmpeg': '0.4.5',         'imagesize': '1.3.0',         'importlib-metadata': '4.10.1',         'iniconfig': '1.1.1',         'install': '1.3.5',         'intervaltree': '3.1.0',         'ipykernel': '6.7.0',         'ipython': '8.0.0',         'ipython-genutils': '0.2.0',         'ipywidgets': '7.6.5',         'jedi': '0.18.1',         'jinja2': '3.0.3',         'jinja2-time': '0.2.0',         'jsonschema': '3.2.0',         'jupyter': '1.0.0',         'jupyter-book': '0.12.1',         'jupyter-cache': '0.4.3',         'jupyter-client': '7.1.1',         'jupyter-console': '6.4.0',         'jupyter-core': '4.9.1',         'jupyter-server': '1.13.3',         'jupyter-server-mathjax': '0.2.3',         'jupyter-sphinx': '0.3.2',         'jupyterlab-pygments': '0.1.2',         'jupyterlab-widgets': '1.0.2',         'jupytext': '1.11.5',         'kiwisolver': '1.3.2',         'latexcodec': '2.0.1',         'linkify-it-py': '1.0.3',         'llvmlite': '0.38.0',         'locket': '0.2.1',         'loguru': '0.5.3',         'lxml': '4.7.1',         'magicgui': '0.3.5.dev18+g78d1687',         'markdown-it-py': '1.1.0',         'markupsafe': '2.0.1',         'matplotlib': '3.5.1',         'matplotlib-inline': '0.1.3',         'mccabe': '0.6.1',         'mdit-py-plugins': '0.2.8',         'meshzoo': '0.9.2',         'mistune': '0.8.4',         'mrc': '0.2.0',         'msgpack': '1.0.3',         'multidict': '5.2.0',         'mypy': '0.931',         'mypy-extensions': '0.4.3',         'myst-nb': '0.13.1',         'myst-parser': '0.15.2',         'napari': '0.4.14rc1.dev4+gcdf58d44b',         'napari-aicsimageio': '0.4.1',         'napari-console': '0.0.4',         'napari-dv': '0.2.7.dev0+g54e1691.d20220128',         'napari-error-reporter': '0.1.dev1+g1b388f2.d20220201',         'napari-hello': '0.0.1',         'napari-math': '0.0.1a0',         'napari-micromanager': '0.0.1rc6.dev14+g5149788.d20220128',         'napari-molecule-reader': '0.1.2.dev1+gc2ec2de',         'napari-plugin-engine': '0.2.0',         'napari-pyclesperanto-assistant': '0.12.0',         'napari-skimage-regionprops': '0.2.9',         'napari-svg': '0.1.6',         'napari-time-slicer': '0.4.2',         'napari-workflows': '0.1.2',         'natsort': '8.0.2',         'nbclient': '0.5.10',         'nbconvert': '6.4.0',         'nbdime': '3.1.1',         'nbformat': '5.1.3',         'nd2': '0.1.4',         'nest-asyncio': '1.5.4',         'networkx': '2.6.3',         'nodeenv': '1.6.0',         'notebook': '6.4.7',         'npe2': '0.1.1',         'numba': '0.55.0',         'numcodecs': '0.9.1',         'numpy': '1.20.3',         'numpydoc': '1.1.0',         'ome-types': '0.2.10',         'opencv-python-headless': '4.5.5.62',         'packaging': '21.3',         'pandas': '1.3.5',         'pandocfilters': '1.5.0',         'paramiko': '2.9.2',         'parso': '0.8.3',         'partd': '1.2.0',         'pathspec': '0.9.0',         'pdbpp': '0.10.3',         'peewee': '3.14.8',         'pep517': '0.12.0',         'pexpect': '4.8.0',         'pickleshare': '0.7.5',         'pillow': '8.4.0',         'pint': '0.18',         'pip': '21.3.1',         'platformdirs': '2.4.1',         'pluggy': '1.0.0',         'pooch': '1.5.2',         'poyo': '0.5.0',         'pre-commit': '2.16.0',         'prometheus-client': '0.12.0',         'prompt-toolkit': '3.0.24',         'psutil': '5.9.0',         'psygnal': '0.2.0',         'ptyprocess': '0.7.0',         'pure-eval': '0.2.1',         'py': '1.11.0',         'pybtex': '0.24.0',         'pybtex-docutils': '1.0.1',         'pyclesperanto-prototype': '0.12.0',         'pycodestyle': '2.8.0',         'pycparser': '2.21',         'pydantic': '1.9.0',         'pydata-sphinx-theme': '0.7.2',         'pyflakes': '2.2.0',         'pygments': '2.11.2',         'pymmcore': '10.1.1.70.5',         'pymmcore-plus': '0.1.8',         'pynacl': '1.5.0',         'pyopencl': '2021.2.13',         'pyopengl': '3.1.5',         'pyparsing': '3.0.6',         'pyperclip': '1.8.2',         'pyrepl': '0.9.0',         'pyro5': '5.13.1',         'pyrsistent': '0.18.1',         'pyside2': '5.15.2.1',         'pytest': '6.2.5',         'pytest-cookies': '0.6.1',         'pytest-cov': '3.0.0',         'pytest-faulthandler': '2.0.1',         'pytest-order': '1.0.1',         'pytest-qt': '4.0.2',         'python-dateutil': '2.8.2',         'python-dotenv': '0.19.2',         'python-slugify': '5.0.2',         'pytomlpp': '1.0.10',         'pytools': '2021.2.9',         'pytz': '2021.3',         'pywavelets': '1.2.0',         'pyyaml': '6.0',         'pyzmq': '22.3.0',         'qtconsole': '5.2.2',         'qtpy': '2.0.0',         'regex': '2021.11.10',         'requests': '2.27.1',         'rich': '11.0.0',         'rmsd': '1.4',         'ruamel.yaml': '0.17.20',         'ruamel.yaml.clib': '0.2.6',         'scikit-image': '0.19.1',         'scipy': '1.7.3',         'semgrep': '0.78.0',         'send2trash': '1.8.0',         'sentry-sdk': '1.5.4',         'serpent': '1.40',         'setuptools': '60.5.0',         'shiboken2': '5.15.2.1',         'six': '1.16.0',         'smmap': '5.0.0',         'sniffio': '1.2.0',         'snowballstemmer': '2.2.0',         'sortedcontainers': '2.4.0',         'soupsieve': '2.3.1',         'sourcery-cli': '0.10.0',         'sphinx': '4.4.0',         'sphinx-autodoc-typehints': '1.12.0',         'sphinx-book-theme': '0.1.10',         'sphinx-comments': '0.0.3',         'sphinx-copybutton': '0.4.0',         'sphinx-external-toc': '0.2.3',         'sphinx-jupyterbook-latex': '0.4.6',         'sphinx-multitoc-numbering': '0.1.3',         'sphinx-panels': '0.6.0',         'sphinx-tabs': '3.2.0',         'sphinx-thebe': '0.0.10',         'sphinx-togglebutton': '0.2.3',         'sphinxcontrib-applehelp': '1.0.2',         'sphinxcontrib-bibtex': '2.2.1',         'sphinxcontrib-devhelp': '1.0.2',         'sphinxcontrib-htmlhelp': '2.0.0',         'sphinxcontrib-jsmath': '1.0.1',         'sphinxcontrib-qthelp': '1.0.3',         'sphinxcontrib-serializinghtml': '1.1.5',         'sqlalchemy': '1.4.29',         'stack-data': '0.1.4',         'superqt': '0.2.5.post2.dev7+ga49bcd7',         'tensorstore': '0.1.16',         'terminado': '0.12.1',         'testpath': '0.5.0',         'text-unidecode': '1.3',         'tifffile': '2021.11.2',         'toml': '0.10.2',         'tomli': '2.0.0',         'toolz': '0.11.2',         'torch': '1.10.1',         'tornado': '6.1',         'tox': '3.24.5',         'tox-conda': '0.9.1',         'tqdm': '4.62.3',         'traitlets': '5.1.1',         'transforms3d': '0.3.1',         'transitions': '0.8.10',         'typed-ast': '1.5.1',         'typer': '0.4.0',         'typing-extensions': '4.0.1',         'uc-micro-py': '1.0.1',         'urllib3': '1.26.8',         'useq-schema': '0.1.1.dev13+g01d1b46.d20220120',         'valerius': '0.2.0',         'virtualenv': '20.13.0',         'vispy': '0.9.4',         'watchdog': '2.1.6',         'wcmatch': '8.3',         'wcwidth': '0.2.5',         'webencodings': '0.5.1',         'websocket-client': '1.2.3',         'wheel': '0.37.1',         'widgetsnbextension': '3.5.2',         'wmctrl': '0.4',         'wrapt': '1.13.3',         'wurlitzer': '3.0.2',         'xarray': '0.20.2',         'xmlschema': '1.9.2',         'yarl': '1.7.2',         'zarr': '2.10.3',         'zipp': '3.7.0'     },     'platform': 'python',     'release': '0.4.14rc1.dev4+gcdf58d44b',     'sdk': {         'integrations': [             'aiohttp',             'argv',             'atexit',             'dedupe',             'excepthook',             'logging',             'modules',             'sqlalchemy',             'stdlib',             'threading',             'tornado'         ],         'name': 'sentry.python',         'packages': [{'name': 'pypi:sentry-sdk', 'version': '1.5.4'}],         'version': '1.5.4'     },     'server_name': '',     'tags': {         'platform.name': 'MacOS 10.15.7',         'platform.system': 'Darwin',         'qtpy.API_NAME': 'PySide2',         'qtpy.QT_VERSION': '5.15.2'     },     'timestamp': '2022-02-02T01:30:00.229122Z' } ```    NOTE: in the opt-in dialog, there is a checkbox labeled \"include local variables\", checking this will include the value of variables in the local scope when an exception occurs.  While these can be very useful when interpreting a bug report, they may occasionally include local file path strings.  If that concerns you, please leave this box unchecked  Install This plugin requires napari version 0.4.15 or greater, or the main branch with PR napari/napari#4055. Install via pip with: sh pip install napari-error-reporter or in the built-in plugin installer (a restart will be required):  Thank you!! Privacy FAQ Even with the multiple layers of opt-ins, and the attempts to wipe all personal info prior to sending reports, we understand that privacy is always a concern. Do you collect personal info? We make every attempt to collect no personally identifiable information.  No name, location, IP address, etc...  We do collect your (uuid.getnode()) to be able to track bug resolution over time. As mentioned above, allowing local variables to be collected may occasionally include a file path in the log. If that concerns you, please leave that unchecked. Is this shipped with napari? napari-error-reporter is not bundled with napari or listed as a napari dependency. In order for reports to be sent, you must first install this plugin yourself, and then opt in on the next launch.  If you uninstall the plugin, no more reports can be sent. Who can access these reports? Only the following napari core developers have access to these reports. If this list changes in the future, you will be asked to opt-in again in napari:  Juan Nunez-Iglesias (@jni) Talley Lambert (@tlambert03)  This plugin is not associated with the Chan Zuckerberg Initiative. How will these reports be used? Commonly occuring errors will be will be manually purged of file paths and local variables and posted to https://github.com/napari/napari/issues How long is data retained Sentry retains event data for 90 days by default.  For complete details, see Sentry's page on Security & Compliance",
    "development_status": ["Development Status :: 3 - Alpha"],
    "display_name": "Napari Error Reporter",
    "documentation": "",
    "first_released": "2022-02-13T12:21:26.571245Z",
    "license": "BSD-3-Clause",
    "name": "napari-error-reporter",
    "npe2": true,
    "operating_system": [],
    "plugin_types": [],
    "project_site": "https://github.com/tlambert03/napari-error-reporter",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-06-21T16:36:23.036467Z",
    "report_issues": "",
    "requirements": [
      "appdirs",
      "qtpy",
      "sentry-sdk",
      "black ; extra == 'dev'",
      "flake8 ; extra == 'dev'",
      "flake8-docstrings ; extra == 'dev'",
      "ipython ; extra == 'dev'",
      "isort ; extra == 'dev'",
      "jedi (<0.18.0) ; extra == 'dev'",
      "mypy ; extra == 'dev'",
      "pre-commit ; extra == 'dev'",
      "pydocstyle ; extra == 'dev'",
      "pytest ; extra == 'dev'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "tox ; extra == 'testing'",
      "tox-conda ; extra == 'testing'"
    ],
    "summary": "Opt-in automated bug/error reporting for napari",
    "support": "",
    "twitter": "",
    "version": "0.3.1",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Marcelo Leomil Zoccoler" }],
    "code_repository": "https://github.com/zoccoler/napari-metroid",
    "conda": [],
    "description": "# napari-metroid  [![License](https://img.shields.io/pypi/l/napari-metroid.svg?color=green)](https://github.com/zoccoler/napari-metroid/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-metroid.svg?color=green)](https://pypi.org/project/napari-metroid) [![Python Version](https://img.shields.io/pypi/pyversions/napari-metroid.svg?color=green)](https://python.org) [![tests](https://github.com/zoccoler/napari-metroid/workflows/tests/badge.svg)](https://github.com/zoccoler/napari-metroid/actions) [![codecov](https://codecov.io/gh/zoccoler/napari-metroid/branch/main/graph/badge.svg)](https://codecov.io/gh/zoccoler/napari-metroid) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-metroid)](https://napari-hub.org/plugins/napari-metroid)  This napari plugin is an adaptation of [metroid](https://github.com/zoccoler/metroid). It creates several regions of interest of similar area over cells in a fluorescence video (2D+time). It then gets ROIs means over time and performs signal denoising: fixes photobleaching and separates signal from noise by means of blind source separation (with or without wavelet filtering).  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/plugins/stable/index.html -->  ## A Picture (to boil down a thousand words)  Below is the graphical abstract of the Metroid software. This napari plugin works very similarly.  ![](https://github.com/zoccoler/metroid/blob/master/Metroid_flowchart.png)  ## Table of Contents  - [Quick Walktrough](#quick-walkthrough) - [Installation](#installation) - [Usage](#usage)   - [Open Sample Data](#open-sample-data)   - [Open Plugin Main Interface](#open-plugin-main-interface)   - [Auto-generate Cell Mask](#auto-generate-cell-mask)   - [Split Mask into ROIs](#split-mask-into-rois)   - [Get ROI Means over Time](#get-roi-means-over-time)   - [Remove Photobleaching](#remove-photobleaching)   - [Filter Signals](#filter-signals)   - [Save outputs](#save-outputs) - [Contributing](#contributing) - [Citing napari-metroid](#citing-napari-metroid) - [License](#license) - [Issues](#issues)  ## Quick Walkthrough  Below is a full demonstration of using napari-metroid. It shows the following:   * Open sample data;   * Create cell mask;   * Split mask into ROIs of similar area;   * Get ROIs signals over time and plots two of them;   * Remove photobleaching;   * Remove noise:     * Use ICA to decompose ROIs signals into independent components;     * Plot 4 components;     * Manually select the component of interest (source);     * Perform inverse transformation with selected source;          ![](https://github.com/zoccoler/napari-metroid/raw/main/figures/napari_metroid_demo.gif)  ## Installation  Download and install [Anaconda](https://www.anaconda.com/products/individual) or [Miniconda](https://docs.conda.io/en/latest/miniconda.html#).  Create a new conda environment:      conda create -n metroid-env python=3.8  Install napari, e.g. via pip:      pip install \"napari[all]\"  Install `napari-metroid` via [pip]:      pip install napari-metroid  To install latest development version :      pip install git+https://github.com/zoccoler/napari-metroid.git  ## Usage ### Open Sample Data  This plugin comes with two sample videos: - Cell1 Video Action Potential: 2D + time fluorescence video of a rat isolated cardiomyocyte labeled with a membrane potential dye upon which an external electrical field pulse is applied. - Cell1 Video Electroporation: Same cell, but submitted to a strong external electrical field pulse.  You can open them under \"File -> Open Sample -> napari-metroid\", as shown below. Both videos are loaded from the [metroid main repository](https://github.com/zoccoler/metroid). To know more about the experimental conditions, please refer to the [original publication](https://doi.org/10.1186/s12859-020-03661-9).  ![](https://github.com/zoccoler/napari-metroid/raw/main/figures/load_sample_data.gif)  ### Open Plugin Main Interface  ![](https://github.com/zoccoler/napari-metroid/raw/main/figures/open_plugin.gif)  ### Auto-generate Cell Mask  Metroid can generate cell binary masks automatically by cumulative sum of images until any pixel saturation happens. It then applies Otsu thresholding and removes small objects.  ![](https://github.com/zoccoler/napari-metroid/raw/main/figures/auto_create_mask.png)  ### Split Mask into ROIs  By default, a cell mask is split into 32 regions of interest (ROIs) in a double-layer fashion: An outer layer of ROIs and an inner layer.  The method is solely based on the shape of the cell mask and the main criteria is that ROIs must have similar areas. The number of ROIs in each layer can be editted.   ![](https://github.com/zoccoler/napari-metroid/raw/main/figures/mess.png)  ### Get ROI Means over Time  The 'Get Signals' button serves to collect each ROI mean fluorescence over time and enable plotting. There, you can optionally provide the frame rate so that the time axis is properly displayed. Double click over a ROI to have its signal plotted. Hold the 'ALT' key to plot multiple signals together.  ![](https://github.com/zoccoler/napari-metroid/raw/main/figures/get_signals.gif)  ### Remove Photobleaching  Metroid removes photobleaching by curve fitting over time periods that lack the cellular signal (which can be an action potential or an electroporation signal). That is why the 'Transitory' parameter is important. Action potentials are transitory signals whereas electroporation (at least for the duration of this experiment) are not, and the algorithm must be informed about that for proper trend removal.  ![](https://github.com/zoccoler/napari-metroid/raw/main/figures/remov_photob.gif)  ### Filter Signals  Cellular signals are filtered by separating signal components with either PCA or ICA (plus optional wavelet filtering). It then chooses one (or several) components and it applies the inverse transform using only the selected components. Metroid can do this component/source selection automatically based on estimations of signal power. Instead, we show below the manual selection procedure, where 4 components are plotted and the user selects one of them.  ![](https://github.com/zoccoler/napari-metroid/raw/main/figures/bssd.gif)  ### Save Outputs  Raw, corrected and filtered signals, as well as time and components, are arranged in a table with values for each time point. The table is displayed as a widget after each Run button click. Estimated signal-to-noise (SNR) in dB for each label/ROI are also provided (in this case, each line corresponds to a ROI, not a time point). The user can save these data by clicking on the buttons \"Copy to clipboard\" or \"Save as csv...\".  ![](https://github.com/zoccoler/napari-metroid/raw/main/figures/table_widget.png)  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## Citing napari-metroid  If you use this plugin in your research, please be kind to cite the original paper below:  Zoccoler, M., de Oliveira, P.X. METROID: an automated method for robust quantification of subcellular fluorescence events at low SNR. BMC Bioinformatics 21, 332 (2020). https://doi.org/10.1186/s12859-020-03661-9  ## License  Distributed under the terms of the [BSD-3] license, \"napari-metroid\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/zoccoler/napari-metroid/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-metroid       This napari plugin is an adaptation of metroid. It creates several regions of interest of similar area over cells in a fluorescence video (2D+time). It then gets ROIs means over time and performs signal denoising: fixes photobleaching and separates signal from noise by means of blind source separation (with or without wavelet filtering).  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  A Picture (to boil down a thousand words) Below is the graphical abstract of the Metroid software. This napari plugin works very similarly.  Table of Contents  Quick Walktrough Installation Usage Open Sample Data Open Plugin Main Interface Auto-generate Cell Mask Split Mask into ROIs Get ROI Means over Time Remove Photobleaching Filter Signals Save outputs Contributing Citing napari-metroid License Issues  Quick Walkthrough Below is a full demonstration of using napari-metroid. It shows the following:   * Open sample data;   * Create cell mask;   * Split mask into ROIs of similar area;   * Get ROIs signals over time and plots two of them;   * Remove photobleaching;   * Remove noise:     * Use ICA to decompose ROIs signals into independent components;     * Plot 4 components;     * Manually select the component of interest (source);     * Perform inverse transformation with selected source;  Installation Download and install Anaconda or Miniconda. Create a new conda environment: conda create -n metroid-env python=3.8  Install napari, e.g. via pip: pip install \"napari[all]\"  Install napari-metroid via pip: pip install napari-metroid  To install latest development version : pip install git+https://github.com/zoccoler/napari-metroid.git  Usage Open Sample Data This plugin comes with two sample videos: - Cell1 Video Action Potential: 2D + time fluorescence video of a rat isolated cardiomyocyte labeled with a membrane potential dye upon which an external electrical field pulse is applied. - Cell1 Video Electroporation: Same cell, but submitted to a strong external electrical field pulse. You can open them under \"File -> Open Sample -> napari-metroid\", as shown below. Both videos are loaded from the metroid main repository. To know more about the experimental conditions, please refer to the original publication.  Open Plugin Main Interface  Auto-generate Cell Mask Metroid can generate cell binary masks automatically by cumulative sum of images until any pixel saturation happens. It then applies Otsu thresholding and removes small objects.  Split Mask into ROIs By default, a cell mask is split into 32 regions of interest (ROIs) in a double-layer fashion: An outer layer of ROIs and an inner layer.  The method is solely based on the shape of the cell mask and the main criteria is that ROIs must have similar areas. The number of ROIs in each layer can be editted.   Get ROI Means over Time The 'Get Signals' button serves to collect each ROI mean fluorescence over time and enable plotting. There, you can optionally provide the frame rate so that the time axis is properly displayed. Double click over a ROI to have its signal plotted. Hold the 'ALT' key to plot multiple signals together.  Remove Photobleaching Metroid removes photobleaching by curve fitting over time periods that lack the cellular signal (which can be an action potential or an electroporation signal). That is why the 'Transitory' parameter is important. Action potentials are transitory signals whereas electroporation (at least for the duration of this experiment) are not, and the algorithm must be informed about that for proper trend removal.  Filter Signals Cellular signals are filtered by separating signal components with either PCA or ICA (plus optional wavelet filtering). It then chooses one (or several) components and it applies the inverse transform using only the selected components. Metroid can do this component/source selection automatically based on estimations of signal power. Instead, we show below the manual selection procedure, where 4 components are plotted and the user selects one of them.  Save Outputs Raw, corrected and filtered signals, as well as time and components, are arranged in a table with values for each time point. The table is displayed as a widget after each Run button click. Estimated signal-to-noise (SNR) in dB for each label/ROI are also provided (in this case, each line corresponds to a ROI, not a time point). The user can save these data by clicking on the buttons \"Copy to clipboard\" or \"Save as csv...\".  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. Citing napari-metroid If you use this plugin in your research, please be kind to cite the original paper below: Zoccoler, M., de Oliveira, P.X. METROID: an automated method for robust quantification of subcellular fluorescence events at low SNR. BMC Bioinformatics 21, 332 (2020). https://doi.org/10.1186/s12859-020-03661-9 License Distributed under the terms of the BSD-3 license, \"napari-metroid\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari METROID",
    "documentation": "https://github.com/zoccoler/napari-metroid#README.md",
    "first_released": "2022-03-24T07:40:31.950920Z",
    "license": "BSD-3-Clause",
    "name": "napari-metroid",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["writer", "widget", "sample_data"],
    "project_site": "https://github.com/zoccoler/napari-metroid",
    "python_version": "<3.9,>=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-07-20T10:56:14.990238Z",
    "report_issues": "https://github.com/zoccoler/napari-metroid/issues",
    "requirements": [
      "numpy",
      "scikit-learn",
      "scikit-image",
      "statsmodels",
      "scipy",
      "matplotlib",
      "napari-skimage-regionprops (>=0.3.1)"
    ],
    "summary": "This napari plugin creates several regions of interest of similar area over cells in a fluorescence video (2D+time). It then gets ROIs means over time and performs signal denoising: fixes photobleaching and separates signal from noise by means of blind source separation (with or without wavelet filtering).",
    "support": "https://github.com/zoccoler/napari-metroid/issues",
    "twitter": "",
    "version": "0.0.5",
    "visibility": "public",
    "writer_file_extensions": [".npy"],
    "writer_save_layers": ["labels*", "image", "image*"]
  },
  {
    "authors": [{ "name": "Marc Boucsein" }, { "name": "Marc Buckmakowski" }],
    "code_repository": "https://github.com/MBPhys/napari-medical-image-formats",
    "conda": [
      { "channel": "conda-forge", "package": "napari-medical-image-formats" }
    ],
    "description": "# napari-medical-image-formats  [![License](https://img.shields.io/pypi/l/napari-medical-image-formats.svg?color=green)](https://github.com/MBPhys/napari-medical-image-formats/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-medical-image-formats.svg?color=green)](https://pypi.org/project/napari-medical-image-formats) [![Python Version](https://img.shields.io/pypi/pyversions/napari-medical-image-formats.svg?color=green)](https://python.org)   A Plugin in order to read and write medical image formats such as DICOM, DICOM Series and NIfTI. The meta information is supported by the package napari-itk-io.   ----------------------------------  ## Installation  You can install `napari-medical-image-formats` via [pip]:      pip install napari-medical-image-formats  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-medical-image-formats\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin [file an issue]: https://github.com/MBPhys/napari-medical-image-formats/issues [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-medical-image-formats    A Plugin in order to read and write medical image formats such as DICOM, DICOM Series and NIfTI. The meta information is supported by the package napari-itk-io.   Installation You can install napari-medical-image-formats via pip: pip install napari-medical-image-formats  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-medical-image-formats\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "napari-medical-image-formats",
    "documentation": "",
    "first_released": "2021-04-24T14:22:51.473417Z",
    "license": "BSD-3-Clause",
    "name": "napari-medical-image-formats",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader", "writer"],
    "project_site": "https://github.com/MBPhys/napari-medical-image-formats",
    "python_version": ">=3.7",
    "reader_file_extensions": ["*"],
    "release_date": "2022-01-11T09:47:49.304049Z",
    "report_issues": "",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "pydicom",
      "SimpleITK",
      "itk",
      "itk-napari-conversion"
    ],
    "summary": "A Plugin in order to read medical image formats such as DICOM and NIfTI",
    "support": "",
    "twitter": "",
    "version": "0.3.8",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": ["image", "labels"]
  },
  {
    "authors": [{ "name": "Blik Team" }],
    "code_repository": "https://github.com/gutsche-lab/blik",
    "description": "![logo](https://github.com/gutsche-lab/blik/raw/main/docs/images/logo.png)  # `blik`  ![Codecov branch](https://img.shields.io/codecov/c/github/gutsche-lab/blik/main?label=codecov) ![PyPI](https://img.shields.io/pypi/v/blik) ![PyPI - Python Version](https://img.shields.io/pypi/pyversions/blik)  *it means glance in Dutch*  ![blik showcase](https://user-images.githubusercontent.com/23482191/161224963-ad746a06-c2e5-46fe-a13b-f356bc4ad72b.png)  **`blik`** is a tool for visualising and interacting with cryo-ET and subtomogram averaging data. It leverages the fast, multi-dimensional [napari viewer](https://napari.org) and the scientific python stack.  **DISCLAIMER**: this package is in development phase. Expect frequent bugs and crashes. Please, report them on the issue tracker and ask if anything is unclear!  ## Installation  You can either install `blik` through the [napari plugin system](https://napari.org/plugins/index.html), through pip, or get both napari and blik directly with:  ```bash pip install \"blik[all]\" ```  The `[all]` qualifier also installs `pyqt5` as the napari GUI backend, and a few additional napari plugins that you might find useful in your workflow: - [napari-properties-plotter](https://github.com/brisvag/napari-properties-plotter) - [napari-properties-viewer](https://github.com/kevinyamauchi/napari-properties-viewer) - [napari-label-interpolator](https://github.com/kevinyamauchi/napari-label-interpolator)  ## Basic Usage  From the command line: ```bash napari -w blik -- /path/to.star /path/to/mrc/files/* ```  The `-w blik` is important for proper initialization of all the layers. Keep the main widget open to ensure nothing goes wrong!  *`blik` is just `napari`*. Particles and images are exposed as simple napari layers, which can be analysed and manipulated with simple python, and most importantly other [napari plugins](https://napari-hub.org/).  ## Widget  The main widget has a few functions:  ",
    "description_content_type": "text/markdown",
    "description_text": " blik    it means glance in Dutch  blik is a tool for visualising and interacting with cryo-ET and subtomogram averaging data. It leverages the fast, multi-dimensional napari viewer and the scientific python stack. DISCLAIMER: this package is in development phase. Expect frequent bugs and crashes. Please, report them on the issue tracker and ask if anything is unclear! Installation You can either install blik through the napari plugin system, through pip, or get both napari and blik directly with: bash pip install \"blik[all]\" The [all] qualifier also installs pyqt5 as the napari GUI backend, and a few additional napari plugins that you might find useful in your workflow: - napari-properties-plotter - napari-properties-viewer - napari-label-interpolator Basic Usage From the command line: bash napari -w blik -- /path/to.star /path/to/mrc/files/* The -w blik is important for proper initialization of all the layers. Keep the main widget open to ensure nothing goes wrong! blik is just napari. Particles and images are exposed as simple napari layers, which can be analysed and manipulated with simple python, and most importantly other napari plugins. Widget The main widget has a few functions:",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "blik",
    "documentation": "https://github.com/gutsche-lab/blik#README.md",
    "first_released": "2021-06-15T13:05:03.510893Z",
    "license": "GPL-3.0",
    "name": "blik",
    "npe2": true,
    "operating_system": [
      "Operating System :: MacOS",
      "Operating System :: Microsoft :: Windows",
      "Operating System :: POSIX",
      "Operating System :: Unix"
    ],
    "plugin_types": ["reader", "writer", "widget"],
    "project_site": "https://github.com/gutsche-lab/blik",
    "python_version": ">=3.8",
    "reader_file_extensions": [
      "*.em",
      "*.box",
      "*.mrcs",
      "*.mrc",
      "*.tbl",
      "*.star",
      "*.cbox",
      "*.st"
    ],
    "release_date": "2022-12-07T11:29:01.544622Z",
    "report_issues": "https://github.com/gutsche-lab/blik/issues",
    "requirements": [
      "numpy",
      "dask",
      "pandas",
      "scipy",
      "magicgui (>=0.4.0)",
      "cryohub (>=0.3.2)",
      "cryotypes",
      "napari[all] (>=0.4.17) ; extra == 'all'",
      "napari-properties-plotter ; extra == 'all'",
      "napari-properties-viewer ; extra == 'all'",
      "napari-label-interpolator ; extra == 'all'",
      "black ; extra == 'dev'",
      "flake8 ; extra == 'dev'",
      "isort ; extra == 'dev'",
      "pre-commit ; extra == 'dev'",
      "napari[all] (>=0.4.17) ; extra == 'dev'",
      "tox ; extra == 'dev'",
      "pytest ; extra == 'dev'",
      "pytest-qt ; extra == 'dev'",
      "pyqt5 ; extra == 'dev'",
      "napari[all] (>=0.4.17) ; extra == 'testing'",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "summary": "Python tool for visualising and interacting with cryo-ET and subtomogram averaging data.",
    "support": "https://github.com/gutsche-lab/blik/issues",
    "twitter": "",
    "version": "0.3.4",
    "visibility": "public",
    "writer_file_extensions": [".mrcs", ".st", ".mrc", ".star"],
    "writer_save_layers": ["labels", "points+", "vectors*", "image"]
  },
  {
    "authors": [{ "name": "Herearii Metuarea" }],
    "code_repository": "https://github.com/hereariim/napari-pixel-correction",
    "conda": [],
    "description": "<!-- This file is a placeholder for customizing description of your plugin  on the napari hub if you wish. The readme file will be used by default if you wish not to do any customization for the napari hub listing.  If you need some help writing a good description, check out our  [guide](https://github.com/chanzuckerberg/napari-hub/wiki/Writing-the-Perfect-Description-for-your-Plugin) -->  Plugin to correct manually pixel wrongly predicted on image by annotation",
    "description_content_type": "text/markdown",
    "description_text": " Plugin to correct manually pixel wrongly predicted on image by annotation",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "Pixel correction",
    "documentation": "https://github.com/hereariim/napari-pixel-correction#README.md",
    "first_released": "2022-09-19T09:19:12.974227Z",
    "license": "BSD-3-Clause",
    "name": "napari-pixel-correction",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader", "writer", "widget"],
    "project_site": "https://github.com/hereariim/napari-pixel-correction",
    "python_version": ">=3.8",
    "reader_file_extensions": ["*.npy"],
    "release_date": "2022-09-21T15:24:28.107693Z",
    "report_issues": "https://github.com/hereariim/napari-pixel-correction/issues",
    "requirements": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "napari",
      "matplotlib",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "summary": "Plugin to correct manually pixel wrongly predicted on image by annotation",
    "support": "https://github.com/hereariim/napari-pixel-correction/issues",
    "twitter": "",
    "version": "0.1.4",
    "visibility": "public",
    "writer_file_extensions": [".npy"],
    "writer_save_layers": ["image*", "image", "labels*"]
  },
  {
    "authors": [{ "name": "Reka Hollandi", "orcid": "0000-0002-4052-6846" }],
    "category": {
      "Image modality": [
        "Fluorescence microscopy",
        "Confocal microscopy",
        "Electron microscopy",
        "Medical imaging"
      ],
      "Supported data": ["2D", "Multi-channel"],
      "Workflow step": [
        "Image annotation",
        "Image Segmentation",
        "Object classification"
      ]
    },
    "category_hierarchy": {
      "Image modality": [
        ["Fluorescence microscopy"],
        ["Confocal microscopy"],
        ["Electron microscopy"],
        ["Medical imaging"]
      ],
      "Supported data": [["2D"], ["Multi-channel"]],
      "Workflow step": [
        ["Image annotation"],
        ["Image Segmentation"],
        ["Image Segmentation", "Cell segmentation"],
        ["Image Segmentation", "Model-based segmentation"],
        ["Object classification"]
      ]
    },
    "code_repository": "https://github.com/spreka/napari-annotatorj",
    "conda": [{ "channel": "conda-forge", "package": "napari-annotatorj" }],
    "description": "  <!-- This file is designed to provide you with a starting template for documenting the functionality of your plugin. Its content will be rendered on your plugin's napari hub page.  The sections below are given as a guide for the flow of information only, and are in no way prescriptive. You should feel free to merge, remove, add and  rename sections at will to make this document work best for your plugin.   # Description  This should be a detailed description of the context of your plugin and its  intended purpose.  If you have videos or screenshots of your plugin in action, you should include them here as well, to make them front and center for new users.   You should use absolute links to these assets, so that we can easily display them  on the hub. The easiest way to include a video is to use a GIF, for example hosted on imgur. You can then reference this GIF as an image.  ![Example GIF hosted on Imgur](https://i.imgur.com/A5phCX4.gif)  Note that GIFs larger than 5MB won't be rendered by GitHub - we will however, render them on the napari hub.  The other alternative, if you prefer to keep a video, is to use GitHub's video embedding feature.  1. Push your `DESCRIPTION.md` to GitHub on your repository (this can also be done as part of a Pull Request) 2. Edit `.napari/DESCRIPTION.md` **on GitHub**. 3. Drag and drop your video into its desired location. It will be uploaded and hosted on GitHub for you, but will not be placed in your repository. 4. We will take the resolved link to the video and render it on the hub.  Here is an example of an mp4 video embedded this way.  https://user-images.githubusercontent.com/17995243/120088305-6c093380-c132-11eb-822d-620e81eb5f0e.mp4  # Intended Audience & Supported Data  This section should describe the target audience for this plugin (any knowledge, skills and experience required), as well as a description of the types of data supported by this plugin.  Try to make the data description as explicit as possible, so that users know the format your plugin expects. This applies both to reader plugins reading file formats and to function/dock widget plugins accepting layers and/or layer data. For example, if you know your plugin only works with 3D integer data in \"tyx\" order, make sure to mention this.  If you know of researchers, groups or labs using your plugin, or if it has been cited anywhere, feel free to also include this information here.  # Quickstart  This section should go through step-by-step examples of how your plugin should be used. Where your plugin provides multiple dock widgets or functions, you should split these out into separate subsections for easy browsing. Include screenshots and videos wherever possible to elucidate your descriptions.   Ideally, this section should start with minimal examples for those who just want a quick overview of the plugin's functionality, but you should definitely link out to more complex and in-depth tutorials highlighting any intricacies of your plugin, and more detailed documentation if you have it.  # Additional Install Steps (uncommon) We will be providing installation instructions on the hub, which will be sufficient for the majority of plugins. They will include instructions to pip install, and to install via napari itself.  Most plugins can be installed out-of-the-box by just specifying the package requirements over in `setup.cfg`. However, if your plugin has any more complex dependencies, or  requires any additional preparation before (or after) installation, you should add  this information here.  # Getting Help  This section should point users to your preferred support tools, whether this be raising an issue on GitHub, asking a question on image.sc, or using some other method of contact. If you distinguish between usage support and bug/feature support, you should state that here.  # How to Cite  Many plugins may be used in the course of published (or publishable) research, as well as during conference talks and other public facing events. If you'd like to be cited in a particular format, or have a DOI you'd like used, you should provide that information here. -->  # Description This plugin allows easy object annotation on 2D images. Annotation is made quick, easy and fun, just start drawing! See a [quick start](#quick-start) guide below.  ![image](https://drive.google.com/uc?export=view&id=1fVfvanffTdrXvLE0m1Yo6FV5TAjh6sb2)  It is the napari version of the ImageJ plugin [AnnotatorJ](https://github.com/spreka/annotatorj).  ## What kind of data it works on **2D images**. That's the only requirement. Whether you have microscopy images of cells or tissue, natural photos of cats and dogs, vehichle dash-cam footage, industrial pipeline monitoring etc., just open the image and you can start annotating.  Annotations are save to ImageJ-compatible roi.zip files. [Export](#export) is possible to several file formats depending on the intended application.  ## Intended users **Anyone**. No experience in computer science or underlying technology is needed; if you know how to use MS Paint, you are ready to start annotating. Biologists, programmers, even children can use it. See [quick start](#quick-start) guide or [demos](#demo). If you experience any issues or have questions, feel free to open an [issue](https://github.com/spreka/napari-annotatorj/issues) on GitHub.  ## Main features  Why choose napari-annotatorj? - Assisted annotation is possible with automatic deep learning-based [contour suggestion](#contour-assist-mode), - freehand contour drawing in [instance annotation](#instance-annotation), - shape [editing](#edit-mode) via painting labels, - [class annotation](#class-mode), - [export](#export) to formats directly suitable for deep CNN training - import of previous annotations as [overlay](#overlay); e.g. when comparing annotations or curating - and more.  See [demos](#demo).  ## Quick start Demo data is available in the GitHub repository's [demo](https://github.com/spreka/napari-annotatorj/tree/main/demo) folder.  napari-annotatorj has several convenient functions to speed up the annotation process, make it easier and more fun. These *modes* can be activated by their corresponding checkboxes on the left side of the main AnnotatorJ widget.  - [Contour assist mode](#contour-assist-mode) - [Edit mode](#edit-mode) - [Class mode](#class-mode) - [Overlay](#overlay)  Freehand drawing is enabled in the plugin. The \"Add polygon\" tool is selected by default upon startup. To draw a freehand object (shape) simply hold the mouse and drag it around the object. The contour is visualized when the mouse button is released.  See the [guide](#how-to-annotate) below or a [demo](#demo-scripts) script.  ## Instance annotation Allows freehand drawing of object contours (shapes) with the mouse as in ImageJ.  Shape contour points are tracked automatically when the left mouse button is held and dragged to draw a shape. The shape is closed when the mouse button is released, automatically, and added to the default shapes layer (named \"ROI\"). In direct selection mode (from the layer controls panel), you can see the saved contour points. The slower you drag the mouse, the more contour points saved, i.e. the more refined your contour will be.  Click to watch demo video below.  [![instance-annot-demo](https://drive.google.com/uc?export=view&id=1sBg19d_hqGH-UI8irkrwame7ZjrldwHr)](https://drive.google.com/uc?export=view&id=1wELreE9MdCZq4Kf4oCWdxIw4e5o05XzK \"Click to watch instance annotation demo\")  ## How to annotate  1. Open --> opens an image 2. (Optionally)  \\t- ... --> Select annotation type --> Ok --> a default tool is selected from the toolbar that fits the selected annotation type \\t- The default annotation type is instance \\t- Selected annotation type is saved to a config file 3. Start annotating objects \\t- [instance](#instance-annotation): draw contours around objects \\t- [semantic](#semantic-annotation): paint the objects' area \\t- [bounding box](#bounding-box-annotation): draw rectangles around the objects 4. Save --> Select class --> saves the annotation to a file in a sub-folder of the original image folder with the name of the selected class  5. (Optionally) \\t- Load --> continue a previous annotation \\t- Overlay --> display a different annotation as overlay (semi-transparent) on the currently opened image \\t- Colours --> select annotation and overlay colours \\t- ... (coming soon) --> set options for semantic segmentation and *Contour assist* mode \\t- checkboxes --> Various options \\t\\t- (default) Add automatically --> adds the most recent annotation to the ROI list automatically when releasing the left mouse button \\t\\t- Smooth (coming soon) --> smooths the contour (in instance annotation type only) \\t\\t- Show contours --> displays all the contours in the ROI list \\t\\t- Contours assist --> suggests a contour in the region of an initial, lazily drawn contour using the deep learning method U-Net \\t\\t- Show overlay --> displays the overlayed annotation if loaded with the Overlay button \\t\\t- Edit mode --> edits a selected, already saved contour in the ROI list by clicking on it on the image \\t\\t- Class mode --> assigns the selected class to the selected contour in the ROI list by clicking on it on the image and displays its contour in the class's colour (can be set in the Class window); clicking on the object a second time unclassifies it \\t- [^] --> quick export in 16-bit multi-labelled .tiff format; if classified, also exports by classes   ## Semantic annotation Allows painting with the brush tool (labels).  Useful for semantic (e.g. scene) annotation. Currently saves all labels to binary mask only (foreground-background).  ## Bounding box annotation Allows drawing bounding boxes (shapes, rectangles) around objects with the mouse.  Useful for object detection annotation.   ## Demo ## Instance annotation mode See [above](#instance-annotation).  ## Contour assist mode Assisted annotation via a pre-trained deep learning model's suggested contour.  1. initialize a contour with mouse drag around an object 2. the suggested contour is displayed automatically 3. modify the contour:     - edit with mouse drag or      - erase holding \"Alt\" or \\t- invert with pressing \"u\" 4. finalize it     - accept with pressing \"q\" or     - reject with pressing \"Ctrl\" + \"Del\"  - if the suggested contour is a merge of multiple objects, you can erase the dividing line around the object you wish to keep, and keep erasing (or splitting with the eraser) until the object you wish to keep is the largest, then press \"q\" to accept it - this mode requires a Keras model to be present in the [model folder](#configure-model-folder)  Click to watch demo video below  [![contour-assist-demo](https://drive.google.com/uc?export=view&id=1Mw2fCPdm5WHBVRgNnp8fGNmqxI84F_9I)](https://drive.google.com/uc?export=view&id=1VTd6RScjNfAwi3vMk-bU87U4ucPmOO_M \"Click to watch contour assist demo\")   ## Edit mode Allows to modify created objects with a brush tool.  1. select an object (shape) to modify by clicking on it 2. an editing layer (labels layer) is created for painting automatically 3. modify the contour:     - edit with mouse drag or      - erase holding \"Alt\" 4. finalize it     - accept with pressing \"q\" or     - delete with pressing \"Ctrl\" + \"Del\" or     - revert changes with pressing \"Esc\" (to the state before editing)  - if the edited contour is a merge of multiple objects, you can erase the dividing line around the object you wish to keep, and keep erasing (or splitting with the eraser) until the object you wish to keep is the largest, then press \"q\" to accept it  Click to watch demo video below  [![edit-mode-demo](https://drive.google.com/uc?export=view&id=1M-XdEWPXMsIOtO0ncyUtvGACS0SRX-3K)](https://drive.google.com/uc?export=view&id=10MQm53hblLKQlfBNrfUsi1vxvIdTbzCZ \"Click to watch edit mode demo\")   ## Class mode Allows to assign class labels to objects by clicking on shapes.  1. select a class from the class list to assign 2. click on an object (shape) to assign the selected class label to it 3. the contour colour of the clicked object will be updated to the selected class colour, plus the class label is updated in the text properties of object (turn on \"display text\" on the layer control panel to see the text properties as `objectID:(classLabel)` e.g. 1:(0) for the first object)  - optionally, you can set a default class for all currently unlabelled objects on the ROI (shapes) layer by selecting a class from the drop-down menu on the right to the text label \"Default class\" - class colours can be changed with the drop-down menu right to the class list; upon selection, all objects whose class label is the currently selected class will have their contour colour updated to the selected colour - clicking on an object that has already been assigned a class label will unclassify it: assign the label *0* to it  Click to watch demo video below  [![class-mode-demo](https://drive.google.com/uc?export=view&id=1EV1cn_mySO11S_ZDFv6Dl1laAk30jGJk)](https://drive.google.com/uc?export=view&id=1uOmznUvfHEFvviWTtOnUHty8rkKyWR7Q \"Click to watch class mode demo\")   ## Export See also: [Quick export](#quick-export)  The exporter plugin AnnotatorJExport can be invoked from the Plugins menu under the plugin name `napari-annotatorj`. It is used for batch export of annotations to various formats directly suitable to train different types of deep learning models. See a [demonstrative figure](https://raw.githubusercontent.com/spreka/annotatorj/master/demos/annotation_and_export_types.png) in the [AnnotatorJ repository](https://github.com/spreka/annotatorj) and further description in its [README](https://github.com/spreka/annotatorj#export) or [documentation](https://github.com/spreka/annotatorj/blob/master/AnnotatorJ_documentation.pdf).  1. browse original image folder with either the     - \"Browse original...\" button or     - text input field next to it 2. browse annotation folder with either the     - \"Browse annot...\" button or     - text input field next to it 3. select the export options you wish to export the annotations to (see tooltips on hover for help)     - at least one export option must be selected to start export     - (optional) right click on the checkbox \"Coordinates\" to switch between the default COCO format and YOLO format; see [explanation](#coordinate-formats) 4. click on \"Export masks\" to start the export     - this will open a progress bar in the napari window and close it upon finish  The folder structure required by the exporter is as follows:  ``` image_folder \\t|--- image1.png \\t|--- another_image.png \\t|--- something.png \\t|--- ...  annotation_folder \\t|--- image1_ROIs.zip \\t|--- another_image_ROIs.zip \\t|--- something_ROIs.zip \\t|--- ... ```  Multiple export options can be selected at once, any selected will create a subfolder in the folder where the annotations are saved.  Click to watch demo video below  [![exporter-demo](https://drive.google.com/uc?export=view&id=1QoaJrI9pKziUzYwiZNdWlfRD7PcvJB9U)](https://drive.google.com/uc?export=view&id=1uJz-x_ypEOjc7SYPUTqrEt0ieyNLFy6u \"Click to watch exporter demo\")   ## Quick export Click on the \"[^]\" button to quickly save annotations and export to mask image. It saves the current annotations (shapes) to an ImageJ-compatible roi.zip file and a generated a 16-bit multi-labelled mask image to the subfolder \"masks\" under the current original image's folder.   ## Coordinate formats In the AnnotatorJExport plugin 2 coordinates formats can be selecting by right clicking on the Coordinates checkbox: COCO or YOLO. The default is COCO.  *COCO format*: - `[x, y, width, height]` based on the top-left corner of the bounding box around the object - coordinates are not normalized - annotations are saved with header to      - .csv file     - tab delimeted  *YOLO format*: - `[class, x, y, width, height]` based on the center point of the bounding box around the object - coordinates are normalized to the image size as floating point values between 0 and 1 - annotations are saved with header to     - .txt file     - whitespace delimeted     - class is saved as the 1st column   ## Overlay A separate annotation file can be loaded as overlay for convenience, e.g. to compare annotations.  1. load another annotation file with the \"Overlay\" button  - (optional) switch its visibility with the \"Show overlay\" checkbox - (optional) change the contour colour of the overlay shapes with the [\"Colours\" button](#change-colours)   ## Change colours Clicking on the \"Colours\" button opens the Colours widget where you can set the annotation and overlay colours.  1. select a colour from the drop-down list either next to the text label \"overlay\" or \"annotation\" 2. click the \"Ok\" button to apply changes  - contour colour of shapes on the annotation shapes layer (named \"ROI\") that already have a class label assigned to them will **not** be updated to the new annotation colour, only those not having a class label (the class label can be displayed with the \"display text\" checkbox on the layer controls panel as `objectID:(classLabel)` e.g. 1:(0) for the first object) - contour colour of shapes on the overlay shapes layer (named \"overlay\") will all have the overlay colour set, regardless of any existing class information saved to the annotation file loaded as overlay   # For coding users ## Demo scripts Run a demo of napari-annotatorj with sample data: a small 3-channel RGB image as original image and an ImageJ roi.zip file as annotations loaded.  ```shell     # from the napari-annotatorj folder \\tpython src/napari_annotatorj/load_imagej_roi.py ``` Alternatively, you can startup the napari-annotatorj plugin by running  ```shell     # from the napari-annotatorj folder \\tpython src/napari_annotatorj/startup_annotatorj.py ```  ## Configure model folder The Contour assist mode imports a pre-trained Keras model from a folder named *models* under exactly the path *napari_annotatorj*. This is automatically created on the first startup in your user folder: - `C:\\\\Users\\\\Username\\\\.napari_annotatorj` on Windows - `\\\\home\\\\username\\\\.napari_annotatorj` on Linux  A pre-trained model for nucleus segmentation is automatically downloaded from the GitHub repository of the [ImageJ version of AnnotatorJ](https://github.com/spreka/annotatorj/releases/tag/v0.0.2-model). The model will be saved to `[your user folder]\\\\.napari_annotatorj\\\\models\\\\model_real.h5`. This location is printed to the console (command prompt or powershell on Windows, terminal on Linux).  (deprecated) When bulding from source the model folder is located at *path\\\\to\\ apari-annotatorj\\\\src\\ apari_annotatorj\\\\models* whereas installing from pypi it is located at *path\\\\to\\\\virtualenv\\\\Lib\\\\site-packages\\ apari_annotatorj\\\\models*.  The model must be in either of these file formats: - config .json file + weights file: *model_real.json* and *model_real_weights.h5* - combined weights file: *model_real.hdf5*  You can also train a new model on your own data in e.g. Python and save it with this code block:  ```python \\t# save model as json \\tmodel_json=model.to_json() \\twith open(‘model_real.json’, ‘w’) as f: \\t\\tf.write(model_json) \\t \\t# save weights too \\tmodel.save_weights(‘model_real_weights.h5’)  ``` This configuration will change in the next release to allow model browse and custom model name in an options widget.  ## Setting device for deep learning model prediction The [Contour assist](#contour-assist-mode) mode uses a pre-trained U-Net model for suggesting contours based on a lazily initialized contour drawn by the user. The default configuration loads and runs the model on the CPU so all users can run it. It is possible to switch to GPU if you have: - a CUDA-capable GPU in your computer - nVidia's CUDA toolkit + cuDNN installed  See installation guide on [nVidia's website](https://developer.nvidia.com/cuda-downloads) according to your system.  To switch to GPU utilization, edit [_dock_widget.py](https://github.com/spreka/napari-annotatorj/blob/main/src/napari_annotatorj/_dock_widget.py#L112) and set to the device you would like to use. Valid values are `'cpu','0','1','2',...`. The default value is `cpu`. The default GPU device is `0` if your system has any CUDA-capable GPU. If the device you set cannot be found or utilized by the code, it will fall back to `cpu`. An informative message is printed to the console upon plugin startup.  ## License Distributed under the terms of the [BSD-3](https://opensource.org/licenses/BSD-3-Clause) license, \"napari-annotatorj\" is free and open source software.  ## Getting help If you experience any issues or have questions, feel free to open an [issue](https://github.com/spreka/napari-annotatorj/issues) on GitHub.  ## How to cite Réka Hollandi, Ákos Diósdi, Gábor Hollandi, Nikita Moshkov, Péter Horváth (2020): “AnnotatorJ: an ImageJ plugin to ease hand-annotation of cellular compartments”, Molecular Biology of the Cell, Vol. 31, No. 20, 2179-2186, https://doi.org/10.1091/mbc.E20-02-0156",
    "description_content_type": "text/markdown",
    "description_text": " Description This plugin allows easy object annotation on 2D images. Annotation is made quick, easy and fun, just start drawing! See a quick start guide below.  It is the napari version of the ImageJ plugin AnnotatorJ. What kind of data it works on 2D images. That's the only requirement. Whether you have microscopy images of cells or tissue, natural photos of cats and dogs, vehichle dash-cam footage, industrial pipeline monitoring etc., just open the image and you can start annotating. Annotations are save to ImageJ-compatible roi.zip files. Export is possible to several file formats depending on the intended application. Intended users Anyone. No experience in computer science or underlying technology is needed; if you know how to use MS Paint, you are ready to start annotating. Biologists, programmers, even children can use it. See quick start guide or demos. If you experience any issues or have questions, feel free to open an issue on GitHub. Main features Why choose napari-annotatorj? - Assisted annotation is possible with automatic deep learning-based contour suggestion, - freehand contour drawing in instance annotation, - shape editing via painting labels, - class annotation, - export to formats directly suitable for deep CNN training - import of previous annotations as overlay; e.g. when comparing annotations or curating - and more. See demos. Quick start Demo data is available in the GitHub repository's demo folder. napari-annotatorj has several convenient functions to speed up the annotation process, make it easier and more fun. These modes can be activated by their corresponding checkboxes on the left side of the main AnnotatorJ widget.  Contour assist mode Edit mode Class mode Overlay  Freehand drawing is enabled in the plugin. The \"Add polygon\" tool is selected by default upon startup. To draw a freehand object (shape) simply hold the mouse and drag it around the object. The contour is visualized when the mouse button is released. See the guide below or a demo script. Instance annotation Allows freehand drawing of object contours (shapes) with the mouse as in ImageJ. Shape contour points are tracked automatically when the left mouse button is held and dragged to draw a shape. The shape is closed when the mouse button is released, automatically, and added to the default shapes layer (named \"ROI\"). In direct selection mode (from the layer controls panel), you can see the saved contour points. The slower you drag the mouse, the more contour points saved, i.e. the more refined your contour will be. Click to watch demo video below.  How to annotate  Open --> opens an image (Optionally)  ... --> Select annotation type --> Ok --> a default tool is selected from the toolbar that fits the selected annotation type The default annotation type is instance Selected annotation type is saved to a config file   Start annotating objects instance: draw contours around objects semantic: paint the objects' area bounding box: draw rectangles around the objects    Save --> Select class --> saves the annotation to a file in a sub-folder of the original image folder with the name of the selected class   (Optionally)  Load --> continue a previous annotation Overlay --> display a different annotation as overlay (semi-transparent) on the currently opened image Colours --> select annotation and overlay colours ... (coming soon) --> set options for semantic segmentation and Contour assist mode checkboxes --> Various options (default) Add automatically --> adds the most recent annotation to the ROI list automatically when releasing the left mouse button Smooth (coming soon) --> smooths the contour (in instance annotation type only) Show contours --> displays all the contours in the ROI list Contours assist --> suggests a contour in the region of an initial, lazily drawn contour using the deep learning method U-Net Show overlay --> displays the overlayed annotation if loaded with the Overlay button Edit mode --> edits a selected, already saved contour in the ROI list by clicking on it on the image Class mode --> assigns the selected class to the selected contour in the ROI list by clicking on it on the image and displays its contour in the class's colour (can be set in the Class window); clicking on the object a second time unclassifies it   [^] --> quick export in 16-bit multi-labelled .tiff format; if classified, also exports by classes    Semantic annotation Allows painting with the brush tool (labels). Useful for semantic (e.g. scene) annotation. Currently saves all labels to binary mask only (foreground-background). Bounding box annotation Allows drawing bounding boxes (shapes, rectangles) around objects with the mouse. Useful for object detection annotation. Demo Instance annotation mode See above. Contour assist mode Assisted annotation via a pre-trained deep learning model's suggested contour.  initialize a contour with mouse drag around an object the suggested contour is displayed automatically modify the contour: edit with mouse drag or  erase holding \"Alt\" or invert with pressing \"u\"    finalize it  accept with pressing \"q\" or reject with pressing \"Ctrl\" + \"Del\"    if the suggested contour is a merge of multiple objects, you can erase the dividing line around the object you wish to keep, and keep erasing (or splitting with the eraser) until the object you wish to keep is the largest, then press \"q\" to accept it  this mode requires a Keras model to be present in the model folder  Click to watch demo video below  Edit mode Allows to modify created objects with a brush tool.  select an object (shape) to modify by clicking on it an editing layer (labels layer) is created for painting automatically modify the contour: edit with mouse drag or  erase holding \"Alt\"    finalize it  accept with pressing \"q\" or delete with pressing \"Ctrl\" + \"Del\" or revert changes with pressing \"Esc\" (to the state before editing)    if the edited contour is a merge of multiple objects, you can erase the dividing line around the object you wish to keep, and keep erasing (or splitting with the eraser) until the object you wish to keep is the largest, then press \"q\" to accept it   Click to watch demo video below  Class mode Allows to assign class labels to objects by clicking on shapes.  select a class from the class list to assign click on an object (shape) to assign the selected class label to it  the contour colour of the clicked object will be updated to the selected class colour, plus the class label is updated in the text properties of object (turn on \"display text\" on the layer control panel to see the text properties as objectID:(classLabel) e.g. 1:(0) for the first object)   optionally, you can set a default class for all currently unlabelled objects on the ROI (shapes) layer by selecting a class from the drop-down menu on the right to the text label \"Default class\"  class colours can be changed with the drop-down menu right to the class list; upon selection, all objects whose class label is the currently selected class will have their contour colour updated to the selected colour clicking on an object that has already been assigned a class label will unclassify it: assign the label 0 to it  Click to watch demo video below  Export See also: Quick export The exporter plugin AnnotatorJExport can be invoked from the Plugins menu under the plugin name napari-annotatorj. It is used for batch export of annotations to various formats directly suitable to train different types of deep learning models. See a demonstrative figure in the AnnotatorJ repository and further description in its README or documentation.  browse original image folder with either the \"Browse original...\" button or text input field next to it   browse annotation folder with either the \"Browse annot...\" button or text input field next to it   select the export options you wish to export the annotations to (see tooltips on hover for help) at least one export option must be selected to start export (optional) right click on the checkbox \"Coordinates\" to switch between the default COCO format and YOLO format; see explanation   click on \"Export masks\" to start the export this will open a progress bar in the napari window and close it upon finish    The folder structure required by the exporter is as follows: ``` image_folder     |--- image1.png     |--- another_image.png     |--- something.png     |--- ... annotation_folder     |--- image1_ROIs.zip     |--- another_image_ROIs.zip     |--- something_ROIs.zip     |--- ... ``` Multiple export options can be selected at once, any selected will create a subfolder in the folder where the annotations are saved. Click to watch demo video below  Quick export Click on the \"[^]\" button to quickly save annotations and export to mask image. It saves the current annotations (shapes) to an ImageJ-compatible roi.zip file and a generated a 16-bit multi-labelled mask image to the subfolder \"masks\" under the current original image's folder. Coordinate formats In the AnnotatorJExport plugin 2 coordinates formats can be selecting by right clicking on the Coordinates checkbox: COCO or YOLO. The default is COCO. COCO format: - [x, y, width, height] based on the top-left corner of the bounding box around the object - coordinates are not normalized - annotations are saved with header to      - .csv file     - tab delimeted YOLO format: - [class, x, y, width, height] based on the center point of the bounding box around the object - coordinates are normalized to the image size as floating point values between 0 and 1 - annotations are saved with header to     - .txt file     - whitespace delimeted     - class is saved as the 1st column Overlay A separate annotation file can be loaded as overlay for convenience, e.g. to compare annotations.   load another annotation file with the \"Overlay\" button   (optional) switch its visibility with the \"Show overlay\" checkbox  (optional) change the contour colour of the overlay shapes with the \"Colours\" button  Change colours Clicking on the \"Colours\" button opens the Colours widget where you can set the annotation and overlay colours.  select a colour from the drop-down list either next to the text label \"overlay\" or \"annotation\"  click the \"Ok\" button to apply changes   contour colour of shapes on the annotation shapes layer (named \"ROI\") that already have a class label assigned to them will not be updated to the new annotation colour, only those not having a class label (the class label can be displayed with the \"display text\" checkbox on the layer controls panel as objectID:(classLabel) e.g. 1:(0) for the first object)  contour colour of shapes on the overlay shapes layer (named \"overlay\") will all have the overlay colour set, regardless of any existing class information saved to the annotation file loaded as overlay  For coding users Demo scripts Run a demo of napari-annotatorj with sample data: a small 3-channel RGB image as original image and an ImageJ roi.zip file as annotations loaded. shell     # from the napari-annotatorj folder     python src/napari_annotatorj/load_imagej_roi.py Alternatively, you can startup the napari-annotatorj plugin by running shell     # from the napari-annotatorj folder     python src/napari_annotatorj/startup_annotatorj.py Configure model folder The Contour assist mode imports a pre-trained Keras model from a folder named models under exactly the path napari_annotatorj. This is automatically created on the first startup in your user folder: - C:\\\\Users\\\\Username\\\\.napari_annotatorj on Windows - \\\\home\\\\username\\\\.napari_annotatorj on Linux A pre-trained model for nucleus segmentation is automatically downloaded from the GitHub repository of the ImageJ version of AnnotatorJ. The model will be saved to [your user folder]\\\\.napari_annotatorj\\\\models\\\\model_real.h5. This location is printed to the console (command prompt or powershell on Windows, terminal on Linux). (deprecated) When bulding from source the model folder is located at path\\\\to\\ apari-annotatorj\\\\src\\ apari_annotatorj\\\\models whereas installing from pypi it is located at path\\\\to\\\\virtualenv\\\\Lib\\\\site-packages\\ apari_annotatorj\\\\models. The model must be in either of these file formats: - config .json file + weights file: model_real.json and model_real_weights.h5 - combined weights file: model_real.hdf5 You can also train a new model on your own data in e.g. Python and save it with this code block: ```python     # save model as json     model_json=model.to_json()     with open(‘model_real.json’, ‘w’) as f:         f.write(model_json) # save weights too model.save_weights(‘model_real_weights.h5’)  ``` This configuration will change in the next release to allow model browse and custom model name in an options widget. Setting device for deep learning model prediction The Contour assist mode uses a pre-trained U-Net model for suggesting contours based on a lazily initialized contour drawn by the user. The default configuration loads and runs the model on the CPU so all users can run it. It is possible to switch to GPU if you have: - a CUDA-capable GPU in your computer - nVidia's CUDA toolkit + cuDNN installed See installation guide on nVidia's website according to your system. To switch to GPU utilization, edit _dock_widget.py and set to the device you would like to use. Valid values are 'cpu','0','1','2',.... The default value is cpu. The default GPU device is 0 if your system has any CUDA-capable GPU. If the device you set cannot be found or utilized by the code, it will fall back to cpu. An informative message is printed to the console upon plugin startup. License Distributed under the terms of the BSD-3 license, \"napari-annotatorj\" is free and open source software. Getting help If you experience any issues or have questions, feel free to open an issue on GitHub. How to cite Réka Hollandi, Ákos Diósdi, Gábor Hollandi, Nikita Moshkov, Péter Horváth (2020): “AnnotatorJ: an ImageJ plugin to ease hand-annotation of cellular compartments”, Molecular Biology of the Cell, Vol. 31, No. 20, 2179-2186, https://doi.org/10.1091/mbc.E20-02-0156",
    "development_status": ["Development Status :: 3 - Alpha"],
    "display_name": "napari-annotatorj",
    "documentation": "https://github.com/spreka/napari-annotatorj#README.md",
    "first_released": "2022-05-26T12:19:13.088484Z",
    "license": "BSD-3-Clause",
    "name": "napari-annotatorj",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader", "writer", "widget"],
    "project_site": "https://github.com/spreka/napari-annotatorj",
    "python_version": ">=3.7",
    "reader_file_extensions": ["<EDIT_ME>"],
    "release_date": "2022-07-21T13:13:37.100077Z",
    "report_issues": "https://github.com/spreka/napari-annotatorj/issues",
    "requirements": [
      "napari",
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "roifile",
      "scikit-image",
      "opencv-python (>=4.5.5)",
      "keras",
      "tensorflow (>=2.5.0)",
      "tifffile",
      "imagecodecs"
    ],
    "summary": "The napari adaptation of the ImageJ/Fiji plugin AnnotatorJ for easy and fun image annotation.",
    "support": "https://github.com/spreka/napari-annotatorj/issues",
    "twitter": "",
    "version": "0.0.7",
    "visibility": "public",
    "writer_file_extensions": [".tiff"],
    "writer_save_layers": ["labels"]
  },
  {
    "authors": [{ "name": "Joel Luethi" }, { "name": "Max Hess" }],
    "code_repository": "https://github.com/fractal-napari-plugins-collection/napari-feature-classifier",
    "conda": [
      { "channel": "conda-forge", "package": "napari-feature-classifier" }
    ],
    "description": "# napari-feature-classifier  [![License](https://img.shields.io/pypi/l/napari-feature-classifier.svg?color=green)](https://github.com/fractal-napari-plugins-collection/napari-feature-classifier/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-feature-classifier.svg?color=green)](https://pypi.org/project/napari-feature-classifier) [![Python Version](https://img.shields.io/pypi/pyversions/napari-feature-classifier.svg?color=green)](https://python.org) [![tests](https://github.com/fractal-napari-plugins-collection/napari-feature-classifier/workflows/tests/badge.svg)](https://github.com/fractal-napari-plugins-collection/napari-feature-classifier/actions) [![codecov](https://codecov.io/gh/fractal-napari-plugins-collection/napari-feature-classifier/branch/main/graph/badge.svg)](https://codecov.io/gh/fractal-napari-plugins-collection/napari-feature-classifier) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-feature-classifier)](https://napari-hub.org/plugins/napari-feature-classifier)  An interactive classifier plugin that allows the user to assign objects in a label image to multiple classes and train a classifier to learn those classes based on a feature dataframe.   ## Usage <p align=\"center\"><img src=\"https://user-images.githubusercontent.com/18033446/153727595-60380204-f299-485f-b762-d2030b75e7d3.gif\" /></p> To use the napari-feature-classifier, you need to have a label image and a csv file containing measurements that correspond to the object in the label image. The csv file needs to contain a column with integer values corresponding to the label values in the label image. These interactive classification workflows are well suited to visually define cell types, find mitotic cells in images, do quality control by automatically detecting missegmented cells and other tasks where a user can easily assign objects to groups.  #### Initialize a classifier: - Start the classifier in napari by going to `Plugins -> napari-feature-classifier -> Initialize a Classifier`   - Provide a csv file that contains feature measurements and a column with the integer labels corresponding to the label layer you are using. - Choose a name (or relative path from the current working directory) for the classifier. The classifier is initially saved in the current working directory (you can change this later on). - Select the features you want to use for the classifier (you need to do the feature selection before initializing. The feature selection can't be changed after initialization anymore). Hold the command key to select multiple features. <img width=\"1831\" alt=\"Initialize Classifier\" src=\"https://user-images.githubusercontent.com/18033446/153727784-d7b7d44b-a7b1-479f-a4af-34e0e280c8d6.png\">   #### Classify objects: - Make sure you have the label layer selected on which you want to classify - Select the current class with the radio buttons or by pressing 0, 1, 2, 3 or 4 - Click on label objects in the viewer to assign them to the currently selected class - While you need to have the label layer active to select, sometimes you want to focus on the intensity images. You can press `v` (or manually change the opacity of the label layer) to focus on the intensity images. - Once you have trained enough examples, click \"Run Classifier\" (or press `t`) to run the classifier and have it make a prediction for all objects. Aim for at least a dozen annotations per class, as the classifier divides your annotations 80/20 in training and test sets. To get good performance readouts, aim for >30 annotations per class. - Once you get predictions, correct mistakes the classifier made and retrain it to improve its performance. - You can save the classifier under a different name (to move it to a new folder or to have a slightly different version of the classifier - but careful, it autosaves whenever you run it). Define the new output location and then click `Save Classifier` (you need to click the Save Classifier button. Just defining the new output path does not save it yet) <img width=\"1831\" alt=\"trainClassifier\" src=\"https://user-images.githubusercontent.com/18033446/153727960-daae2955-4368-4081-88da-1a1cdbda6e69.png\">   #### Apply the classifier to additional images: - You can apply a classifier trained on one image to additional label images. Use `Plugins -> napari-feature-classifier -> Load Classifier`   - Select the classifier (.clf file with the name you gave above) and a csv file containing the same features as the past images. - Click Load Classifier, proceed as above. <img width=\"1831\" alt=\"LoadClassifier\" src=\"https://user-images.githubusercontent.com/18033446/153728100-dd60918d-c9a4-4de8-8f0e-8fd8c6a51700.png\">   #### Export classifier results - To export the training data and the results of the classifier, define an Export Name (full path to an output file or just a filename ending in .csv) where the results of the classifier shall be saved - Click `Export Classifier Result` (Just selecting a filename is not enough, you need to click the export button) - The results of the classifier are save in a csv file. The first two columns are index columns: path describes the Feature Path used (and allows you to understand which image / feature dataframe a result is from) and label is an integer of the label object within that image. The predict column contains predictions of the classifier for all objects (except those that contained NaNs in their feature data) and the train column contains the annotations you made (0 for unclassified objects, 1, 2, 3 or 4 for the classes) ![DataStructure](https://user-images.githubusercontent.com/18033446/153728461-d685987d-e1a9-46ff-834b-073008252ccb.png)   There is a simple workflow for the classifier in the examples folder: - Install jupyter-lab (`pip install jupyterlab`) - Open the notebook in jupyter lab (Type `jupyter-lab` in the terminal when you are in the examples folder) - Follow the instructions to generate an example dataframe and an example label image - Use the classifier in napari with this simplified data  ## Installation  This plugin is written for the new napari npe2 plugin engine. Thus, it requires napari >= 0.4.13. Activate your environment where you have napari installed (or install napari using `pip install \"napari[all]\"`), then install the classifier plugin:      pip install napari-feature-classifier       ## Contributing  Contributions are very welcome.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-feature-classifier\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  ## Contributors [Joel Lüthi](https://github.com/jluethi) & [Max Hess](https://github.com/MaksHess)  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-feature-classifier       An interactive classifier plugin that allows the user to assign objects in a label image to multiple classes and train a classifier to learn those classes based on a feature dataframe. Usage  To use the napari-feature-classifier, you need to have a label image and a csv file containing measurements that correspond to the object in the label image. The csv file needs to contain a column with integer values corresponding to the label values in the label image. These interactive classification workflows are well suited to visually define cell types, find mitotic cells in images, do quality control by automatically detecting missegmented cells and other tasks where a user can easily assign objects to groups. Initialize a classifier:  Start the classifier in napari by going to Plugins -> napari-feature-classifier -> Initialize a Classifier  Provide a csv file that contains feature measurements and a column with the integer labels corresponding to the label layer you are using. Choose a name (or relative path from the current working directory) for the classifier. The classifier is initially saved in the current working directory (you can change this later on). Select the features you want to use for the classifier (you need to do the feature selection before initializing. The feature selection can't be changed after initialization anymore). Hold the command key to select multiple features.   Classify objects:  Make sure you have the label layer selected on which you want to classify Select the current class with the radio buttons or by pressing 0, 1, 2, 3 or 4 Click on label objects in the viewer to assign them to the currently selected class While you need to have the label layer active to select, sometimes you want to focus on the intensity images. You can press v (or manually change the opacity of the label layer) to focus on the intensity images. Once you have trained enough examples, click \"Run Classifier\" (or press t) to run the classifier and have it make a prediction for all objects. Aim for at least a dozen annotations per class, as the classifier divides your annotations 80/20 in training and test sets. To get good performance readouts, aim for >30 annotations per class. Once you get predictions, correct mistakes the classifier made and retrain it to improve its performance. You can save the classifier under a different name (to move it to a new folder or to have a slightly different version of the classifier - but careful, it autosaves whenever you run it). Define the new output location and then click Save Classifier (you need to click the Save Classifier button. Just defining the new output path does not save it yet)   Apply the classifier to additional images:  You can apply a classifier trained on one image to additional label images. Use Plugins -> napari-feature-classifier -> Load Classifier  Select the classifier (.clf file with the name you gave above) and a csv file containing the same features as the past images. Click Load Classifier, proceed as above.   Export classifier results  To export the training data and the results of the classifier, define an Export Name (full path to an output file or just a filename ending in .csv) where the results of the classifier shall be saved Click Export Classifier Result (Just selecting a filename is not enough, you need to click the export button) The results of the classifier are save in a csv file. The first two columns are index columns: path describes the Feature Path used (and allows you to understand which image / feature dataframe a result is from) and label is an integer of the label object within that image. The predict column contains predictions of the classifier for all objects (except those that contained NaNs in their feature data) and the train column contains the annotations you made (0 for unclassified objects, 1, 2, 3 or 4 for the classes)   There is a simple workflow for the classifier in the examples folder: - Install jupyter-lab (pip install jupyterlab) - Open the notebook in jupyter lab (Type jupyter-lab in the terminal when you are in the examples folder) - Follow the instructions to generate an example dataframe and an example label image - Use the classifier in napari with this simplified data Installation This plugin is written for the new napari npe2 plugin engine. Thus, it requires napari >= 0.4.13. Activate your environment where you have napari installed (or install napari using pip install \"napari[all]\"), then install the classifier plugin: pip install napari-feature-classifier  Contributing Contributions are very welcome. License Distributed under the terms of the BSD-3 license, \"napari-feature-classifier\" is free and open source software Issues If you encounter any problems, please [file an issue] along with a detailed description. Contributors Joel Lüthi & Max Hess",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari feature classifier",
    "documentation": "",
    "first_released": "2022-02-12T22:19:29.954607Z",
    "license": "BSD-3-Clause",
    "name": "napari-feature-classifier",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/fractal-napari-plugins-collection/napari-feature-classifier",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-02-12T22:19:29.954607Z",
    "report_issues": "",
    "requirements": [
      "numpy",
      "napari",
      "matplotlib",
      "magicgui",
      "pandas",
      "sklearn"
    ],
    "summary": "An interactive classifier plugin to use with label images and feature measurements",
    "support": "",
    "twitter": "",
    "version": "0.0.1",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      { "email": "giorgiatortora2@gmail.com", "name": "Giorgia Tortora" }
    ],
    "code_repository": "https://github.com/GiorgiaTortora/napari-power-spectrum",
    "description": "# napari-power-spectrum  [![License](https://img.shields.io/pypi/l/napari-power-spectrum.svg?color=green)](https://github.com/GiorgiaTortora/napari-power-spectrum/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-power-spectrum.svg?color=green)](https://pypi.org/project/napari-power-spectrum) [![Python Version](https://img.shields.io/pypi/pyversions/napari-power-spectrum.svg?color=green)](https://python.org) [![tests](https://github.com/GiorgiaTortora/napari-power-spectrum/workflows/tests/badge.svg)](https://github.com/GiorgiaTortora/napari-power-spectrum/actions) [![codecov](https://codecov.io/gh/GiorgiaTortora/napari-power-spectrum/branch/main/graph/badge.svg)](https://codecov.io/gh/GiorgiaTortora/napari-power-spectrum) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-power-spectrum)](https://napari-hub.org/plugins/napari-power-spectrum)  A simple plugin to get the power spectrum of frames of a stack image  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/plugins/stable/index.html -->  ## Installation  You can install `napari-power-spectrum` via [pip]:      pip install napari-power-spectrum    To install latest development version :      pip install git+https://github.com/GiorgiaTortora/napari-power-spectrum.git   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-power-spectrum\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/GiorgiaTortora/napari-power-spectrum/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-power-spectrum       A simple plugin to get the power spectrum of frames of a stack image  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install napari-power-spectrum via pip: pip install napari-power-spectrum  To install latest development version : pip install git+https://github.com/GiorgiaTortora/napari-power-spectrum.git  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-power-spectrum\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "Power Spectrum",
    "documentation": "https://github.com/GiorgiaTortora/napari-power-spectrum#README.md",
    "first_released": "2022-04-08T10:06:35.233809Z",
    "license": "BSD-3-Clause",
    "name": "napari-power-spectrum",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/GiorgiaTortora/napari-power-spectrum",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-04-22T15:54:51.976065Z",
    "report_issues": "https://github.com/GiorgiaTortora/napari-power-spectrum/issues",
    "requirements": [
      "numpy",
      "magicgui",
      "qtpy",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "summary": "A simple plugin to get the power spectrum of frames of a stack image",
    "support": "https://github.com/GiorgiaTortora/napari-power-spectrum/issues",
    "twitter": "",
    "version": "0.0.6",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Zach Marin" }, { "name": "Robert Haase" }],
    "code_repository": "https://github.com/zacsimile/napari-pymeshlab",
    "conda": [],
    "description": "# napari-pymeshlab  [![License](https://img.shields.io/pypi/l/napari-pymeshlab.svg?color=green)](https://github.com/zacsimile/napari-pymeshlab/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-pymeshlab.svg?color=green)](https://pypi.org/project/napari-pymeshlab) [![Python Version](https://img.shields.io/pypi/pyversions/napari-pymeshlab.svg?color=green)](https://python.org) [![tests](https://github.com/zacsimile/napari-pymeshlab/workflows/tests/badge.svg)](https://github.com/zacsimile/napari-pymeshlab/actions) [![codecov](https://codecov.io/gh/zacsimile/napari-pymeshlab/branch/main/graph/badge.svg)](https://codecov.io/gh/zacsimile/napari-pymeshlab) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-pymeshlab)](https://napari-hub.org/plugins/napari-pymeshlab)  Interfaces between `napari` and the `pymeshlab` library to allow import, export, construction and processing of surfaces.   This is a WIP and feature requests are welcome. Please check [PyMeshLab](https://pymeshlab.readthedocs.io/en/latest/) for possible features.  ![img.png](docs/screenshot.png)  ## Feature list  - Read/write .3ds, .apts, .asc, .bre, .ctm, .dae, .e57, .es, .fbx, .glb, .gltf, .obj, .off, .pdb, .ply,                   .ptx, .qobj, .stl, .vmi, .wrl, .x3d, .x3dv - [Screened Poisson Surface Reconstruction](https://www.cs.jhu.edu/~misha/MyPapers/ToG13.pdf) - [Convex hull of a surface](https://pymeshlab.readthedocs.io/en/0.1.9/tutorials/apply_filter.html) - [Laplacian smoothing of surfaces](https://pymeshlab.readthedocs.io/en/0.1.9/filter_list.html#laplacian_smooth) - [Smoothing surfaces using Taubin's method](https://pymeshlab.readthedocs.io/en/0.1.9/filter_list.html#taubin_smooth) - [Surface simplification using clustering decimation](https://pymeshlab.readthedocs.io/en/0.1.9/filter_list.html#simplification_clustering_decimation) - [colorize_curvature_apss](https://pymeshlab.readthedocs.io/en/0.1.9/filter_list.html#colorize_curvature_apss)  Some functions are shown in the [demo notebook](docs/demo.ipynb).  ----------------------------------  <!--  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/plugins/stable/index.html -->  ## Installation   You can install `napari-pymeshlab` via [pip]:      pip install napari-pymeshlab     ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [GNU GPL v3.0] license, \"napari-pymeshlab\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue](https://github.com/zacsimile/napari-pymeshlab/issues) along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-pymeshlab       Interfaces between napari and the pymeshlab library to allow import, export, construction and processing of surfaces.  This is a WIP and feature requests are welcome. Please check PyMeshLab for possible features.  Feature list  Read/write .3ds, .apts, .asc, .bre, .ctm, .dae, .e57, .es, .fbx, .glb, .gltf, .obj, .off, .pdb, .ply,                   .ptx, .qobj, .stl, .vmi, .wrl, .x3d, .x3dv Screened Poisson Surface Reconstruction Convex hull of a surface Laplacian smoothing of surfaces Smoothing surfaces using Taubin's method Surface simplification using clustering decimation colorize_curvature_apss  Some functions are shown in the demo notebook.   Installation You can install napari-pymeshlab via pip: pip install napari-pymeshlab  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the GNU GPL v3.0 license, \"napari-pymeshlab\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari pymeshlab",
    "documentation": "https://github.com/zacsimile/napari-pymeshlab#README.md",
    "first_released": "2022-01-14T04:47:54.142489Z",
    "license": "GPL-3.0",
    "name": "napari-pymeshlab",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader", "writer", "widget", "sample_data"],
    "project_site": "https://github.com/zacsimile/napari-pymeshlab",
    "python_version": ">=3.7",
    "reader_file_extensions": [
      "*.fbx",
      "*.apts",
      "*.asc",
      "*.off",
      "*.glb",
      "*.wrl",
      "*.qobj",
      "*.dae",
      "*.pdb",
      "*.bre",
      "*.es",
      "*.vmi",
      "*.ptx",
      "*.stl",
      "*.ply",
      ".x3dv",
      "*.ctm",
      "*.obj",
      "*.x3d",
      "*.3ds",
      "*.e57",
      "*.gltf"
    ],
    "release_date": "2022-03-29T00:49:46.964913Z",
    "report_issues": "https://github.com/zacsimile/napari-pymeshlab/issues",
    "requirements": ["npe2", "numpy", "pymeshlab"],
    "summary": "Interfaces between napari and pymeshlab library to allow import, export and construction of surfaces.",
    "support": "https://github.com/zacsimile/napari-pymeshlab/issues",
    "twitter": "",
    "version": "0.0.5",
    "visibility": "public",
    "writer_file_extensions": [
      ".es",
      ".dae",
      ".fbx",
      ".apts",
      ".off",
      ".ptx",
      ".wrl",
      ".bre",
      ".obj",
      ".3ds",
      ".vmi",
      ".x3d",
      ".ply",
      ".x3dv",
      ".e57",
      ".stl",
      ".qobj",
      ".pdb",
      ".glb",
      ".asc",
      ".ctm",
      ".gltf"
    ],
    "writer_save_layers": ["surface"]
  },
  {
    "authors": [{ "name": "Nathan Heath Patterson" }],
    "category": {
      "Image modality": ["Fluorescence microscopy"],
      "Supported data": ["2D", "Multi-channel"],
      "Workflow step": ["Image registration"]
    },
    "category_hierarchy": {
      "Image modality": [["Fluorescence microscopy"]],
      "Supported data": [["2D"], ["Multi-channel"]],
      "Workflow step": [
        ["Image registration"],
        ["Image registration", "Affine registration"],
        ["Image registration", "Affine registration", "Rigid registration"],
        ["Image registration", "Deformable registration"],
        ["Image registration", "Intensity-based registration"]
      ]
    },
    "code_repository": "https://github.com/nhpatterson/napari-wsireg",
    "conda": [],
    "description": "# napari-wsireg  [//]: # ([![License]&#40;https://img.shields.io/pypi/l/napari-wsireg.svg?color=green&#41;]&#40;https://github.com/nhpatterson/napari-wsireg/raw/main/LICENSE&#41;)  [//]: # ([![PyPI]&#40;https://img.shields.io/pypi/v/napari-wsireg.svg?color=green&#41;]&#40;https://pypi.org/project/napari-wsireg&#41;)  [//]: # ([![Python Version]&#40;https://img.shields.io/pypi/pyversions/napari-wsireg.svg?color=green&#41;]&#40;https://python.org&#41;)  [//]: # ([![tests]&#40;https://github.com/nhpatterson/napari-wsireg/workflows/tests/badge.svg&#41;]&#40;https://github.com/nhpatterson/napari-wsireg/actions&#41;)  [//]: # ([![napari hub]&#40;https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-wsireg&#41;]&#40;https://napari-hub.org/plugins/napari-wsireg&#41;)   Plugin to perform whole slide image registration based on wsireg.  Please see [wsireg](https://github.com/nhpatterson/wsireg) for more info image formats, features and how registration works.   ## Usage  Add images from napari layers or from file and set up \"registration paths\" between them. OME-TIFF is best supported format.  ## Installation  You can install `napari-wsireg` via [pip]:      pip install napari-wsireg    To install latest development version :      pip install git+https://github.com/nhpatterson/napari-wsireg.git   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-wsireg\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/nhpatterson/napari-wsireg/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-wsireg Plugin to perform whole slide image registration based on wsireg. Please see wsireg for more info image formats, features and how registration works. Usage Add images from napari layers or from file and set up \"registration paths\" between them. OME-TIFF is best supported format. Installation You can install napari-wsireg via pip: pip install napari-wsireg  To install latest development version : pip install git+https://github.com/nhpatterson/napari-wsireg.git  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-wsireg\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "napari-wsireg",
    "documentation": "https://github.com/nhpatterson/napari-wsireg#README.md",
    "first_released": "2022-04-27T18:49:51.526126Z",
    "license": "BSD-3-Clause",
    "name": "napari-wsireg",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/nhpatterson/napari-wsireg",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-08-16T17:32:20.887373Z",
    "report_issues": "https://github.com/nhpatterson/napari-wsireg/issues",
    "requirements": [
      "wsireg (>=0.3.6)",
      "SimpleITK",
      "czifile",
      "dask",
      "imagecodecs",
      "napari",
      "numpy",
      "ome-types",
      "pint",
      "qtpy",
      "tifffile",
      "zarr (>=2.10.3)",
      "napari-geojson",
      "networkx",
      "matplotlib"
    ],
    "summary": "plugin to perform whole slide image registration with wsireg",
    "support": "https://github.com/nhpatterson/napari-wsireg/issues",
    "twitter": "",
    "version": "0.1.2",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Robert Haase" }],
    "code_repository": "https://github.com/haesleinhuepf/napari-script-editor",
    "conda": [{ "channel": "conda-forge", "package": "napari-script-editor" }],
    "description": "# napari-script-editor  [![License](https://img.shields.io/pypi/l/napari-script-editor.svg?color=green)](https://github.com/haesleinhuepf/napari-script-editor/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-script-editor.svg?color=green)](https://pypi.org/project/napari-script-editor) [![Python Version](https://img.shields.io/pypi/pyversions/napari-script-editor.svg?color=green)](https://python.org) [![tests](https://github.com/haesleinhuepf/napari-script-editor/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-script-editor/actions) [![codecov](https://codecov.io/gh/haesleinhuepf/napari-script-editor/branch/main/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-script-editor) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-script-editor)](https://napari-hub.org/plugins/napari-script-editor)  A python script editor for napari based on [haesleinhuepf's fork of PyQode](https://github.com/haesleinhuepf/pyqode.core).  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.  ![](https://github.com/haesleinhuepf/napari-script-editor/raw/main/docs/screenshot2.png)  ## Usage  Start the script editor from the menu `Tools > Scripts > Script Editor`. Use the auto-completion while typing,  check out the [napari tutorials](https://napari.org/tutorials/) and the [example scripts](https://github.com/haesleinhuepf/napari-script-editor/tree/main/example_scripts).  Use the `Run` button to execute a script.  ![](https://github.com/haesleinhuepf/napari-script-editor/raw/main/docs/type_and_run_screencast.gif)  If you save the script to the folder \".napari-scripts\" in your home directory, you will find the script in the  `Tools > Scripts` menu in napari. You can then also start it from there.  ![](https://github.com/haesleinhuepf/napari-script-editor/raw/main/docs/run_from_menu_screencast.gif)  Note: If you have scripts, that might be useful to others, please send them as  [pull-request](https://github.com/haesleinhuepf/napari-script-editor/pulls) to the examples in  repository or share them in any other way that suits you.  ## Installation * Get a python environment, e.g. via [mini-conda](https://docs.conda.io/en/latest/miniconda.html).    If you never used python/conda environments before, please follow the instructions    [here](https://mpicbg-scicomp.github.io/ipf_howtoguides/guides/Python_Conda_Environments) first. * Install [napari](https://github.com/napari/napari) using conda.   ``` conda install -c conda-forge napari ```  Afterwards, install `napari-script-editor` using pip:  ``` pip install napari-script-editor ```  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-script-editor\" is free and open source software  ## Known issues  * Sometimes, the script editor thinks, the file has been changed on disk and asks to reload it.  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/haesleinhuepf/napari-script-editor/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-script-editor       A python script editor for napari based on haesleinhuepf's fork of PyQode.  This napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.  Usage Start the script editor from the menu Tools > Scripts > Script Editor. Use the auto-completion while typing,  check out the napari tutorials and the example scripts.  Use the Run button to execute a script.  If you save the script to the folder \".napari-scripts\" in your home directory, you will find the script in the  Tools > Scripts menu in napari. You can then also start it from there.  Note: If you have scripts, that might be useful to others, please send them as  pull-request to the examples in  repository or share them in any other way that suits you. Installation  Get a python environment, e.g. via mini-conda.    If you never used python/conda environments before, please follow the instructions    here first. Install napari using conda.   conda install -c conda-forge napari Afterwards, install napari-script-editor using pip: pip install napari-script-editor Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-script-editor\" is free and open source software Known issues  Sometimes, the script editor thinks, the file has been changed on disk and asks to reload it.  Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 3 - Alpha"],
    "display_name": "napari-script-editor",
    "documentation": "https://github.com/haesleinhuepf/napari-script-editor#README.md",
    "first_released": "2021-11-06T10:19:52.572418Z",
    "license": "BSD-3-Clause",
    "name": "napari-script-editor",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/haesleinhuepf/napari-script-editor",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-10-06T07:04:12.157901Z",
    "report_issues": "https://github.com/haesleinhuepf/napari-script-editor/issues",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "napari",
      "haesleinhuepf-pyqode.core (>=2.15.5)",
      "haesleinhuepf-pyqode.python (>=2.15.2)",
      "napari-tools-menu",
      "jedi (>=0.18.0)",
      "pyflakes (<=2.4.0)"
    ],
    "summary": "A python script editor for napari",
    "support": "https://github.com/haesleinhuepf/napari-script-editor/issues",
    "twitter": "",
    "version": "0.2.9",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Leo Guignard" }],
    "code_repository": "https://github.com/GuignardLab/napari-sc3D-viewer",
    "description": "# napari-sc3D-viewer  [![License](https://img.shields.io/pypi/l/napari-sc3D-viewer.svg?color=green)](https://raw.githubusercontent.com/GuignardLab/napari-sc3D-viewer/main/LICENSE) [![Python Version](https://img.shields.io/pypi/pyversions/napari-sc3D-viewer.svg?color=green)](https://python.org) [![PyPI](https://img.shields.io/pypi/v/napari-sc3D-viewer.svg?color=green)](https://pypi.org/project/napari-sc3D-viewer) [![tests](https://github.com/GuignardLab/napari-sc3D-viewer/workflows/tests/badge.svg)](https://github.com/GuignardLab/napari-sc3D-viewer/actions) [![codecov](https://codecov.io/gh/GuignardLab/napari-sc3D-viewer/branch/main/graph/badge.svg)](https://codecov.io/gh/GuignardLab/napari-sc3D-viewer) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-sc3D-viewer)](https://napari-hub.org/plugins/napari-sc3D-viewer)   A plugin to visualise 3D spatial single cell omics  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  ## Installation  You can install `napari-sc3D-viewer` via [pip]:      pip install . (from the correct folder) or      pip install napari-sc3D-viewer  To install latest development version :      pip install git+https://github.com/GuignardLab/napari-sc3D-viewer.git  To install the surface computation enabled version it is necessary to use Python 3.9 (until [VTK] is ported to Python 3.10) and you can run one of the following commands:      pip install '.[pyvista]' from the correct folder or      pip install 'napari-sc3D-viewer[pyvista]'  to install directly from pip or      pip install 'napari-sc3D-viewer[pyvista] @ git+https://github.com/GuignardLab/napari-sc3D-viewer.git'  to install the latest version  ## Usage  `napari-sc3D-viewer` allows users to easily visualise and navigate 3D spatial single-cell transcriptomics using napari.  ### Loading and opening a dataset  <!-- To test your the plugin you can download the following dataset composed of a id to tissue name file located [there](https://github.com/GuignardLab/sc3D/tree/main/data) and a scanpy h5ad dataset [there](https://figshare.com/s/1c29d867bc8b90d754d2). The dataset is from the following publication: [pub] -->  The expected dataset is a [scanpy]/[anndata] h5ad file together with an optional json file that maps cluster id numbers to actual tissue/cluster name.  The json file should look like that: ```json {     \"1\": \"Endoderm\",     \"2\": \"Heart\",     \"10\": \"Anterior neuroectoderm\" } ``` If no json file or a wrong json file is given, the original cluster id numbers are used.  The h5ad file should be informed in (1) and the json file in (2). ![loading image](images/1.loading.png)  Let `data` be your h5ad data structure. To work properly, the viewer is expecting 4 different columns to be present in the h5ad file: - the cluster id column (by default named 'predicted.id' that can be accessed as `data.obs['predicted.id']`) - the 3D position column (by default named 'X_spatial_registered' that can be accessed as `data.obsm['X_spatial_registered']`) - the gene names if not already in the column name (by default named 'feature_name' that can be accessed as `data.var['feature_name']`) - umap coordinates (by default named 'X_umap' that can be accessed as `data.obsm['X_umap']`)  If the default column names are not consistent with your dataset, they can be changed in the tab `Parameters` (3) next to the tab `Loading files`  Once all the data paths and fields are correctly informed pressing the `Load Atlas` button (4) will load the dataset.  ### Exploring a dataset  Once the dataset is loaded there are few options to explore it.  The viewer should look like to the following: ![viewer](images/2.viewer.png)  It is divided in two main parts, the Tissue visualisation (1) part and the Metric visualisation (2) one. Both of them are themselves split in two and three tabs respectively. All these tabs allow you to visualise and explore the dataset in different fashions.  The Tissues tab (1.1) allows to select the tissues to display, to show the legend and to colour the cells according to their tissue types.  The Surfaces tab (1.2) allows to construct coarse surfaces of tissues and to display them.  The Single metric tab (2.1) allows to display a metric, whether it is a gene intensity or a numerical metric that is embedded in the visualised dataset. This tab also allows to threshold cells according to the viewed metric, to change the contrast and the colour map.  The 2 Genes (2.2) tab allows to display gene coexpression.  The umap tab (2.3) allows to display the umap of the selected cells and to manually select subcategories of cells to be displayed.  ![viewer](images/3.description.png)  #### Explanatory \"videos\". The plugin is meant to be easy to use. That means that you should be able to play with it and figure things out by yourself.  That being said, it is not always that easy. You can find below a series of videos showing how to perform some of the main features.  #### Loading data ![Loading data video](images/loading.gif)  #### Selecting tissues ![Selecting tissues video](images/tissue-select.gif)  #### Displaying one gene ![Displaying one gene video](images/gene1.gif)  #### Displaying two genes co-expression ![Displaying genes video](images/gene2.gif)  #### Playing with the umap ![Playing with the umap video](images/umap.gif)  #### Computing and processing the surface ![Computing and processing the surface video](images/surfaces.gif)  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [MIT] license, \"napari-sc3D-viewer\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/GuignardLab/napari-sc3D-viewer/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ [VTK]: https://vtk.org/ [scanpy]: https://scanpy.readthedocs.io/en/latest/index.html [anndata]: https://anndata.readthedocs.io/en/latest/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-sc3D-viewer       A plugin to visualise 3D spatial single cell omics  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template. Installation You can install napari-sc3D-viewer via pip: pip install .  (from the correct folder) or pip install napari-sc3D-viewer  To install latest development version : pip install git+https://github.com/GuignardLab/napari-sc3D-viewer.git  To install the surface computation enabled version it is necessary to use Python 3.9 (until VTK is ported to Python 3.10) and you can run one of the following commands: pip install '.[pyvista]'  from the correct folder or pip install 'napari-sc3D-viewer[pyvista]'  to install directly from pip or pip install 'napari-sc3D-viewer[pyvista] @ git+https://github.com/GuignardLab/napari-sc3D-viewer.git'  to install the latest version Usage napari-sc3D-viewer allows users to easily visualise and navigate 3D spatial single-cell transcriptomics using napari. Loading and opening a dataset  The expected dataset is a scanpy/anndata h5ad file together with an optional json file that maps cluster id numbers to actual tissue/cluster name. The json file should look like that: json {     \"1\": \"Endoderm\",     \"2\": \"Heart\",     \"10\": \"Anterior neuroectoderm\" } If no json file or a wrong json file is given, the original cluster id numbers are used. The h5ad file should be informed in (1) and the json file in (2).  Let data be your h5ad data structure. To work properly, the viewer is expecting 4 different columns to be present in the h5ad file: - the cluster id column (by default named 'predicted.id' that can be accessed as data.obs['predicted.id']) - the 3D position column (by default named 'X_spatial_registered' that can be accessed as data.obsm['X_spatial_registered']) - the gene names if not already in the column name (by default named 'feature_name' that can be accessed as data.var['feature_name']) - umap coordinates (by default named 'X_umap' that can be accessed as data.obsm['X_umap']) If the default column names are not consistent with your dataset, they can be changed in the tab Parameters (3) next to the tab Loading files Once all the data paths and fields are correctly informed pressing the Load Atlas button (4) will load the dataset. Exploring a dataset Once the dataset is loaded there are few options to explore it. The viewer should look like to the following:  It is divided in two main parts, the Tissue visualisation (1) part and the Metric visualisation (2) one. Both of them are themselves split in two and three tabs respectively. All these tabs allow you to visualise and explore the dataset in different fashions. The Tissues tab (1.1) allows to select the tissues to display, to show the legend and to colour the cells according to their tissue types. The Surfaces tab (1.2) allows to construct coarse surfaces of tissues and to display them. The Single metric tab (2.1) allows to display a metric, whether it is a gene intensity or a numerical metric that is embedded in the visualised dataset. This tab also allows to threshold cells according to the viewed metric, to change the contrast and the colour map. The 2 Genes (2.2) tab allows to display gene coexpression. The umap tab (2.3) allows to display the umap of the selected cells and to manually select subcategories of cells to be displayed.  Explanatory \"videos\". The plugin is meant to be easy to use. That means that you should be able to play with it and figure things out by yourself. That being said, it is not always that easy. You can find below a series of videos showing how to perform some of the main features. Loading data  Selecting tissues  Displaying one gene  Displaying two genes co-expression  Playing with the umap  Computing and processing the surface  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the MIT license, \"napari-sc3D-viewer\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "sc3D Viewer",
    "documentation": "https://github.com/GuignardLab/napari-sc3D-viewer#README.md",
    "first_released": "2022-06-21T09:59:47.002122Z",
    "license": "MIT",
    "name": "napari-sc3D-viewer",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/GuignardLab/napari-sc3D-viewer",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-11-22T10:37:36.236467Z",
    "report_issues": "https://github.com/GuignardLab/napari-sc3D-viewer/issues",
    "requirements": [
      "numpy",
      "magicgui",
      "qtpy",
      "sc-3D",
      "matplotlib",
      "pyvista ; extra == 'pyvista'",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "summary": "A plugin to visualize 3D single cell omics",
    "support": "https://github.com/GuignardLab/napari-sc3D-viewer/issues",
    "twitter": "https://twitter.com/guignardlab",
    "version": "0.2.0",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Hanjin Liu" }],
    "code_repository": "https://github.com/hanjinliu/napari-power-widgets",
    "description": "# napari-power-widgets  [![License BSD-3](https://img.shields.io/pypi/l/napari-power-widgets.svg?color=green)](https://github.com/hanjinliu/napari-power-widgets/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-power-widgets.svg?color=green)](https://pypi.org/project/napari-power-widgets) [![Python Version](https://img.shields.io/pypi/pyversions/napari-power-widgets.svg?color=green)](https://python.org) [![tests](https://github.com/hanjinliu/napari-power-widgets/workflows/tests/badge.svg)](https://github.com/hanjinliu/napari-power-widgets/actions) [![codecov](https://codecov.io/gh/hanjinliu/napari-power-widgets/branch/main/graph/badge.svg)](https://codecov.io/gh/hanjinliu/napari-power-widgets) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-power-widgets)](https://napari-hub.org/plugins/napari-power-widgets)  Powerful `magicgui` widgets and type annotations for general-purpose napari plugin development.  `napari-power-widgets` makes the full use of type-to-widget mapping strategy of `magicgui` to provide napari-specific types and value-widgets, which will be very useful to improve UI/UX of your napari plugins with simple codes.  Currently, `napari-power-widgets` does not provide any reader, writer or widget. It is supposed to be used programmatically.  ### Examples  Some types/widgets and the possible usage are picked up here ([&rarr; check all](https://github.com/hanjinliu/napari-power-widgets/blob/main/src/napari_power_widgets/types.py)). If you have any neat ideas, please open an issue.  #### 1. `BoxSelection`  Alias of a four-float tuple for 2D selection. You can set the value by drawing a interaction box in the viewer.  *e. g. : image cropper, rectangular labeling etc.*  ```python @magicgui def f(box: BoxSelection):     print(box) viewer.window.add_dock_widget(f) ```  ![](images/BoxSelection.gif)  #### 2. `OneOfRectangles`  Alias of `np.ndarray` for one of rectangles in a `Shapes` layer.  *e. g. : image cropper, rectangular labeling etc.*  ```python @magicgui def f(rect: OneOfRectangles):     print(rect) viewer.window.add_dock_widget(f) ```  ![](images/OneOfRectangles.gif)  #### 3. `LineData`  Alias of `np.ndarray` for a line data. You can obtain the data by manually drawing a line in the viewer.  *e. g. : line profiling, kymograph etc.*  ```python @magicgui def f(line: LineData):     print(line) viewer.window.add_dock_widget(f) ```  ![](images/LineData.gif)  #### 4. `OneOfLabels`  Alias of boolean `np.ndarray` for a labeled region. You can choose ones by directly clicking the viewer.  *e. g. : image masking, feature measurement etc.*  ```python @magicgui def f(label: OneOfLabels):     pass viewer.window.add_dock_widget(f) ```  ![](images/OneOfLabels.gif)   #### 5. `ZRange`  Alias of a tuple of float that represents the limit of the third dimension. You can select the values by moving the dimension slider.  *e. g. : movie trimming, partial image projection etc.*  ```python @magicgui def f(zrange: ZRange):     print(zrange) viewer.window.add_dock_widget(f) ```  ![](images/ZRange.gif)  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/stable/plugins/index.html -->  ## Installation  You can install `napari-power-widgets` via [pip]:      pip install napari-power-widgets    To install latest development version :      pip install git+https://github.com/hanjinliu/napari-power-widgets.git   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-power-widgets\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/hanjinliu/napari-power-widgets/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-power-widgets       Powerful magicgui widgets and type annotations for general-purpose napari plugin development. napari-power-widgets makes the full use of type-to-widget mapping strategy of magicgui to provide napari-specific types and value-widgets, which will be very useful to improve UI/UX of your napari plugins with simple codes. Currently, napari-power-widgets does not provide any reader, writer or widget. It is supposed to be used programmatically. Examples Some types/widgets and the possible usage are picked up here (→ check all). If you have any neat ideas, please open an issue. 1. BoxSelection Alias of a four-float tuple for 2D selection. You can set the value by drawing a interaction box in the viewer. e. g. : image cropper, rectangular labeling etc. python @magicgui def f(box: BoxSelection):     print(box) viewer.window.add_dock_widget(f)  2. OneOfRectangles Alias of np.ndarray for one of rectangles in a Shapes layer. e. g. : image cropper, rectangular labeling etc. python @magicgui def f(rect: OneOfRectangles):     print(rect) viewer.window.add_dock_widget(f)  3. LineData Alias of np.ndarray for a line data. You can obtain the data by manually drawing a line in the viewer. e. g. : line profiling, kymograph etc. python @magicgui def f(line: LineData):     print(line) viewer.window.add_dock_widget(f)  4. OneOfLabels Alias of boolean np.ndarray for a labeled region. You can choose ones by directly clicking the viewer. e. g. : image masking, feature measurement etc. python @magicgui def f(label: OneOfLabels):     pass viewer.window.add_dock_widget(f)  5. ZRange Alias of a tuple of float that represents the limit of the third dimension. You can select the values by moving the dimension slider. e. g. : movie trimming, partial image projection etc. python @magicgui def f(zrange: ZRange):     print(zrange) viewer.window.add_dock_widget(f)   This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install napari-power-widgets via pip: pip install napari-power-widgets  To install latest development version : pip install git+https://github.com/hanjinliu/napari-power-widgets.git  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-power-widgets\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari-power-widgets",
    "documentation": "https://github.com/hanjinliu/napari-power-widgets#README.md",
    "first_released": "2022-11-11T15:48:22.469969Z",
    "license": "BSD-3-Clause",
    "name": "napari-power-widgets",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": [],
    "project_site": "https://github.com/hanjinliu/napari-power-widgets",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-11-11T15:48:22.469969Z",
    "report_issues": "https://github.com/hanjinliu/napari-power-widgets/issues",
    "requirements": [
      "numpy",
      "pandas",
      "typing-extensions",
      "magicgui",
      "napari",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "summary": "Powerful widgets and type annotations for napari plugin widgets",
    "support": "https://github.com/hanjinliu/napari-power-widgets/issues",
    "twitter": "",
    "version": "0.0.1",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Elaine Ho" }],
    "code_repository": "https://github.com/rosalindfranklininstitute/napari-quoll",
    "conda": [],
    "description": "<!-- This file is a placeholder for customizing description of your plugin  on the napari hub if you wish. The readme file will be used by default if you wish not to do any customization for the napari hub listing.  If you need some help writing a good description, check out our  [guide](https://github.com/chanzuckerberg/napari-hub/wiki/Writing-the-Perfect-Description-for-your-Plugin) -->  # Description  This Napari plugin allows you to estimate the local resolution of images using the Fourier Ring Correlation (FRC).   ## How it works  Quoll splits the input image into square tiles of a user specified size. This tile is then split into two sets of sub-image pairs, and the average FRC resolution of these sets is calculated. A calibration factor is applied to match this one-image FRC measurement to the standard two-image FRC resolution.  ## What was Quoll developed for?  Quoll was developed for cryo-electron microscopy images.   # Who is this for?  Anyone who might want to measure spatial variations in image resolution. See [Quoll](https://github.com/rosalindfranklininstitute/quoll) if you would like to access the command-line interface option for batch-scripting, or if you would like to customise the tool for your needs.  # Getting started  ## Installation  In a terminal, create a new conda environment with Python 3.7 and install the latest version of `napari-quoll`.   ``` conda create -n napari-quoll python=3.7 conda activate napari-quoll pip install git+https://github.com/rosalindfranklininstitute/napari-quoll.git ```  Launch Napari ``` python -m napari ```  (If that does not work, run `pip install napari[all]` and try again)  ## Napari-quoll plugin  Napari-quoll should be under the plugins menu at the top.  The menu should pop up on the right.   1. Choose the filename for the image you would like to try this with (a test image is included [here](https://github.com/rosalindfranklininstitute/napari-quoll/tree/main/test_data)).  2. Specify the pixel size in physical units. The test image pixel size is 3 nm. 3. Specify the tile-size in pixel units (how fine the resolution map should be) 4. Specify if you would like to save the results as .csv and the filepath for this. 5. Click run!   Once complete, the resolution map should be shown in the main window, and summary statistics and a histogram of resolutions should pop up on the bottom right, though these windows might need resizing.  # FAQ  ### 1. What tile size should I use?  The minimum tile size is 128 x 128 pixels, the smaller the tile the finer the resolution map, though if the tile is too small, there might not be any features to calculate resolution on.  ### 2. How was the one-image FRC calibrated to match the two-image FRC?  The original implementation by [Koho et al](https://github.com/sakoho81/miplib) obtained a calibration function based on a set of pairs of optical microscopy images taken at different pixel sizes. We have repeated this with cryo-electron microscopy images to ensure that our calibration is correct for EM images.  # Getting help  Submit an [issue](https://github.com/rosalindfranklininstitute/napari-quoll/issues) :)  # Citations/Links  ### 1. Original implementation of single-image FRC resolution measurement  Koho, S. et al. Fourier ring correlation simplifies image restoration in fluorescence microscopy. [Nat. Commun. 10 3103 (2019).](https://www.nature.com/articles/s41467-019-11024-z)  ### 2. Quoll - adapted implementation for local resolution measurement of cryo-EM images Elaine Ho. (2022). rosalindfranklininstitute/quoll: v0.0.1 (v0.0.1). Zenodo. https://doi.org/10.5281/zenodo.6958768 ",
    "description_content_type": "text/markdown",
    "description_text": " Description This Napari plugin allows you to estimate the local resolution of images using the Fourier Ring Correlation (FRC).  How it works Quoll splits the input image into square tiles of a user specified size. This tile is then split into two sets of sub-image pairs, and the average FRC resolution of these sets is calculated. A calibration factor is applied to match this one-image FRC measurement to the standard two-image FRC resolution. What was Quoll developed for? Quoll was developed for cryo-electron microscopy images.  Who is this for? Anyone who might want to measure spatial variations in image resolution. See Quoll if you would like to access the command-line interface option for batch-scripting, or if you would like to customise the tool for your needs. Getting started Installation In a terminal, create a new conda environment with Python 3.7 and install the latest version of napari-quoll.  conda create -n napari-quoll python=3.7 conda activate napari-quoll pip install git+https://github.com/rosalindfranklininstitute/napari-quoll.git Launch Napari python -m napari (If that does not work, run pip install napari[all] and try again) Napari-quoll plugin Napari-quoll should be under the plugins menu at the top. The menu should pop up on the right.   Choose the filename for the image you would like to try this with (a test image is included here).  Specify the pixel size in physical units. The test image pixel size is 3 nm. Specify the tile-size in pixel units (how fine the resolution map should be) Specify if you would like to save the results as .csv and the filepath for this. Click run!   Once complete, the resolution map should be shown in the main window, and summary statistics and a histogram of resolutions should pop up on the bottom right, though these windows might need resizing. FAQ 1. What tile size should I use? The minimum tile size is 128 x 128 pixels, the smaller the tile the finer the resolution map, though if the tile is too small, there might not be any features to calculate resolution on. 2. How was the one-image FRC calibrated to match the two-image FRC? The original implementation by Koho et al obtained a calibration function based on a set of pairs of optical microscopy images taken at different pixel sizes. We have repeated this with cryo-electron microscopy images to ensure that our calibration is correct for EM images. Getting help Submit an issue :) Citations/Links 1. Original implementation of single-image FRC resolution measurement Koho, S. et al. Fourier ring correlation simplifies image restoration in fluorescence microscopy. Nat. Commun. 10 3103 (2019). 2. Quoll - adapted implementation for local resolution measurement of cryo-EM images Elaine Ho. (2022). rosalindfranklininstitute/quoll: v0.0.1 (v0.0.1). Zenodo. https://doi.org/10.5281/zenodo.6958768",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari Quoll",
    "documentation": "https://github.com/rosalindfranklininstitute/napari-quoll#README.md",
    "first_released": "2022-08-26T15:11:31.357188Z",
    "license": "Apache-2.0",
    "name": "napari-quoll",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/rosalindfranklininstitute/napari-quoll",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-08-26T15:11:31.357188Z",
    "report_issues": "https://github.com/rosalindfranklininstitute/napari-quoll/issues",
    "requirements": [
      "numpy",
      "magicgui",
      "qtpy",
      "quoll",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "summary": "Resolution estimation for electron tomography",
    "support": "https://github.com/rosalindfranklininstitute/napari-quoll/issues",
    "twitter": "",
    "version": "0.0.1",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Robert Haase" }, { "name": "Johannes Müller" }],
    "category": {
      "Workflow step": [
        "Clustering",
        "Visualization",
        "Image annotation",
        "Image Segmentation"
      ]
    },
    "category_hierarchy": {
      "Workflow step": [
        ["Clustering", "Distribution-based clustering"],
        ["Visualization", "Image visualisation", "Surface rendering"],
        ["Image annotation", "Sparse image annotation", "Landmark assignment"],
        ["Image Segmentation", "Cell segmentation"]
      ]
    },
    "code_repository": "https://github.com/haesleinhuepf/napari-process-points-and-surfaces",
    "conda": [],
    "description": "# napari-process-points-and-surfaces (nppas)  [![License](https://img.shields.io/pypi/l/napari-process-points-and-surfaces.svg?color=green)](https://github.com/haesleinhuepf/napari-process-points-and-surfaces/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-process-points-and-surfaces.svg?color=green)](https://pypi.org/project/napari-process-points-and-surfaces) [![Python Version](https://img.shields.io/pypi/pyversions/napari-process-points-and-surfaces.svg?color=green)](https://python.org) [![tests](https://github.com/haesleinhuepf/napari-process-points-and-surfaces/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-process-points-and-surfaces/actions) [![codecov](https://codecov.io/gh/haesleinhuepf/napari-process-points-and-surfaces/branch/master/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-process-points-and-surfaces) [![Development Status](https://img.shields.io/pypi/status/napari-process-points-and-surfaces.svg)](https://en.wikipedia.org/wiki/Software_release_life_cycle#Alpha) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-process-points-and-surfaces)](https://napari-hub.org/plugins/napari-process-points-and-surfaces)  Process and analyze surfaces using [open3d](http://www.open3d.org/) and [vedo](https://vedo.embl.es/) in [napari].  ## Usage  You find a couple of surface generation, smoothing and analysis functions in the menu `Tools > Surfaces` and `Tools > Points`. For detailed explanation of the underlying algorithms, please refer to the [open3d](http://www.open3d.org/docs/release/) documentation. Some code snippets and the knot example data have been taken from the open3d project which is  [MIT licensed](https://github.com/haesleinhuepf/napari-process-points-and-surfaces/blob/main/licenses_third_party/open3d_LICENSE)  and from the [vedo documentation](https://vedo.embl.es/autodocs/index.html)  which is [MIT licensed](https://github.com/haesleinhuepf/napari-process-points-and-surfaces/blob/main/licenses_third_party/vedo_LICENSE). The Standford Bunny example dataset has been taken from the [The Stanford 3D Scanning Repository](http://graphics.stanford.edu/data/3Dscanrep/).  For processing meshes in Python scripts, see the [demo notebook](https://github.com/haesleinhuepf/napari-process-points-and-surfaces/blob/main/docs/demo.ipynb). There you also learn how this screenshot is made:  ![img.png](https://github.com/haesleinhuepf/napari-process-points-and-surfaces/raw/main/docs/screenshot.png)  For performing quantitative measurements of meshes in Python scripts, see the [demo notebook](https://github.com/haesleinhuepf/napari-process-points-and-surfaces/blob/main/docs/quality_measurements.ipynb).  There you also learn how this screenshot is made:  ![img.png](https://github.com/haesleinhuepf/napari-process-points-and-surfaces/raw/main/docs/screenshot2.png)  ### Surface measurements and annotations  Using the menu `Tools > Measurement > Surface quality table (vedo, nppas)` you can derived quantiative measurements of the vertices in a given surface layer.   ![img_1.png](https://github.com/haesleinhuepf/napari-process-points-and-surfaces/raw/main/docs/surface_measurements.png)  To differentiate regions when analyzing those measurements it is recommended to use the menu `Tools > Surfaces > Annotate surface manually (nppas)` after measurements have been made. This tool allows you to draw annotation label values on the surface.  It is recommended to do activate a colorful colormap such as `hsv` before starting to draw annotations.  Furthermore, set the maximum of the contrast limit range to the number of regions you want to annotate + 1. Annotations can be drawn as freehand lines and circles.  ![img.png](https://github.com/haesleinhuepf/napari-process-points-and-surfaces/raw/main/docs/surface_annotation.png)  After measurements and annotations were done, you can save the annotation in the same measurement table using the menu `Tools > Measurement > Surface quality/annotation to table (nppas)`  ![img.png](https://github.com/haesleinhuepf/napari-process-points-and-surfaces/raw/main/docs/surface_annotation_in_table.png)  ### Measurement visualization  To visualize measurements on the surface, just double-click on the table column headers. This also allows to visualize  measurements and annotations side-by-side.  ![img.png](https://github.com/haesleinhuepf/napari-process-points-and-surfaces/raw/main/docs/measurement_visualization.gif)  ## Installation  You can install `napari-process-points-and-surfaces` via [pip]:  ``` pip install napari-process-points-and-surfaces ```  ## See also  There are other napari plugins with similar / overlapping functionality * [pymeshlab](https://www.napari-hub.org/plugins/napari-pymeshlab) * [morphometrics](https://www.napari-hub.org/plugins/morphometrics)   * [napari-pyclesperanto-assistant](https://www.napari-hub.org/plugins/napari-pyclesperanto-assistant)  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-process-points-and-surfaces\" is free and open source software  ## Issues  If you encounter any problems, please create a thread on [image.sc] along with a detailed description and tag [@haesleinhuepf].  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/haesleinhuepf/napari-process-points-and-surfaces/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/  [image.sc]: https://image.sc [@haesleinhuepf]: https://twitter.com/haesleinhuepf ",
    "description_content_type": "text/markdown",
    "description_text": "napari-process-points-and-surfaces (nppas)        Process and analyze surfaces using open3d and vedo in napari. Usage You find a couple of surface generation, smoothing and analysis functions in the menu Tools > Surfaces and Tools > Points. For detailed explanation of the underlying algorithms, please refer to the open3d documentation. Some code snippets and the knot example data have been taken from the open3d project which is  MIT licensed  and from the vedo documentation  which is MIT licensed. The Standford Bunny example dataset has been taken from the The Stanford 3D Scanning Repository. For processing meshes in Python scripts, see the demo notebook. There you also learn how this screenshot is made:  For performing quantitative measurements of meshes in Python scripts, see the demo notebook.  There you also learn how this screenshot is made:  Surface measurements and annotations Using the menu Tools > Measurement > Surface quality table (vedo, nppas) you can derived quantiative measurements of the vertices in a given surface layer.   To differentiate regions when analyzing those measurements it is recommended to use the menu Tools > Surfaces > Annotate surface manually (nppas) after measurements have been made. This tool allows you to draw annotation label values on the surface.  It is recommended to do activate a colorful colormap such as hsv before starting to draw annotations.  Furthermore, set the maximum of the contrast limit range to the number of regions you want to annotate + 1. Annotations can be drawn as freehand lines and circles.  After measurements and annotations were done, you can save the annotation in the same measurement table using the menu Tools > Measurement > Surface quality/annotation to table (nppas)  Measurement visualization To visualize measurements on the surface, just double-click on the table column headers. This also allows to visualize  measurements and annotations side-by-side.  Installation You can install napari-process-points-and-surfaces via pip: pip install napari-process-points-and-surfaces See also There are other napari plugins with similar / overlapping functionality * pymeshlab * morphometrics * napari-pyclesperanto-assistant Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-process-points-and-surfaces\" is free and open source software Issues If you encounter any problems, please create a thread on image.sc along with a detailed description and tag @haesleinhuepf.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari-process-points-and-surfaces",
    "documentation": "https://github.com/haesleinhuepf/napari-process-points-and-surfaces#README.md",
    "first_released": "2022-02-05T13:31:53.600823Z",
    "license": "BSD-3-Clause",
    "name": "napari-process-points-and-surfaces",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/haesleinhuepf/napari-process-points-and-surfaces",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-10-15T21:45:49.977082Z",
    "report_issues": "https://github.com/haesleinhuepf/napari-process-points-and-surfaces/issues",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "napari",
      "napari-tools-menu (>=0.1.14)",
      "napari-time-slicer (>=0.4.5)",
      "napari-workflows (>=0.2.3)",
      "open3d",
      "ipyvtklink",
      "vedo",
      "napari-skimage-regionprops (>=0.5.5)",
      "pandas",
      "imageio (!=2.22.1)"
    ],
    "summary": "Process and analyze surfaces using open3d and vedo in napari",
    "support": "https://github.com/haesleinhuepf/napari-process-points-and-surfaces/issues",
    "twitter": "",
    "version": "0.3.3",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      { "email": "kevin.yamauchi@gmail.com", "name": "Kevin Yamauchi" }
    ],
    "code_repository": "https://github.com/kevinyamauchi/napari-properties-viewer",
    "description": "# napari-properties-viewer  [![License](https://img.shields.io/pypi/l/napari-properties-viewer.svg?color=green)](https://github.com/napari/napari-properties-viewer/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-properties-viewer.svg?color=green)](https://pypi.org/project/napari-properties-viewer) [![Python Version](https://img.shields.io/pypi/pyversions/napari-properties-viewer.svg?color=green)](https://python.org) [![tests](https://github.com/kevinyamauchi/napari-properties-viewer/workflows/tests/badge.svg)](https://github.com/kevinyamauchi/napari-properties-viewer/actions) [![codecov](https://codecov.io/gh/kevinyamauchi/napari-properties-viewer/branch/master/graph/badge.svg)](https://codecov.io/gh/kevinyamauchi/napari-properties-viewer)  A viewer for napari layer properties  ![image](resources/properties_viewer.gif) ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/docs/plugins/index.html -->  ## Installation  You can install `napari-properties-viewer` via [pip]:      pip install napari-properties-viewer      ## Using the properties viewer table  1. Open a a napari viewer with a layer with properties (e.g., Points) 2. View the properties by opening the properties viewer plugin from Plugins menu -> Add dock widget -> napari-propertiews-viewer: properties table 3. The layer property values are now displayed in the table widget. You can edit the values by double clicking the cell of interest and entering a new value.  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-properties-viewer\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin [file an issue]: https://github.com/kevinyamauchi/napari-properties-viewer/issues [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-properties-viewer      A viewer for napari layer properties  This napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.  Installation You can install napari-properties-viewer via pip: pip install napari-properties-viewer  Using the properties viewer table  Open a a napari viewer with a layer with properties (e.g., Points) View the properties by opening the properties viewer plugin from Plugins menu -> Add dock widget -> napari-propertiews-viewer: properties table The layer property values are now displayed in the table widget. You can edit the values by double clicking the cell of interest and entering a new value.  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-properties-viewer\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "napari-properties-viewer",
    "documentation": "https://github.com/kevinyamauchi/napari-properties-viewer",
    "first_released": "2021-04-28T13:47:44.032297Z",
    "license": "BSD-3-Clause",
    "name": "napari-properties-viewer",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/kevinyamauchi/napari-properties-viewer",
    "python_version": ">=3.6",
    "reader_file_extensions": [],
    "release_date": "2021-06-30T14:08:46.312555Z",
    "report_issues": "https://github.com/kevinyamauchi/napari-properties-viewer/issues",
    "requirements": ["napari-plugin-engine (>=0.1.4)", "numpy"],
    "summary": "A viewer for napari layer properties",
    "support": "https://github.com/kevinyamauchi/napari-properties-viewer/issues",
    "twitter": "",
    "version": "0.0.2",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      { "email": "lorenzo.gaifas@gmail.com", "name": "Lorenzo Gaifas" }
    ],
    "category": { "Workflow step": ["Visualization"] },
    "category_hierarchy": { "Workflow step": [["Visualization"]] },
    "code_repository": "https://github.com/brisvag/napari-properties-plotter",
    "description": "# napari-properties-plotter  [![License](https://img.shields.io/pypi/l/napari-properties-plotter.svg?color=green)](https://github.com/brisvag/napari-properties-plotter/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-properties-plotter.svg?color=green)](https://pypi.org/project/napari-properties-plotter) [![Python Version](https://img.shields.io/pypi/pyversions/napari-properties-plotter.svg?color=green)](https://python.org) [![tests](https://github.com/brisvag/napari-properties-plotter/workflows/tests/badge.svg)](https://github.com/brisvag/napari-properties-plotter/actions) [![codecov](https://codecov.io/gh/brisvag/napari-properties-plotter/branch/master/graph/badge.svg)](https://codecov.io/gh/brisvag/napari-properties-plotter)  A napari plugin that automatically generates interactive plots based on layer properties.  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/docs/plugins/index.html -->  ## Installation  You can install `napari-properties-plotter` via [pip]:      pip install napari-properties-plotter  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-properties-plotter\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin [file an issue]: https://github.com/brisvag/napari-properties-plotter/issues [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-properties-plotter      A napari plugin that automatically generates interactive plots based on layer properties.  This napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.  Installation You can install napari-properties-plotter via pip: pip install napari-properties-plotter  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-properties-plotter\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari-properties-plotter",
    "documentation": "https://github.com/brisvag/napari-properties-plotter#README.md",
    "first_released": "2021-05-26T13:31:54.874796Z",
    "license": "BSD-3-Clause",
    "name": "napari-properties-plotter",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/brisvag/napari-properties-plotter",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2021-12-01T11:04:29.153379Z",
    "report_issues": "https://github.com/brisvag/napari-properties-plotter/issues",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "pandas",
      "pyqtgraph",
      "qtpy"
    ],
    "summary": "A napari plugin that automatically generates interactive plots based on layer properties.",
    "support": "https://github.com/brisvag/napari-properties-plotter/issues",
    "twitter": "",
    "version": "0.2.2",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      { "email": "delgrosso.nick@gmail.com", "name": "Nicholas A. Del Grosso" }
    ],
    "code_repository": "https://github.com/nickdelgrosso/napari-video-cvdask",
    "description": " # Description  This is a simple video reader plugin that creates Dask Arrays using OpenCV' for file reading.   Anything OpenCV's VideoCapture() function can read should work here.  # Supported Data  Supports mp4, mov, and avi extensions.s  # Quickstart  ```python napari.view_image('myvid.avi') ```  ",
    "description_content_type": "text/markdown",
    "description_text": "Description This is a simple video reader plugin that creates Dask Arrays using OpenCV' for file reading. Anything OpenCV's VideoCapture() function can read should work here. Supported Data Supports mp4, mov, and avi extensions.s Quickstart python napari.view_image('myvid.avi')",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari VideoCVDask",
    "documentation": "https://github.com/nickdelgrosso/napari-video-cvdask#README.md",
    "first_released": "2022-02-24T01:38:30.436652Z",
    "license": "MIT",
    "name": "napari-video-cvdask",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader"],
    "project_site": "https://github.com/nickdelgrosso/napari-video-cvdask",
    "python_version": ">=3.7",
    "reader_file_extensions": ["*.avi", "*.mp4", "*.mov"],
    "release_date": "2022-02-25T19:53:29.796712Z",
    "report_issues": "https://github.com/nickdelgrosso/napari-video-cvdask/issues",
    "requirements": ["dask-image", "av"],
    "summary": "A Video File Reader that uses OpenCV2 and Dask Arrays",
    "support": "https://github.com/nickdelgrosso/napari-video-cvdask/issues",
    "twitter": "",
    "version": "0.2.1",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Seongbin Lim" }],
    "code_repository": null,
    "conda": [],
    "description": "# napari-proofread-brainbow  [![License](https://img.shields.io/pypi/l/napari-proofread-brainbow.svg?color=green)](https://github.com/sbinnee/napari-proofread-brainbow/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-proofread-brainbow.svg?color=green)](https://pypi.org/project/napari-proofread-brainbow) [![Python Version](https://img.shields.io/pypi/pyversions/napari-proofread-brainbow.svg?color=green)](https://python.org) [![tests](https://github.com/sbinnee/napari-proofread-brainbow/workflows/tests/badge.svg)](https://github.com/sbinnee/napari-proofread-brainbow/actions) [![codecov](https://codecov.io/gh/sbinnee/napari-proofread-brainbow/branch/main/graph/badge.svg)](https://codecov.io/gh/sbinnee/napari-proofread-brainbow) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-proofread-brainbow)](https://napari-hub.org/plugins/napari-proofread-brainbow)  proofreading with napari  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/plugins/stable/index.html -->  ## Installation  You can install `napari-proofread-brainbow` via [pip]:      pip install napari-proofread-brainbow     ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [MIT] license, \"napari-proofread-brainbow\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-proofread-brainbow       proofreading with napari  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install napari-proofread-brainbow via pip: pip install napari-proofread-brainbow  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the MIT license, \"napari-proofread-brainbow\" is free and open source software Issues If you encounter any problems, please [file an issue] along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari proofread brainbow",
    "documentation": "",
    "first_released": "2022-08-30T09:45:51.714619Z",
    "license": "MIT",
    "name": "napari-proofread-brainbow",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-09-21T08:33:03.270553Z",
    "report_issues": "",
    "requirements": null,
    "summary": "proofreading Brainbow images with napari",
    "support": "",
    "twitter": "",
    "version": "0.3.0",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Jonas Windhager" }],
    "code_repository": "https://github.com/BodenmillerGroup/napari-roi",
    "conda": [{ "channel": "conda-forge", "package": "napari-roi" }],
    "description": "# napari-roi  [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-roi)](https://napari-hub.org/plugins/napari-roi) [![PyPI](https://img.shields.io/pypi/v/napari-roi.svg?color=green)](https://pypi.org/project/napari-roi) [![License](https://img.shields.io/pypi/l/napari-roi.svg?color=green)](https://github.com/BodenmillerGroup/napari-roi/raw/main/LICENSE) [![Python Version](https://img.shields.io/pypi/pyversions/napari-roi.svg?color=green)](https://python.org) [![Issues](https://img.shields.io/github/issues/BodenmillerGroup/napari-roi)](https://github.com/BodenmillerGroup/napari-roi/issues) [![Pull requests](https://img.shields.io/github/issues-pr/BodenmillerGroup/napari-roi)](https://github.com/BodenmillerGroup/napari-roi/pulls)  Select regions of interest (ROIs) using napari  ## Installation  You can install napari-roi via [pip](https://pypi.org/project/pip/):      pip install napari-roi  Alternatively, you can install napari-roi via [conda](https://conda.io/):      conda install -c conda-forge napari-roi  ## Usage  The *napari-roi* plugin can be opened from within napari (`napari -> napari-roi: regions of interest`) and operates on napari *Shapes* layers.  ROIs can be added to any napari *Shapes* layer, either by drawing a standard napari shape (e.g. rectangle), or by adding a rectangular ROI of specified size using the `Add ROI` functionality in the *napari-roi* widget. Each ROI is associated with a name, a position (X/Y origin), and a size (width/height). The location of the X/Y origin of all ROIs can be chosen in the *napari-roi* widget. Note that any shape supported by napari (e.g. ellipse, rectangle, polygon, line, path) can serve as an ROI; for non-rectangular shapes, *napari-roi* computes rectangular bounding boxes aligned with the napari coordinate system to determine their positions and sizes. ROIs can be edited or deleted by modifying the corresponding shapes in napari, or by editing the corresponding row in the *napari-roi* widget.  All ROIs in the current *Shapes* layer can be saved to a comma-separated values (CSV) file using the `Save` functionality in the *napari-roi* widget. When the `Autosave` option is checked, the file is automatically updated on every ROI change. Note that the selected file is specific to the current *Shapes* layer; ROIs from different *Shapes* layers cannot be saved to the same file. ROIs can be loaded from a previously saved file and added to the current *Shapes* layer by opening the file in the *napari-roi* widget.  CSV files saved using *napari-roi* adhere to the following format:  | Columns | Description | | --- | --- | | `Name` | ROI name | | `X`, `Y` | Position (X/Y origin) | | `W`, `H` | Size (width/height) |  ## Authors  Created and maintained by Jonas Windhager [jonas.windhager@uzh.ch](mailto:jonas.windhager@uzh.ch)  ## Contributing  [Contributing](https://github.com/BodenmillerGroup/napari-roi/blob/main/CONTRIBUTING.md)  ## Changelog  [Changelog](https://github.com/BodenmillerGroup/napari-roi/blob/main/CHANGELOG.md)  ## License  [MIT](https://github.com/BodenmillerGroup/napari-roi/blob/main/LICENSE) ",
    "description_content_type": "text/markdown",
    "description_text": "napari-roi       Select regions of interest (ROIs) using napari Installation You can install napari-roi via pip: pip install napari-roi  Alternatively, you can install napari-roi via conda: conda install -c conda-forge napari-roi  Usage The napari-roi plugin can be opened from within napari (napari -> napari-roi: regions of interest) and operates on napari Shapes layers. ROIs can be added to any napari Shapes layer, either by drawing a standard napari shape (e.g. rectangle), or by adding a rectangular ROI of specified size using the Add ROI functionality in the napari-roi widget. Each ROI is associated with a name, a position (X/Y origin), and a size (width/height). The location of the X/Y origin of all ROIs can be chosen in the napari-roi widget. Note that any shape supported by napari (e.g. ellipse, rectangle, polygon, line, path) can serve as an ROI; for non-rectangular shapes, napari-roi computes rectangular bounding boxes aligned with the napari coordinate system to determine their positions and sizes. ROIs can be edited or deleted by modifying the corresponding shapes in napari, or by editing the corresponding row in the napari-roi widget. All ROIs in the current Shapes layer can be saved to a comma-separated values (CSV) file using the Save functionality in the napari-roi widget. When the Autosave option is checked, the file is automatically updated on every ROI change. Note that the selected file is specific to the current Shapes layer; ROIs from different Shapes layers cannot be saved to the same file. ROIs can be loaded from a previously saved file and added to the current Shapes layer by opening the file in the napari-roi widget. CSV files saved using napari-roi adhere to the following format: | Columns | Description | | --- | --- | | Name | ROI name | | X, Y | Position (X/Y origin) | | W, H | Size (width/height) | Authors Created and maintained by Jonas Windhager jonas.windhager@uzh.ch Contributing Contributing Changelog Changelog License MIT",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "napari-roi",
    "documentation": "https://github.com/BodenmillerGroup/napari-roi#README.md",
    "first_released": "2021-11-19T19:28:29.299053Z",
    "license": "MIT",
    "name": "napari-roi",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/BodenmillerGroup/napari-roi",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-08-10T14:46:04.023324Z",
    "report_issues": "https://github.com/BodenmillerGroup/napari-roi/issues",
    "requirements": ["numpy", "pandas", "qtpy"],
    "summary": "Select regions of interest (ROIs) using napari",
    "support": "https://github.com/BodenmillerGroup/napari-roi/issues",
    "twitter": "",
    "version": "0.1.7",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      { "email": "meriadec.prigent@gmail.com", "name": "Sylvain Prigent" }
    ],
    "code_repository": "https://github.com/sylvainprigent/napari-sairyscan",
    "conda": [],
    "description": "This plugin implements various methods to reconstruct high resolution images for the Airyscan  microscope raw data   ## Description  Airyscan raw images are confocal images obtained from 32 sub-detectors. Several methods can be used to reconstruct a high resolution image from these 32 sub-detectors images. This plugin implements the following methods: - **Pseudo-confocal**: creates a pseudo confocal image by summing 7, 19 or 19 detectors - **ISM**: creates a higher resolution image by summing the images from the 32 sub-detectors after  co-registering all the images to the central detector. A deconvolution algorithm can be applied in  post-processing to gain more resolution - **IFED**: reconstructs a high resolution image by subtracting the outer ring detector to the central  detector. This method can be interpreted as a 'virtual STED' - **ISFED**: reconstructs a high resolution image by combining the co-registered detectors images and the raw detectors images      - **Join deconvolution**: reconstructs a high resolution image by jointly deblurring all 32 detectors  with a variational approach.  ![Example image](https://raw.githubusercontent.com/sylvainprigent/napari-sairyscan/main/docs/images/screenshot.png)  ## Intended Audience & Supported Data  Supported data are raw images from the Airyscan microscope. These images must be stacks of 32  layers corresponding to the 32 detectors. The Airyscan reader plugin can open .czi raw files. Data can also be stored in any format that napari can open.   ## Quickstart  - Open the sample image from the menu *File > Open samples > napari-sairyscan > SAiryscan*  ![Open image](https://raw.githubusercontent.com/sylvainprigent/napari-sairyscan/main/docs/images/open_sample.png) ![Open image](https://raw.githubusercontent.com/sylvainprigent/napari-sairyscan/main/docs/images/samples.png)   - Open the SAiryscan plugin from the menu *Plugins > napari-sairyscan: Airyscan reconstruction*  ![Open image](https://raw.githubusercontent.com/sylvainprigent/napari-sairyscan/main/docs/images/open_plugin.png) ![Open image](https://raw.githubusercontent.com/sylvainprigent/napari-sairyscan/main/docs/images/join_deconv_plugin.png)  - We select the `join deconvolution` method and run it with the default parameters. Default parameters are optimized for the sample image:  ![Open image](https://raw.githubusercontent.com/sylvainprigent/napari-sairyscan/main/docs/images/join_deconv_result.png)   ## Getting Help  For any bug report or feature request please [file an issue]  ## How to Cite  If you use this plugin please cite the [paper](https://ieeexplore.ieee.org/document/9054640):          @INPROCEEDINGS{9054640,     author={Prigent, Sylvain and Dutertre, Stephanie and Kervrann, Charles},     booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},      title={Empirical Sure-Guided Microscopy Super-Resolution Image Reconstruction from Confocal Multi-Array Detectors},      year={2020},     volume={},     number={},     pages={1075-1079},     doi={10.1109/ICASSP40776.2020.9054640}}    [file an issue]: https://github.com/sylvainprigent/napari-sairyscan/issues",
    "description_content_type": "text/markdown",
    "description_text": "This plugin implements various methods to reconstruct high resolution images for the Airyscan  microscope raw data  Description Airyscan raw images are confocal images obtained from 32 sub-detectors. Several methods can be used to reconstruct a high resolution image from these 32 sub-detectors images. This plugin implements the following methods: - Pseudo-confocal: creates a pseudo confocal image by summing 7, 19 or 19 detectors - ISM: creates a higher resolution image by summing the images from the 32 sub-detectors after  co-registering all the images to the central detector. A deconvolution algorithm can be applied in  post-processing to gain more resolution - IFED: reconstructs a high resolution image by subtracting the outer ring detector to the central  detector. This method can be interpreted as a 'virtual STED' - ISFED: reconstructs a high resolution image by combining the co-registered detectors images and the raw detectors images    - Join deconvolution: reconstructs a high resolution image by jointly deblurring all 32 detectors  with a variational approach.  Intended Audience & Supported Data Supported data are raw images from the Airyscan microscope. These images must be stacks of 32  layers corresponding to the 32 detectors. The Airyscan reader plugin can open .czi raw files. Data can also be stored in any format that napari can open.  Quickstart  Open the sample image from the menu File > Open samples > napari-sairyscan > SAiryscan     Open the SAiryscan plugin from the menu Plugins > napari-sairyscan: Airyscan reconstruction     We select the join deconvolution method and run it with the default parameters. Default parameters are optimized for the sample image:   Getting Help For any bug report or feature request please file an issue How to Cite If you use this plugin please cite the paper: @INPROCEEDINGS{9054640, author={Prigent, Sylvain and Dutertre, Stephanie and Kervrann, Charles}, booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},  title={Empirical Sure-Guided Microscopy Super-Resolution Image Reconstruction from Confocal Multi-Array Detectors},  year={2020}, volume={}, number={}, pages={1075-1079}, doi={10.1109/ICASSP40776.2020.9054640}} ",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari sairyscan",
    "documentation": "https://github.com/sylvainprigent/napari-sairyscan#README.md",
    "first_released": "2022-06-03T17:56:24.998564Z",
    "license": "BSD-3-Clause",
    "name": "napari-sairyscan",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader", "widget", "sample_data"],
    "project_site": "https://github.com/sylvainprigent/napari-sairyscan",
    "python_version": ">=3.8",
    "reader_file_extensions": ["*.czi"],
    "release_date": "2022-06-07T10:20:46.204193Z",
    "report_issues": "https://github.com/sylvainprigent/napari-sairyscan/issues",
    "requirements": [
      "numpy",
      "magicgui",
      "qtpy",
      "sairyscan (>=0.0.2)",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "summary": "Airyscan image reconstruction",
    "support": "https://github.com/sylvainprigent/napari-sairyscan/issues",
    "twitter": "",
    "version": "0.0.2",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Andrea Bassi" }, { "name": "Giorgia Tortora" }],
    "category": {
      "Supported data": ["Time series"],
      "Workflow step": ["Image registration"]
    },
    "category_hierarchy": {
      "Supported data": [["Time series"]],
      "Workflow step": [["Image registration"]]
    },
    "code_repository": "https://github.com/GiorgiaTortora/napari-roi-registration",
    "conda": [
      { "channel": "conda-forge", "package": "napari-roi-registration" }
    ],
    "description": "# napari-roi-registration  [![License](https://img.shields.io/pypi/l/napari-roi-registration.svg?color=green)](https://github.com/GiorgiaTortora/napari-roi-registration/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-roi-registration.svg?color=green)](https://pypi.org/project/napari-roi-registration) [![Python Version](https://img.shields.io/pypi/pyversions/napari-roi-registration.svg?color=green)](https://python.org) [![tests](https://github.com/GiorgiaTortora/napari-roi-registration/workflows/tests/badge.svg)](https://github.com/GiorgiaTortora/napari-roi-registration/actions) [![codecov](https://codecov.io/gh/GiorgiaTortora/napari-roi-registration/branch/main/graph/badge.svg)](https://codecov.io/gh/GiorgiaTortora/napari-roi-registration) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-roi-registration)](https://napari-hub.org/plugins/napari-roi-registration)  A Napari plugin for the registration of regions of interests (ROI) in a time lapse acquistion and processing of the intensity of the registered data.  The ROI are defined using a Labels layer. Registration of multiple ROIs is supported.    The `Registration` widget uses the user-defined labels, constructs a rectangular ROI around each of them and registers the ROIs in each time frame.  The `Processing` widget measures the ROI displacements and extracts the average intensity of the ROI, calculated on the area of the labels.  The `Subtract background` widget subtracts a background on each frame, calculated as the mean intensity on a Labels layer. Tipically useful when ambient light affects the measurement.    ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/plugins/stable/index.html -->  ![raw](https://github.com/GiorgiaTortora/napari-roi-registration/raw/main/images/roi_registration.gif)  ## Installation  You can install `napari-roi-registration` via [pip]:      pip install napari-roi-registration    To install latest development version :      pip install git+https://github.com/GiorgiaTortora/napari-roi-registration.git  ## Usage  ### Registration Widget  1. Create a new Labels layer and draw one or more labels where you want to select a ROI (Region Of Interest). Each color in the same Labels layer represents a different label which will correspond to a different ROI.  ![raw](https://github.com/GiorgiaTortora/napari-roi-registration/raw/main/images/Picture1.png)  2. Push the `Register ROIs` button: registration of the entire stack will be performed. When the registration is finished two new layers will appear in the viewer. One layer contains the centroids of the drawn labels while the other contains the bounding boxes enclosing the ROIs. The registration starts from the currently selected frame. If `register entire stack` is selected, the registration will create a new layer for each label, with the registered ROI stacks.  ![raw](https://github.com/GiorgiaTortora/napari-roi-registration/raw/main/images/Picture2.png)  ### Processing Widget  Pushing the `Process registered ROIs` button, the registered ROIs will be analyzed. The intensity of the registered ROIs (measured on the area of the selected label) and the displacement of the ROIs will be calculated. If `plot results` is selected the plot of displacement vs time index and mean intensity vs time index will appear in the Console. Choosing the `save results` option, an excel file containing ROIs positions, displacements and intensities, will be saved.   ![raw](https://github.com/GiorgiaTortora/napari-roi-registration/raw/main/images/Picture3.png)  ### Background Widget  1. Create a new Labels layer and draw a label on the area where you want to calculate the background.   ![raw](https://github.com/GiorgiaTortora/napari-roi-registration/raw/main/images/Picture4.png)  2. Push the `Subtract background` button. A new image layer will appear in the viewer. This layer contains the image to which the background was subtracted.  ## Contributing   Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-roi-registration\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/GiorgiaTortora/napari-roi-registration/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-roi-registration       A Napari plugin for the registration of regions of interests (ROI) in a time lapse acquistion and processing of the intensity of the registered data. The ROI are defined using a Labels layer. Registration of multiple ROIs is supported.   The Registration widget uses the user-defined labels, constructs a rectangular ROI around each of them and registers the ROIs in each time frame. The Processing widget measures the ROI displacements and extracts the average intensity of the ROI, calculated on the area of the labels. The Subtract background widget subtracts a background on each frame, calculated as the mean intensity on a Labels layer. Tipically useful when ambient light affects the measurement.    This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.   Installation You can install napari-roi-registration via pip: pip install napari-roi-registration  To install latest development version : pip install git+https://github.com/GiorgiaTortora/napari-roi-registration.git  Usage Registration Widget  Create a new Labels layer and draw one or more labels where you want to select a ROI (Region Of Interest). Each color in the same Labels layer represents a different label which will correspond to a different ROI.    Push the Register ROIs button: registration of the entire stack will be performed. When the registration is finished two new layers will appear in the viewer. One layer contains the centroids of the drawn labels while the other contains the bounding boxes enclosing the ROIs. The registration starts from the currently selected frame. If register entire stack is selected, the registration will create a new layer for each label, with the registered ROI stacks.   Processing Widget Pushing the Process registered ROIs button, the registered ROIs will be analyzed. The intensity of the registered ROIs (measured on the area of the selected label) and the displacement of the ROIs will be calculated. If plot results is selected the plot of displacement vs time index and mean intensity vs time index will appear in the Console. Choosing the save results option, an excel file containing ROIs positions, displacements and intensities, will be saved.   Background Widget  Create a new Labels layer and draw a label on the area where you want to calculate the background.     Push the Subtract background button. A new image layer will appear in the viewer. This layer contains the image to which the background was subtracted.  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-roi-registration\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "Roi Registration",
    "documentation": "https://github.com/GiorgiaTortora/napari-roi-registration#README.md",
    "first_released": "2022-05-04T11:35:41.317049Z",
    "license": "BSD-3-Clause",
    "name": "napari-roi-registration",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/GiorgiaTortora/napari-roi-registration",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-06-28T09:44:37.605195Z",
    "report_issues": "https://github.com/GiorgiaTortora/napari-roi-registration/issues",
    "requirements": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "opencv-python",
      "matplotlib",
      "openpyxl",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'",
      "scikit-image ; extra == 'testing'",
      "opencv-python-headless ; extra == 'testing'",
      "matplotlib ; extra == 'testing'",
      "openpyxl ; extra == 'testing'"
    ],
    "summary": "A plugin to perform roi registration.",
    "support": "https://github.com/GiorgiaTortora/napari-roi-registration/issues",
    "twitter": "",
    "version": "0.1.2",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Léo Guignard" }],
    "code_repository": "https://github.com/leoguignard/napari-boids",
    "conda": [],
    "description": "# napari-boids  [![License BSD-3](https://img.shields.io/pypi/l/napari-boids.svg?color=green)](https://github.com/leoguignard/napari-boids/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-boids.svg?color=green)](https://pypi.org/project/napari-boids) [![Python Version](https://img.shields.io/pypi/pyversions/napari-boids.svg?color=green)](https://python.org) [![tests](https://github.com/leoguignard/napari-boids/workflows/tests/badge.svg)](https://github.com/leoguignard/napari-boids/actions) [![codecov](https://codecov.io/gh/leoguignard/napari-boids/branch/main/graph/badge.svg)](https://codecov.io/gh/leoguignard/napari-boids) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-boids)](https://napari-hub.org/plugins/napari-boids)  A plugin to look at boids  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/plugins/index.html -->  ## Installation  You can install `napari-boids` via [pip]:      pip install napari-boids    To install latest development version :      pip install git+https://github.com/leoguignard/napari-boids.git   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-boids\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/leoguignard/napari-boids/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-boids       A plugin to look at boids  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install napari-boids via pip: pip install napari-boids  To install latest development version : pip install git+https://github.com/leoguignard/napari-boids.git  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-boids\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "Boids",
    "documentation": "https://github.com/leoguignard/napari-boids#README.md",
    "first_released": "2022-08-08T17:10:01.488885Z",
    "license": "BSD-3-Clause",
    "name": "napari-boids",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/leoguignard/napari-boids",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-08-08T17:10:01.488885Z",
    "report_issues": "https://github.com/leoguignard/napari-boids/issues",
    "requirements": null,
    "summary": "A plugin to look at boids",
    "support": "https://github.com/leoguignard/napari-boids/issues",
    "twitter": "",
    "version": "0.0.1",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      { "name": "Robert Haase" },
      { "name": "Marcelo Leomil Zoccoler" }
    ],
    "code_repository": "https://github.com/haesleinhuepf/napari-plot-profile",
    "conda": [{ "channel": "conda-forge", "package": "napari-plot-profile" }],
    "description": "# napari-plot-profile (npp)  [![License](https://img.shields.io/pypi/l/napari-plot-profile.svg?color=green)](https://github.com/haesleinhuepf/napari-plot-profile/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-plot-profile.svg?color=green)](https://pypi.org/project/napari-plot-profile) [![Python Version](https://img.shields.io/pypi/pyversions/napari-plot-profile.svg?color=green)](https://python.org) [![tests](https://github.com/haesleinhuepf/napari-plot-profile/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-plot-profile/actions) [![codecov](https://codecov.io/gh/haesleinhuepf/napari-plot-profile/branch/master/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-plot-profile) [![Development Status](https://img.shields.io/pypi/status/napari-plot-profile.svg)](https://en.wikipedia.org/wiki/Software_release_life_cycle#Alpha) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-plot-profile)](https://napari-hub.org/plugins/napari-plot-profile)  ## Plot a Line Profile  Plot intensities along a line in [napari].  ![img.png](https://github.com/haesleinhuepf/napari-plot-profile/raw/main/docs/napari-plot-profile-screencast.gif)  * Open some images in [napari].    * Add a shapes layer.  ![img.png](https://github.com/haesleinhuepf/napari-plot-profile/raw/main/docs/add_shapes_layer_screenshot.png)    * Activate the line drawing tool or the path tool and draw a line.  ![img.png](https://github.com/haesleinhuepf/napari-plot-profile/raw/main/docs/draw_line_tool_screenshot.png)    * After drawing a line, click on the menu Plugins > Measurements (Plot Profile) * If you modify the line, you may want to click the \"Refresh\" button to redraw the profile.  ![img.png](https://github.com/haesleinhuepf/napari-plot-profile/raw/main/docs/redraw_screenshot.png)  To see how these steps can be done programmatically from python, check out the [demo notebook](https://github.com/haesleinhuepf/napari-plot-profile/blob/main/docs/demo.ipynb)  ## Create a Topographical View  Create a 3D view of a 2D image by warping pixel intensities to heights. It can be displayed as a 3D image layer, a points cloud layer or a surface layer.  ![](https://github.com/haesleinhuepf/napari-plot-profile/raw/main/docs/topographical_view_screencast.gif)  This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.  ----------------------------------  ## Installation  You can install `napari-plot-profile` via [pip]:      pip install napari-plot-profile  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-plot-profile\" is free and open source software  ## Issues  If you encounter any problems, please create a thread on [image.sc] along with a detailed description and tag [@haesleinhuepf].  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/haesleinhuepf/napari-plot-profile/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ [image.sc]: https://image.sc [@haesleinhuepf]: https://twitter.com/haesleinhuepf ",
    "description_content_type": "text/markdown",
    "description_text": "napari-plot-profile (npp)        Plot a Line Profile Plot intensities along a line in napari.    Open some images in napari.   Add a shapes layer.     Activate the line drawing tool or the path tool and draw a line.    After drawing a line, click on the menu Plugins > Measurements (Plot Profile) If you modify the line, you may want to click the \"Refresh\" button to redraw the profile.   To see how these steps can be done programmatically from python, check out the demo notebook Create a Topographical View Create a 3D view of a 2D image by warping pixel intensities to heights. It can be displayed as a 3D image layer, a points cloud layer or a surface layer.  This napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.  Installation You can install napari-plot-profile via pip: pip install napari-plot-profile  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-plot-profile\" is free and open source software Issues If you encounter any problems, please create a thread on image.sc along with a detailed description and tag @haesleinhuepf.",
    "development_status": ["Development Status :: 3 - Alpha"],
    "display_name": "napari-plot-profile",
    "documentation": "https://github.com/haesleinhuepf/napari-plot-profile#README.md",
    "first_released": "2021-08-20T20:36:22.546745Z",
    "license": "BSD-3-Clause",
    "name": "napari-plot-profile",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/haesleinhuepf/napari-plot-profile",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-10-05T15:15:04.990462Z",
    "report_issues": "https://github.com/haesleinhuepf/napari-plot-profile/issues",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "pyqtgraph",
      "napari",
      "napari-tools-menu",
      "napari-skimage-regionprops (>=0.2.4)",
      "imageio (!=2.22.1)"
    ],
    "summary": "Plot intensity along a line and create topographical views in napari",
    "support": "https://github.com/haesleinhuepf/napari-plot-profile/issues",
    "twitter": "",
    "version": "0.2.2",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "William Patton" }],
    "code_repository": "https://github.com/pattonw/napari-pssr",
    "description": "# napari-pssr  [![License](https://img.shields.io/pypi/l/napari-pssr.svg?color=green)](https://github.com/pattonw/napari-pssr/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-pssr.svg?color=green)](https://pypi.org/project/napari-pssr) [![Python Version](https://img.shields.io/pypi/pyversions/napari-pssr.svg?color=green)](https://python.org) [![tests](https://github.com/pattonw/napari-pssr/workflows/tests/badge.svg)](https://github.com/pattonw/napari-pssr/actions) [![codecov](https://codecov.io/gh/pattonw/napari-pssr/branch/main/graph/badge.svg)](https://codecov.io/gh/pattonw/napari-pssr) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-pssr)](https://napari-hub.org/plugins/napari-pssr)  A plugin for training and applying pssr  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/plugins/stable/index.html -->  ## Installation  You can install `napari-pssr` via [pip]:      pip install napari-pssr  Some libraries need to be updated to the most recent version to get all features. These will be updated once they are released on pypi          pip install git+https://github.com/bioimage-io/core-bioimage-io-python\",     pip install git+https://github.com/funkey/gunpowder.git@patch-1.2.3\",   To install latest development version :      pip install git+https://github.com/pattonw/napari-pssr.git  ## Model download  A sample model can be downloaded from `https://github.com/pattonw/model-specs/tree/main/pssr`. This model comes with some restrictive dependencies. To use follow these steps. 1) install this plugin following the directions provided above 2) install bioimageio.core via `pip install bioimageio.core` or `conda install -c conda-forge bioimageio.core` 3) `pip install fastai==1.0.55 tifffile libtiff czifile scikit-image` 4) `pip uninstall torch torchvision` (may need multiple runs) 5) `conda install pytorch==1.1.0 torchvision==0.3.0 cudatoolkit=10.0 -c pytorch` 6) `pip install pillow==6.1.0`  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [MIT] license, \"napari-pssr\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/pattonw/napari-pssr/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-pssr       A plugin for training and applying pssr  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install napari-pssr via pip: pip install napari-pssr  Some libraries need to be updated to the most recent version to get all features. These will be updated once they are released on pypi pip install git+https://github.com/bioimage-io/core-bioimage-io-python\", pip install git+https://github.com/funkey/gunpowder.git@patch-1.2.3\",  To install latest development version : pip install git+https://github.com/pattonw/napari-pssr.git  Model download A sample model can be downloaded from https://github.com/pattonw/model-specs/tree/main/pssr. This model comes with some restrictive dependencies. To use follow these steps. 1) install this plugin following the directions provided above 2) install bioimageio.core via pip install bioimageio.core or conda install -c conda-forge bioimageio.core 3) pip install fastai==1.0.55 tifffile libtiff czifile scikit-image 4) pip uninstall torch torchvision (may need multiple runs) 5) conda install pytorch==1.1.0 torchvision==0.3.0 cudatoolkit=10.0 -c pytorch 6) pip install pillow==6.1.0 Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the MIT license, \"napari-pssr\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari PSSR",
    "documentation": "https://github.com/pattonw/napari-pssr#README.md",
    "first_released": "2022-11-18T01:03:14.478148Z",
    "license": "MIT",
    "name": "napari-pssr",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget", "sample_data"],
    "project_site": "https://github.com/pattonw/napari-pssr",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-11-18T01:03:14.478148Z",
    "report_issues": "https://github.com/pattonw/napari-pssr/issues",
    "requirements": [
      "numpy",
      "zarr",
      "magicgui",
      "bioimageio.core",
      "gunpowder",
      "matplotlib",
      "torch",
      "napari"
    ],
    "summary": "A plugin for training and applying pssr",
    "support": "https://github.com/pattonw/napari-pssr/issues",
    "twitter": "",
    "version": "0.1",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      { "email": "robert.haase@tu-dresden.de", "name": "Robert Haase" }
    ],
    "code_repository": "https://github.com/haesleinhuepf/napari-plugin-search",
    "description": "# napari-plugin-search  [![License](https://img.shields.io/pypi/l/napari-plugin-search.svg?color=green)](https://github.com/haesleinhuepf/napari-plugin-search/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-plugin-search.svg?color=green)](https://pypi.org/project/napari-plugin-search) [![Python Version](https://img.shields.io/pypi/pyversions/napari-plugin-search.svg?color=green)](https://python.org) [![tests](https://github.com/haesleinhuepf/napari-plugin-search/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-plugin-search/actions) [![codecov](https://codecov.io/gh/haesleinhuepf/napari-plugin-search/branch/master/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-plugin-search)  Find napari plugins ![img.png](https://github.com/haesleinhuepf/napari-plugin-search/raw/main/docs/napari-plugin-search-screencast.gif)  ## Usage Enter the name of the plugin you are searching for and use the `up` and `down` arrow keys to navigate between them.  Hit `Enter` to start a plugin.  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.  ## Installation  You can install `napari-plugin-search` via [pip]:      pip install napari-plugin-search  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-plugin-search\" is free and open source software  ## Issues  If you encounter any problems, please create a thread on [image.sc] along with a detailed description and tag [@haesleinhuepf].  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/haesleinhuepf/napari-plugin-search/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ [image.sc]: https://image.sc [@haesleinhuepf]: https://twitter.com/haesleinhuepf   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-plugin-search      Find napari plugins  Usage Enter the name of the plugin you are searching for and use the up and down arrow keys to navigate between them.  Hit Enter to start a plugin.  This napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template. Installation You can install napari-plugin-search via pip: pip install napari-plugin-search  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-plugin-search\" is free and open source software Issues If you encounter any problems, please create a thread on image.sc along with a detailed description and tag @haesleinhuepf.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari-plugin-search",
    "documentation": "https://github.com/haesleinhuepf/napari-plugin-search#README.md",
    "first_released": "2021-08-21T17:18:51.077813Z",
    "license": "BSD-3-Clause",
    "name": "napari-plugin-search",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/haesleinhuepf/napari-plugin-search",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-01-01T15:54:04.108367Z",
    "report_issues": "https://github.com/haesleinhuepf/napari-plugin-search/issues",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "napari-tools-menu"
    ],
    "summary": "Find napari plugins",
    "support": "https://github.com/haesleinhuepf/napari-plugin-search/issues",
    "twitter": "",
    "version": "0.1.3",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Robert Haase" }],
    "code_repository": "https://github.com/haesleinhuepf/napari-owncloud",
    "description": "# napari-owncloud\\r \\r [![License](https://img.shields.io/pypi/l/napari-owncloud.svg?color=green)](https://github.com/haesleinhuepf/napari-owncloud/raw/master/LICENSE)\\r [![PyPI](https://img.shields.io/pypi/v/napari-owncloud.svg?color=green)](https://pypi.org/project/napari-owncloud)\\r [![Python Version](https://img.shields.io/pypi/pyversions/napari-owncloud.svg?color=green)](https://python.org)\\r [![tests](https://github.com/haesleinhuepf/napari-owncloud/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-owncloud/actions)\\r [![codecov](https://codecov.io/gh/haesleinhuepf/napari-owncloud/branch/master/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-owncloud)\\r \\r ## Usage\\r \\r Browse folders and images in [owncloud](https://owncloud.com/) / [nextcloud](https://nextcloud.com/) servers and open them using just a double-click! \\r \\r Login to an owncloud or nextcloud server by clicking the menu `Tools > Utilities > Browse owncloud / nextcloud storage`\\r \\r ![](https://github.com/haesleinhuepf/napari-owncloud/raw/main/docs/login.png)\\r \\r You can then navigate through folders by double-clicking `folder/` items in the list.\\r You can also open images by double-clicking them. Alternatively, use the arrow-up and arrow-down key to navigate the list and hit ENTER to open an image or folder.\\r \\r ![](https://github.com/haesleinhuepf/napari-owncloud/raw/main/docs/browse.png)\\r \\r Store images in your cloud storage using the button `Save / upload current layer`. Note: Currently, only single selected layers can be saved.\\r \\r ![](https://github.com/haesleinhuepf/napari-owncloud/raw/main/docs/upload.png)\\r \\r [Demo](https://github.com/haesleinhuepf/napari-owncloud/raw/main/docs/demo.mp4)\\r \\r ![](https://github.com/haesleinhuepf/napari-owncloud/raw/main/docs/demo.gif)\\r \\r ## Installation\\r \\r You can install `napari-owncloud` via [pip]:\\r \\r     pip install napari-owncloud\\r \\r ## Related plugins\\r \\r There are other napari plugins that allow you browsing local and online image storage\\r * [napari-omero](https://www.napari-hub.org/plugins/napari-omero)\\r * [napari-folder-browser](https://www.napari-hub.org/plugins/napari-folder-browser)\\r \\r ## Contributing\\r \\r Contributions are very welcome. Tests can be run with [tox], please ensure\\r the coverage at least stays the same before you submit a pull request.\\r \\r ## License\\r \\r Distributed under the terms of the [BSD-3] license,\\r \"napari-owncloud\" is free and open source software\\r \\r ## Issues\\r \\r If you encounter any problems, please create a thread on [image.sc] along with a detailed description and tag [@haesleinhuepf].\\r \\r [napari]: https://github.com/napari/napari\\r [Cookiecutter]: https://github.com/audreyr/cookiecutter\\r [@napari]: https://github.com/napari\\r [MIT]: http://opensource.org/licenses/MIT\\r [BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\r [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\r [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\r [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\r [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\r [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\r \\r [file an issue]: https://github.com/haesleinhuepf/napari-owncloud/issues\\r \\r [napari]: https://github.com/napari/napari\\r [tox]: https://tox.readthedocs.io/en/latest/\\r [pip]: https://pypi.org/project/pip/\\r [PyPI]: https://pypi.org/\\r [image.sc]: https://image.sc\\r [@haesleinhuepf]: https://twitter.com/haesleinhuepf\\r \\r ",
    "description_content_type": "text/markdown",
    "description_text": "napari-owncloud      Usage Browse folders and images in owncloud / nextcloud servers and open them using just a double-click!  Login to an owncloud or nextcloud server by clicking the menu Tools > Utilities > Browse owncloud / nextcloud storage  You can then navigate through folders by double-clicking folder/ items in the list. You can also open images by double-clicking them. Alternatively, use the arrow-up and arrow-down key to navigate the list and hit ENTER to open an image or folder.  Store images in your cloud storage using the button Save / upload current layer. Note: Currently, only single selected layers can be saved.  Demo  Installation You can install napari-owncloud via pip: pip install napari-owncloud  Related plugins There are other napari plugins that allow you browsing local and online image storage * napari-omero * napari-folder-browser Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-owncloud\" is free and open source software Issues If you encounter any problems, please create a thread on image.sc along with a detailed description and tag @haesleinhuepf.",
    "development_status": ["Development Status :: 3 - Alpha"],
    "display_name": "napari-owncloud",
    "documentation": "https://github.com/haesleinhuepf/napari-owncloud#README.md",
    "first_released": "2022-11-16T11:40:47.276624Z",
    "license": "BSD-3-Clause",
    "name": "napari-owncloud",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/haesleinhuepf/napari-owncloud",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-11-19T10:16:27.903262Z",
    "report_issues": "https://github.com/haesleinhuepf/napari-owncloud/issues",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "napari-tools-menu",
      "pyocclient"
    ],
    "summary": "Browse folders and images in owncloud / nextcloud servers and open them using just a double-click!",
    "support": "https://github.com/haesleinhuepf/napari-owncloud/issues",
    "twitter": "",
    "version": "0.1.2",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Tom Burke" }],
    "code_repository": "https://github.com/juglab/napari-patchcreator",
    "conda": [],
    "description": "# napari-patchcreator  [![License BSD-3](https://img.shields.io/pypi/l/napari-patchcreator.svg?color=green)](https://github.com/juglab/napari-patchcreator/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-patchcreator.svg?color=green)](https://pypi.org/project/napari-patchcreator) [![Python Version](https://img.shields.io/pypi/pyversions/napari-patchcreator.svg?color=green)](https://python.org) [![tests](https://github.com/juglab/napari-patchcreator/workflows/tests/badge.svg)](https://github.com/juglab/napari-patchcreator/actions) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-patchcreator)](https://napari-hub.org/plugins/napari-patchcreator)  A simple plugin to create quadratic patches from images through selection and clicking with the left mouse button. The patches can then be exported to a folder of your own choosing.  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/stable/plugins/index.html -->  ## Installation  You can install `napari-patchcreator` via [pip]:      pip install napari-patchcreator    To install latest development version :      pip install git+https://github.com/juglab/napari-patchcreator.git   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-patchcreator\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/juglab/napari-patchcreator/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-patchcreator      A simple plugin to create quadratic patches from images through selection and clicking with the left mouse button. The patches can then be exported to a folder of your own choosing.  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install napari-patchcreator via pip: pip install napari-patchcreator  To install latest development version : pip install git+https://github.com/juglab/napari-patchcreator.git  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-patchcreator\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 5 - Production/Stable"],
    "display_name": "napari patch creator",
    "documentation": "https://github.com/juglab/napari-patchcreator#README.md",
    "first_released": "2022-08-23T12:21:19.519400Z",
    "license": "BSD-3-Clause",
    "name": "napari-patchcreator",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/juglab/napari-patchcreator",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-08-24T10:40:52.431008Z",
    "report_issues": "https://github.com/juglab/napari-patchcreator/issues",
    "requirements": [
      "numpy",
      "napari",
      "napari-plugin-engine (>=0.1.4)",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "summary": "A simple plugin to use with napari",
    "support": "https://github.com/juglab/napari-patchcreator/issues",
    "twitter": "",
    "version": "0.1.4",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Gregor Lichtner" }],
    "code_repository": "https://github.com/glichtner/napari-pystackreg",
    "description": "# napari-pystackreg  [![License](https://img.shields.io/pypi/l/napari-pystackreg.svg?color=green)](https://github.com/glichtner/napari-pystackreg/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-pystackreg.svg?color=green)](https://pypi.org/project/napari-pystackreg) [![Python Version](https://img.shields.io/pypi/pyversions/napari-pystackreg.svg?color=green)](https://python.org) [![tests](https://github.com/glichtner/napari-pystackreg/workflows/tests/badge.svg)](https://github.com/glichtner/napari-pystackreg/actions) [![codecov](https://codecov.io/gh/glichtner/napari-pystackreg/branch/main/graph/badge.svg)](https://codecov.io/gh/glichtner/napari-pystackreg) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-pystackreg)](https://napari-hub.org/plugins/napari-pystackreg)  Robust image registration for napari.  ## Summary napari-pystackreg offers the image registration capabilities of the python package [pystackreg](https://github.com/glichtner/pystackreg) for napari.  ![](https://github.com/glichtner/napari-pystackreg/raw/main/docs/napari-pystackreg.gif)  ## Description  pyStackReg is used to align (register) one or more images to a common reference image, as is required usually in time-resolved fluorescence or wide-field microscopy. It is directly ported from the source code of the ImageJ plugin ``TurboReg`` and provides additionally the functionality of the ImageJ plugin ``StackReg``, both of which were written by Philippe Thevenaz/EPFL (available at http://bigwww.epfl.ch/thevenaz/turboreg/).  pyStackReg provides the following five types of distortion:  - Translation - Rigid body (translation + rotation) - Scaled rotation (translation + rotation + scaling) - Affine (translation + rotation + scaling + shearing) - Bilinear (non-linear transformation; does not preserve straight lines)  pyStackReg supports the full functionality of StackReg plus some additional options, e.g., using different reference images and having access to the actual transformation matrices (please see the examples below). Note that pyStackReg uses the high quality (i.e. high accuracy) mode of TurboReg that uses cubic spline interpolation for transformation.  Please note: The bilinear transformation cannot be propagated, as a combination of bilinear transformations does not generally result in a bilinear transformation. Therefore, stack registration/transform functions won't work with bilinear transformation when using \"previous\" image as reference image. You can either use another reference ( \"first\" or \"mean\" for first or mean image, respectively), or try to register/transform each image of the stack separately to its respective previous image (and use the already transformed previous image as reference for the next image).  ## Installation  You can install ``napari-pystackreg`` via [pip](https://pypi.org/project/pip/) from [PyPI](https://pypi.org/):      pip install napari-pystackreg  You can also install ``napari-pystackreg`` via [conda](https://docs.conda.io/en/latest/):      conda install -c conda-forge napari-pystackreg  Or install it via napari's plugin installer.      Plugins > Install/Uninstall Plugins... > Filter for \"napari-pystackreg\" > Install  To install latest development version:      pip install git+https://github.com/glichtner/napari-pystackreg.git  ## Usage   ### Open Plugin User Interface  Start up napari, e.g. from the command line:      napari  Then, load an image stack (e.g. via ``File > Open Image...``) that you want to register. You can also use the example stack provided by the pluging (``File > Open Sample > napari-pystackreg: PC12 moving example``). Then, select the ``napari-pystackreg`` plugin from the ``Plugins > napari-pystackreg: pystackreg`` menu.  ![](https://github.com/glichtner/napari-pystackreg/raw/main/docs/ui-initial.png)  ### User Interface Options A variety of options are available to control the registration process:  * `Image Stack`: The image layer that should be registered/transformed. * `Transformation`: The type of transformation that should be applied.   - `Translation`: translation   - `Rigid body`: translation + rotation   - `Scaled rotation`: translation + rotation + scaling   - `Affine`: translation + rotation + scaling + shearing   - `Bilinear`: non-linear transformation; does not preserve straight lines * `Reference frame:` The reference image for registration.   - `Previous frame`: Aligns each frame (image) to its previous frame in the stack   - `Mean (all frames)`: Aligns each frame (image) to the average of all images in the stack   - `Mean (first n frames)`: Aligns each frame (image) to the mean of the first n frames in the stack. n is a tuneable parameter. * `Moving-average stack before register`: Apply a moving average to the stack before registration. This can be useful to   reduce noise in the stack (if the signal-to-noise ratio is very low). The moving average is applied to the stack only   for determining the transformation matrices, but not for the actual transforming of the stack. * `Transformation matrix file`: Transformation matrices can be saved to or loaded from a file for permanent storage.  ### Reference frame The reference frame is the frame to which the other frames are aligned. The default option is to use the `Previous frame`, which will register each frame to its respective previous frame in the stack. Alternatively, the reference frame can be set to the mean of all frames in the stack (`Mean (all frames)`) or the mean of the first n frames in the stack (`Mean (first n frames)`). The latter option can be useful if the first frames in the stack are more stable than the later frames (e.g. if the first frames are taken before the sample is moved). When selecting the `Mean (first n frames)` option, the number of frames to use for the mean can be set via the spinbox below the option.  ![](https://github.com/glichtner/napari-pystackreg/raw/main/docs/ui-reference-mean-n.png)  ### Moving average before registration To increase registration performance with low signal-to-noise ratio stacks, a moving average can be applied to the stack before registration. The moving average is applied to the stack only for determining the transformation matrices, but not for the actual transforming of the stack. That means that the transformed stack will still contain the original frames (however registered), but not the averaged frames.  When selecting the `Moving-average stack before register` option, the number of frames to use for the moving average can be set via the spinbox below the option.  ![](https://github.com/glichtner/napari-pystackreg/raw/main/docs/ui-moving-average.png)  ### Transformation matrix file The transformation matrices can be saved to or loaded from a file for permanent storage. This can be useful if you want to apply the same transformation to another stack (e.g. a different channel of the same sample). The transformation matrices are saved as a numpy array in a binary file (``.npy``). The file can be loaded via the `Load` button and saved via the `Save` button.  ![](https://github.com/glichtner/napari-pystackreg/raw/main/docs/ui-register-tmat.png)  ### Register/Transform To perform the actual registration and transformation steps, click the `Register` and `Transform` buttons, respectively.  The `Register` button will register the stack to the reference by determining the appropriate transformation matrices, without actually transforming the stack. The transformation matrices can be saved to a file via the `Save` button in the `Transformation matrix file` section.  ![](https://github.com/glichtner/napari-pystackreg/raw/main/docs/ui-registered.png)  The `Transform` button (1) will transform the stack to the reference by applying the transformation matrices that are currently loaded to the stack selected in `Image Stack`. For the button to become active, either the transformation matrices have to be loaded from a file via the `Load` button in the `Transformation matrix file` section, or the `Register` button has to be clicked first to determine the transformation matrices.  The `Transform` button will also add a new image layer to the napari viewer (2) with the transformed stack. The name of the new layer will be the name of the original stack with the prefix `Registered`.  ![](https://github.com/glichtner/napari-pystackreg/raw/main/docs/ui-transformed.png)  Finally, the `Register & Transform` button will perform both the registration and transformation steps in one go.  ----------------------------------  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [Apache Software License 2.0] license, \"napari-pystackreg\" is free and open source software.  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  ## Acknowledgments  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/glichtner/napari-pystackreg/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-pystackreg       Robust image registration for napari. Summary napari-pystackreg offers the image registration capabilities of the python package pystackreg for napari.  Description pyStackReg is used to align (register) one or more images to a common reference image, as is required usually in time-resolved fluorescence or wide-field microscopy. It is directly ported from the source code of the ImageJ plugin TurboReg and provides additionally the functionality of the ImageJ plugin StackReg, both of which were written by Philippe Thevenaz/EPFL (available at http://bigwww.epfl.ch/thevenaz/turboreg/). pyStackReg provides the following five types of distortion:  Translation Rigid body (translation + rotation) Scaled rotation (translation + rotation + scaling) Affine (translation + rotation + scaling + shearing) Bilinear (non-linear transformation; does not preserve straight lines)  pyStackReg supports the full functionality of StackReg plus some additional options, e.g., using different reference images and having access to the actual transformation matrices (please see the examples below). Note that pyStackReg uses the high quality (i.e. high accuracy) mode of TurboReg that uses cubic spline interpolation for transformation. Please note: The bilinear transformation cannot be propagated, as a combination of bilinear transformations does not generally result in a bilinear transformation. Therefore, stack registration/transform functions won't work with bilinear transformation when using \"previous\" image as reference image. You can either use another reference ( \"first\" or \"mean\" for first or mean image, respectively), or try to register/transform each image of the stack separately to its respective previous image (and use the already transformed previous image as reference for the next image). Installation You can install napari-pystackreg via pip from PyPI: pip install napari-pystackreg  You can also install napari-pystackreg via conda: conda install -c conda-forge napari-pystackreg  Or install it via napari's plugin installer. Plugins > Install/Uninstall Plugins... > Filter for \"napari-pystackreg\" > Install  To install latest development version: pip install git+https://github.com/glichtner/napari-pystackreg.git  Usage Open Plugin User Interface Start up napari, e.g. from the command line: napari  Then, load an image stack (e.g. via File > Open Image...) that you want to register. You can also use the example stack provided by the pluging (File > Open Sample > napari-pystackreg: PC12 moving example). Then, select the napari-pystackreg plugin from the Plugins > napari-pystackreg: pystackreg menu.  User Interface Options A variety of options are available to control the registration process:  Image Stack: The image layer that should be registered/transformed. Transformation: The type of transformation that should be applied. Translation: translation Rigid body: translation + rotation Scaled rotation: translation + rotation + scaling Affine: translation + rotation + scaling + shearing Bilinear: non-linear transformation; does not preserve straight lines Reference frame: The reference image for registration. Previous frame: Aligns each frame (image) to its previous frame in the stack Mean (all frames): Aligns each frame (image) to the average of all images in the stack Mean (first n frames): Aligns each frame (image) to the mean of the first n frames in the stack. n is a tuneable parameter. Moving-average stack before register: Apply a moving average to the stack before registration. This can be useful to   reduce noise in the stack (if the signal-to-noise ratio is very low). The moving average is applied to the stack only   for determining the transformation matrices, but not for the actual transforming of the stack. Transformation matrix file: Transformation matrices can be saved to or loaded from a file for permanent storage.  Reference frame The reference frame is the frame to which the other frames are aligned. The default option is to use the Previous frame, which will register each frame to its respective previous frame in the stack. Alternatively, the reference frame can be set to the mean of all frames in the stack (Mean (all frames)) or the mean of the first n frames in the stack (Mean (first n frames)). The latter option can be useful if the first frames in the stack are more stable than the later frames (e.g. if the first frames are taken before the sample is moved). When selecting the Mean (first n frames) option, the number of frames to use for the mean can be set via the spinbox below the option.  Moving average before registration To increase registration performance with low signal-to-noise ratio stacks, a moving average can be applied to the stack before registration. The moving average is applied to the stack only for determining the transformation matrices, but not for the actual transforming of the stack. That means that the transformed stack will still contain the original frames (however registered), but not the averaged frames. When selecting the Moving-average stack before register option, the number of frames to use for the moving average can be set via the spinbox below the option.  Transformation matrix file The transformation matrices can be saved to or loaded from a file for permanent storage. This can be useful if you want to apply the same transformation to another stack (e.g. a different channel of the same sample). The transformation matrices are saved as a numpy array in a binary file (.npy). The file can be loaded via the Load button and saved via the Save button.  Register/Transform To perform the actual registration and transformation steps, click the Register and Transform buttons, respectively. The Register button will register the stack to the reference by determining the appropriate transformation matrices, without actually transforming the stack. The transformation matrices can be saved to a file via the Save button in the Transformation matrix file section.  The Transform button (1) will transform the stack to the reference by applying the transformation matrices that are currently loaded to the stack selected in Image Stack. For the button to become active, either the transformation matrices have to be loaded from a file via the Load button in the Transformation matrix file section, or the Register button has to be clicked first to determine the transformation matrices. The Transform button will also add a new image layer to the napari viewer (2) with the transformed stack. The name of the new layer will be the name of the original stack with the prefix Registered.  Finally, the Register & Transform button will perform both the registration and transformation steps in one go.  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the Apache Software License 2.0 license, \"napari-pystackreg\" is free and open source software. Issues If you encounter any problems, please file an issue along with a detailed description. Acknowledgments This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "napari pystackreg",
    "documentation": "https://github.com/glichtner/napari-pystackreg#README.md",
    "first_released": "2022-07-07T10:39:13.117097Z",
    "license": "Apache-2.0",
    "name": "napari-pystackreg",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget", "sample_data"],
    "project_site": "https://github.com/glichtner/napari-pystackreg",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-11-28T08:34:40.623774Z",
    "report_issues": "https://github.com/glichtner/napari-pystackreg/issues",
    "requirements": [
      "magicgui",
      "numpy",
      "pystackreg (>=0.2.6)",
      "qtpy",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "tox ; extra == 'testing'"
    ],
    "summary": "Robust image registration for napari",
    "support": "https://github.com/glichtner/napari-pystackreg/issues",
    "twitter": "",
    "version": "0.1.3",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Tim-Oliver Buchholz" }],
    "code_repository": "https://github.com/fmi-faim/napari-psf-analysis",
    "description": "# napari-psf-analysis  [![License](https://img.shields.io/badge/License-BSD_3--Clause-blue.svg)](https://opensource.org/licenses/BSD-3-Clause) [![PyPI](https://img.shields.io/pypi/v/napari-psf-analysis.svg?color=green)](https://pypi.org/project/napari-psf-analysis) [![Python Version](https://img.shields.io/pypi/pyversions/napari-psf-analysis.svg?color=green)](https://python.org) [![tests](https://github.com/fmi-faim/napari-psf-analysis/workflows/tests/badge.svg)](https://github.com/fmi-faim/napari-psf-analysis/actions) [![codecov](https://codecov.io/gh/fmi-faim/napari-psf-analysis/branch/main/graph/badge.svg)](https://codecov.io/gh/fmi-faim/napari-psf-analysis) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-psf-analysis)](https://napari-hub.org/plugins/napari-psf-analysis)  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  --- ![application_screenshot](./figs/napari-psf-analysis_demo.gif) A plugin to analyse point spread funcitons (PSFs) of optical systems.  ## Usage ### Starting Point To run a PSF analysis open an image of acquired beads. Add a point-layer and indicate the beads you want to measure by adding a point.  ### Run Analyis Open the plugin (Plugins > napari-psf-analysis > PSF Analysis) and ensure that your bead image and point layers are select in the `Basic` tab under `Image` and `Points` respectively. In the `Advanced` tab further information can be provided. Only the filled in fields of the `Advanced` tab are saved in the output.  After verifying all input fields click `Extract PSFs`.  ### Discard and Save Measurement Once the PSF extraction has finished a new layer (`Analyzed Beads`) appears, holding a summary image for every selected bead. Individual summaries can be discarded by clicking the `Delete Displayed Measurement` button.  Results are saved to the selected `Save Dir` by clicking the `Save Measurements` button.  Note: Beads for which the bounding box does not fit within the image are automatically excluded from the analysis and no output is generated.   ### Saved Data Every image of the `Analyzed Beads` layer is saved as `{source_image_name}_X {bead-centroid-x}_Y{bead-centroid-y}_Z{bead-centroid-z}.png` file. Additionally a `PSFMeasurement_{source_image_acquisition_date}_ {source_image_name}_{microscope_name}_{magnification}_{NA}.csv` file is stored containing the measured values and all filled in fields.  For the demo gif above the following table is saved:  |ImageName               |Date      |Microscope|Magnification|NA |Amplitude        |Amplitude_2D      |Background        |Background_2D     |X                 |Y                 |Z                 |X_2D             |Y_2D              |FWHM_X            |FWHM_Y            |FWHM_Z           |FWHM_X_2D         |FWHM_Y_2D         |PrincipalAxis_1  |PrincipalAxis_2   |PrincipalAxis_3   |PrincipalAxis_1_2D|PrincipalAxis_2_2D|SignalToBG        |SignalToBG_2D     |XYpixelsize|Zspacing|cov_xx           |cov_xy            |cov_xz            |cov_yy           |cov_yz            |cov_zz           |cov_xx_2D        |cov_xy_2D         |cov_yy_2D        |sde_peak         |sde_background      |sde_X               |sde_Y              |sde_Z              |sde_cov_xx        |sde_cov_xy        |sde_cov_xz        |sde_cov_yy        |sde_cov_yz        |sde_cov_zz        |sde_peak_2D      |sde_background_2D   |sde_X_2D            |sde_Y_2D           |sde_cov_xx_2D      |sde_cov_xy_2D     |sde_cov_yy_2D     |version                      |Objective_id|PSF_path                                                   | |------------------------|----------|----------|-------------|---|-----------------|------------------|------------------|------------------|------------------|------------------|------------------|-----------------|------------------|------------------|------------------|-----------------|------------------|------------------|-----------------|------------------|------------------|------------------|------------------|------------------|------------------|-----------|--------|-----------------|------------------|------------------|-----------------|------------------|-----------------|-----------------|------------------|-----------------|-----------------|--------------------|--------------------|-------------------|-------------------|------------------|------------------|------------------|------------------|------------------|------------------|-----------------|--------------------|--------------------|-------------------|-------------------|------------------|------------------|-----------------------------|------------|-----------------------------------------------------------| |100x_1_conf488Virtex.tif|2022-03-03|Microscope|100          |1.4|4969.668887708917|5337.819377008636 |108.68118565810401|112.06260978398689|3304.2501743562757|3250.4033761925416|3065.7405670931444|3298.302439467146|3250.4113027135386|200.8139074557401 |197.03885411472754|674.9482557309917|183.72294307475494|176.22103947242965|675.8518951846926|199.23103476771948|195.54256297143823|184.93322114936828|174.9505023717054 |45.72703966759075 |47.63247426860641 |65.0       |200.0   |7272.305680278791|199.51201664296516|3544.234164883923 |7001.454943429444|1986.6819006579824|82153.39409342635|6087.112657801992|213.67205054291261|5600.155281533672|4.505314027916024|0.020623059963186995|0.06914746933299017 |0.06784783324982839|0.23241058486654181|11.794695129256654|8.185734859446441 |28.322224373959276|11.355300093289646|27.596443942593737|133.24416529759324|4.505314027916024|0.020623059963186995|0.06914746933299017 |0.06784783324982839|0.23241058486654181|11.794695129256654|8.185734859446441 |0.2.2.dev0+g1cb747a.d20221017|obj_1       |./100x_1_conf488Virtex.tif_Bead_X3304.3_Y3250.4_Z3065.7.png| |100x_1_conf488Virtex.tif|2022-03-03|Microscope|100          |1.4|6131.783156459655|7007.7128858909955|108.97903673830632|112.03049813071806|3283.1815179892947|3369.970476029713 |3032.554247929097 |3276.851481699453|3370.657390994046 |210.4996203597178 |197.86302004157108|689.3745507955736|190.91529553428666|174.67418701487333|689.6867779885374|209.54607959880923|197.78706972073468|190.9553524915874 |174.63039550165533|56.265712562536564|62.55183189236865 |65.0       |200.0   |7990.743418986118|71.21282284528938 |2449.8383466578707|7060.148226446074|195.46702412069595|85702.80681598671|6573.035837695339|54.41831030118964 |5502.271462869397|4.621842915212504|0.02204379426143539 |0.060262260154465876|0.05664426296450384|0.19735347823753954|10.774935265711402|7.1611338777206415|25.058014334542662|9.520360761496262 |23.451758416706767|115.56192359712198|4.621842915212504|0.02204379426143539 |0.060262260154465876|0.05664426296450384|0.19735347823753954|10.774935265711402|7.1611338777206415|0.2.2.dev0+g1cb747a.d20221017|obj_1       |./100x_1_conf488Virtex.tif_Bead_X3283.2_Y3370.0_Z3032.6.png| |100x_1_conf488Virtex.tif|2022-03-03|Microscope|100          |1.4|5619.498212394354|5796.371864919072 |108.65622592515462|111.58266064326322|3091.108753635228 |3279.722510310466 |3076.2986816645853|3084.318731921503|3278.6653904046307|214.82526951473534|210.45093794302085|708.1707538388272|205.59236614714763|202.64724874262387|709.557918556187 |211.60340602370195|209.0379608329753 |206.12587984384623|202.10455239686695|51.718142835783915|51.946887011866814|65.0       |200.0   |8322.52833820903 |217.27766177392047|4878.624412287828 |7987.047795051033|2326.0083470459012|90439.99432189878|7622.519106230307|100.7865902930646 |7405.697623587783|4.830591872545995|0.0241438047271005  |0.07013922523759801 |0.06871098346239177|0.23121554192278806|12.798834896781521|8.867905979672457 |30.29851519735856 |12.282968059322448|29.332165284940636|139.08589313352223|4.830591872545995|0.0241438047271005  |0.07013922523759801 |0.06871098346239177|0.23121554192278806|12.798834896781521|8.867905979672457 |0.2.2.dev0+g1cb747a.d20221017|obj_1       |./100x_1_conf488Virtex.tif_Bead_X3091.1_Y3279.7_Z3076.3.png|   With the three summary images:  ![summaries](figs/summaries.png)  ---  ## Installation We recommend installation into a fresh conda environment.  ### 1. Install napari ```shell conda create -y -n psf-analysis -c conda-forge python=3.9  conda activate psf-analysis  python -m pip install \"napari[all]\" ```  ### 2. Install napari-aicsimageio and bioformats Required if you want to open other files than `.tif` e.g. `.stk. `.  __Note:__ See [napari-aicsimageio](https://www.napari-hub.org/plugins/napari-aicsimageio) for more information about opening images. ```shell conda install -c conda-forge openjdk  conda deactivate conda activate psf-analysis  python -m pip install \"bfio[bioformats]\" python -m pip install \"aicsimageio[all]\" python -m pip install napari-aicsimageio ```  ### 3. Install napari-psf-analysis You can install `napari-psf-analysis` via [pip]:  ```shell python -m pip install napari-psf-analysis ```  ### 4. Optional `Set Config` You can provide a config yaml file with the available microscope names and a default save directory. This will change the `Microscope` text field to a drop down menu and change the default save directory.  `example_config.yaml` ```yaml microscopes:   - TIRF   - Zeiss Z1 output_path: \"D:\\\\\\\\psf_analysis\\\\\\\\measurements\" ```  To use this config navigate to `Plugins > napari-psf-analysis > Set Config` and select the config file.  __Note:__ The save path is OS specific.  ### 5. Desktop Icon for Windows Follow [this](https://twitter.com/haesleinhuepf/status/1537030855843094529) thread by Robert Haase.  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-psf-analysis\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue](https://github.com/fmi-faim/napari-psf-analysis/issues) along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin [@napari]: https://github.com/napari [BSD-3]: http://opensource.org/licenses/BSD-3-Clause  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-psf-analysis       This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.   A plugin to analyse point spread funcitons (PSFs) of optical systems. Usage Starting Point To run a PSF analysis open an image of acquired beads. Add a point-layer and indicate the beads you want to measure by adding a point. Run Analyis Open the plugin (Plugins > napari-psf-analysis > PSF Analysis) and ensure that your bead image and point layers are select in the Basic tab under Image and Points respectively. In the Advanced tab further information can be provided. Only the filled in fields of the Advanced tab are saved in the output. After verifying all input fields click Extract PSFs. Discard and Save Measurement Once the PSF extraction has finished a new layer (Analyzed Beads) appears, holding a summary image for every selected bead. Individual summaries can be discarded by clicking the Delete Displayed Measurement button. Results are saved to the selected Save Dir by clicking the Save Measurements button. Note: Beads for which the bounding box does not fit within the image are automatically excluded from the analysis and no output is generated. Saved Data Every image of the Analyzed Beads layer is saved as {source_image_name}_X {bead-centroid-x}_Y{bead-centroid-y}_Z{bead-centroid-z}.png file. Additionally a PSFMeasurement_{source_image_acquisition_date}_ {source_image_name}_{microscope_name}_{magnification}_{NA}.csv file is stored containing the measured values and all filled in fields. For the demo gif above the following table is saved: |ImageName               |Date      |Microscope|Magnification|NA |Amplitude        |Amplitude_2D      |Background        |Background_2D     |X                 |Y                 |Z                 |X_2D             |Y_2D              |FWHM_X            |FWHM_Y            |FWHM_Z           |FWHM_X_2D         |FWHM_Y_2D         |PrincipalAxis_1  |PrincipalAxis_2   |PrincipalAxis_3   |PrincipalAxis_1_2D|PrincipalAxis_2_2D|SignalToBG        |SignalToBG_2D     |XYpixelsize|Zspacing|cov_xx           |cov_xy            |cov_xz            |cov_yy           |cov_yz            |cov_zz           |cov_xx_2D        |cov_xy_2D         |cov_yy_2D        |sde_peak         |sde_background      |sde_X               |sde_Y              |sde_Z              |sde_cov_xx        |sde_cov_xy        |sde_cov_xz        |sde_cov_yy        |sde_cov_yz        |sde_cov_zz        |sde_peak_2D      |sde_background_2D   |sde_X_2D            |sde_Y_2D           |sde_cov_xx_2D      |sde_cov_xy_2D     |sde_cov_yy_2D     |version                      |Objective_id|PSF_path                                                   | |------------------------|----------|----------|-------------|---|-----------------|------------------|------------------|------------------|------------------|------------------|------------------|-----------------|------------------|------------------|------------------|-----------------|------------------|------------------|-----------------|------------------|------------------|------------------|------------------|------------------|------------------|-----------|--------|-----------------|------------------|------------------|-----------------|------------------|-----------------|-----------------|------------------|-----------------|-----------------|--------------------|--------------------|-------------------|-------------------|------------------|------------------|------------------|------------------|------------------|------------------|-----------------|--------------------|--------------------|-------------------|-------------------|------------------|------------------|-----------------------------|------------|-----------------------------------------------------------| |100x_1_conf488Virtex.tif|2022-03-03|Microscope|100          |1.4|4969.668887708917|5337.819377008636 |108.68118565810401|112.06260978398689|3304.2501743562757|3250.4033761925416|3065.7405670931444|3298.302439467146|3250.4113027135386|200.8139074557401 |197.03885411472754|674.9482557309917|183.72294307475494|176.22103947242965|675.8518951846926|199.23103476771948|195.54256297143823|184.93322114936828|174.9505023717054 |45.72703966759075 |47.63247426860641 |65.0       |200.0   |7272.305680278791|199.51201664296516|3544.234164883923 |7001.454943429444|1986.6819006579824|82153.39409342635|6087.112657801992|213.67205054291261|5600.155281533672|4.505314027916024|0.020623059963186995|0.06914746933299017 |0.06784783324982839|0.23241058486654181|11.794695129256654|8.185734859446441 |28.322224373959276|11.355300093289646|27.596443942593737|133.24416529759324|4.505314027916024|0.020623059963186995|0.06914746933299017 |0.06784783324982839|0.23241058486654181|11.794695129256654|8.185734859446441 |0.2.2.dev0+g1cb747a.d20221017|obj_1       |./100x_1_conf488Virtex.tif_Bead_X3304.3_Y3250.4_Z3065.7.png| |100x_1_conf488Virtex.tif|2022-03-03|Microscope|100          |1.4|6131.783156459655|7007.7128858909955|108.97903673830632|112.03049813071806|3283.1815179892947|3369.970476029713 |3032.554247929097 |3276.851481699453|3370.657390994046 |210.4996203597178 |197.86302004157108|689.3745507955736|190.91529553428666|174.67418701487333|689.6867779885374|209.54607959880923|197.78706972073468|190.9553524915874 |174.63039550165533|56.265712562536564|62.55183189236865 |65.0       |200.0   |7990.743418986118|71.21282284528938 |2449.8383466578707|7060.148226446074|195.46702412069595|85702.80681598671|6573.035837695339|54.41831030118964 |5502.271462869397|4.621842915212504|0.02204379426143539 |0.060262260154465876|0.05664426296450384|0.19735347823753954|10.774935265711402|7.1611338777206415|25.058014334542662|9.520360761496262 |23.451758416706767|115.56192359712198|4.621842915212504|0.02204379426143539 |0.060262260154465876|0.05664426296450384|0.19735347823753954|10.774935265711402|7.1611338777206415|0.2.2.dev0+g1cb747a.d20221017|obj_1       |./100x_1_conf488Virtex.tif_Bead_X3283.2_Y3370.0_Z3032.6.png| |100x_1_conf488Virtex.tif|2022-03-03|Microscope|100          |1.4|5619.498212394354|5796.371864919072 |108.65622592515462|111.58266064326322|3091.108753635228 |3279.722510310466 |3076.2986816645853|3084.318731921503|3278.6653904046307|214.82526951473534|210.45093794302085|708.1707538388272|205.59236614714763|202.64724874262387|709.557918556187 |211.60340602370195|209.0379608329753 |206.12587984384623|202.10455239686695|51.718142835783915|51.946887011866814|65.0       |200.0   |8322.52833820903 |217.27766177392047|4878.624412287828 |7987.047795051033|2326.0083470459012|90439.99432189878|7622.519106230307|100.7865902930646 |7405.697623587783|4.830591872545995|0.0241438047271005  |0.07013922523759801 |0.06871098346239177|0.23121554192278806|12.798834896781521|8.867905979672457 |30.29851519735856 |12.282968059322448|29.332165284940636|139.08589313352223|4.830591872545995|0.0241438047271005  |0.07013922523759801 |0.06871098346239177|0.23121554192278806|12.798834896781521|8.867905979672457 |0.2.2.dev0+g1cb747a.d20221017|obj_1       |./100x_1_conf488Virtex.tif_Bead_X3091.1_Y3279.7_Z3076.3.png| With the three summary images:   Installation We recommend installation into a fresh conda environment. 1. Install napari ```shell conda create -y -n psf-analysis -c conda-forge python=3.9 conda activate psf-analysis python -m pip install \"napari[all]\" ``` 2. Install napari-aicsimageio and bioformats Required if you want to open other files than .tif e.g. .stk.. Note: See napari-aicsimageio for more information about opening images. ```shell conda install -c conda-forge openjdk conda deactivate conda activate psf-analysis python -m pip install \"bfio[bioformats]\" python -m pip install \"aicsimageio[all]\" python -m pip install napari-aicsimageio ``` 3. Install napari-psf-analysis You can install napari-psf-analysis via pip: shell python -m pip install napari-psf-analysis 4. Optional Set Config You can provide a config yaml file with the available microscope names and a default save directory. This will change the Microscope text field to a drop down menu and change the default save directory. example_config.yaml yaml microscopes:   - TIRF   - Zeiss Z1 output_path: \"D:\\\\\\\\psf_analysis\\\\\\\\measurements\" To use this config navigate to Plugins > napari-psf-analysis > Set Config and select the config file. Note: The save path is OS specific. 5. Desktop Icon for Windows Follow this thread by Robert Haase. Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-psf-analysis\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 3 - Alpha"],
    "display_name": "napari-psf-analysis",
    "documentation": "https://github.com/fmi-faim/napari-psf-analysis#README.md",
    "first_released": "2022-03-30T15:58:04.217762Z",
    "license": "BSD-3-Clause",
    "name": "napari-psf-analysis",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/fmi-faim/napari-psf-analysis.git",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-11-23T15:12:40.589495Z",
    "report_issues": "https://github.com/fmi-faim/napari-psf-analysis/issues",
    "requirements": [
      "bfio",
      "matplotlib",
      "matplotlib-scalebar",
      "napari",
      "numpy",
      "pandas",
      "scikit-image"
    ],
    "summary": "A plugin to analyse point spread functions (PSFs).",
    "support": "https://github.com/fmi-faim/napari-psf-analysis/issues",
    "twitter": "",
    "version": "0.2.5",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "christinab12" }],
    "code_repository": "https://github.com/HelmholtzAI-Consultants-Munich/napari-organoid-counter",
    "conda": [],
    "description": "## Description  A napari plugin to automatically count lung organoids from microscopy imaging data. The original implementation can be found in the [Organoid-Counting](https://github.com/HelmholtzAI-Consultants-Munich/Organoid-Counting) repository, which has been adapted here to work as a napari plugin. The CannyEdgeDetection algorithm is used for detecting the organoids and pre-processing steps ahve been added, specific to this type of data to obtain optimal resutls.  ![Alt Text](https://github.com/HelmholtzAI-Consultants-Munich/napari-organoid-counter/blob/main/readme-content/demo-plugin.gif)  ## Intended Audience & Supported Data  This plugin has been developed and tested with 2D CZI microscopy images of lunch organoids. The images had been previously converted from a 3D stack to 2D using an extended focus algorithm. This plugin may be used as a baseline for developers who wish to extend the plugin to work with other types of input images and/or improve the detection algorithm.   ## Dependencies  ```napari-organoid-counter``` uses the ```napari-aicsimageio```<sup>[1]</sup> plugin for reading and processing CZI images.  [1] AICSImageIO Contributors (2021). AICSImageIO: Image Reading, Metadata Conversion, and Image Writing for Microscopy Images in Pure Python [Computer software]. GitHub. https://github.com/AllenCellModeling/aicsimageio  ## Quickstart  The use of the napari-organoid-counter plugin is straightforward. After loading the image or images you wish to process into the napari viewer, you must first pre-process them by clicking the _Preprocess_ button and the image layer will automatically be updated with the result. Next, you can adjust any of the parameters used in the algorithm (downsamppling, minimum organoid diamtere and sigma, i.e. kernel sixe for the Cannny Edge Detection algorithm) by using the corresponding sliders. By clicking the _Run Organoid Counter_ button the detection algorithm will run and a new shapes layer will be added to the viewer, with bounding boxes are placed around the detected organoid. You can add, edit or remove boxes using the _layer controls_ window and update the _Number of detected organoids_ displayed by clicking the _Update Number_ button.   The _Take Screenshot_ button has the same functionality as _File -> Save screenshot_. The _Reset Configs_ button will reset the image and all parameters to the original settings. To save your results, first select the shapes layer you wish to save form the dropdown menu. To save features of the detected organoids (diameters when approximating organoid as an ellipse and organoid area) in a csv file click _Save features_. To save the bounding boxes as a json file click _Save boxes_.   ## Getting Help  If you encounter any problems, please [file an issue] along with a detailed description.  ## How to Cite If you use this plugin for your work, please cite it using the following: ``` @software{christina_bukas_2022_6457904,   author       = {Christina Bukas},   title        = {{HelmholtzAI-Consultants-Munich/napari-organoid-                     counter: first release of napari plugin for lung                    organoid counting}},   month        = apr,   year         = 2022,   publisher    = {Zenodo},   version      = {v0.1.0-beta},   doi          = {10.5281/zenodo.6457904},   url          = {https://doi.org/10.5281/zenodo.6457904} } ```    ",
    "description_content_type": "text/markdown",
    "description_text": "Description A napari plugin to automatically count lung organoids from microscopy imaging data. The original implementation can be found in the Organoid-Counting repository, which has been adapted here to work as a napari plugin. The CannyEdgeDetection algorithm is used for detecting the organoids and pre-processing steps ahve been added, specific to this type of data to obtain optimal resutls.  Intended Audience & Supported Data This plugin has been developed and tested with 2D CZI microscopy images of lunch organoids. The images had been previously converted from a 3D stack to 2D using an extended focus algorithm. This plugin may be used as a baseline for developers who wish to extend the plugin to work with other types of input images and/or improve the detection algorithm.  Dependencies napari-organoid-counter uses the napari-aicsimageio[1] plugin for reading and processing CZI images. [1] AICSImageIO Contributors (2021). AICSImageIO: Image Reading, Metadata Conversion, and Image Writing for Microscopy Images in Pure Python [Computer software]. GitHub. https://github.com/AllenCellModeling/aicsimageio Quickstart The use of the napari-organoid-counter plugin is straightforward. After loading the image or images you wish to process into the napari viewer, you must first pre-process them by clicking the Preprocess button and the image layer will automatically be updated with the result. Next, you can adjust any of the parameters used in the algorithm (downsamppling, minimum organoid diamtere and sigma, i.e. kernel sixe for the Cannny Edge Detection algorithm) by using the corresponding sliders. By clicking the Run Organoid Counter button the detection algorithm will run and a new shapes layer will be added to the viewer, with bounding boxes are placed around the detected organoid. You can add, edit or remove boxes using the layer controls window and update the Number of detected organoids displayed by clicking the Update Number button.  The Take Screenshot button has the same functionality as File -> Save screenshot. The Reset Configs button will reset the image and all parameters to the original settings. To save your results, first select the shapes layer you wish to save form the dropdown menu. To save features of the detected organoids (diameters when approximating organoid as an ellipse and organoid area) in a csv file click Save features. To save the bounding boxes as a json file click Save boxes. Getting Help If you encounter any problems, please [file an issue] along with a detailed description. How to Cite If you use this plugin for your work, please cite it using the following: @software{christina_bukas_2022_6457904,   author       = {Christina Bukas},   title        = {{HelmholtzAI-Consultants-Munich/napari-organoid-                     counter: first release of napari plugin for lung                    organoid counting}},   month        = apr,   year         = 2022,   publisher    = {Zenodo},   version      = {v0.1.0-beta},   doi          = {10.5281/zenodo.6457904},   url          = {https://doi.org/10.5281/zenodo.6457904} }",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari OrganoidCounter",
    "documentation": "https://github.com/HelmholtzAI-Consultants-Munich/napari-organoid-counter#README.md",
    "first_released": "2022-04-13T11:15:10.687948Z",
    "license": "MIT",
    "name": "napari-organoid-counter",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/HelmholtzAI-Consultants-Munich/napari-organoid-counter",
    "python_version": "<3.10,>=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-09-09T16:11:38.477206Z",
    "report_issues": "https://github.com/HelmholtzAI-Consultants-Munich/napari-organoid-counter/issues",
    "requirements": [
      "napari[all] (>=0.4.15)",
      "napari-aicsimageio (>=0.6.1)",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "summary": "A plugin to automatically count lung organoids",
    "support": "https://github.com/HelmholtzAI-Consultants-Munich/napari-organoid-counter/issues",
    "twitter": "",
    "version": "0.1.1",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      {
        "email": "talley.lambert@gmail.com",
        "name": "The Open Microscopy and napari teams"
      }
    ],
    "code_repository": "https://github.com/tlambert03/napari-omero",
    "description": "# napari-omero  [![License](https://img.shields.io/github/license/tlambert03/napari-omero)](LICENSE) [![Version](https://img.shields.io/pypi/v/napari-omero.svg)](https://pypi.python.org/pypi/napari-omero) [![Python Version](https://img.shields.io/pypi/pyversions/napari-omero.svg)](https://python.org) [![CI](https://github.com/tlambert03/napari-omero/workflows/CI/badge.svg)](https://github.com/tlambert03/napari-omero/actions) <!-- [![conda-forge](https://img.shields.io/conda/vn/conda-forge/napari-omero)](https://anaconda.org/conda-forge/napari-omero) -->  This package provides interoperability between the [OMERO](https://www.openmicroscopy.org/omero/) image management platform, and [napari](https://github.com/napari/napari): a fast, multi-dimensional image viewer for python.  It provides a GUI interface for browsing an OMERO instance from within napari, as well as command line interface extensions for both OMERO and napari CLIs.  ![demo](https://github.com/tlambert03/napari-omero/blob/master/demo.gif?raw=true)  ## Features  - GUI interface to browse remote OMERO data, with thumbnail previews. - Loads remote nD images from an OMERO server into napari - Planes are loading on demand as sliders are moved (\"lazy loading\"). - session management (login memory) - OMERO rendering settings (contrast limits, colormaps, active channels, current   Z/T position) are applied in napari  ### as a napari dock widget  To launch napari with the OMERO browser added, [install](#installation) this package and run:  ```bash napari-omero ```  The OMERO browser widget can also be manually added to the napari viewer:  ```python import napari  viewer = napari.Viewer() viewer.window.add_plugin_dock_widget('napari-omero')  napari.run() ```  ### as a napari plugin  This package provides a napari reader plugin that accepts OMERO resources as \"proxy strings\" (e.g. `omero://Image:<ID>`) or as [OMERO webclient URLS](https://help.openmicroscopy.org/urls-to-data.html).  ```python viewer = napari.Viewer()  # omero object identifier string viewer.open(\"omero://Image:1\")  # or URLS: https://help.openmicroscopy.org/urls-to-data.html viewer.open(\"http://yourdomain.example.org/omero/webclient/?show=image-314\") ```  these will also work on the napari command line interface, e.g.:  ```bash napari omero://Image:1 # or napari http://yourdomain.example.org/omero/webclient/?show=image-314 ```  ### as an OMERO CLI plugin  This package also serves as a plugin to the OMERO CLI  ```bash omero napari view Image:1 ```  - ROIs created in napari can be saved back to OMERO via a \"Save ROIs\" button. - napari viewer console has BlitzGateway 'conn' and 'omero_image' in context.  ## installation  Requires python 3.7 - 3.10.  ### from conda  It's easiest to install `omero-py` from conda, so the recommended procedure is to install everything from conda, using the `conda-forge` channel  ```python conda install -c conda-forge napari-omero ```  ### from pip  `napari-omero` itself can be installed from pip, but you will still need `omero-py`  ```sh conda create -n omero -c conda-forge python=3.9 omero-py conda activate omero pip install napari-omero[all]  # the [all] here is the same as `napari[all]` ```  ## issues  | ❗  | This is alpha software & some things will be broken or sub-optimal!  | | --- | -------------------------------------------------------------------- |  - experimental & definitely still buggy!  [Bug   reports](https://github.com/tlambert03/napari-omero/issues/new) are welcome! - remote loading can be very slow still... though this is not strictly an issue   of this plugin.  Datasets are wrapped as delayed dask stacks, and remote data   fetching time can be significant.  Plans for [asynchronous   rendering](https://napari.org/guides/stable/rendering.html) in   napari and   [tiled loading from OMERO](https://github.com/tlambert03/napari-omero/pull/1)   may eventually improve the subjective performance... but remote data loading   will likely always be a limitation here.   To try asyncronous loading, start the program with `NAPARI_ASYNC=1 napari-omero`.  ## contributing  Contributions are welcome!  To get setup with a development environment:  ```bash # clone this repo: git clone https://github.com/tlambert03/napari-omero.git # change into the new directory cd napari-omero # create conda environment conda env create -f environment.yml # activate the new env conda activate napari-omero  # install in editable mode pip install -e . ```  To maintain good code quality, this repo uses [flake8](https://gitlab.com/pycqa/flake8), [mypy](https://github.com/python/mypy), and [black](https://github.com/psf/black).  To enforce code quality when you commit code, you can install pre-commit  ```bash # install pre-commit which will run code checks prior to commits pre-commit install ```  The original OMERO data loader and CLI extension was created by [Will Moore](https://github.com/will-moore).  The napari reader plugin and GUI browser was created by [Talley Lambert](https://github.com/tlambert03/) ",
    "description_content_type": "text/markdown",
    "description_text": "napari-omero      This package provides interoperability between the OMERO image management platform, and napari: a fast, multi-dimensional image viewer for python. It provides a GUI interface for browsing an OMERO instance from within napari, as well as command line interface extensions for both OMERO and napari CLIs.  Features  GUI interface to browse remote OMERO data, with thumbnail previews. Loads remote nD images from an OMERO server into napari Planes are loading on demand as sliders are moved (\"lazy loading\"). session management (login memory) OMERO rendering settings (contrast limits, colormaps, active channels, current   Z/T position) are applied in napari  as a napari dock widget To launch napari with the OMERO browser added, install this package and run: bash napari-omero The OMERO browser widget can also be manually added to the napari viewer: ```python import napari viewer = napari.Viewer() viewer.window.add_plugin_dock_widget('napari-omero') napari.run() ``` as a napari plugin This package provides a napari reader plugin that accepts OMERO resources as \"proxy strings\" (e.g. omero://Image:<ID>) or as OMERO webclient URLS. ```python viewer = napari.Viewer() omero object identifier string viewer.open(\"omero://Image:1\") or URLS: https://help.openmicroscopy.org/urls-to-data.html viewer.open(\"http://yourdomain.example.org/omero/webclient/?show=image-314\") ``` these will also work on the napari command line interface, e.g.: ```bash napari omero://Image:1 or napari http://yourdomain.example.org/omero/webclient/?show=image-314 ``` as an OMERO CLI plugin This package also serves as a plugin to the OMERO CLI bash omero napari view Image:1  ROIs created in napari can be saved back to OMERO via a \"Save ROIs\" button. napari viewer console has BlitzGateway 'conn' and 'omero_image' in context.  installation Requires python 3.7 - 3.10. from conda It's easiest to install omero-py from conda, so the recommended procedure is to install everything from conda, using the conda-forge channel python conda install -c conda-forge napari-omero from pip napari-omero itself can be installed from pip, but you will still need omero-py sh conda create -n omero -c conda-forge python=3.9 omero-py conda activate omero pip install napari-omero[all]  # the [all] here is the same as `napari[all]` issues | ❗  | This is alpha software & some things will be broken or sub-optimal!  | | --- | -------------------------------------------------------------------- |  experimental & definitely still buggy!  Bug   reports are welcome! remote loading can be very slow still... though this is not strictly an issue   of this plugin.  Datasets are wrapped as delayed dask stacks, and remote data   fetching time can be significant.  Plans for asynchronous   rendering in   napari and   tiled loading from OMERO   may eventually improve the subjective performance... but remote data loading   will likely always be a limitation here.   To try asyncronous loading, start the program with NAPARI_ASYNC=1 napari-omero.  contributing Contributions are welcome!  To get setup with a development environment: ```bash clone this repo: git clone https://github.com/tlambert03/napari-omero.git change into the new directory cd napari-omero create conda environment conda env create -f environment.yml activate the new env conda activate napari-omero install in editable mode pip install -e . ``` To maintain good code quality, this repo uses flake8, mypy, and black.  To enforce code quality when you commit code, you can install pre-commit ```bash install pre-commit which will run code checks prior to commits pre-commit install ``` The original OMERO data loader and CLI extension was created by Will Moore. The napari reader plugin and GUI browser was created by Talley Lambert",
    "development_status": ["Development Status :: 3 - Alpha"],
    "display_name": "napari-omero",
    "documentation": "",
    "first_released": "2020-06-24T20:52:31.571671Z",
    "license": "BSD-3-Clause",
    "name": "napari-omero",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader", "widget"],
    "project_site": "https://github.com/tlambert03/napari-omero",
    "python_version": ">=3.7",
    "reader_file_extensions": ["omero://*"],
    "release_date": "2022-05-22T23:27:08.824649Z",
    "report_issues": "https://github.com/tlambert03/napari-omero/issues",
    "requirements": [
      "napari (>=0.4.13)",
      "omero-py",
      "omero-rois",
      "napari[all] ; extra == 'all'",
      "black ; extra == 'dev'",
      "flake8 ; extra == 'dev'",
      "mypy ; extra == 'dev'",
      "pre-commit ; extra == 'dev'",
      "pytest ; extra == 'tests'",
      "pytest-cov ; extra == 'tests'",
      "pytest-qt ; extra == 'tests'",
      "pytest-xvfb ; (sys_platform == \"linux\") and extra == 'tests'",
      "pywin32 ; (sys_platform == \"win32\") and extra == 'tests'"
    ],
    "summary": "napari/OMERO interoperability",
    "support": "",
    "twitter": "",
    "version": "0.2.0",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "email": "ome-team@openmicroscopy.org", "name": "OME Team" }],
    "citations": {
      "APA": "Moore J., Moore W., Besson S., Doncila Pop D. napari-ome-zarr DOI: 10.5281/zenodo.5620851 URL: https://github.com/ome/napari-ome-zarr ",
      "BibTex": "@misc{YourReferenceHere, author = {Moore, Josh and Moore, Will and Besson, Sébastien and Doncila Pop, Draga}, doi = {10.5281/zenodo.5620851}, title = {napari-ome-zarr}, url = {https://github.com/ome/napari-ome-zarr} } ",
      "RIS": "TY  - GEN AU  - Moore, Josh AU  - Moore, Will AU  - Besson, Sébastien AU  - Doncila Pop, Draga DO  - 10.5281/zenodo.5620851 TI  - napari-ome-zarr UR  - https://github.com/ome/napari-ome-zarr ER ",
      "citation": "# YAML 1.2 # Metadata for citation of this software according to the CFF format (https://citation-file-format.github.io/) cff-version: 1.2.0 message: If you use this software, please cite it using these metadata. title: napari-ome-zarr doi: 10.5281/zenodo.5620851 authors: - given-names: Josh   family-names: Moore   affiliation: '@openmicroscopy' - given-names: Will   family-names: Moore   affiliation: '@openmicroscopy' - given-names: Sébastien   family-names: Besson   affiliation: University of Dundee - given-names: Draga   family-names: Doncila Pop repository-code: https://github.com/ome/napari-ome-zarr references:   - type: article     title: \"OME-NGFF: a next-generation file format for expanding bioimaging data-access strategies \"     journal: Nature Methods     volume: 18     number: 12     pages: 1496–1498     doi: 10.1038/s41592-021-01326-w     year: 2021     date-published: 2021-11-29     authors:       - family-names: Moore         given-names: Josh         orcid: \"https://orcid.org/0000-0003-4028-811X\"       - family-names: Allan         given-names: Chris       - family-names: Besson         given-names: Sébastien         orcid: \"https://orcid.org/0000-0001-8783-1429\"       - family-names: Burel         given-names: Jean-Marie       - family-names: Diel         given-names: Erin         orcid: \"https://orcid.org/0000-0003-2526-3512\"       - family-names: Gault         given-names: David       - family-names: Kozlowski         given-names: Kevin       - family-names: Lindner         given-names: Dominik       - family-names: Linkert         given-names: Melissa       - family-names: Manz         given-names: Trevor         orcid: \"https://orcid.org/0000-0001-7694-5164\"       - family-names: Moore         given-names: Will         orcid: \"https://orcid.org/0000-0002-7264-8338\"       - family-names: Pape         given-names: Constantin         orcid: \"https://orcid.org/0000-0001-6562-7187\"       - family-names: Tischer         given-names: Christian       - family-names: Swedlow         given-names: Jason         orcid: \"https://orcid.org/0000-0002-2198-1958\" "
    },
    "code_repository": "https://github.com/ome/napari-ome-zarr",
    "conda": [{ "channel": "conda-forge", "package": "napari-ome-zarr" }],
    "description": "# Description  This plugin provides a reader for zarr backed OME-NGFF images in napari. The reader will inspect the `.zattrs` metadata provided and pass any relevant metadata, including channel, scale and colormap metadata.  ![Opening an ome-zarr image in napari](https://i.imgur.com/tf9IqRA.gif)  The example above uses the image at https://idr.openmicroscopy.org/webclient/?show=image-6001240  # Supported Data  This plugin is designed to allow bioimaging researchers and analysts to explore their multi-resolution images stored in Zarr filesets (according to the [OME zarr spec](https://ngff.openmicroscopy.org/latest/)) without needing an intricate understanding of zarr, or the spec itself.  This plugin supports reading all images recognised as ome-zarr, namely, containing well-formed `.zattrs` and `.zgroup` files, as well as the appropriate directory hierarchy as described in the [spec](https://ngff.openmicroscopy.org/latest/). The image metadata from OMERO will be used to set channel names, colormaps and rendering settings in napari.  # Quickstart  You can open local or remote images using `napari` at the terminal and the path to your file:  ``` $ napari 'https://uk1s3.embassy.ebi.ac.uk/idr/zarr/v0.1/6001240.zarr/'  # also works with local files $ napari 6001240.zarr ```  OR in python:  ```python import napari  viewer = napari.Viewer() viewer.open('https://uk1s3.embassy.ebi.ac.uk/idr/zarr/v0.1/6001240.zarr/') napari.run() ``` If a single zarray is passed to the plugin, it will be opened without the use of the metadata:  ``` $ napari '/tmp/6001240.zarr/0' ```  If an image group contains labels, they will also be opened, and added as a separate layer in napari.  When the labels group metadata additionally contains `\"rgba\"` and `\"properties\"` keys, the labels will be given appropriate colors and the properties will be displayed in the status bar.  Working with ome-zarr images can be more convenient using the command-line interface and utility functions of our associated library `ome-zarr`. For more information please see the [package documentation](https://pypi.org/project/ome-zarr/) for `ome-zarr`.  # Getting Help  If you discover a bug with the plugin, or would like to request a new feature, please raise an issue on our repository at https://github.com/ome/napari-ome-zarr.  If you would like assistance with using the plugin, or converting images to ome-zarr format, please reach out on [image.sc](https://forum.image.sc/).  # How to Cite OME-NGFF:  [Next-generation file format (NGFF) specifications for storing bioimaging data in the cloud](https://ngff.openmicroscopy.org/0.1/). J. Moore, et al. Editors. Open Microscopy Environment Consortium, 20 November 2020. This edition of the specification is https://ngff.openmicroscopy.org/0.1/. The latest edition is available at https://ngff.openmicroscopy.org/latest/. ([doi:10.5281/zenodo.4282107](https://doi.org/10.5281/zenodo.4282107)) ",
    "description_content_type": "text/markdown",
    "description_text": "Description This plugin provides a reader for zarr backed OME-NGFF images in napari. The reader will inspect the .zattrs metadata provided and pass any relevant metadata, including channel, scale and colormap metadata.  The example above uses the image at https://idr.openmicroscopy.org/webclient/?show=image-6001240 Supported Data This plugin is designed to allow bioimaging researchers and analysts to explore their multi-resolution images stored in Zarr filesets (according to the OME zarr spec) without needing an intricate understanding of zarr, or the spec itself. This plugin supports reading all images recognised as ome-zarr, namely, containing well-formed .zattrs and .zgroup files, as well as the appropriate directory hierarchy as described in the spec. The image metadata from OMERO will be used to set channel names, colormaps and rendering settings in napari. Quickstart You can open local or remote images using napari at the terminal and the path to your file: ``` $ napari 'https://uk1s3.embassy.ebi.ac.uk/idr/zarr/v0.1/6001240.zarr/' also works with local files $ napari 6001240.zarr ``` OR in python: ```python import napari viewer = napari.Viewer() viewer.open('https://uk1s3.embassy.ebi.ac.uk/idr/zarr/v0.1/6001240.zarr/') napari.run() ``` If a single zarray is passed to the plugin, it will be opened without the use of the metadata: $ napari '/tmp/6001240.zarr/0' If an image group contains labels, they will also be opened, and added as a separate layer in napari. When the labels group metadata additionally contains \"rgba\" and \"properties\" keys, the labels will be given appropriate colors and the properties will be displayed in the status bar. Working with ome-zarr images can be more convenient using the command-line interface and utility functions of our associated library ome-zarr. For more information please see the package documentation for ome-zarr. Getting Help If you discover a bug with the plugin, or would like to request a new feature, please raise an issue on our repository at https://github.com/ome/napari-ome-zarr. If you would like assistance with using the plugin, or converting images to ome-zarr format, please reach out on image.sc. How to Cite OME-NGFF: Next-generation file format (NGFF) specifications for storing bioimaging data in the cloud. J. Moore, et al. Editors. Open Microscopy Environment Consortium, 20 November 2020. This edition of the specification is https://ngff.openmicroscopy.org/0.1/. The latest edition is available at https://ngff.openmicroscopy.org/latest/. (doi:10.5281/zenodo.4282107)",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "napari-ome-zarr",
    "documentation": "https://github.com/ome/napari-ome-zarr#README.md",
    "first_released": "2021-06-14T08:43:26.418782Z",
    "license": "BSD-3-Clause",
    "name": "napari-ome-zarr",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader"],
    "project_site": "https://github.com/ome/napari-ome-zarr",
    "python_version": ">=3.7",
    "reader_file_extensions": ["*.zarr*", "*.zarr"],
    "release_date": "2022-07-06T14:07:17.676453Z",
    "report_issues": "https://github.com/ome/napari-ome-zarr/issues",
    "requirements": [
      "ome-zarr (>=0.3.0)",
      "numpy",
      "vispy",
      "napari (>=0.4.13)"
    ],
    "summary": "A reader for zarr backed OME-NGFF images.",
    "support": "https://github.com/ome/napari-ome-zarr/issues",
    "twitter": "https://twitter.com/openmicroscopy",
    "version": "0.5.2",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Tom Burke" }, { "name": "Joran Deschamps" }],
    "code_repository": "https://github.com/juglab/napari-n2v",
    "conda": [],
    "description": "# napari-n2v  [![License](https://img.shields.io/pypi/l/napari-n2v.svg?color=green)](https://github.com/juglab/napari-n2v/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-n2v.svg?color=green)](https://pypi.org/project/napari-n2v) [![Python Version](https://img.shields.io/pypi/pyversions/napari-n2v.svg?color=green)](https://python.org) [![tests](https://github.com/juglab/napari-n2v/workflows/build/badge.svg)](https://github.com/juglab/napari-n2v/actions) [![codecov](https://codecov.io/gh/juglab/napari-n2v/branch/main/graph/badge.svg)](https://codecov.io/gh/juglab/napari-n2v) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-n2v)](https://napari-hub.org/plugins/napari-n2v)  A self-supervised denoising algorithm now usable by all in napari.  <img src=\"https://raw.githubusercontent.com/juglab/napari-n2v/master/docs/images/noisy_denoised.png\" width=\"800\" /> ----------------------------------  ## Installation  Check out the [documentation](https://juglab.github.io/napari-n2v/installation.html) for more detailed installation  instructions.    ## Quick demo  You can try out a demo by loading the `N2V Demo prediction` plugin and directly clicking on `Predict`. This model was trained using the [N2V2 example](https://juglab.github.io/napari-n2v/examples.html).   <img src=\"https://raw.githubusercontent.com/juglab/napari-n2v/master/docs/images/demo.gif\" width=\"800\" />   ## Documentation  Documentation is available on the [project website](https://juglab.github.io/napari-n2v/).   ## Contributing and feedback  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request. You can also  help us improve by [filing an issue] along with a detailed description or contact us through the [image.sc](https://forum.image.sc/) forum (tag @jdeschamps).   ## Citations  ### N2V  Alexander Krull, Tim-Oliver Buchholz, and Florian Jug. \"[Noise2void-learning denoising from single noisy images.](https://ieeexplore.ieee.org/document/8954066)\"  *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*. 2019.  ### structN2V  Coleman Broaddus, et al. \"[Removing structured noise with self-supervised blind-spot networks.](https://ieeexplore.ieee.org/document/9098336)\" *2020 IEEE 17th  International Symposium on Biomedical Imaging (ISBI)*. IEEE, 2020.  ### N2V2  Eva Hoeck, Tim-Oliver Buchholz, et al. \"[N2V2 - Fixing Noise2Void Checkerboard Artifacts with Modified Sampling Strategies and a Tweaked Network Architecture](https://openreview.net/forum?id=IZfQYb4lHVq)\", (2022).   ## Acknowledgements  This plugin was developed thanks to the support of the Silicon Valley Community Foundation (SCVF) and the  Chan-Zuckerberg Initiative (CZI) with the napari Plugin Accelerator grant _2021-240383_.   Distributed under the terms of the [BSD-3] license, \"napari-n2v\" is a free and open source software.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [filing an issue]: https://github.com/juglab/napari-n2v/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-n2v       A self-supervised denoising algorithm now usable by all in napari.  Installation Check out the documentation for more detailed installation  instructions.  Quick demo You can try out a demo by loading the N2V Demo prediction plugin and directly clicking on Predict. This model was trained using the N2V2 example.  Documentation Documentation is available on the project website. Contributing and feedback Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. You can also  help us improve by filing an issue along with a detailed description or contact us through the image.sc forum (tag @jdeschamps). Citations N2V Alexander Krull, Tim-Oliver Buchholz, and Florian Jug. \"Noise2void-learning denoising from single noisy images.\"  Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019. structN2V Coleman Broaddus, et al. \"Removing structured noise with self-supervised blind-spot networks.\" 2020 IEEE 17th  International Symposium on Biomedical Imaging (ISBI). IEEE, 2020. N2V2 Eva Hoeck, Tim-Oliver Buchholz, et al. \"N2V2 - Fixing Noise2Void Checkerboard Artifacts with Modified Sampling Strategies and a Tweaked Network Architecture\", (2022).  Acknowledgements This plugin was developed thanks to the support of the Silicon Valley Community Foundation (SCVF) and the  Chan-Zuckerberg Initiative (CZI) with the napari Plugin Accelerator grant 2021-240383. Distributed under the terms of the BSD-3 license, \"napari-n2v\" is a free and open source software.",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "napari n2v",
    "documentation": "https://juglab.github.io/napari-n2v/",
    "first_released": "2022-10-24T13:33:45.533918Z",
    "license": "BSD-3-Clause",
    "name": "napari-n2v",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget", "sample_data"],
    "project_site": "https://github.com/juglab/napari-n2v",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-10-30T17:42:32.966075Z",
    "report_issues": "https://github.com/juglab/napari-n2v/issues",
    "requirements": [
      "numpy",
      "magicgui",
      "qtpy",
      "n2v (>=0.3.2)",
      "bioimageio.core",
      "pyqtgraph",
      "scikit-image",
      "napari-time-slicer (>=0.4.9)",
      "napari (<=0.4.15)",
      "vispy (<=0.9.6)",
      "imageio (!=2.11.0,!=2.22.1,>=2.5.0)",
      "tensorflow ; platform_system != \"Darwin\" or platform_machine != \"arm64\"",
      "tensorflow-macos ; platform_system == \"Darwin\" and platform_machine == \"arm64\"",
      "tensorflow-metal ; platform_system == \"Darwin\" and platform_machine == \"arm64\"",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "summary": "A self-supervised denoising algorithm now usable by all in napari.",
    "support": "https://github.com/juglab/napari-n2v/issues",
    "twitter": "",
    "version": "0.0.4",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Robert Haase" }, { "name": "Talley Lambert" }],
    "category": {
      "Supported data": ["2D", "3D"],
      "Workflow step": [
        "Image Segmentation",
        "Image correction",
        "Image reconstruction",
        "Image enhancement",
        "Object feature extraction",
        "Morphological operations",
        "Image feature detection",
        "Visualization"
      ]
    },
    "category_hierarchy": {
      "Supported data": [["2D"], ["3D"]],
      "Workflow step": [
        ["Image Segmentation", "Cell segmentation"],
        ["Image Segmentation", "Connected-component analysis"],
        ["Image correction", "Illumination correction"],
        ["Image correction"],
        ["Image reconstruction", "Image denoising"],
        ["Image enhancement", "Image denoising"],
        ["Image Segmentation", "Image thresholding"],
        ["Image Segmentation"],
        ["Object feature extraction"],
        ["Image enhancement", "Smoothing"],
        ["Morphological operations"],
        ["Image feature detection"],
        ["Visualization", "Image visualisation", "Image projection"],
        ["Image feature detection", "Spot detection"],
        ["Image Segmentation", "Semi-automatic segmentation"],
        ["Image Segmentation", "Overlap analysis"],
        ["Object feature extraction", "Overlap analysis"],
        ["Image feature detection", "Edge detection"],
        ["Morphological operations", "Top-hat transform"],
        ["Morphological operations", "Closing"],
        ["Morphological operations", "Dilation"],
        ["Morphological operations", "Opening"],
        ["Morphological operations", "Erosion"],
        ["Image enhancement"]
      ]
    },
    "code_repository": "https://github.com/clEsperanto/napari_pyclesperanto_assistant",
    "description": "# napari-pyclesperanto-assistant\\r [![Image.sc forum](https://img.shields.io/badge/dynamic/json.svg?label=forum&url=https%3A%2F%2Fforum.image.sc%2Ftag%2Fclesperanto.json&query=%24.topic_list.tags.0.topic_count&colorB=brightgreen&suffix=%20topics&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA4AAAAOCAYAAAAfSC3RAAABPklEQVR42m3SyyqFURTA8Y2BER0TDyExZ+aSPIKUlPIITFzKeQWXwhBlQrmFgUzMMFLKZeguBu5y+//17dP3nc5vuPdee6299gohUYYaDGOyyACq4JmQVoFujOMR77hNfOAGM+hBOQqB9TjHD36xhAa04RCuuXeKOvwHVWIKL9jCK2bRiV284QgL8MwEjAneeo9VNOEaBhzALGtoRy02cIcWhE34jj5YxgW+E5Z4iTPkMYpPLCNY3hdOYEfNbKYdmNngZ1jyEzw7h7AIb3fRTQ95OAZ6yQpGYHMMtOTgouktYwxuXsHgWLLl+4x++Kx1FJrjLTagA77bTPvYgw1rRqY56e+w7GNYsqX6JfPwi7aR+Y5SA+BXtKIRfkfJAYgj14tpOF6+I46c4/cAM3UhM3JxyKsxiOIhH0IO6SH/A1Kb1WBeUjbkAAAAAElFTkSuQmCC)](https://forum.image.sc/tag/clesperanto)\\r [![website](https://img.shields.io/website?url=http%3A%2F%2Fclesperanto.net)](http://clesperanto.net)\\r [![License](https://img.shields.io/pypi/l/napari-pyclesperanto-assistant.svg?color=green)](https://github.com/clesperanto/napari-pyclesperanto-assistant/raw/master/LICENSE)\\r [![PyPI](https://img.shields.io/pypi/v/napari-pyclesperanto-assistant.svg?color=green)](https://pypi.org/project/napari-pyclesperanto-assistant)\\r [![Python Version](https://img.shields.io/pypi/pyversions/napari-pyclesperanto-assistant.svg?color=green)](https://python.org)\\r [![tests](https://github.com/clesperanto/napari_pyclesperanto_assistant/workflows/tests/badge.svg)](https://github.com/clesperanto/napari_pyclesperanto_assistant/actions)\\r [![codecov](https://codecov.io/gh/clesperanto/napari_pyclesperanto_assistant/branch/master/graph/badge.svg)](https://codecov.io/gh/clesperanto/napari_pyclesperanto_assistant)\\r [![Development Status](https://img.shields.io/pypi/status/napari_pyclesperanto_assistant.svg)](https://en.wikipedia.org/wiki/Software_release_life_cycle#Alpha)\\r [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-pyclesperanto-assistant)](https://napari-hub.org/plugins/napari-pyclesperanto-assistant)\\r [![DOI](https://zenodo.org/badge/322312181.svg)](https://zenodo.org/badge/latestdoi/322312181)\\r \\r The py-clEsperanto-assistant is a yet experimental [napari](https://github.com/napari/napari) plugin for building GPU-accelerated image processing workflows. \\r It is part of the [clEsperanto](http://clesperanto.net) project and thus, aims at removing programming language related barriers between image processing ecosystems in the life sciences. \\r It uses [pyclesperanto](https://github.com/clEsperanto/pyclesperanto_prototype) and with that [pyopencl](https://documen.tician.de/pyopencl/) as backend for processing images.\\r \\r This napari plugin adds some menu entries to the Tools menu. You can recognize them with their suffix `(clEsperanto)` in brackets.\\r Furthermore, it can be used from the [napari-assistant](https://www.napari-hub.org/plugins/napari-assistant) graphical user interface. \\r Therefore, just click the menu `Tools > Utilities > Assistant (na)` or run `naparia` from the command line.\\r \\r ![](https://github.com/clEsperanto/napari_pyclesperanto_assistant/raw/master/docs/images/virtual_4d_support1.gif)\\r \\r ## Usage\\r \\r ### Start up the assistant\\r Start up napari, e.g. from the command line:\\r ```\\r napari\\r ```\\r \\r Load example data, e.g. from the menu `File > Open Samples > clEsperanto > CalibZAPWfixed` and \\r start the assistant from the menu `Tools > Utilities > Assistant (na)`.\\r \\r ![](https://github.com/clEsperanto/napari_pyclesperanto_assistant/raw/master/docs/images/screenshot1.png)\\r \\r In case of two dimensional timelapse data, an initial conversion step might be necessary depending on your data source. \\r Click the menu `Tools > Utilities > Convert to 2d timelapse`. In the dialog, select the dataset and click ok. \\r You can delete the original dataset afterwards:\\r \\r ![](https://github.com/clEsperanto/napari_pyclesperanto_assistant/raw/master/docs/images/screenshot1a.png)\\r \\r ### Set up a workflow\\r \\r Choose categories of operations in the top right panel, for example start with denoising using a Gaussian Blur with sigma 1 in x and y.\\r \\r ![](https://github.com/clEsperanto/napari_pyclesperanto_assistant/raw/master/docs/images/screenshot2.png)\\r \\r Continue with background removal using the top-hat filter with radius 5 in x and y.\\r \\r ![](https://github.com/clEsperanto/napari_pyclesperanto_assistant/raw/master/docs/images/screenshot2a.png)\\r \\r For labeling the objects, use [Voronoi-Otsu-Labeling](https://nbviewer.jupyter.org/github/clEsperanto/pyclesperanto_prototype/blob/master/demo/segmentation/voronoi_otsu_labeling.ipynb) with both sigma parameters set to 2.\\r \\r ![](https://github.com/clEsperanto/napari_pyclesperanto_assistant/raw/master/docs/images/screenshot2b.png)\\r \\r The labeled objects can be extended using a Voronoi diagram to derive a estimations of cell boundaries.\\r \\r ![](https://github.com/clEsperanto/napari_pyclesperanto_assistant/raw/master/docs/images/screenshot2c.png)\\r \\r You can then configure napari to show the label boundaries on top of the original image:\\r \\r ![](https://github.com/clEsperanto/napari_pyclesperanto_assistant/raw/master/docs/images/screenshot2d.png)\\r \\r When your workflow is set up, click the play button below your dataset:\\r \\r ![](https://github.com/clEsperanto/napari_pyclesperanto_assistant/raw/master/docs/images/timelapse_2d.gif)\\r \\r ### Neighbor statistics\\r \\r When working with 2D or 3D data you can analyze measurements in relationship with their neighbors. \\r For example, you can measure the area of blobs as shown in the example shown below using the menu \\r `Tools > Measurements > Statistics of labeled pixels (clesperant)` and visualize it as `area` image by double-clicking on the table column (1).\\r Additionally, you can measure the maximum area of the 6 nearest neighbors using the menu `Tools > Measurments > Neighborhood statistics of measurements`.\\r The new column will then be called \"max_nn6_area...\" (2). When visualizing such parametric images next by each other, it is recommended to use\\r [napari-brightness-contrast](https://www.napari-hub.org/plugins/napari-brightness-contrast) and visualize the same intensity range to see differences correctly.\\r \\r ![](https://github.com/clEsperanto/napari_pyclesperanto_assistant/raw/master/docs/images/neighbor_statistics.png)\\r \\r ### Code generation\\r You can also export your workflow as Python/Jython code or as notebook. See the [napari-assistant documentation](https://www.napari-hub.org/plugins/napari-assistant) for details.\\r \\r ## Features\\r [pyclesperanto](https://github.com/clEsperanto/pyclesperanto_prototype) offers various possibilities for processing images. It comes from developers who work in life sciences and thus, it may be focused towards processing two- and three-dimensional microscopy image data showing cells and tissues. A selection of pyclesperanto's functionality is available via the assistant user interface. Typical workflows which can be built with this assistant include\\r * image filtering\\r   * denoising / noise reduction (mean, median, Gaussian blur)\\r   * background subtraction for uneven illumination or out-of-focus light (bottom-hat, top-hat, subtract Gaussian background)\\r   * grey value morphology (local minimum, maximum. variance)\\r   * gamma correction\\r   * Laplace operator\\r   * Sobel operator\\r * combining images\\r   * masking\\r   * image math (adding, subtracting, multiplying, dividing images) \\r   * absolute / squared difference\\r * image transformations\\r   * translation\\r   * rotation\\r   * scale\\r   * reduce stack  \\r   * sub-stacks\\r * image projections\\r   * minimum / mean / maximum / sum / standard deviation projections\\r * image segmentation\\r   * binarization (thresholding, local maxima detection)\\r   * labeling\\r   * regionalization\\r   * instance segmentation\\r   * semantic segmentation\\r   * detect label edges\\r   * label spots\\r   * connected component labeling\\r   * Voronoi-Otsu-labeling\\r * post-processing of binary images\\r   * dilation\\r   * erosion\\r   * binary opening\\r   * binary closing \\r   * binary and / or / xor\\r * post-processing of label images\\r   * dilation (expansion) of labels\\r   * extend labels via Voronoi\\r   * exclude labels on edges\\r   * exclude labels within / out of size / value range\\r   * merge touching labels\\r * parametric maps\\r   * proximal / touching neighbor count\\r   * distance measurements to touching / proximal / n-nearest neighbors\\r   * pixel count map\\r   * mean / maximum / extension ratio map\\r * label measurements / post processing of parametric maps\\r   * minimum / mean / maximum / standard deviation intensity maps\\r   * minimum / mean / maximum / standard deviation of touching / n-nearest / neighbors\\r * neighbor meshes\\r   * touching neighbors\\r   * n-nearest neighbors\\r   * proximal neighbors\\r   * distance meshes\\r * measurements based on label images\\r   * bounding box 2D / 3D\\r   * minimum / mean / maximum / sum / standard deviation intensity\\r   * center of mass\\r   * centroid\\r   * mean / maximum distance to centroid (and extension ratio shape descriptor)\\r   * mean / maximum distance to center of mass (and extension ratio shape descriptor)\\r   * statistics of neighbors (See related [publication](https://www.frontiersin.org/articles/10.3389/fcomp.2021.774396/full))\\r * code export\\r   * python / Fiji-compatible jython\\r   * python jupyter notebooks\\r * pyclesperanto scripting\\r   * cell segmentation\\r   * cell counting\\r   * cell differentiation\\r   * tissue classification\\r \\r ## Installation\\r \\r It is recommended to install the assistant using conda. If you have never used conda before, it is recommended to read \\r [this blog post](https://biapol.github.io/blog/johannes_mueller/anaconda_getting_started/) first. \\r \\r ```shell\\r conda create --name cle_39 python=3.9 napari-pyclesperanto-assistant\\r conda activate cle_39\\r ```\\r \\r Mac-users please also install this:\\r \\r     conda install -c conda-forge ocl_icd_wrapper_apple\\r     \\r Linux users please also install this:\\r     \\r     conda install -c conda-forge ocl-icd-system\\r \\r You can then start the napari-assistant using this command:\\r \\r ```\\r naparia\\r ```\\r \\r \\r ## Feedback and contributions welcome!\\r clEsperanto is developed in the open because we believe in the open source community. See our [community guidelines](https://clij.github.io/clij2-docs/community_guidelines). Feel free to drop feedback as [github issue](https://github.com/clEsperanto/pyclesperanto_prototype/issues) or via [image.sc](https://image.sc)\\r \\r ## Acknowledgements\\r This project was supported by the Deutsche Forschungsgemeinschaft under Germanyâ€™s Excellence Strategy â€“ EXC2068 - Cluster of Excellence \"Physics of Life\" of TU Dresden.\\r This project has been made possible in part by grant number [2021-240341 (Napari plugin accelerator grant)](https://chanzuckerberg.com/science/programs-resources/imaging/napari/improving-image-processing/) from the Chan Zuckerberg Initiative DAF, an advised fund of the Silicon Valley Community Foundation.\\r \\r [Imprint](https://clesperanto.github.io/imprint)\\r \\r ",
    "description_content_type": "text/markdown",
    "description_text": "napari-pyclesperanto-assistant           The py-clEsperanto-assistant is a yet experimental napari plugin for building GPU-accelerated image processing workflows.  It is part of the clEsperanto project and thus, aims at removing programming language related barriers between image processing ecosystems in the life sciences.  It uses pyclesperanto and with that pyopencl as backend for processing images. This napari plugin adds some menu entries to the Tools menu. You can recognize them with their suffix (clEsperanto) in brackets. Furthermore, it can be used from the napari-assistant graphical user interface.  Therefore, just click the menu Tools > Utilities > Assistant (na) or run naparia from the command line.  Usage Start up the assistant Start up napari, e.g. from the command line: napari Load example data, e.g. from the menu File > Open Samples > clEsperanto > CalibZAPWfixed and  start the assistant from the menu Tools > Utilities > Assistant (na).  In case of two dimensional timelapse data, an initial conversion step might be necessary depending on your data source.  Click the menu Tools > Utilities > Convert to 2d timelapse. In the dialog, select the dataset and click ok.  You can delete the original dataset afterwards:  Set up a workflow Choose categories of operations in the top right panel, for example start with denoising using a Gaussian Blur with sigma 1 in x and y.  Continue with background removal using the top-hat filter with radius 5 in x and y.  For labeling the objects, use Voronoi-Otsu-Labeling with both sigma parameters set to 2.  The labeled objects can be extended using a Voronoi diagram to derive a estimations of cell boundaries.  You can then configure napari to show the label boundaries on top of the original image:  When your workflow is set up, click the play button below your dataset:  Neighbor statistics When working with 2D or 3D data you can analyze measurements in relationship with their neighbors.  For example, you can measure the area of blobs as shown in the example shown below using the menu  Tools > Measurements > Statistics of labeled pixels (clesperant) and visualize it as area image by double-clicking on the table column (1). Additionally, you can measure the maximum area of the 6 nearest neighbors using the menu Tools > Measurments > Neighborhood statistics of measurements. The new column will then be called \"max_nn6_area...\" (2). When visualizing such parametric images next by each other, it is recommended to use napari-brightness-contrast and visualize the same intensity range to see differences correctly.  Code generation You can also export your workflow as Python/Jython code or as notebook. See the napari-assistant documentation for details. Features pyclesperanto offers various possibilities for processing images. It comes from developers who work in life sciences and thus, it may be focused towards processing two- and three-dimensional microscopy image data showing cells and tissues. A selection of pyclesperanto's functionality is available via the assistant user interface. Typical workflows which can be built with this assistant include * image filtering   * denoising / noise reduction (mean, median, Gaussian blur)   * background subtraction for uneven illumination or out-of-focus light (bottom-hat, top-hat, subtract Gaussian background)   * grey value morphology (local minimum, maximum. variance)   * gamma correction   * Laplace operator   * Sobel operator * combining images   * masking   * image math (adding, subtracting, multiplying, dividing images)    * absolute / squared difference * image transformations   * translation   * rotation   * scale   * reduce stack   * sub-stacks * image projections   * minimum / mean / maximum / sum / standard deviation projections * image segmentation   * binarization (thresholding, local maxima detection)   * labeling   * regionalization   * instance segmentation   * semantic segmentation   * detect label edges   * label spots   * connected component labeling   * Voronoi-Otsu-labeling * post-processing of binary images   * dilation   * erosion   * binary opening   * binary closing    * binary and / or / xor * post-processing of label images   * dilation (expansion) of labels   * extend labels via Voronoi   * exclude labels on edges   * exclude labels within / out of size / value range   * merge touching labels * parametric maps   * proximal / touching neighbor count   * distance measurements to touching / proximal / n-nearest neighbors   * pixel count map   * mean / maximum / extension ratio map * label measurements / post processing of parametric maps   * minimum / mean / maximum / standard deviation intensity maps   * minimum / mean / maximum / standard deviation of touching / n-nearest / neighbors * neighbor meshes   * touching neighbors   * n-nearest neighbors   * proximal neighbors   * distance meshes * measurements based on label images   * bounding box 2D / 3D   * minimum / mean / maximum / sum / standard deviation intensity   * center of mass   * centroid   * mean / maximum distance to centroid (and extension ratio shape descriptor)   * mean / maximum distance to center of mass (and extension ratio shape descriptor)   * statistics of neighbors (See related publication) * code export   * python / Fiji-compatible jython   * python jupyter notebooks * pyclesperanto scripting   * cell segmentation   * cell counting   * cell differentiation   * tissue classification Installation It is recommended to install the assistant using conda. If you have never used conda before, it is recommended to read  this blog post first.  shell conda create --name cle_39 python=3.9 napari-pyclesperanto-assistant conda activate cle_39 Mac-users please also install this: conda install -c conda-forge ocl_icd_wrapper_apple  Linux users please also install this: conda install -c conda-forge ocl-icd-system  You can then start the napari-assistant using this command: naparia Feedback and contributions welcome! clEsperanto is developed in the open because we believe in the open source community. See our community guidelines. Feel free to drop feedback as github issue or via image.sc Acknowledgements This project was supported by the Deutsche Forschungsgemeinschaft under Germanyâ€™s Excellence Strategy â€“ EXC2068 - Cluster of Excellence \"Physics of Life\" of TU Dresden. This project has been made possible in part by grant number 2021-240341 (Napari plugin accelerator grant) from the Chan Zuckerberg Initiative DAF, an advised fund of the Silicon Valley Community Foundation. Imprint",
    "development_status": ["Development Status :: 3 - Alpha"],
    "display_name": "napari-pyclesperanto-assistant",
    "documentation": "https://github.com/clEsperanto/napari_pyclesperanto_assistant/",
    "first_released": "2020-12-19T16:01:10.190324Z",
    "license": "BSD-3-Clause",
    "name": "napari-pyclesperanto-assistant",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget", "sample_data"],
    "project_site": "https://github.com/clesperanto/napari_pyclesperanto_assistant",
    "python_version": ">=3.6",
    "reader_file_extensions": [],
    "release_date": "2022-12-26T22:39:52.792703Z",
    "report_issues": "https://github.com/clEsperanto/napari_pyclesperanto_assistant/issues",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "pyopencl",
      "toolz",
      "scikit-image",
      "napari (>=0.4.15)",
      "pyclesperanto-prototype (>=0.21.1)",
      "magicgui",
      "numpy (!=1.19.4)",
      "pyperclip",
      "loguru",
      "jupytext",
      "jupyter",
      "pandas",
      "napari-tools-menu (>=0.1.8)",
      "napari-time-slicer (>=0.4.0)",
      "napari-skimage-regionprops (>=0.2.0)",
      "napari-workflows (>=0.1.1)",
      "napari-assistant (>=0.2.0)"
    ],
    "summary": "GPU-accelerated image processing in napari using OpenCL",
    "support": "https://forum.image.sc/tag/clij",
    "twitter": "",
    "version": "0.21.2",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Loic A. Royer" }],
    "code_repository": "https://github.com/royerloic/napari-nasa-samples",
    "conda": [],
    "description": "# napari-nasa-samples  [![License Mozilla Public License 2.0](https://img.shields.io/pypi/l/napari-nasa-samples.svg?color=green)](https://github.com/royerlab/napari-nasa-samples/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-nasa-samples.svg?color=green)](https://pypi.org/project/napari-nasa-samples) [![Python Version](https://img.shields.io/pypi/pyversions/napari-nasa-samples.svg?color=green)](https://python.org) [![tests](https://github.com/royerloic/napari-nasa-samples/workflows/tests/badge.svg)](https://github.com/royerlab/napari-nasa-samples/actions) [![codecov](https://codecov.io/gh/royerloic/napari-nasa-samples/branch/main/graph/badge.svg)](https://codecov.io/gh/royerlab/napari-nasa-samples) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-nasa-samples)](https://napari-hub.org/plugins/napari-nasa-samples)  This napari plugin written [by Loic A. Royer](https://twitter.com/loicaroyer) provides sample datasets from NASA. In particular, you can access directly from napari the recently released images for the [James Webb Space Telescope](https://webb.nasa.gov/), as well as some of the classic and most beautiful images obtained by the venerable and still strong [Hubble Space Telescope](https://hubblesite.org/).  More images will be added over time.  Thanks to (NASA)[https://www.nasa.gov/] for releasing these incredible images!  ![](https://github.com/royerloic/napari-nasa-samples/raw/main/docs/images/teaser.gif)  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/plugins/index.html -->  ## Installation  You can install `napari-nasa-samples` via [pip]:      pip install napari-nasa-samples    To install latest development version :      pip install git+https://github.com/royerloic/napari-nasa-samples.git   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [Mozilla Public License 2.0] license, \"napari-nasa-samples\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/royerlab/napari-nasa-samples/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-nasa-samples       This napari plugin written by Loic A. Royer provides sample datasets from NASA. In particular, you can access directly from napari the recently released images for the James Webb Space Telescope, as well as some of the classic and most beautiful images obtained by the venerable and still strong Hubble Space Telescope.  More images will be added over time. Thanks to (NASA)[https://www.nasa.gov/] for releasing these incredible images!   This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install napari-nasa-samples via pip: pip install napari-nasa-samples  To install latest development version : pip install git+https://github.com/royerloic/napari-nasa-samples.git  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the Mozilla Public License 2.0 license, \"napari-nasa-samples\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "NASA sample images",
    "documentation": "https://github.com/royerloic/napari-nasa-samples#README.md",
    "first_released": "2022-07-12T22:51:43.112742Z",
    "license": "MPL-2.0",
    "name": "napari-nasa-samples",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["sample_data"],
    "project_site": "https://github.com/royerloic/napari-nasa-samples",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-07-16T14:41:18.493102Z",
    "report_issues": "https://github.com/royerloic/napari-nasa-samples/issues",
    "requirements": [
      "numpy",
      "requests",
      "pillow",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'"
    ],
    "summary": "This napari plugin provides sample datasets from NASA.",
    "support": "https://github.com/royerloic/napari-nasa-samples/issues",
    "twitter": "",
    "version": "0.0.5",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "John Fozard" }],
    "code_repository": "https://github.com/jfozard/napari-sift-registration",
    "conda": [],
    "description": "# skimage-sift-registration  [![License BSD-3](https://img.shields.io/pypi/l/napari-sift-registration.svg?color=green)](https://github.com/jfozard/napari-sift-registration/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-sift-registration.svg?color=green)](https://pypi.org/project/napari-sift-registration) [![Python Version](https://img.shields.io/pypi/pyversions/napari-sift-registration.svg?color=green)](https://python.org) [![tests](https://github.com/jfozard/napari-sift-registration/workflows/tests/badge.svg)](https://github.com/jfozard/napari-sift-registration/actions) [![codecov](https://codecov.io/gh/jfozard/napari-sift-registration/branch/main/graph/badge.svg)](https://codecov.io/gh/jfozard/napari-sift-registration) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-sift-registration)](https://napari-hub.org/plugins/napari-sift-registration)  Simple plugin for 2D keypoint detection and affine registration with RANSAC.  ----------------------------------  ![moving image](test_data/test1.png) ![fixed image](test_data/test2.png)  Artificial data   ![moving image with inlier keypoints](doc/moving_keypoints.png) ![fixed image with inlier keypoints](doc/fixed_keypoints.png)  Moving and fixed images showing inlier keypoints after RANSAC   ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template. It uses the [scikit-image] SIFT keypoint detection routines to find distinctive image points and generate local descriptions of the image around them. Correspondences between the two images are then found by looking for pairs of keypoints, one in each of the two images, with closely matching descriptors.    For typical images, many of these correspondences will be wrong. To reduce these false correspondences, the plugin applies the RANSAC algorithm. This randomly selects a small subset of the matching pairs of keypoints, estimates the affine transformation between this subset of keypoints, and then evaluates how many of the other pairs of keypoints also closely agree with this affine transformation (\"inliers\"). A large number of random samples are tested, and the transformation with the most inliers retained.  The plugin outputs two points layers, one for each image, containing all the corresponding (inlier) SIFT keypoints. It also uses the estimated affine transformation between the two images to deform the \"moving\" image layer onto the \"fixed\" image layer.  This approach is an attempt to provide similar functionality to the Stephan Saalfeld's Fiji \"Extract SIFT Correspondences\" plugin [extract], and more-or-less just provides a napari interface to the existing routines in scikit-image. There are great examples in the scikit-image documentation (e.g. [SIFT-example] and [RANSAC-example]) that can be used if you would like to use these routines in your own analysis scripts.   ## Installation  You can install `napari-sift-registration` via [pip]:      pip install napari-sift-registration  To install the latest development version :      pip install git+https://github.com/jfozard/napari-sift-registration.git  ## Usage  ### Basic usage  - Load two 2D single channel images in Napari. - Select the menu item Plugins > napari-sift-registration - Select these two images as the \"Moving image layer\" and the \"Fixed image layer\". The moving image will be deformed by the transformation to look like the fixed image. - The remaining parameters are the default settings from scikit-image; try these default values first.  ### Advanced usage  The parameter values for SIFT feature detection, keypoint matching and RANSAC are accessible from the plugin gui. For further information about their use, see the appropriate scikit-image documentation:  Upsampling before feature detection, maximum number of octaves, maximum number of scales in every octave, blur level of seed image, feature descriptor size, feature descriptor orientation bins: see [scikit-image-SIFT].  Closest/next closest ratio: see [scikit-image-match_descriptors]  Minimum number of points sampled for each RANSAC model, distance for points to be inliers in RANSAC model, maximum number of trials in RANSAC model: see [scikit-image-RANSAC]  Only show inlier keypoints: If checked, only show corresponding keypoints that are inliers after RANSAC. If unchecked, show all corresponding keypoints.  ### Limitations  Only 2D, single channel images (for now).  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-sift-registration\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [extract]: https://imagej.net/plugins/feature-extraction [scikit-image]: https://scikit-image.org/ [SIFT-example]: https://scikit-image.org/docs/stable/auto_examples/features_detection/plot_sift.html [RANSAC-example]: https://scikit-image.org/docs/stable/auto_examples/transform/plot_matching.html [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [scikit-image-SIFT]: https://scikit-image.org/docs/stable/api/skimage.feature.html#skimage.feature.SIFT [scikit-image-match_descriptors]: https://scikit-image.org/docs/stable/api/skimage.feature.html#skimage.feature.match_descriptors [scikit-image-RANSAC]: https://scikit-image.org/docs/stable/api/skimage.measure.html#skimage.measure.ransac  [file an issue]: https://github.com/jfozard/napari-sift-registration/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "skimage-sift-registration       Simple plugin for 2D keypoint detection and affine registration with RANSAC.    Artificial data    Moving and fixed images showing inlier keypoints after RANSAC  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template. It uses the scikit-image SIFT keypoint detection routines to find distinctive image points and generate local descriptions of the image around them. Correspondences between the two images are then found by looking for pairs of keypoints, one in each of the two images, with closely matching descriptors. For typical images, many of these correspondences will be wrong. To reduce these false correspondences, the plugin applies the RANSAC algorithm. This randomly selects a small subset of the matching pairs of keypoints, estimates the affine transformation between this subset of keypoints, and then evaluates how many of the other pairs of keypoints also closely agree with this affine transformation (\"inliers\"). A large number of random samples are tested, and the transformation with the most inliers retained. The plugin outputs two points layers, one for each image, containing all the corresponding (inlier) SIFT keypoints. It also uses the estimated affine transformation between the two images to deform the \"moving\" image layer onto the \"fixed\" image layer. This approach is an attempt to provide similar functionality to the Stephan Saalfeld's Fiji \"Extract SIFT Correspondences\" plugin extract, and more-or-less just provides a napari interface to the existing routines in scikit-image. There are great examples in the scikit-image documentation (e.g. SIFT-example and RANSAC-example) that can be used if you would like to use these routines in your own analysis scripts. Installation You can install napari-sift-registration via pip: pip install napari-sift-registration  To install the latest development version : pip install git+https://github.com/jfozard/napari-sift-registration.git  Usage Basic usage  Load two 2D single channel images in Napari. Select the menu item Plugins > napari-sift-registration Select these two images as the \"Moving image layer\" and the \"Fixed image layer\". The moving image will be deformed by the transformation to look like the fixed image. The remaining parameters are the default settings from scikit-image; try these default values first.  Advanced usage The parameter values for SIFT feature detection, keypoint matching and RANSAC are accessible from the plugin gui. For further information about their use, see the appropriate scikit-image documentation: Upsampling before feature detection, maximum number of octaves, maximum number of scales in every octave, blur level of seed image, feature descriptor size, feature descriptor orientation bins: see scikit-image-SIFT. Closest/next closest ratio: see scikit-image-match_descriptors Minimum number of points sampled for each RANSAC model, distance for points to be inliers in RANSAC model, maximum number of trials in RANSAC model: see scikit-image-RANSAC Only show inlier keypoints: If checked, only show corresponding keypoints that are inliers after RANSAC. If unchecked, show all corresponding keypoints. Limitations Only 2D, single channel images (for now). Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-sift-registration\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 3 - Alpha"],
    "display_name": "SIFTReg",
    "documentation": "https://github.com/jfozard/napari-sift-registration#README.md",
    "first_released": "2022-07-27T09:51:31.420762Z",
    "license": "BSD-3-Clause",
    "name": "napari-sift-registration",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/jfozard/napari-sift-registration",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-07-27T17:59:06.719331Z",
    "report_issues": "https://github.com/jfozard/napari-sift-registration/issues",
    "requirements": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "summary": "Simple plugin for SIFT keypoint detection, and affine registration with RANSAC, based on scikit-image",
    "support": "https://github.com/jfozard/napari-sift-registration/issues",
    "twitter": "",
    "version": "0.1.2",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "giovanni palla" }],
    "code_repository": "https://github.com/scverse/napari-spatialdata",
    "conda": [],
    "description": "# napari-spatialdata  [![License](https://img.shields.io/pypi/l/napari-spatialdata.svg?color=green)](https://github.com/scverse/napari-spatialdata/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-spatialdata.svg?color=green)](https://pypi.org/project/napari-spatialdata) [![Python Version](https://img.shields.io/pypi/pyversions/napari-spatialdata.svg?color=green)](https://python.org) [![tests](https://github.com/scverse/napari-spatialdata/workflows/tests/badge.svg)](https://github.com/scverse/napari-spatialdata/actions) [![codecov](https://codecov.io/gh/scverse/napari-spatialdata/branch/main/graph/badge.svg?token=ASqlOKnOj7)](https://codecov.io/gh/scverse/napari-spatialdata) [![pre-commit.ci status](https://results.pre-commit.ci/badge/github/scverse/napari-spatialdata/main.svg)](https://results.pre-commit.ci/latest/github/scverse/napari-spatialdata/main) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-spatialdata)](https://napari-hub.org/plugins/napari-spatialdata)  Interactive visualization of spatial omics data with napari  ---  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/plugins/stable/index.html -->  # UNDER DEVELOPMENT  Clone repo and `pip install -e .`. Run examples e.g. `python shape_regions.py` or via notebook for the shape export function.  More documentation on usage will be added later on.  Expect breaking changes.  ## Installation  You can install `napari-spatialdata` via [pip]:      pip install napari-spatialdata  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-spatialdata\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [mit]: http://opensource.org/licenses/MIT [bsd-3]: http://opensource.org/licenses/BSD-3-Clause [gnu gpl v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [gnu lgpl v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [apache software license 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [mozilla public license 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [pypi]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-spatialdata        Interactive visualization of spatial omics data with napari  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  UNDER DEVELOPMENT Clone repo and pip install -e .. Run examples e.g. python shape_regions.py or via notebook for the shape export function. More documentation on usage will be added later on. Expect breaking changes. Installation You can install napari-spatialdata via pip: pip install napari-spatialdata  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-spatialdata\" is free and open source software Issues If you encounter any problems, please [file an issue] along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari spatialdata",
    "documentation": "https://github.com/scverse/napari-spatialdata#README.md",
    "first_released": "2022-07-06T14:57:06.453889Z",
    "license": "BSD-3-Clause",
    "name": "napari-spatialdata",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/scverse/napari-spatialdata.git",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-08-15T13:18:56.856934Z",
    "report_issues": "https://github.com/scverse/napari-spatialdata/issues",
    "requirements": [
      "numpy",
      "magicgui",
      "qtpy",
      "numba",
      "anndata",
      "scikit-image",
      "squidpy",
      "napari[all]",
      "loguru",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pre-commit (>=2.9.0) ; extra == 'testing'",
      "towncrier (>=21.3.0) ; extra == 'testing'"
    ],
    "summary": "Interactive visualization of spatial omics data with napari",
    "support": "https://github.com/scverse/napari-spatialdata/issues",
    "twitter": "",
    "version": "0.1.0",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      { "email": "sebgoti8@gmail.com", "name": "Sebastian Gonzalez-Tirado" }
    ],
    "code_repository": "https://github.com/sebgoti/napari-spatial-omics",
    "description": "# napari-spatial-omics  [![License](https://img.shields.io/pypi/l/napari-spatial-omics.svg?color=green)](https://github.com/sebgoti/napari-spatial-omics/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-spatial-omics.svg?color=green)](https://pypi.org/project/napari-spatial-omics) [![Python Version](https://img.shields.io/pypi/pyversions/napari-spatial-omics.svg?color=green)](https://python.org) [![tests](https://github.com/sebgoti/napari-spatial-omics/workflows/tests/badge.svg)](https://github.com/sebgoti/napari-spatial-omics/actions) [![codecov](https://codecov.io/gh/sebgoti/napari-spatial-omics/branch/main/graph/badge.svg)](https://codecov.io/gh/sebgoti/napari-spatial-omics) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-spatial-omics)](https://napari-hub.org/plugins/napari-spatial-omics)  A simple plugin to visualize spatial omic data stored in CSV format  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/plugins/stable/index.html -->  ## Installation  You can install `napari-spatial-omics` via [pip]:      pip install napari-spatial-omics    To install latest development version :      pip install git+https://github.com/sebgoti/napari-spatial-omics.git   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-spatial-omics\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/sebgoti/napari-spatial-omics/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-spatial-omics       A simple plugin to visualize spatial omic data stored in CSV format  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install napari-spatial-omics via pip: pip install napari-spatial-omics  To install latest development version : pip install git+https://github.com/sebgoti/napari-spatial-omics.git  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-spatial-omics\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari-spatial-omics",
    "documentation": "https://github.com/sebgoti/napari-spatial-omics#README.md",
    "first_released": "2021-12-10T12:06:23.898797Z",
    "license": "BSD-3-Clause",
    "name": "napari-spatial-omics",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader", "widget"],
    "project_site": "https://github.com/sebgoti/napari-spatial-omics",
    "python_version": ">=3.7",
    "reader_file_extensions": ["*"],
    "release_date": "2021-12-12T11:24:57.465896Z",
    "report_issues": "https://github.com/sebgoti/napari-spatial-omics/issues",
    "requirements": ["napari-plugin-engine (>=0.1.4)", "numpy", "pandas"],
    "summary": "A simple plugin to visualize spatial omic data stored in CSV format",
    "support": "https://github.com/sebgoti/napari-spatial-omics/issues",
    "twitter": "",
    "version": "0.0.8",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Kyle Harrington" }],
    "code_repository": "https://github.com/kephale/napari-tyssue",
    "description": "# napari-tyssue  [![License BSD-3](https://img.shields.io/pypi/l/napari-tyssue.svg?color=green)](https://github.com/kephale/napari-tyssue/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-tyssue.svg?color=green)](https://pypi.org/project/napari-tyssue) [![Python Version](https://img.shields.io/pypi/pyversions/napari-tyssue.svg?color=green)](https://python.org) [![tests](https://github.com/kephale/napari-tyssue/workflows/tests/badge.svg)](https://github.com/kephale/napari-tyssue/actions) [![codecov](https://codecov.io/gh/kephale/napari-tyssue/branch/main/graph/badge.svg)](https://codecov.io/gh/kephale/napari-tyssue) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-tyssue)](https://napari-hub.org/plugins/napari-tyssue)  A napari plugin for use with the tyssue library  ![napari-tyssue demo of apoptosis model](https://github.com/kephale/napari-tyssue/raw/main/assets/napari_tyssue_apoptosis.gif)   Example video of apoptosis demo simulation created based on the apoptosis demo from [tyssue-demo](https://github.com/DamCB/tyssue-demo).  ![napari-tyssue demo of invagination model](https://github.com/kephale/napari-tyssue/raw/main/assets/napari_tyssue_invagination_3x.gif)   Example video of apoptosis demo simulation created based on work under revision by Suzanne group at U Toulouse entitled \"Epithelio-mesenchymal transition generates an apico-basal driving force required for tissue remodeling\" [available here](https://github.com/DamCB/invagination).  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/stable/plugins/index.html -->  ## Installation  You are better off using conda. You will need pytables, and ideally CGAL.  You can install `napari-tyssue` via [pip]:      pip install napari-tyssue    To install latest development version :      pip install git+https://github.com/kephale/napari-tyssue.git   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-tyssue\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/kephale/napari-tyssue/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-tyssue       A napari plugin for use with the tyssue library  Example video of apoptosis demo simulation created based on the apoptosis demo from tyssue-demo.  Example video of apoptosis demo simulation created based on work under revision by Suzanne group at U Toulouse entitled \"Epithelio-mesenchymal transition generates an apico-basal driving force required for tissue remodeling\" available here.  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You are better off using conda. You will need pytables, and ideally CGAL. You can install napari-tyssue via pip: pip install napari-tyssue  To install latest development version : pip install git+https://github.com/kephale/napari-tyssue.git  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-tyssue\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari tyssue",
    "documentation": "https://github.com/kephale/napari-tyssue#README.md",
    "first_released": "2022-10-20T13:58:45.132746Z",
    "license": "BSD-3-Clause",
    "name": "napari-tyssue",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/kephale/napari-tyssue",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-11-10T13:41:10.697955Z",
    "report_issues": "https://github.com/kephale/napari-tyssue/issues",
    "requirements": [
      "numpy",
      "magicgui",
      "qtpy",
      "tyssue",
      "quantities",
      "pooch",
      "tables",
      "imageio-ffmpeg",
      "invagination (==0.0.2)",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "summary": "A napari plugin for use with the tyssue library",
    "support": "https://github.com/kephale/napari-tyssue/issues",
    "twitter": "",
    "version": "0.1.1",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      { "email": "maurosilber@gmail.com", "name": "Mauro Silberberg" }
    ],
    "citations": {
      "APA": "Silberberg M., Grecco H.E. Robust and unbiased estimation of the background distribution for automated quantitative imaging DOI: 10.1101/2021.11.09.467975 ",
      "BibTex": "@misc{YourReferenceHere, author = {Silberberg, Mauro and Grecco, Hernán Edgardo}, doi = {10.1101/2021.11.09.467975}, title = {Robust and unbiased estimation of the background distribution for automated quantitative imaging} } ",
      "RIS": "TY  - GEN AB  - Quantitative analysis of high-throughput microscopy images requires robust automated algorithms. Background estimation is usually the first step and has an impact on all subsequent analysis, in particular for foreground detection and calculation of ratiometric quantities. Most methods recover only a single background value, such as the median. Those that aim to retrieve a background distribution by dividing the intensity histogram yield a biased estimation in images in non-trivial cases. In this work, we present the first method to recover an unbiased estimation of the background distribution directly from an image and without any additional input. Through a robust statistical test, our method leverages the lack of local spatial correlation in background pixels to select a subset of pixels that accurately represent the background distribution. This method is both fast and simple to implement, as it only uses standard mathematical operations and an averaging filter. Additionally, the only parameter, the size of the averaging filter, does not require fine tuning. The obtained background distribution can be used to test for foreground membership of individual pixels, or to estimate confidence intervals in derived quantities. We expect that the concepts described in this work can help to develop a novel family of robust segmentation methods. AU  - Silberberg, Mauro AU  - Grecco, Hernán Edgardo DO  - 10.1101/2021.11.09.467975 TI  - Robust and unbiased estimation of the background distribution for automated quantitative imaging ER ",
      "citation": "# This CITATION.cff file was generated with cffinit. # Visit https://bit.ly/cffinit to generate yours today!  cff-version: 1.2.0 title: >-   Robust and unbiased estimation of the background   distribution for automated quantitative imaging message: >-   If you use this software, please cite it using the   metadata from this file. type: software authors:   - given-names: Mauro     family-names: Silberberg     email: maurosilber@df.uba.ar     orcid: 'https://orcid.org/0000-0002-2402-1100'     affiliation: >-       Department of Physics, FCEN, University of       Buenos Aires and IFIBA, CONICET, Buenos Aires.       C1428EHA, Argentina   - given-names: Hernán Edgardo     family-names: Grecco     email: hgrecco@df.uba.ar     affiliation: >-       Department of Physics, FCEN, University of       Buenos Aires and IFIBA, CONICET, Buenos Aires.       C1428EHA, Argentina; and, Department of       Systemic Cell Biology, Max Planck Institute of       Molecular Physiology, Dortmund, 44227, Germany     orcid: 'https://orcid.org/0000-0002-1165-4320' identifiers:   - type: doi     value: 10.1101/2021.11.09.467975     description: BioRxiv pre-print abstract: >-   Quantitative analysis of high-throughput microscopy   images requires robust automated algorithms.   Background estimation is usually the first step and   has an impact on all subsequent analysis, in   particular for foreground detection and calculation   of ratiometric quantities. Most methods recover   only a single background value, such as the median.   Those that aim to retrieve a background   distribution by dividing the intensity histogram   yield a biased estimation in images in non-trivial   cases. In this work, we present the first method to   recover an unbiased estimation of the background   distribution directly from an image and without any   additional input. Through a robust statistical   test, our method leverages the lack of local   spatial correlation in background pixels to select   a subset of pixels that accurately represent the   background distribution. This method is both fast   and simple to implement, as it only uses standard   mathematical operations and an averaging filter.   Additionally, the only parameter, the size of the   averaging filter, does not require fine tuning. The   obtained background distribution can be used to   test for foreground membership of individual   pixels, or to estimate confidence intervals in   derived quantities. We expect that the concepts   described in this work can help to develop a novel   family of robust segmentation methods. license: MIT "
    },
    "code_repository": "https://github.com/maurosilber/smo",
    "description": "# SMO  SMO is a Python package that implements the Silver Mountain Operator (SMO), which allows to recover an unbiased estimation of the background intensity distribution in a robust way.  We provide an easy to use Python package and plugins for some of the major image processing softwares: [napari](https://napari.org), [CellProfiler](https://cellprofiler.org), and [ImageJ](https://imagej.net) / [FIJI](https://fiji.sc). See Plugins section below.  ## Usage  To obtain a background-corrected image, it is as straightforward as:  ```python import skimage.data from smo import SMO  image = skimage.data.human_mitosis() smo = SMO(sigma=0, size=7, shape=(1024, 1024)) background_corrected_image = smo.bg_corrected(image) ```  where we used a sample image from `scikit-image`.  A notebook explaining in more detail the meaning of the parameters and other possible uses for SMO is available here: [smo/examples/usage.ipynb](smo/examples/usage.ipynb) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/maurosilber/SMO/blob/main/smo/examples/usage.ipynb).  ## Installation  It can be installed with `pip` from PyPI:  ``` pip install smo ```  ## Plugins ### Napari  A [napari](https://napari.org) plugin is available.  To install:  - Option 1: in napari, go to `Plugins > Install/Uninstall Plugins...` in the top menu, search for `smo` and click on the install button.  - Option 2: just `pip` install this package in the napari environment.  It will appear in the `Plugins` menu.  ### CellProfiler  A [CellProfiler](https://cellprofiler.org) plugin in available in the [smo/plugins/cellprofiler](smo/plugins/cellprofiler) folder.  ![](images/CellProfiler_SMO.png)  To install, save [this file](https://raw.githubusercontent.com/maurosilber/SMO/main/smo/plugins/cellprofiler/smo.py) into your CellProfiler plugins folder. You can find (or change) the location of your plugins directory in `File > Preferences > CellProfiler plugins directory`.  ### ImageJ / FIJI  An [ImageJ](https://imagej.net) / [FIJI](https://fiji.sc) plugin is available in the [smo/plugins/imagej](smo/plugins/imagej) folder.  ![](images/ImageJ_SMO.png)  To install, download [this file](https://raw.githubusercontent.com/maurosilber/SMO/main/smo/plugins/imagej/smo.py) and:  - Option 1: in the ImageJ main window, click on `Plugins > Install... (Ctrl+Shift+M)`, which opens a file chooser dialog. Browse and select the downloaded file. It will prompt to restart ImageJ for changes to take effect.  - Option 2: copy into your ImageJ plugins folder (`File > Show Folder > Plugins`).  To use the plugin, type `smo` on the bottom right search box:  ![](images/ImageJ_MainWindow.png)  select `smo` in the `Quick Search` window and click on the `Run` button.  ![](images/ImageJ_QuickSearch.png)  Note: the ImageJ plugin does not check that saturated pixels are properly excluded.  ## Development  Code style is enforced via pre-commit hooks. To set up a development environment, clone the repository, optionally create a virtual environment, install the [dev] extras and the pre-commit hooks:  ``` git clone https://github.com/maurosilber/SMO cd SMO conda create -n smo python pip numpy scipy pip install -e .[dev] pre-commit install ```   ",
    "description_content_type": "text/markdown; charset=UTF-8; variant=GFM",
    "description_text": "SMO SMO is a Python package that implements the Silver Mountain Operator (SMO), which allows to recover an unbiased estimation of the background intensity distribution in a robust way. We provide an easy to use Python package and plugins for some of the major image processing softwares: napari, CellProfiler, and ImageJ / FIJI. See Plugins section below. Usage To obtain a background-corrected image, it is as straightforward as: ```python import skimage.data from smo import SMO image = skimage.data.human_mitosis() smo = SMO(sigma=0, size=7, shape=(1024, 1024)) background_corrected_image = smo.bg_corrected(image) ``` where we used a sample image from scikit-image. A notebook explaining in more detail the meaning of the parameters and other possible uses for SMO is available here: smo/examples/usage.ipynb . Installation It can be installed with pip from PyPI: pip install smo Plugins Napari A napari plugin is available. To install:   Option 1: in napari, go to Plugins > Install/Uninstall Plugins... in the top menu, search for smo and click on the install button.   Option 2: just pip install this package in the napari environment.   It will appear in the Plugins menu. CellProfiler A CellProfiler plugin in available in the smo/plugins/cellprofiler folder.  To install, save this file into your CellProfiler plugins folder. You can find (or change) the location of your plugins directory in File > Preferences > CellProfiler plugins directory. ImageJ / FIJI An ImageJ / FIJI plugin is available in the smo/plugins/imagej folder.  To install, download this file and:   Option 1: in the ImageJ main window, click on Plugins > Install... (Ctrl+Shift+M), which opens a file chooser dialog. Browse and select the downloaded file. It will prompt to restart ImageJ for changes to take effect.   Option 2: copy into your ImageJ plugins folder (File > Show Folder > Plugins).   To use the plugin, type smo on the bottom right search box:  select smo in the Quick Search window and click on the Run button.  Note: the ImageJ plugin does not check that saturated pixels are properly excluded. Development Code style is enforced via pre-commit hooks. To set up a development environment, clone the repository, optionally create a virtual environment, install the [dev] extras and the pre-commit hooks: git clone https://github.com/maurosilber/SMO cd SMO conda create -n smo python pip numpy scipy pip install -e .[dev] pre-commit install",
    "development_status": [],
    "display_name": "smo",
    "documentation": "",
    "first_released": "2021-09-21T15:54:33.842241Z",
    "license": "MIT",
    "name": "smo",
    "npe2": false,
    "operating_system": [
      "Operating System :: MacOS :: MacOS X",
      "Operating System :: Microsoft :: Windows",
      "Operating System :: POSIX"
    ],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/maurosilber/smo",
    "python_version": "",
    "reader_file_extensions": [],
    "release_date": "2021-11-09T14:47:06.973881Z",
    "report_issues": "",
    "requirements": [
      "numpy",
      "scipy",
      "typing-extensions ; python_version < \"3.9\"",
      "pre-commit ; extra == 'dev'",
      "pytest ; extra == 'dev'",
      "tox ; extra == 'dev'"
    ],
    "summary": "Implementation of the Silver Mountain Operator (SMO) for the estimation of background distributions.",
    "support": "",
    "twitter": "",
    "version": "2.0.0",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Jun Ma" }],
    "code_repository": "https://github.com/JunMa11/napari-unicell",
    "description": "# napari-unicell  [![License Apache Software License 2.0](https://img.shields.io/pypi/l/napari-unicell.svg?color=green)](https://github.com/JunMa11/napari-unicell/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-unicell.svg?color=green)](https://pypi.org/project/napari-unicell) [![Python Version](https://img.shields.io/pypi/pyversions/napari-unicell.svg?color=green)](https://python.org) [![tests](https://github.com/JunMa11/napari-unicell/workflows/tests/badge.svg)](https://github.com/JunMa11/napari-unicell/actions) [![codecov](https://codecov.io/gh/JunMa11/napari-unicell/branch/main/graph/badge.svg)](https://codecov.io/gh/JunMa11/napari-unicell) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-unicell)](https://napari-hub.org/plugins/napari-unicell)  universal cell segmentation models  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/plugins/index.html -->  ## Installation  You can install `napari-unicell` via [pip]:      pip install napari-unicell     ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [Apache Software License 2.0] license, \"napari-unicell\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/JunMa11/napari-unicell/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-unicell       universal cell segmentation models  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install napari-unicell via pip: pip install napari-unicell  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the Apache Software License 2.0 license, \"napari-unicell\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "unicell",
    "documentation": "https://github.com/JunMa11/napari-unicell#README.md",
    "first_released": "2022-11-12T22:55:14.228265Z",
    "license": "Apache-2.0",
    "name": "napari-unicell",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/JunMa11/napari-unicell",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-11-12T23:14:45.085988Z",
    "report_issues": "https://github.com/JunMa11/napari-unicell/issues",
    "requirements": [
      "torch",
      "imagecodecs",
      "scipy",
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "monai",
      "einops",
      "PyQt5",
      "napari",
      "napari-plugin-engine",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "summary": "universal cell segmentation models",
    "support": "https://github.com/JunMa11/napari-unicell/issues",
    "twitter": "",
    "version": "0.0.1.post3",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Jordao Bragantini" }],
    "code_repository": "https://github.com/royerlab/napari-umap",
    "conda": [],
    "description": "# napari-umap  [![License BSD-3](https://img.shields.io/pypi/l/napari-umap.svg?color=green)](https://github.com/royerlab/napari-umap/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-umap.svg?color=green)](https://pypi.org/project/napari-umap) [![Python Version](https://img.shields.io/pypi/pyversions/napari-umap.svg?color=green)](https://python.org) [![tests](https://github.com/royerlab/napari-umap/workflows/tests/badge.svg)](https://github.com/royerlab/napari-umap/actions) [![codecov](https://codecov.io/gh/royerlab/napari-umap/branch/main/graph/badge.svg)](https://codecov.io/gh/royerlab/napari-umap) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-umap)](https://napari-hub.org/plugins/napari-umap)  A simple plugin to use with napari  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/stable/plugins/index.html -->  ## Installation  You can install `napari-umap` via [pip]:      pip install napari-umap    To install latest development version :      pip install git+https://github.com/royerlab/napari-umap.git   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-umap\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/royerlab/napari-umap/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-umap       A simple plugin to use with napari  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install napari-umap via pip: pip install napari-umap  To install latest development version : pip install git+https://github.com/royerlab/napari-umap.git  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-umap\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "UMAP",
    "documentation": "https://github.com/royerlab/napari-umap#README.md",
    "first_released": "2022-11-01T11:50:17.650265Z",
    "license": "BSD-3-Clause",
    "name": "napari-umap",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/royerlab/napari-umap",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-11-01T11:50:17.650265Z",
    "report_issues": "https://github.com/royerlab/napari-umap/issues",
    "requirements": [
      "numpy",
      "magicgui",
      "qtpy",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "summary": "A simple plugin to use with napari",
    "support": "https://github.com/royerlab/napari-umap/issues",
    "twitter": "",
    "version": "0.0.1",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      { "email": "sylvain.prigent@inria.fr", "name": "Sylvain Prigent" }
    ],
    "category": {
      "Supported data": ["Time series"],
      "Workflow step": ["Object tracking"]
    },
    "category_hierarchy": {
      "Supported data": [["Time series"]],
      "Workflow step": [["Object tracking"]]
    },
    "code_repository": "https://github.com/sylvainprigent/napari-tracks-reader",
    "description": "# Description  This plugin allows to open particle tracking results from multiple formats into the Napari  Tracks Layer.  # Supported formats  The formats currently supported by this plugin are:  ## CSV  The most basic format to store particle tracking tracks is a CSV file containing the tracks table. In this format each line is a particle and each column a property of the particle. The table headers must be `TrackID`, `t`, `x`, `y`, `z`. Note that the header order does not matter:  | TrackID       | t | x | y | z | | :------------ | :----------: | :----------: | :----------: | -----------: | | 0 | 16   | 41.5828343348868  | 47.505930020081664| 0 | | 0 | 17   | 41.48425270538317 | 51.6023835597057 | 0 |  The raw CSV file is a classical comma-separated values format:   ```csv TrackID,t,x,y,z 0,16, 41.5828343348868, 47.505930020081664, 0 0,17, 41.48425270538317, 51.6023835597057, 0 ... ```  [!NOTE] This CSV format does **not** support split and merge events  ## TrackMate  The TrackMate format is the XML model file générated by the [TrackMate](https://imagej.net/plugins/trackmate/) Fiji  plugin.  The XML file from TrackMate should not be manually modified and and contains a `Model` element:  ```xml <?xml version=\"1.0\" encoding=\"UTF-8\"?> <TrackMate version=\"6.0.2\"> ... <Model spatialunits=\"pixel\" timeunits=\"sec\">     <FeatureDeclarations>       <SpotFeatures>         <Feature feature=\"QUALITY\" name=\"Quality\" shortname=\"Quality\" dimension=\"QUALITY\" isint=\"false\" />         <Feature feature=\"POSITION_X\" name=\"X\" shortname=\"X\" dimension=\"POSITION\" isint=\"false\" /> ... ```  All the particles features from the TrackMate model file are loaded in the napari tracks properties.   [!NOTE] This format supports split and merge events  ## Icy  The Icy format is a XML file generated by the [Icy](http://icy.bioimageanalysis.org/plugin/spot-tracking/) software. The XML file from ICY should not be manually modified and starts with the `root` element:  ```xml <?xml version=\"1.0\" encoding=\"UTF-8\"?> <root>   <trackfile version=\"1\" />   <trackgroup>     <track id=\"265713726\">       <detection t=\"16\" x=\"41.5828343348868\" y=\"47.505930020081664\" z=\"0\" classname=\"plugins.nchenouard.particleTracking.sequenceGenerator.ProfileSpotTrack\" type=\"1\" />       <detection t=\"17\" x=\"41.48425270538317\" y=\"51.6023835597057\" z=\"0\" classname=\"plugins.nchenouard.particleTracking.sequenceGenerator.ProfileSpotTrack\" type=\"1\" />       ... ```  [!TIP] This format supports split and merge events  ## ISBI  The ISBI format is a XML format used for the ISBI tracking challenge. This format must contain  a `root` element and a list of particles in a `TrackContestISBI2012` element:  ```xml <?xml version=\"1.0\" encoding=\"UTF-8\"?> <root>   <TrackContestISBI2012 snr=\"?\" density=\"?\" scenario=\"FakeTracks.tif\" generationDateTime=\"Wed May 12 14:07:09 CEST 2021\">     <particle>       <detection t=\"0\" x=\"64.00558680057657\" y=\"3.9587411612103076\" z=\"0.0\" />       <detection t=\"1\" x=\"63.98171495578894\" y=\"6.04150382894106\" z=\"0.0\" />       <detection t=\"2\" x=\"63.95406806092088\" y=\"10.06085170348766\" z=\"0.0\" /> ```   [!NOTE] This format does **not** support split and merge events   # Quickstart  You can open local tracks using `napari` at the terminal and the path to your file:  ``` $ napari /path/to/your/tracks.xml ```  OR in python:  ```python import napari  viewer = napari.Viewer() viewer.open('/path/to/your/tracks.xml') napari.run() ```  # Getting Help  If you discover a bug with the plugin, or would like to request a new feature, please raise an issue on our repository at https://github.com/sylvainprigent/napari-tracks-reader. ",
    "description_content_type": "text/markdown",
    "description_text": "Description This plugin allows to open particle tracking results from multiple formats into the Napari  Tracks Layer. Supported formats The formats currently supported by this plugin are: CSV The most basic format to store particle tracking tracks is a CSV file containing the tracks table. In this format each line is a particle and each column a property of the particle. The table headers must be TrackID, t, x, y, z. Note that the header order does not matter: | TrackID       | t | x | y | z | | :------------ | :----------: | :----------: | :----------: | -----------: | | 0 | 16   | 41.5828343348868  | 47.505930020081664| 0 | | 0 | 17   | 41.48425270538317 | 51.6023835597057 | 0 | The raw CSV file is a classical comma-separated values format:  csv TrackID,t,x,y,z 0,16, 41.5828343348868, 47.505930020081664, 0 0,17, 41.48425270538317, 51.6023835597057, 0 ... [!NOTE] This CSV format does not support split and merge events TrackMate The TrackMate format is the XML model file générated by the TrackMate Fiji  plugin.  The XML file from TrackMate should not be manually modified and and contains a Model element: ```xml   ...      ... ``` All the particles features from the TrackMate model file are loaded in the napari tracks properties.  [!NOTE] This format supports split and merge events Icy The Icy format is a XML file generated by the Icy software. The XML file from ICY should not be manually modified and starts with the root element: ```xml              ... ``` [!TIP] This format supports split and merge events ISBI The ISBI format is a XML format used for the ISBI tracking challenge. This format must contain  a root element and a list of particles in a TrackContestISBI2012 element: ```xml        ```  [!NOTE] This format does not support split and merge events Quickstart You can open local tracks using napari at the terminal and the path to your file: $ napari /path/to/your/tracks.xml OR in python: ```python import napari viewer = napari.Viewer() viewer.open('/path/to/your/tracks.xml') napari.run() ``` Getting Help If you discover a bug with the plugin, or would like to request a new feature, please raise an issue on our repository at https://github.com/sylvainprigent/napari-tracks-reader.",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "napari-tracks-reader",
    "documentation": "https://sylvainprigent.github.io/napari-tracks-reader/description.html",
    "first_released": "2021-05-11T18:46:21.984641Z",
    "license": "GPL-3.0",
    "name": "napari-tracks-reader",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader"],
    "project_site": "https://sylvainprigent.github.io/napari-tracks-reader/",
    "python_version": ">=3.6",
    "reader_file_extensions": ["*"],
    "release_date": "2021-05-26T17:18:32.900741Z",
    "report_issues": "",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "pandas (>=1.2.4)"
    ],
    "summary": "Read tracks from txt (xml, csv) files to napari",
    "support": "https://github.com/sylvainprigent/napari-tracks-reader/issues",
    "twitter": "https://twitter.com/SylvainMPrigent",
    "version": "0.1.3",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "email": "", "name": "Tee Li" }],
    "code_repository": "https://github.com/teeli8/skeleton-finder",
    "description": "# Skeleton_Finder  ## Description A Napari plugin for skeleton computation on 2D gray-scale images. Currently support:  1. Skeleton finding and drawing 2. A customizable segmentation parameter for thresholding the area of interest 3. A customizable pruning parameter to keep a part of the skeleton 4. Skeleton colored by distance to the boundary 5. Only support single png or jpg images  ![example](../imgs/horse.png)   ## Usage Install with  ``` pip install skeleton-finder ```  Open the plugin widget and select an image layer.  Adjust parameters and find skeleton.  Re-adjusting parameters after computing a skeleton is also supported.  Be sure to use the Reset button after finishing the computation for an image  ## License The plugin is distributed under the terms of [BSD-3](https://opensource.org/licenses/BSD-3-Clause) license.  ## Issues If you encounter any issues, please file an issue along with a detailed description ",
    "description_content_type": "",
    "description_text": "Skeleton_Finder Description A Napari plugin for skeleton computation on 2D gray-scale images. Currently support:  Skeleton finding and drawing A customizable segmentation parameter for thresholding the area of interest A customizable pruning parameter to keep a part of the skeleton Skeleton colored by distance to the boundary Only support single png or jpg images   Usage Install with pip install skeleton-finder Open the plugin widget and select an image layer. Adjust parameters and find skeleton. Re-adjusting parameters after computing a skeleton is also supported. Be sure to use the Reset button after finishing the computation for an image License The plugin is distributed under the terms of BSD-3 license. Issues If you encounter any issues, please file an issue along with a detailed description",
    "development_status": [],
    "display_name": "",
    "documentation": "",
    "first_released": "2022-05-25T18:47:44.045175Z",
    "license": "BSD-3-Clause",
    "name": "skeleton-finder",
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": [],
    "project_site": "https://github.com/teeli8/skeleton-finder",
    "python_version": "",
    "reader_file_extensions": [],
    "release_date": "2022-05-25T20:11:26.199968Z",
    "report_issues": "",
    "requirements": ["napari", "plyfile"],
    "summary": "",
    "support": "",
    "twitter": "",
    "version": "0.0.2",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Jacopo Abramo" }],
    "code_repository": "https://github.com/jacopoabramo/napari-trait2d",
    "conda": [],
    "description": "# napari-trait2d  [![License BSD-3](https://img.shields.io/pypi/l/napari-trait2d.svg?color=green)](https://github.com/jacopoabramo/napari-trait2d/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-trait2d.svg?color=green)](https://pypi.org/project/napari-trait2d) [![Python Version](https://img.shields.io/pypi/pyversions/napari-trait2d.svg?color=green)](https://python.org) [![tests](https://github.com/jacopoabramo/napari-trait2d/workflows/tests/badge.svg)](https://github.com/jacopoabramo/napari-trait2d/actions) [![codecov](https://codecov.io/gh/jacopoabramo/napari-trait2d/branch/main/graph/badge.svg)](https://codecov.io/gh/jacopoabramo/napari-trait2d) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-trait2d)](https://napari-hub.org/plugins/napari-trait2d)  A napari plugin for TRAIT2D, a software for quantitative analysis of single particle diffusion data  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/plugins/index.html -->  ## Installation  You can install `napari-trait2d` via [pip]:      pip install napari-trait2d    To install latest development version :      pip install git+https://github.com/jacopoabramo/napari-trait2d.git   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-trait2d\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/jacopoabramo/napari-trait2d/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-trait2d       A napari plugin for TRAIT2D, a software for quantitative analysis of single particle diffusion data  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install napari-trait2d via pip: pip install napari-trait2d  To install latest development version : pip install git+https://github.com/jacopoabramo/napari-trait2d.git  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-trait2d\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari TRAIT2D",
    "documentation": "https://github.com/jacopoabramo/napari-trait2d#README.md",
    "first_released": "2022-07-09T19:06:50.319717Z",
    "license": "BSD-3-Clause",
    "name": "napari-trait2d",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/jacopoabramo/napari-trait2d",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-07-21T16:41:08.060506Z",
    "report_issues": "https://github.com/jacopoabramo/napari-trait2d/issues",
    "requirements": [
      "numpy",
      "qtpy",
      "napari[pyqt5]",
      "dacite",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'"
    ],
    "summary": "A napari plugin for TRAIT2D, a software for quantitative analysis of single particle diffusion data",
    "support": "https://github.com/jacopoabramo/napari-trait2d/issues",
    "twitter": "",
    "version": "0.1.4",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Luis Perdigao" }],
    "code_repository": null,
    "description": "# okapi-em\\r \\r https://github.com/rosalindfranklininstitute/okapi-em\\r \\r <!--\\r [![License](https://img.shields.io/pypi/l/okapi-em.svg?color=green)](https://github.com/rosalindfranklininstitute/okapi-em/raw/main/LICENSE)\\r [![PyPI](https://img.shields.io/pypi/v/okapi-em.svg?color=green)](https://pypi.org/project/okapi-em)\\r [![Python Version](https://img.shields.io/pypi/pyversions/okapi-em.svg?color=green)](https://python.org)\\r [![tests](https://github.com/perdigao1/okapi-em/workflows/tests/badge.svg)](https://github.com/rosalindfranklininstitute/okapi-em/actions)\\r [![codecov](https://codecov.io/gh/perdigao1/okapi-em/branch/main/graph/badge.svg)](https://codecov.io/gh/rosalindfranklininstitute/okapi-em)\\r [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/okapi-em)](https://napari-hub.org/plugins/okapi-em)\\r -->\\r \\r A napari plugin for processing serial-FIB-SEM data.\\r \\r Powered by [chafer] and [quoll].\\r \\r This [napari] plugin contains the following tools:\\r \\r - Two charge artifact suppression filters\\r     - directional fourier bandapass filter\\r     - line-by-line filter function optimiser and subtraction (requires charge artifact labels)\\r - slice alignment using constrained SIFT\\r - FRC estimation\\r \\r ----------------------------------\\r \\r This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\\r \\r <!--\\r Don't miss the full getting started guide to set up your new package:\\r https://github.com/napari/cookiecutter-napari-plugin#getting-started\\r \\r and review the napari docs for plugin developers:\\r https://napari.org/plugins/stable/index.html\\r -->\\r \\r ## Installation\\r \\r You can install `okapi-em` via [pip]:\\r \\r `pip install okapi-em`\\r \\r For development mode it can be installed by navigating to the cloned `okapi-em` folder and run:\\r \\r `pip install -e .`\\r \\r This should install in any machine, however ...\\r \\r Currently the FRC calculation provided by the [quoll] package which is optional because\\r of its stringent environmemt requirements from miplib package. These currently are:\\r \\r - python 3.7\\r - linux OS\\r \\r This issue will be addressed in future version.\\r \\r \\r To install okapi-em with quoll included:\\r \\r `pip install okapi-em[all]`\\r \\r Note that to run napari in python 3.7 you will need to use the command:\\r \\r `python -m napari`\\r \\r \\r \\r ## Contributing\\r \\r Contributions are very welcome. Tests can be run with [tox], please ensure\\r the coverage at least stays the same before you submit a pull request.\\r \\r ## License\\r \\r Distributed under the terms of the [Apache Software License 2.0] license,\\r \"okapi-em\" is free and open source software\\r \\r ## Issues\\r \\r If you encounter any problems, please [file an issue] along with a detailed description.\\r \\r [quoll]: https://github.com/rosalindfranklininstitute/quoll\\r [chafer]: https://github.com/rosalindfranklininstitute/chafer\\r [napari]: https://github.com/napari/napari\\r [Cookiecutter]: https://github.com/audreyr/cookiecutter\\r [@napari]: https://github.com/napari\\r [MIT]: http://opensource.org/licenses/MIT\\r [BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\r [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\r [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\r [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\r [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\r [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\r \\r \\r [tox]: https://tox.readthedocs.io/en/latest/\\r [pip]: https://pypi.org/project/pip/\\r [PyPI]: https://pypi.org/\\r ",
    "description_content_type": "text/markdown",
    "description_text": "okapi-em https://github.com/rosalindfranklininstitute/okapi-em  A napari plugin for processing serial-FIB-SEM data. Powered by chafer and quoll. This napari plugin contains the following tools:  Two charge artifact suppression filters directional fourier bandapass filter line-by-line filter function optimiser and subtraction (requires charge artifact labels)   slice alignment using constrained SIFT FRC estimation   This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install okapi-em via pip: pip install okapi-em For development mode it can be installed by navigating to the cloned okapi-em folder and run: pip install -e . This should install in any machine, however ... Currently the FRC calculation provided by the quoll package which is optional because of its stringent environmemt requirements from miplib package. These currently are:  python 3.7 linux OS  This issue will be addressed in future version. To install okapi-em with quoll included: pip install okapi-em[all] Note that to run napari in python 3.7 you will need to use the command: python -m napari Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the Apache Software License 2.0 license, \"okapi-em\" is free and open source software Issues If you encounter any problems, please [file an issue] along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari okapi-em",
    "documentation": "",
    "first_released": "2022-10-19T18:19:19.663408Z",
    "license": "Apache-2.0",
    "name": "okapi-em",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-12-08T13:08:14.216203Z",
    "report_issues": "",
    "requirements": [
      "numpy",
      "magicgui",
      "qtpy",
      "chafer",
      "napari[all]",
      "opencv-python",
      "quoll ; extra == 'all'",
      "imageio-ffmpeg ; extra == 'all'",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'"
    ],
    "summary": "napari plugin to deal with charging artifacts in tomography electron microscopy data",
    "support": "",
    "twitter": "",
    "version": "0.0.7",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Robert Haase" }],
    "category": {
      "Image modality": ["Fluorescence microscopy"],
      "Workflow step": [
        "Image Segmentation",
        "Image enhancement",
        "Image reconstruction"
      ]
    },
    "category_hierarchy": {
      "Image modality": [["Fluorescence microscopy"]],
      "Workflow step": [
        ["Image Segmentation", "Cell segmentation"],
        ["Image enhancement", "Smoothing"],
        ["Image Segmentation", "Image thresholding"],
        ["Image Segmentation", "Region growing", "Watershed segmentation"],
        ["Image reconstruction", "Image denoising"],
        ["Image enhancement", "Image denoising"]
      ]
    },
    "code_repository": "https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes",
    "conda": [
      {
        "channel": "conda-forge",
        "package": "napari-segment-blobs-and-things-with-membranes"
      }
    ],
    "description": "# napari-segment-blobs-and-things-with-membranes (nsbatwm)  [![License](https://img.shields.io/pypi/l/napari-segment-blobs-and-things-with-membranes.svg?color=green)](https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-segment-blobs-and-things-with-membranes.svg?color=green)](https://pypi.org/project/napari-segment-blobs-and-things-with-membranes) [![Python Version](https://img.shields.io/pypi/pyversions/napari-segment-blobs-and-things-with-membranes.svg?color=green)](https://python.org) [![tests](https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes/actions) [![codecov](https://codecov.io/gh/haesleinhuepf/napari-segment-blobs-and-things-with-membranes/branch/master/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-segment-blobs-and-things-with-membranes) [![Development Status](https://img.shields.io/pypi/status/napari-segment-blobs-and-things-with-membranes.svg)](https://en.wikipedia.org/wiki/Software_release_life_cycle#Alpha) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-segment-blobs-and-things-with-membranes)](https://napari-hub.org/plugins/napari-segment-blobs-and-things-with-membranes) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.7027634.svg)](https://doi.org/10.5281/zenodo.7027634)  This napari-plugin is based on scikit-image and allows segmenting nuclei and cells based on fluorescence microscopy images with high intensity in nuclei and/or membranes.  ## Usage  This plugin populates image processing operations to the `Tools` menu in napari. You can recognize them with their suffix `(nsbatwm)` in brackets. Furthermore, it can be used from the [napari-assistant](https://www.napari-hub.org/plugins/napari-assistant) graphical user interface.  Therefore, just click the menu `Tools > Utilities > Assistant (na)` or run `naparia` from the command line.  ![img.png](https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes/raw/main/docs/tools_menu_screenshot.png)  You can also call these functions as shown in [the demo notebook](https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes/blob/main/docs/demo.ipynb).  ### Voronoi-Otsu-Labeling  This algorithm uses [Otsu's thresholding method](https://ieeexplore.ieee.org/document/4310076) in combination with  [Gaussian blur](https://scikit-image.org/docs/dev/api/skimage.filters.html#skimage.filters.gaussian) and a  [Voronoi-Tesselation](https://en.wikipedia.org/wiki/Voronoi_diagram)  approach to label bright objects such as nuclei in an intensity image. The alogrithm has two sigma parameters which allow you to fine-tune where objects should be cut (`spot_sigma`) and how smooth outlines should be (`outline_sigma`). This implementation aims to be similar to [Voronoi-Otsu-Labeling in clesperanto](https://github.com/clEsperanto/pyclesperanto_prototype/blob/master/demo/segmentation/voronoi_otsu_labeling.ipynb).  ![img.png](https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes/raw/main/docs/voronoi_otsu_labeling.png)  ### Seeded Watershed  Starting from an image showing high-intensity membranes and a seed-image where objects have been labeled (e.g. using Voronoi-Otsu-Labeling), objects are labeled that are constrained by the membranes.  ![img.png](https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes/raw/main/docs/seeded_watershed.png)  ### Seeded Watershed with mask  If there is additionally a mask image available, one can use the `Seeded Watershed with mask`, to constraint the flooding  on a membrane image (1), starting from nuclei (2), limited by a mask image (3) to produce a cell segmentation within the mask (4).  ![img.png](https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes/raw/main/docs/seeded_watershed_with_mask.png)  ### Seeded Watershed using local minima as starting points  Similar to the Seeded Watershed and Voronoi-Otsu-Labeling explained above, you can use this tool to segment an image showing membranes without an additional image showing nuclei. The two sigma parameters allow to fine tune how close  objects can be and how precise their boundaries are detected.  ![img.png](https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes/raw/main/docs/local_minima_seeded_watershed.png)  ### Gaussian blur  Applies a [Gaussian blur](https://scikit-image.org/docs/dev/api/skimage.filters.html#skimage.filters.gaussian) to an image. This might be useful for denoising, e.g. before applying the Threshold-Otsu method.  ![img.png](https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes/raw/main/docs/gaussian_blur.png)  ### Subtract background  Subtracts background using [scikit-image's rolling-ball algorithm](https://scikit-image.org/docs/stable/auto_examples/segmentation/plot_rolling_ball.html).  This might be useful, for example to make intensity of membranes more similar in different regions of an image.  ![img.png](https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes/raw/main/docs/subtract_background.png)  ### Threshold Otsu  Binarizes an image using [scikit-image's threshold Otsu algorithm](https://scikit-image.org/docs/dev/auto_examples/segmentation/plot_thresholding.html), also known as  [Otsu's method](https://ieeexplore.ieee.org/document/4310076).  ![img.png](https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes/raw/main/docs/threshold_otsu.png)  ### Split touching objects (formerly known as binary watershed).  In case objects stick together after thresholding, this tool might help. It aims to deliver similar results as [ImageJ's watershed implementation](https://imagej.nih.gov/ij/docs/menus/process.html#watershed).  ![img.png](https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes/raw/main/docs/binary_watershed.png)  ### Connected component labeling  Takes a binary image and produces a label image with all separated objects labeled differently. Under the hood, it uses [scikit-image's label function](https://scikit-image.org/docs/dev/auto_examples/segmentation/plot_label.html).  ![img.png](https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes/raw/main/docs/connected_component_labeling.png)  ### Manual split and merge labels  Split and merge labels in napari manually via the `Tools > Utilities menu`:  ![](https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes/raw/main/docs/split_and_merge_demo.gif)  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.  ## Installation  You can install `napari-segment-blobs-and-things-with-membranes` using `conda` and `pip`. If you have never used `conda` before, please go through [this tutorial](https://biapol.github.io/blog/johannes_mueller/anaconda_getting_started/) first.      conda install -c conda-forge napari     pip install napari-segment-blobs-and-things-with-membranes  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-segment-blobs-and-things-with-membranes\" is free and open source software  ## Issues  If you encounter any problems, please create a thread on [image.sc] along with a detailed description and tag [@haesleinhuepf].  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/  [image.sc]: https://image.sc [@haesleinhuepf]: https://twitter.com/haesleinhuepf ",
    "description_content_type": "text/markdown",
    "description_text": "napari-segment-blobs-and-things-with-membranes (nsbatwm)         This napari-plugin is based on scikit-image and allows segmenting nuclei and cells based on fluorescence microscopy images with high intensity in nuclei and/or membranes. Usage This plugin populates image processing operations to the Tools menu in napari. You can recognize them with their suffix (nsbatwm) in brackets. Furthermore, it can be used from the napari-assistant graphical user interface.  Therefore, just click the menu Tools > Utilities > Assistant (na) or run naparia from the command line.  You can also call these functions as shown in the demo notebook. Voronoi-Otsu-Labeling This algorithm uses Otsu's thresholding method in combination with  Gaussian blur and a  Voronoi-Tesselation  approach to label bright objects such as nuclei in an intensity image. The alogrithm has two sigma parameters which allow you to fine-tune where objects should be cut (spot_sigma) and how smooth outlines should be (outline_sigma). This implementation aims to be similar to Voronoi-Otsu-Labeling in clesperanto.  Seeded Watershed Starting from an image showing high-intensity membranes and a seed-image where objects have been labeled (e.g. using Voronoi-Otsu-Labeling), objects are labeled that are constrained by the membranes.  Seeded Watershed with mask If there is additionally a mask image available, one can use the Seeded Watershed with mask, to constraint the flooding  on a membrane image (1), starting from nuclei (2), limited by a mask image (3) to produce a cell segmentation within the mask (4).  Seeded Watershed using local minima as starting points Similar to the Seeded Watershed and Voronoi-Otsu-Labeling explained above, you can use this tool to segment an image showing membranes without an additional image showing nuclei. The two sigma parameters allow to fine tune how close  objects can be and how precise their boundaries are detected.  Gaussian blur Applies a Gaussian blur to an image. This might be useful for denoising, e.g. before applying the Threshold-Otsu method.  Subtract background Subtracts background using scikit-image's rolling-ball algorithm.  This might be useful, for example to make intensity of membranes more similar in different regions of an image.  Threshold Otsu Binarizes an image using scikit-image's threshold Otsu algorithm, also known as  Otsu's method.  Split touching objects (formerly known as binary watershed). In case objects stick together after thresholding, this tool might help. It aims to deliver similar results as ImageJ's watershed implementation.  Connected component labeling Takes a binary image and produces a label image with all separated objects labeled differently. Under the hood, it uses scikit-image's label function.  Manual split and merge labels Split and merge labels in napari manually via the Tools > Utilities menu:   This napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template. Installation You can install napari-segment-blobs-and-things-with-membranes using conda and pip. If you have never used conda before, please go through this tutorial first. conda install -c conda-forge napari pip install napari-segment-blobs-and-things-with-membranes  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-segment-blobs-and-things-with-membranes\" is free and open source software Issues If you encounter any problems, please create a thread on image.sc along with a detailed description and tag @haesleinhuepf.",
    "development_status": ["Development Status :: 3 - Alpha"],
    "display_name": "napari-segment-blobs-and-things-with-membranes",
    "documentation": "https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes#README.md",
    "first_released": "2021-09-25T14:21:45.864946Z",
    "license": "BSD-3-Clause",
    "name": "napari-segment-blobs-and-things-with-membranes",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-09-08T12:29:31.639886Z",
    "report_issues": "https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes/issues",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "scikit-image",
      "scipy",
      "napari-tools-menu (>=0.1.17)",
      "napari-time-slicer (>=0.4.8)",
      "napari-assistant",
      "stackview (>=0.3.2)"
    ],
    "summary": "A plugin based on scikit-image for segmenting nuclei and cells based on fluorescent microscopy images with high intensity in nuclei and/or membranes",
    "support": "https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes/issues",
    "twitter": "",
    "version": "0.3.3",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Robert Haase" }],
    "category": { "Workflow step": ["Object feature extraction"] },
    "category_hierarchy": {
      "Workflow step": [
        ["Object feature extraction", "Shape features extraction"],
        ["Object feature extraction"]
      ]
    },
    "code_repository": "https://github.com/haesleinhuepf/napari-skimage-regionprops",
    "description": "# napari-skimage-regionprops (nsr)\\r \\r \\r \\r [![License](https://img.shields.io/pypi/l/napari-skimage-regionprops.svg?color=green)](https://github.com/haesleinhuepf/napari-skimage-regionprops/raw/master/LICENSE)\\r \\r [![PyPI](https://img.shields.io/pypi/v/napari-skimage-regionprops.svg?color=green)](https://pypi.org/project/napari-skimage-regionprops)\\r \\r [![Python Version](https://img.shields.io/pypi/pyversions/napari-skimage-regionprops.svg?color=green)](https://python.org)\\r \\r [![tests](https://github.com/haesleinhuepf/napari-skimage-regionprops/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-skimage-regionprops/actions)\\r \\r [![codecov](https://codecov.io/gh/haesleinhuepf/napari-skimage-regionprops/branch/master/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-skimage-regionprops)\\r \\r [![Development Status](https://img.shields.io/pypi/status/napari-skimage-regionprops.svg)](https://en.wikipedia.org/wiki/Software_release_life_cycle#Alpha)\\r \\r [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-skimage-regionprops)](https://napari-hub.org/plugins/napari-skimage-regionprops)\\r \\r \\r \\r  \\r \\r A [napari] plugin for measuring properties of labeled objects based on [scikit-image]\\r \\r \\r \\r ![](https://github.com/haesleinhuepf/napari-skimage-regionprops/raw/master/images/interactive.gif)\\r \\r \\r \\r ## Usage: measure region properties\\r \\r \\r \\r From the menu `Tools > Measurement > Regionprops (nsr)` you can open a dialog where you can choose an intensity image, a corresponding label image and the features you want to measure:\\r \\r \\r \\r ![img.png](https://github.com/haesleinhuepf/napari-skimage-regionprops/raw/master/images/dialog.png)\\r \\r \\r \\r If you want to interface with the labels and see which table row corresponds to which labeled object, use the label picker and\\r \\r activate the `show selected` checkbox.\\r \\r \\r \\r ![](https://github.com/haesleinhuepf/napari-skimage-regionprops/raw/master/images/interactive.png)\\r \\r \\r \\r If you closed a table and want to reopen it, you can use the menu `Tools > Measurements > Show table (nsr)` to reopen it. \\r \\r You just need to select the labels layer the properties are associated with.\\r \\r \\r \\r For visualizing measurements with different grey values, as parametric images, you can double-click table headers.\\r \\r \\r \\r ![img.png](https://github.com/haesleinhuepf/napari-skimage-regionprops/raw/master/images/label_value_visualization.gif)\\r \\r \\r \\r ## Usage: measure point intensities\\r \\r \\r \\r Analogously, also the intensity and coordinates of point layers can be measured using the menu `Tools > Measurement > Measure intensity at point coordinates (nsr)`. \\r \\r Also these measurements can be visualized by double-clicking table headers:\\r \\r \\r \\r ![img.png](measure_point_intensity.png)\\r \\r \\r \\r ![img_1.png](measure_point_coordinate.png)\\r \\r \\r \\r ## Working with time-lapse and tracking data\\r \\r \\r \\r Note that tables for time-lapse data should include a column named \"frame\", which indicates which slice in\\r \\r time the given row refers to. If you want to import your own csv files for time-lapse data make sure to include this column.\\r \\r If you have tracking data where each column specifies measurements for a track instead of a label at a specific time point,\\r \\r this column must not be added.\\r \\r \\r \\r In case you have 2D time-lapse data you need to convert it into a suitable shape using the function: `Tools > Utilities > Convert 3D stack to 2D time-lapse (time-slicer)`,\\r \\r which can be found in the [napari time slicer](https://www.napari-hub.org/plugins/napari-time-slicer).\\r \\r \\r \\r Last but not least, make sure that in case of time-lapse data the label image has labels that are subsquently labeled per timepoint.\\r \\r E.g. a dataset where label 5 is missing at timepoint 4 may be visualized incorrectly.\\r \\r \\r \\r ## Usage, programmatically\\r \\r \\r \\r You can also control the tables programmatically. See this \\r \\r [example notebook](https://github.com/haesleinhuepf/napari-skimage-regionprops/blob/master/demo/tables.ipynb) for details on regionprops and\\r \\r [this example notebook](https://github.com/haesleinhuepf/napari-skimage-regionprops/blob/master/demo/measure_points.ipynb) for details on measuring intensity at point coordinates.\\r \\r \\r \\r \\r \\r ## Features\\r \\r The user can select categories of features for feature extraction in the user interface. These categories contain measurements from the scikit-image [regionprops list of measurements](https://scikit-image.org/docs/dev/api/skimage.measure.html#skimage.measure.regionprops) library:\\r \\r * size:\\r \\r   * area\\r \\r   * bbox_area\\r \\r   * convex_area\\r \\r   * equivalent_diameter\\r \\r * intensity:\\r \\r   * max_intensity \\r \\r   * mean_intensity\\r \\r   * min_intensity\\r \\r   * standard_deviation_intensity (`extra_properties` implementation using numpy)\\r \\r * perimeter:\\r \\r   * perimeter\\r \\r   * perimeter_crofton\\r \\r * shape\\r \\r   * major_axis_length\\r \\r   * minor_axis_length\\r \\r   * orientation\\r \\r   * solidity\\r \\r   * eccentricity\\r \\r   * extent\\r \\r   * feret_diameter_max\\r \\r   * local_centroid\\r \\r   * roundness as defined for 2D labels [by ImageJ](https://imagej.nih.gov/ij/docs/menus/analyze.html)\\r \\r   * circularity as defined for 2D labels  [by ImageJ](https://imagej.nih.gov/ij/docs/menus/analyze.html)\\r \\r   * aspect_ratio as defined for 2D labels [by ImageJ](https://imagej.nih.gov/ij/docs/menus/analyze.html)\\r \\r * position:\\r \\r   * centroid\\r \\r   * bbox\\r \\r   * weighted_centroid\\r \\r * moments:\\r \\r   * moments\\r \\r   * moments_central\\r \\r   * moments_hu\\r \\r   * moments_normalized\\r \\r \\r \\r \\r \\r \\r \\r This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\\r \\r \\r \\r ## See also\\r \\r \\r \\r There are other napari plugins with similar functionality for extracting features:\\r \\r * [morphometrics](https://www.napari-hub.org/plugins/morphometrics)\\r \\r * [PartSeg](https://www.napari-hub.org/plugins/PartSeg)\\r \\r * [napari-simpleitk-image-processing](https://www.napari-hub.org/plugins/napari-simpleitk-image-processing)\\r \\r * [napari-cupy-image-processing](https://www.napari-hub.org/plugins/napari-cupy-image-processing)\\r \\r * [napari-pyclesperanto-assistant](https://www.napari-hub.org/plugins/napari-pyclesperanto-assistant)\\r \\r \\r \\r Furthermore, there are plugins for postprocessing extracted measurements\\r \\r * [napari-feature-classifier](https://www.napari-hub.org/plugins/napari-feature-classifier)\\r \\r * [napari-clusters-plotter](https://www.napari-hub.org/plugins/napari-clusters-plotter)\\r \\r * [napari-accelerated-pixel-and-object-classification](https://www.napari-hub.org/plugins/napari-accelerated-pixel-and-object-classification)\\r \\r \\r \\r ## Installation\\r \\r \\r \\r You can install `napari-skimage-regionprops` via [pip]:\\r \\r \\r \\r     pip install napari-skimage-regionprops\\r \\r \\r \\r Or if you plan to develop it:\\r \\r \\r \\r     git clone https://github.com/haesleinhuepf/napari-skimage-regionprops\\r \\r     cd napari-skimage-regionprops\\r \\r     pip install -e .\\r \\r \\r \\r If there is an error message suggesting that git is not installed, run `conda install git`.\\r \\r \\r \\r ## Contributing\\r \\r \\r \\r Contributions are very welcome. Tests can be run with [tox], please ensure\\r \\r the coverage at least stays the same before you submit a pull request.\\r \\r \\r \\r ## License\\r \\r \\r \\r Distributed under the terms of the [BSD-3] license,\\r \\r \"napari-skimage-regionprops\" is free and open source software\\r \\r \\r \\r ## Issues\\r \\r \\r \\r If you encounter any problems, please create a thread on [image.sc] along with a detailed description and tag [@haesleinhuepf].\\r \\r \\r \\r [napari]: https://github.com/napari/napari\\r \\r [Cookiecutter]: https://github.com/audreyr/cookiecutter\\r \\r [@napari]: https://github.com/napari\\r \\r [BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\r \\r [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\r \\r [image.sc]: https://image.sc\\r \\r [napari]: https://github.com/napari/napari\\r \\r [tox]: https://tox.readthedocs.io/en/latest/\\r \\r [pip]: https://pypi.org/project/pip/\\r \\r [PyPI]: https://pypi.org/\\r \\r [scikit-image]: https://scikit-image.org/\\r \\r [@haesleinhuepf]: https://twitter.com/haesleinhuepf\\r \\r ",
    "description_content_type": "text/markdown",
    "description_text": "napari-skimage-regionprops (nsr)        A napari plugin for measuring properties of labeled objects based on scikit-image  Usage: measure region properties From the menu Tools > Measurement > Regionprops (nsr) you can open a dialog where you can choose an intensity image, a corresponding label image and the features you want to measure:  If you want to interface with the labels and see which table row corresponds to which labeled object, use the label picker and activate the show selected checkbox.  If you closed a table and want to reopen it, you can use the menu Tools > Measurements > Show table (nsr) to reopen it.  You just need to select the labels layer the properties are associated with. For visualizing measurements with different grey values, as parametric images, you can double-click table headers.  Usage: measure point intensities Analogously, also the intensity and coordinates of point layers can be measured using the menu Tools > Measurement > Measure intensity at point coordinates (nsr).  Also these measurements can be visualized by double-clicking table headers:   Working with time-lapse and tracking data Note that tables for time-lapse data should include a column named \"frame\", which indicates which slice in time the given row refers to. If you want to import your own csv files for time-lapse data make sure to include this column. If you have tracking data where each column specifies measurements for a track instead of a label at a specific time point, this column must not be added. In case you have 2D time-lapse data you need to convert it into a suitable shape using the function: Tools > Utilities > Convert 3D stack to 2D time-lapse (time-slicer), which can be found in the napari time slicer. Last but not least, make sure that in case of time-lapse data the label image has labels that are subsquently labeled per timepoint. E.g. a dataset where label 5 is missing at timepoint 4 may be visualized incorrectly. Usage, programmatically You can also control the tables programmatically. See this  example notebook for details on regionprops and this example notebook for details on measuring intensity at point coordinates. Features The user can select categories of features for feature extraction in the user interface. These categories contain measurements from the scikit-image regionprops list of measurements library:   size:   area   bbox_area   convex_area   equivalent_diameter   intensity:   max_intensity    mean_intensity   min_intensity   standard_deviation_intensity (extra_properties implementation using numpy)   perimeter:   perimeter   perimeter_crofton   shape   major_axis_length   minor_axis_length   orientation   solidity   eccentricity   extent   feret_diameter_max   local_centroid   roundness as defined for 2D labels by ImageJ   circularity as defined for 2D labels  by ImageJ   aspect_ratio as defined for 2D labels by ImageJ   position:   centroid   bbox   weighted_centroid   moments:   moments   moments_central   moments_hu   moments_normalized   This napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template. See also There are other napari plugins with similar functionality for extracting features:   morphometrics   PartSeg   napari-simpleitk-image-processing   napari-cupy-image-processing   napari-pyclesperanto-assistant   Furthermore, there are plugins for postprocessing extracted measurements   napari-feature-classifier   napari-clusters-plotter   napari-accelerated-pixel-and-object-classification   Installation You can install napari-skimage-regionprops via pip: pip install napari-skimage-regionprops  Or if you plan to develop it: git clone https://github.com/haesleinhuepf/napari-skimage-regionprops  cd napari-skimage-regionprops  pip install -e .  If there is an error message suggesting that git is not installed, run conda install git. Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-skimage-regionprops\" is free and open source software Issues If you encounter any problems, please create a thread on image.sc along with a detailed description and tag @haesleinhuepf.",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "napari-skimage-regionprops",
    "documentation": "",
    "first_released": "2021-06-07T19:49:01.148199Z",
    "license": "BSD-3-Clause",
    "name": "napari-skimage-regionprops",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/haesleinhuepf/napari-skimage-regionprops",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-12-20T23:09:31.486345Z",
    "report_issues": "",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "scikit-image",
      "napari",
      "pandas",
      "napari-tools-menu (>=0.1.11)",
      "napari-workflows",
      "imageio (!=2.22.1)"
    ],
    "summary": "A regionprops table widget plugin for napari",
    "support": "",
    "twitter": "",
    "version": "0.6.0",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Léo Guignard" }],
    "code_repository": "https://github.com/leoguignard/napari-turing",
    "conda": [{ "channel": "conda-forge", "package": "napari-turing" }],
    "description": "# napari-turing  [![License MIT](https://img.shields.io/pypi/l/napari-turing.svg?color=green)](https://github.com/leoguignard/napari-turing/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-turing.svg?color=green)](https://pypi.org/project/napari-turing) [![Python Version](https://img.shields.io/pypi/pyversions/napari-turing.svg?color=green)](https://python.org) [![tests](https://github.com/leoguignard/napari-turing/workflows/tests/badge.svg)](https://github.com/leoguignard/napari-turing/actions) [![codecov](https://codecov.io/gh/leoguignard/napari-turing/branch/main/graph/badge.svg)](https://codecov.io/gh/leoguignard/napari-turing) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-turing)](https://napari-hub.org/plugins/napari-turing)  A plugin to run simple simulations of Turing patterns  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/plugins/index.html --> ![example 1](img/turing_patterns.gif) ![example 2](img/turing_patterns2.gif)  ## Installation  You can install `napari-turing` via [pip] after downloading the content of      pip install napari-turing   To install latest version :      pip install git+https://github.com/leoguignard/napari-turing.git  ## Troubleshooting  If the installation does not work just with the previous command, it might be useful to first install [napari] for example that way:      conda install napari  ## Creating a new model  To create your own model, you can use the template for models [here](src/napari_turing/Models/ModelTemplate.py).  Note that a bit of knowledge in Python is probably necessary and it might not be completely trivial at first but you'll manage :)  First you need to name the concentrations that you will use with the following [line](src/napari_turing/Models/ModelTemplate.py#L40): ```python     _concentration_names = [\"A\", \"I\"] ```  Then, you need to declare its variables. For example you can create a parameter named `mu_a` the following way ([here in the code](src/napari_turing/Models/ModelTemplate.py#L53-L60)): ```python     mu_a = ModelParameter(         name=\"mu_a\",  # Name of the parameter         description=\"Activator diffusion coefficient (10^-4)\",  # Description of the parameter for napari         value=2.8,  # Initial and default value         min=1,  # Minimum value the parameter can take         max=5,  # Maximum value the parameter can take         exponent=1e-4,  # All values given to this instance of the class will but multiplied by this value     ) ```  Then you need to list the parameters that are necessary to run the model (usually all the paramaters declared previously) and the parameters that you will allow the user to tune (for example, sometimes some of the parameters are co-dependent and there is no point in being able to tune both of them). That should be done the following way ([here in the code](src/napari_turing/Models/ModelTemplate.py#L86-89)): ```python     # These are the parameters that are necessary to run the equations.     _necessary_parameters = [tau, k, mu_a, mu_i]     # These are the parameters that can be modified via napari     _tunable_parameters = _necessary_parameters ```  If you want, you can specify what the method will return as a string, it will be displayed in the napari viewer ([here in the code](src/napari_turing/Models/ModelTemplate.py#L90-L98)): ```python     # This function allows to display some information about the model     # in napari     def __str__(self) -> str:         return (             \"Equations (FitzHugh-Nagumo model):\\ \"             \"  Concentration of Activator (a) and Inhibitor (i)\\ \"             \"    - da/dt = mu_a * diffusion(a) + a - a^3 - i + k\\ \"             \"    - tau * di/dt = mu_i * diffusion(i) + a - i\"         ) ```  Now that the basics are declared, you will need to declare how to initialize your concentrations the following way ([here in the code](src/napari_turing/Models/ModelTemplate.py#L100-L116)): ```python     # The following allows to reset the values of the concentrations.     # The function takes the name of the concentration to initialize.     # If no name is given or if it is None all the concentrations are     # reinitialized.     #     # The reason why this function is useful is that some models      # require specific initialisations for them to work correctly     # In the following example the concentrations are reintinalized     # to a random value between -1 and 1.     # This is the default behavior, so if you don't need to change     # it you don't have to implement the function.     def init_concentrations(self, C: Optional[str] = None) -> None:         if C is None:             for ci in self.concentration_names():                 self[ci] = np.random.random((self.size, self.size)) * 2 - 1         else:             self[C] = np.random.random((self.size, self.size)) * 2 - 1 ``` In the previous example, the all concentrations are initialized the same way. If you need to have different initializations, you can do it the following way for example ([from the GrayScott model](src/napari_turing/Models/GrayScott.py#L68-L76)): ```python     def init_concentrations(self, C: Optional[str] = None) -> None:         if C == \"X\" or C is None:             self[\"X\"] = np.ones((self.size, self.size))         if C == \"Y\" or C is None:             Y = np.zeros((self.size, self.size))             nb_pos = 20             pos = (np.random.random((2, nb_pos)) * self.size).astype(int)             Y[pos[0], pos[1]] = 1             self[\"Y\"] = Y ``` In this model, there are two concentrations, `X` and `Y` which are initialized differenty. Note that they can be accessed using `self[\"X\"]` or `self.X`.  Finally, you of course have to define the reaction equations and the diffusion equations. The way it is defined is with two functions, one for the reaction and one for the diffusion, that take as an input the name of the concentration to apply the function to and returns the new values. Then for each of your concentrations, their new values will be computed as followed: ```python new_concentration = current_concentration + dt*(reaction + diffusion) ```  Here is an example for the reaction function ([here in the code](src/napari_turing/Models/ModelTemplate.py#L127-L136)): ```python      # This function defines the equations of the reactions.     # It takes as an input which concentration to compute     # (in this example we have to define how to compute A and I)     def _reaction(self, c: str) -> np.ndarray:         if c == \"A\":             # Below is the reaction part of the equation (1)             return self.A - self.A**3 - self.I + self.k          elif c == \"I\":             # Below is the reaction part of the equation (2)             return (self.A - self.I) / self.tau ``` Of course, if you have more concentrations, you will need to define more equations.  Here is an example for the reaction function ([here in the code](src/napari_turing/Models/ModelTemplate.py#L138-L166)): ```python     # This function defines the equations of the diffusion.     # It takes as an input which concentration to compute     # (in this example we have to define how to compute A and I)     # Here we compute the diffusion as follow:     # A cell gives an equal fraction mu of its concentration to its neighbors     # A cell recieves an equal fraction mu of concentration from its neighbors     # Neighbors = (left, right, above, below)     # In the case of oriented diffusion the amount recieved and given to the neighbors     # is imbalanced according to the position of the neighbor.     def _diffusion(self, c: str) -> np.ndarray:         if c == \"A\":             arr = self.A # Define the array of concentrations to diffuse for the reageant A             mu = self.mu_a # Define the diffusion coefficient for the reageant A         elif c == \"I\":             arr = self.I # Define the array of concentrations to diffuse for the reageant I             mu = self.mu_i # Define the diffusion coefficient for the reageant I                  # Computes what is recieved from neighboring cells         from_cell = convolve(arr, self.kernel.value, mode=\"constant\", cval=0)         # Computes what is given to neighboring cells         to_cell = self.nb_neighbs * arr          # Computes the diffusion         out = mu * (from_cell - to_cell) / (self.dx * self.dy)          # In our case, the equation (2), for I specify that it has to be divided by tau         if c == \"I\":             out /= self.tau         return out ``` The diffusion function is usually a standard one so it might not be necessary to overly change it.  You can find other model examples: - [Brusselator](src/napari_turing/Models/Brusselator.py) - [GrayScott](src/napari_turing/Models/GrayScott.py) - [GameOfLife](src/napari_turing/Models/GameOfLife.py)  Once all that is done, let say you've saved your new model in the folder [Models](src/napari_turing/Models) under the name `NewModel.py` and the model class created is name `NewModel`. Then you need to declare you model in the [`_model_list.py`](src/napari_turing/Models/_model_list.py) file. To do so you need to add the following lines in the file: ```python from enum import Enum from .FitzHughNagumo import FitzHughNagumo from .Brusselator import Brusselator from .GrayScott import GrayScott from .GameOfLife import GameOfLife from .NewModel import NewModel ## THAT LINE HERE  class AvailableModels(Enum):     FitzHughNagumo = FitzHughNagumo     Brusselator = Brusselator     GrayScott = GrayScott     GameOfLife = GameOfLife     NewModel = NewModel ## AND THAT OTHER LINE HERE ```  ## Contributing  Contributions are very welcome.  ## License  Distributed under the terms of the [MIT] license, \"napari-turing\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/leoguignard/napari-turing/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-turing       A plugin to run simple simulations of Turing patterns  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.    Installation You can install napari-turing via pip after downloading the content of pip install napari-turing  To install latest version : pip install git+https://github.com/leoguignard/napari-turing.git  Troubleshooting If the installation does not work just with the previous command, it might be useful to first install napari for example that way: conda install napari  Creating a new model To create your own model, you can use the template for models here. Note that a bit of knowledge in Python is probably necessary and it might not be completely trivial at first but you'll manage :) First you need to name the concentrations that you will use with the following line: python     _concentration_names = [\"A\", \"I\"] Then, you need to declare its variables. For example you can create a parameter named mu_a the following way (here in the code): python     mu_a = ModelParameter(         name=\"mu_a\",  # Name of the parameter         description=\"Activator diffusion coefficient (10^-4)\",  # Description of the parameter for napari         value=2.8,  # Initial and default value         min=1,  # Minimum value the parameter can take         max=5,  # Maximum value the parameter can take         exponent=1e-4,  # All values given to this instance of the class will but multiplied by this value     ) Then you need to list the parameters that are necessary to run the model (usually all the paramaters declared previously) and the parameters that you will allow the user to tune (for example, sometimes some of the parameters are co-dependent and there is no point in being able to tune both of them). That should be done the following way (here in the code): python     # These are the parameters that are necessary to run the equations.     _necessary_parameters = [tau, k, mu_a, mu_i]     # These are the parameters that can be modified via napari     _tunable_parameters = _necessary_parameters If you want, you can specify what the method will return as a string, it will be displayed in the napari viewer (here in the code): python     # This function allows to display some information about the model     # in napari     def __str__(self) -> str:         return (             \"Equations (FitzHugh-Nagumo model):\\ \"             \"  Concentration of Activator (a) and Inhibitor (i)\\ \"             \"    - da/dt = mu_a * diffusion(a) + a - a^3 - i + k\\ \"             \"    - tau * di/dt = mu_i * diffusion(i) + a - i\"         ) Now that the basics are declared, you will need to declare how to initialize your concentrations the following way (here in the code): python     # The following allows to reset the values of the concentrations.     # The function takes the name of the concentration to initialize.     # If no name is given or if it is None all the concentrations are     # reinitialized.     #     # The reason why this function is useful is that some models      # require specific initialisations for them to work correctly     # In the following example the concentrations are reintinalized     # to a random value between -1 and 1.     # This is the default behavior, so if you don't need to change     # it you don't have to implement the function.     def init_concentrations(self, C: Optional[str] = None) -> None:         if C is None:             for ci in self.concentration_names():                 self[ci] = np.random.random((self.size, self.size)) * 2 - 1         else:             self[C] = np.random.random((self.size, self.size)) * 2 - 1 In the previous example, the all concentrations are initialized the same way. If you need to have different initializations, you can do it the following way for example (from the GrayScott model): python     def init_concentrations(self, C: Optional[str] = None) -> None:         if C == \"X\" or C is None:             self[\"X\"] = np.ones((self.size, self.size))         if C == \"Y\" or C is None:             Y = np.zeros((self.size, self.size))             nb_pos = 20             pos = (np.random.random((2, nb_pos)) * self.size).astype(int)             Y[pos[0], pos[1]] = 1             self[\"Y\"] = Y In this model, there are two concentrations, X and Y which are initialized differenty. Note that they can be accessed using self[\"X\"] or self.X. Finally, you of course have to define the reaction equations and the diffusion equations. The way it is defined is with two functions, one for the reaction and one for the diffusion, that take as an input the name of the concentration to apply the function to and returns the new values. Then for each of your concentrations, their new values will be computed as followed: python new_concentration = current_concentration + dt*(reaction + diffusion) Here is an example for the reaction function (here in the code): python      # This function defines the equations of the reactions.     # It takes as an input which concentration to compute     # (in this example we have to define how to compute A and I)     def _reaction(self, c: str) -> np.ndarray:         if c == \"A\":             # Below is the reaction part of the equation (1)             return self.A - self.A**3 - self.I + self.k          elif c == \"I\":             # Below is the reaction part of the equation (2)             return (self.A - self.I) / self.tau Of course, if you have more concentrations, you will need to define more equations. Here is an example for the reaction function (here in the code): ```python     # This function defines the equations of the diffusion.     # It takes as an input which concentration to compute     # (in this example we have to define how to compute A and I)     # Here we compute the diffusion as follow:     # A cell gives an equal fraction mu of its concentration to its neighbors     # A cell recieves an equal fraction mu of concentration from its neighbors     # Neighbors = (left, right, above, below)     # In the case of oriented diffusion the amount recieved and given to the neighbors     # is imbalanced according to the position of the neighbor.     def _diffusion(self, c: str) -> np.ndarray:         if c == \"A\":             arr = self.A # Define the array of concentrations to diffuse for the reageant A             mu = self.mu_a # Define the diffusion coefficient for the reageant A         elif c == \"I\":             arr = self.I # Define the array of concentrations to diffuse for the reageant I             mu = self.mu_i # Define the diffusion coefficient for the reageant I     # Computes what is recieved from neighboring cells     from_cell = convolve(arr, self.kernel.value, mode=\"constant\", cval=0)     # Computes what is given to neighboring cells     to_cell = self.nb_neighbs * arr      # Computes the diffusion     out = mu * (from_cell - to_cell) / (self.dx * self.dy)      # In our case, the equation (2), for I specify that it has to be divided by tau     if c == \"I\":         out /= self.tau     return out  ``` The diffusion function is usually a standard one so it might not be necessary to overly change it. You can find other model examples: - Brusselator - GrayScott - GameOfLife Once all that is done, let say you've saved your new model in the folder Models under the name NewModel.py and the model class created is name NewModel. Then you need to declare you model in the _model_list.py file. To do so you need to add the following lines in the file: ```python from enum import Enum from .FitzHughNagumo import FitzHughNagumo from .Brusselator import Brusselator from .GrayScott import GrayScott from .GameOfLife import GameOfLife from .NewModel import NewModel ## THAT LINE HERE class AvailableModels(Enum):     FitzHughNagumo = FitzHughNagumo     Brusselator = Brusselator     GrayScott = GrayScott     GameOfLife = GameOfLife     NewModel = NewModel ## AND THAT OTHER LINE HERE ``` Contributing Contributions are very welcome. License Distributed under the terms of the MIT license, \"napari-turing\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "Turing Patterns",
    "documentation": "https://github.com/leoguignard/napari-turing#README.md",
    "first_released": "2022-08-08T16:42:05.972803Z",
    "license": "MIT",
    "name": "napari-turing",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/leoguignard/napari-turing",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-10-27T14:43:28.246116Z",
    "report_issues": "https://github.com/leoguignard/napari-turing/issues",
    "requirements": [
      "numpy",
      "scipy",
      "scikit-image",
      "magicgui",
      "qtpy",
      "napari",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "summary": "A plugin to run simmple simulations of Turing patterns",
    "support": "https://github.com/leoguignard/napari-turing/issues",
    "twitter": "",
    "version": "0.3.2",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Guillaume Witz" }],
    "code_repository": "https://github.com/guiwitz/napari-serialcellpose",
    "conda": [],
    "description": "# napari-serialcellpose  [![License](https://img.shields.io/pypi/l/napari-serialcellpose.svg?color=green)](https://github.com/guiwitz/napari-serialcellpose/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-serialcellpose.svg?color=green)](https://pypi.org/project/napari-serialcellpose) [![Python Version](https://img.shields.io/pypi/pyversions/napari-serialcellpose.svg?color=green)](https://python.org) [![tests](https://github.com/guiwitz/napari-serialcellpose/workflows/tests/badge.svg)](https://github.com/guiwitz/napari-serialcellpose/actions) [![codecov](https://codecov.io/gh/guiwitz/napari-serialcellpose/branch/main/graph/badge.svg)](https://codecov.io/gh/guiwitz/napari-serialcellpose) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-serialcellpose)](https://napari-hub.org/plugins/napari-serialcellpose)  This napari plugin allows you to segment single images or series of images using built-in or custom Cellpose models as well as to analyze the properties of these segmented regions (\"region properties\"). Properties can be visualized for a single image or a complete experiment in the form of histograms that can also be filtered (e.g. based on area size, mean intensity etc.) Thanks to the [napari-skimage-regionprops](https://github.com/haesleinhuepf/napari-skimage-regionprops) plugin, properties of segmented objects can be interactively explored at a single object level.  ## Main goal  The main goal of this plugin is to simplify the classical image processing pipeline of image segmentation followed by region analysis via Cellpose. It allows to quickly get a quantification of a set of images without the need for any scripting.  ## Installation  In order to use this plugin, whe highly recommend to create a specific environment and to install the required software in it. You can create a conda environment using:      conda create -n serialcellpose python=3.8.5 napari -c conda-forge  Then activate it and install the plugin:          conda activate serialcellpose     pip install napari-serialcellpose  ### Potential issue with PyTorch  Cellpose and therefore the plugin and napari can crash without warning in some cases with ```torch==1.12.0```. This can be fixed by reverting to an earlier version using:          pip install torch==1.11.0  ### GPU  In order to use a GPU:  1. Uninstall the PyTorch version that gets installed by default with Cellpose:          pip uninstall torch  2. Make sure your have up-to-date drivers for your NVIDIA card installed.  3. Re-install a GPU version of PyTorch via conda using a command that you can find [here](https://pytorch.org/get-started/locally/) (this takes care of the cuda toolkit, cudnn etc. so **no need to install manually anything more than the driver**). The command will look like this:          conda install pytorch torchvision cudatoolkit=11.3 -c pytorch  ### Plugin Updates  To update the plugin, you only need to activate the existing environment and install the new version:      conda activate serialcellpose     pip install napari-serialcellpose -U  ## Usage: segmentation  The main interface is shown below. The sequence of events should be the following:  1. Select a folder containing images. The list of files within that folder will appear in the area above. You can also just drag and drop a folder or an image in that area. When selecting an image, it gets displayed in the viewer. Images are opened via [aicsimageio](https://allencellmodeling.github.io/aicsimageio/). You can use grayscale images, RGB images or multi-channel images. In the latter case, **make sure each channel opens as a separate layer when you open them using the napari-aicsimagio importer**. 2. If you want to save the segmentation and tables with properties, select a folder that will contain the output. 3. Select the type of cellpose model. 4. If you use a custom model, select its location. 5. Run the analysis on the currently selected image or on all files in the folder. ### Options  6. Select if you want to use a GPU or not. 7. If you are using multi-channel images, you can specify which channel to segment and optionally which to use as \"nuclei\" channel to help cell segmentation. 8. In case you are using one of the built-in models, you can set the estimated diameter of your objects. 9. In the Options tab you will find a few more options for segmentation, including the two thresholds ```flow_threshold``` and ```cellprob_threshold```. You can also decide to discard objects touching the border. Using the ```Select options yml file``` you can select a ```.yml``` file which contains a list of additional options to pass to the ```eval``` method of the Cellpose model. **Note that options specified in the yml file will override options set in the GUI**. The file [my_options.yml](https://raw.githubusercontent.com/guiwitz/napari-serialcellpose/main/src/napari_serialcellpose/_tests/my_options.yml) is an example of such a file where for example the ```diameter``` (also available in the GUI) and ```resample``` (not available in the GUI) options are set.   <img src=\"https://github.com/guiwitz/napari-serialcellpose/raw/main/illustrations/napari_serialcellpose_gui1.png\" alt=\"image\" width=\"500\"> <img src=\"https://github.com/guiwitz/napari-serialcellpose/raw/main/illustrations/napari_serialcellpose_gui1b.png\" alt=\"image\" width=\"500\">  ### Properties  10. After segmentation, properties of the objects can automatically be computed. You can select which properties should be computed in the Options tab. As defined in ```napari-skimage-regionprops``` properties are grouped by types. If you want to measure intensity properties such as mean intensity, you have to specify which channel (```Analysis channel```) you want to perform the measurement on.  ### Output  The results of the analysis are saved in the folder chosen in #2. The segmentation mask is saved with the same name as the original image with the suffix ```_mask.tif```. A table with properties is saved in the subfolder ```tables``` also with the same name as the image with the suffix ```props.csv```. If you run the plugin on multiple files in a folder, a ```summary.csv``` file is also generated which compiles all the data. ## Usage: post-processing  After the analysis is done, when you select an image, the corresponding segmentation mask is shown on top of the image as shown below. This also works for saved segmentations: in that case you just select a folder with data and the corresponding output folder.  <img src=\"https://github.com/guiwitz/napari-serialcellpose/raw/main/illustrations/napari_serialcellpose_gui2.png\" alt=\"image\" width=\"500\">  ### Properties  If you head to the **Properties** tab, you will find there two histograms showing the distribution of two properties that you can choose from a list at the top of the window. Below the plot you find the table containing information for each cell (each line is a cell).  As shown below, if you select the box ```show selected```, you can select items in the properties table and it will highlight the corresponding cell in the viewer. If you select the pipet tool, you can also select a cell and see the corresponding line in the table highlighted.  <img src=\"https://github.com/guiwitz/napari-serialcellpose/raw/main/illustrations/napari_serialcellpose_gui3.png\" alt=\"image\" width=\"500\">  ### Summary  Finally if you select the **Summary** tab, and click on ```Load summary```, it will load all data of the current output folder and create histograms of two properties that can be selected. An additional property can be used for filtering the data. Using the sliders, one can set a minimum and maximum threshold on the \"filtering property\", which will create a sub-selection of the data.  <img src=\"https://github.com/guiwitz/napari-serialcellpose/raw/main/illustrations/napari_serialcellpose_gui4.png\" alt=\"image\" width=\"500\">  ## Data  Sample data were acquired by Fabian Blank at the DBMR, University of Bern.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-serialcellpose\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/guiwitz/napari-serialcellpose/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-serialcellpose       This napari plugin allows you to segment single images or series of images using built-in or custom Cellpose models as well as to analyze the properties of these segmented regions (\"region properties\"). Properties can be visualized for a single image or a complete experiment in the form of histograms that can also be filtered (e.g. based on area size, mean intensity etc.) Thanks to the napari-skimage-regionprops plugin, properties of segmented objects can be interactively explored at a single object level. Main goal The main goal of this plugin is to simplify the classical image processing pipeline of image segmentation followed by region analysis via Cellpose. It allows to quickly get a quantification of a set of images without the need for any scripting. Installation In order to use this plugin, whe highly recommend to create a specific environment and to install the required software in it. You can create a conda environment using: conda create -n serialcellpose python=3.8.5 napari -c conda-forge  Then activate it and install the plugin: conda activate serialcellpose pip install napari-serialcellpose  Potential issue with PyTorch Cellpose and therefore the plugin and napari can crash without warning in some cases with torch==1.12.0. This can be fixed by reverting to an earlier version using: pip install torch==1.11.0  GPU In order to use a GPU:   Uninstall the PyTorch version that gets installed by default with Cellpose: pip uninstall torch    Make sure your have up-to-date drivers for your NVIDIA card installed.   Re-install a GPU version of PyTorch via conda using a command that you can find here (this takes care of the cuda toolkit, cudnn etc. so no need to install manually anything more than the driver). The command will look like this: conda install pytorch torchvision cudatoolkit=11.3 -c pytorch    Plugin Updates To update the plugin, you only need to activate the existing environment and install the new version: conda activate serialcellpose pip install napari-serialcellpose -U  Usage: segmentation The main interface is shown below. The sequence of events should be the following:  Select a folder containing images. The list of files within that folder will appear in the area above. You can also just drag and drop a folder or an image in that area. When selecting an image, it gets displayed in the viewer. Images are opened via aicsimageio. You can use grayscale images, RGB images or multi-channel images. In the latter case, make sure each channel opens as a separate layer when you open them using the napari-aicsimagio importer. If you want to save the segmentation and tables with properties, select a folder that will contain the output. Select the type of cellpose model. If you use a custom model, select its location. Run the analysis on the currently selected image or on all files in the folder.  Options  Select if you want to use a GPU or not. If you are using multi-channel images, you can specify which channel to segment and optionally which to use as \"nuclei\" channel to help cell segmentation. In case you are using one of the built-in models, you can set the estimated diameter of your objects. In the Options tab you will find a few more options for segmentation, including the two thresholds flow_threshold and cellprob_threshold. You can also decide to discard objects touching the border. Using the Select options yml file you can select a .yml file which contains a list of additional options to pass to the eval method of the Cellpose model. Note that options specified in the yml file will override options set in the GUI. The file my_options.yml is an example of such a file where for example the diameter (also available in the GUI) and resample (not available in the GUI) options are set.     Properties  After segmentation, properties of the objects can automatically be computed. You can select which properties should be computed in the Options tab. As defined in napari-skimage-regionprops properties are grouped by types. If you want to measure intensity properties such as mean intensity, you have to specify which channel (Analysis channel) you want to perform the measurement on.  Output The results of the analysis are saved in the folder chosen in #2. The segmentation mask is saved with the same name as the original image with the suffix _mask.tif. A table with properties is saved in the subfolder tables also with the same name as the image with the suffix props.csv. If you run the plugin on multiple files in a folder, a summary.csv file is also generated which compiles all the data. Usage: post-processing After the analysis is done, when you select an image, the corresponding segmentation mask is shown on top of the image as shown below. This also works for saved segmentations: in that case you just select a folder with data and the corresponding output folder.  Properties If you head to the Properties tab, you will find there two histograms showing the distribution of two properties that you can choose from a list at the top of the window. Below the plot you find the table containing information for each cell (each line is a cell). As shown below, if you select the box show selected, you can select items in the properties table and it will highlight the corresponding cell in the viewer. If you select the pipet tool, you can also select a cell and see the corresponding line in the table highlighted.  Summary Finally if you select the Summary tab, and click on Load summary, it will load all data of the current output folder and create histograms of two properties that can be selected. An additional property can be used for filtering the data. Using the sliders, one can set a minimum and maximum threshold on the \"filtering property\", which will create a sub-selection of the data.  Data Sample data were acquired by Fabian Blank at the DBMR, University of Bern. License Distributed under the terms of the BSD-3 license, \"napari-serialcellpose\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "serialcellpose",
    "documentation": "https://github.com/guiwitz/napari-serialcellpose#README.md",
    "first_released": "2022-07-28T14:16:35.456233Z",
    "license": "BSD-3-Clause",
    "name": "napari-serialcellpose",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/guiwitz/napari-serialcellpose",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-08-08T20:39:23.176539Z",
    "report_issues": "https://github.com/guiwitz/napari-serialcellpose/issues",
    "requirements": [
      "cellpose",
      "numpy",
      "magicgui",
      "qtpy",
      "matplotlib",
      "napari-skimage-regionprops",
      "napari-aicsimageio",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "summary": "A simple plugin to batch segment cells with cellpose",
    "support": "https://github.com/guiwitz/napari-serialcellpose/issues",
    "twitter": "",
    "version": "0.2.2",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "email": "tasnadi.ervin@brc.hu", "name": "Ervin Tasnadi" }],
    "category": { "Image modality": ["Bright-field microscopy"] },
    "category_hierarchy": { "Image modality": [["Bright-field microscopy"]] },
    "code_repository": "https://github.com/etasnadi/napari_nucleaizer",
    "conda": [],
    "description": "# napari_nucleaizer  [![License](https://img.shields.io/pypi/l/napari-nucleaizer.svg?color=green)](https://github.com/etasnadi/napari-nucleaizer/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-nucleaizer.svg?color=green)](https://pypi.org/project/napari-nucleaizer) [![Python package](https://github.com/etasnadi/napari_nucleaizer/actions/workflows/test_and_deploy.yml/badge.svg)](https://github.com/etasnadi/napari_nucleaizer/actions/workflows/test_and_deploy.yml) [![codecov](https://codecov.io/gh/etasnadi/napari_nucleaizer/branch/master/graph/badge.svg?token=5XC36PA6OQ)](https://codecov.io/gh/etasnadi/napari_nucleaizer) [![Documentation Status](https://readthedocs.org/projects/napari-nucleaizer-docs/badge/?version=latest)](https://napari-nucleaizer-docs.readthedocs.io/en/latest/?badge=latest)  <!-- [![Python Version](https://img.shields.io/pypi/pyversions/napari-nucleaizer.svg?color=green)](https://python.org) [![tests](https://github.com/etasnadi/napari_nucleaizer/workflows/tests/badge.svg)](https://github.com/etasnadi/napari-nucleaizer/actions) [![codecov](https://codecov.io/gh/etasnadi/napari-nucleaizer/branch/master/graph/badge.svg)](https://codecov.io/gh/etasnadi/napari-nucleaizer) -->  GUI for the nucleaAIzer method in Napari.  ![Plugin interface in napari.](https://github.com/etasnadi/napari_nucleaizer/blob/main/napari_screenshot.png?raw=true)  ## Overview  This is a napari plugin to execute the nucleaizer nuclei segmentation algorithm.  ### Main functionalities  Using this plugin will be able to  1. Load your image into Napar, then outline the nuclei. 2. Specify an image folder containing lots of images and an output folder, and automatically segment all of the images in the input folder. 3. If you are not satisfied with the results, you can train your own model:     1. You can use our pretrained models and fine tune them on your data.     2. You can skip the nucleaizer pipeline and train only on your data.   ### Supported image types  We have several pretrained models for the following image modelities: * fluorescent microscopy images * IHC stained images * brightfield microscopy images,  among others. For the detailed descriptions of our models, see: https://zenodo.org/record/6800341.  ### How it works?  For the description of the algorithm, see our paper: \"Hollandi et al.: nucleAIzer: A Parameter-free Deep Learning Framework for Nucleus Segmentation Using Image Style Transfer, Cell Systems, 2020. https://doi.org/10.1016/j.cels.2020.04.003\"  The original code (https://github.com/spreka/biomagdsb) is partially transformed into a python package (nucleaizer_backend) to actually perform the operations. See the project page of the backend at: https://github.com/etasnadi/nucleaizer_backend.  If you wish to use the web interface, check: http://nucleaizer.org.  ![All functionalities.](https://github.com/etasnadi/napari_nucleaizer/blob/main/nucleaizer_screenshot.png?raw=true)  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/docs/plugins/index.html -->  ## Install  1. Create an environment (recommended).  2. Install napari: `pip install \"napari[pyqt5]\"`. Other methods: https://napari.org/tutorials/fundamentals/installation.html  3. Install the plugin into napari:      * User mode from [PyPI](https://pypi.org/project/napari-nucleaizer/): start Napari (command line: `napari`) and select he **Install/Uninstall Plugins...** under the **Plugins** menu. In the popup, filter for `napari-nucleaizer`.      * Developer mode: clone this project and use `pythhon3 -m pip install -e <path>` to install the project locally **into the same evnrionment as napari**. It has the advantage that you will have the latest version. ## Run  1. Start Napari by calling `napari` from the command line. 2. Then, activate the plugin in the `Plugins` menu. If you successfully installed the plugin, you have to see something like this:  ![Plugin interface in napari.](https://github.com/etasnadi/napari_nucleaizer/blob/main/napari_plugin_launch.png?raw=true)  ## Further help  See the [documentation](https://napari-nucleaizer-docs.readthedocs.io/en/latest/index.html) (work in progress).  ## Issues  Use the github issue tracker if you experinece unexpected behaviour.  ## Contact  You can contact me in [e-mail](mailto:tasnadi.ervin@MY-INSTITUTE) where MY-INSTITUTE is `brc.hu`. ",
    "description_content_type": "text/markdown",
    "description_text": "napari_nucleaizer       GUI for the nucleaAIzer method in Napari.  Overview This is a napari plugin to execute the nucleaizer nuclei segmentation algorithm. Main functionalities Using this plugin will be able to  Load your image into Napar, then outline the nuclei. Specify an image folder containing lots of images and an output folder, and automatically segment all of the images in the input folder. If you are not satisfied with the results, you can train your own model: You can use our pretrained models and fine tune them on your data. You can skip the nucleaizer pipeline and train only on your data.    Supported image types We have several pretrained models for the following image modelities: * fluorescent microscopy images * IHC stained images * brightfield microscopy images, among others. For the detailed descriptions of our models, see: https://zenodo.org/record/6800341. How it works? For the description of the algorithm, see our paper: \"Hollandi et al.: nucleAIzer: A Parameter-free Deep Learning Framework for Nucleus Segmentation Using Image Style Transfer, Cell Systems, 2020. https://doi.org/10.1016/j.cels.2020.04.003\" The original code (https://github.com/spreka/biomagdsb) is partially transformed into a python package (nucleaizer_backend) to actually perform the operations. See the project page of the backend at: https://github.com/etasnadi/nucleaizer_backend. If you wish to use the web interface, check: http://nucleaizer.org.   Install   Create an environment (recommended).   Install napari: pip install \"napari[pyqt5]\". Other methods: https://napari.org/tutorials/fundamentals/installation.html   Install the plugin into napari:   User mode from PyPI: start Napari (command line: napari) and select he Install/Uninstall Plugins... under the Plugins menu. In the popup, filter for napari-nucleaizer.   Developer mode: clone this project and use pythhon3 -m pip install -e <path> to install the project locally into the same evnrionment as napari. It has the advantage that you will have the latest version.   Run   Start Napari by calling napari from the command line.  Then, activate the plugin in the Plugins menu. If you successfully installed the plugin, you have to see something like this:   Further help See the documentation (work in progress). Issues Use the github issue tracker if you experinece unexpected behaviour. Contact You can contact me in e-mail where MY-INSTITUTE is brc.hu.",
    "development_status": ["Development Status :: 3 - Alpha"],
    "display_name": "Napari nucleAIzer plugin",
    "documentation": "https://napari-nucleaizer-docs.readthedocs.io/en/latest/index.html",
    "first_released": "2021-09-08T17:45:54.165468Z",
    "license": "BSD-3-Clause",
    "name": "napari-nucleaizer",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/etasnadi/napari_nucleaizer",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-07-06T00:33:13.614307Z",
    "report_issues": "https://github.com/etasnadi/napari_nucleaizer/issues",
    "requirements": [
      "napari",
      "qtpy",
      "jsonpickle",
      "numpy",
      "scikit-image",
      "imageio",
      "nucleaizer-backend"
    ],
    "summary": "A GUI interface for training and prediction using the nucleAIzer nuclei detection method.",
    "support": "https://github.com/etasnadi/napari_nucleaizer/issues",
    "twitter": "",
    "version": "0.2.4",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "email": "susmi06@yahoo.com", "name": "SUSMITA SAHA" }],
    "code_repository": null,
    "description": "# napari-mri  [![License](https://img.shields.io/pypi/l/napari-mri.svg?color=green)](https://github.com/sahas111/napari-mri/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-mri.svg?color=green)](https://pypi.org/project/napari-mri) [![Python Version](https://img.shields.io/pypi/pyversions/napari-mri.svg?color=green)](https://python.org) [![tests](https://github.com/sahas111/napari-mri/workflows/tests/badge.svg)](https://github.com/sahas111/napari-mri/actions) [![codecov](https://codecov.io/gh/sahas111/napari-mri/branch/master/graph/badge.svg)](https://codecov.io/gh/sahas111/napari-mri)  A simple plugin to use with napari for 3D-viewing of Magnetic Resonance Imaging file formats  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/docs/plugins/index.html -->  ## Installation  You can install `napari-mri` via [pip]:      pip install napari-mri  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-mri\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin [file an issue]: https://github.com/sahas111/napari-mri/issues [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-mri      A simple plugin to use with napari for 3D-viewing of Magnetic Resonance Imaging file formats  This napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.  Installation You can install napari-mri via pip: pip install napari-mri  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-mri\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "",
    "documentation": "",
    "first_released": "2021-03-21T06:12:30.232144Z",
    "license": "",
    "name": "napari-mri",
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": [],
    "project_site": "",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2021-03-21T06:12:30.232144Z",
    "report_issues": "",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "nibabel",
      "numpy",
      "pytest-runner (>=5.2) ; extra == 'setup'",
      "setuptools-scm ; extra == 'setup'",
      "codecov (>=2.1.4) ; extra == 'test'",
      "flake8 (>=3.8.3) ; extra == 'test'",
      "flake8-debugger (>=3.2.1) ; extra == 'test'",
      "pytest (>=5.4.3) ; extra == 'test'",
      "pytest-cov (>=2.9.0) ; extra == 'test'",
      "pytest-raises (>=0.11) ; extra == 'test'"
    ],
    "summary": "A simple plugin to use with napari for 3D-viewing of                  Magnetic Resonance Imaging file formats",
    "support": "",
    "twitter": "",
    "version": "0.1.0",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Ruben Lopez" }],
    "code_repository": "https://github.com/rjlopez2/napari-sif-reader",
    "conda": [],
    "description": "# napari-sif-reader  [![License BSD-3](https://img.shields.io/pypi/l/napari-sif-reader.svg?color=green)](https://github.com/rjlopez2/napari-sif-reader/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-sif-reader.svg?color=green)](https://pypi.org/project/napari-sif-reader) [![Python Version](https://img.shields.io/pypi/pyversions/napari-sif-reader.svg?color=green)](https://python.org) [![tests](https://github.com/rjlopez2/napari-sif-reader/workflows/tests/badge.svg)](https://github.com/rjlopez2/napari-sif-reader/actions) [![codecov](https://codecov.io/gh/rjlopez2/napari-sif-reader/branch/main/graph/badge.svg)](https://codecov.io/gh/rjlopez2/napari-sif-reader) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-sif-reader)](https://napari-hub.org/plugins/napari-sif-reader)  This is a simple wraper to read .sif format files from Andor Technology.  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/stable/plugins/index.html -->  ## Installation  You can install `napari-sif-reader` via [pip]:      pip install napari-sif-reader    To install latest development version :      pip install git+https://github.com/rjlopez2/napari-sif-reader.git   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-sif-reader\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/rjlopez2/napari-sif-reader/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-sif-reader       This is a simple wraper to read .sif format files from Andor Technology.  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install napari-sif-reader via pip: pip install napari-sif-reader  To install latest development version : pip install git+https://github.com/rjlopez2/napari-sif-reader.git  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-sif-reader\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari sif file reader",
    "documentation": "https://github.com/rjlopez2/napari-sif-reader#README.md",
    "first_released": "2022-11-03T08:11:24.248334Z",
    "license": "BSD-3-Clause",
    "name": "napari-sif-reader",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader", "sample_data"],
    "project_site": "https://github.com/rjlopez2/napari-sif-reader",
    "python_version": ">=3.8",
    "reader_file_extensions": ["*.sif"],
    "release_date": "2022-11-03T08:11:24.248334Z",
    "report_issues": "https://github.com/rjlopez2/napari-sif-reader/issues",
    "requirements": [
      "numpy",
      "sif-parser",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'"
    ],
    "summary": "This is a simple wraper to read .sif format files from Andor Technology.",
    "support": "https://github.com/rjlopez2/napari-sif-reader/issues",
    "twitter": "",
    "version": "0.0.1",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Dr. Thorsten Beier" }],
    "code_repository": "https://github.com/DerThorsten/napari-splinedist",
    "conda": [],
    "description": "# napari-splinedist  [![License MIT](https://img.shields.io/pypi/l/napari-splinedist.svg?color=green)](https://github.com/DerThorsten/napari-splinedist/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-splinedist.svg?color=green)](https://pypi.org/project/napari-splinedist) [![Python Version](https://img.shields.io/pypi/pyversions/napari-splinedist.svg?color=green)](https://python.org) [![tests](https://github.com/DerThorsten/napari-splinedist/workflows/tests/badge.svg)](https://github.com/DerThorsten/napari-splinedist/actions) [![codecov](https://codecov.io/gh/DerThorsten/napari-splinedist/branch/main/graph/badge.svg)](https://codecov.io/gh/DerThorsten/napari-splinedist) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-splinedist)](https://napari-hub.org/plugins/napari-splinedist)  A napari SplineDist plugin  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/stable/plugins/index.html -->  ## Installation  You can install `napari-splinedist` via [pip]:      pip install napari-splinedist    To install latest development version :      pip install git+https://github.com/DerThorsten/napari-splinedist.git   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [MIT] license, \"napari-splinedist\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/DerThorsten/napari-splinedist/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-splinedist       A napari SplineDist plugin  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install napari-splinedist via pip: pip install napari-splinedist  To install latest development version : pip install git+https://github.com/DerThorsten/napari-splinedist.git  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the MIT license, \"napari-splinedist\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "SplineDist",
    "documentation": "https://github.com/DerThorsten/napari-splinedist#README.md",
    "first_released": "2022-10-18T14:03:01.283427Z",
    "license": "MIT",
    "name": "napari-splinedist",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget", "sample_data"],
    "project_site": "https://github.com/DerThorsten/napari-splinedist",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-10-31T07:43:11.399242Z",
    "report_issues": "https://github.com/DerThorsten/napari-splinedist/issues",
    "requirements": [
      "pydantic",
      "numpy",
      "magicgui",
      "qtpy",
      "stardist (>=0.8.3)",
      "splinedist (>=0.1.2)",
      "napari-splineit (>=0.3.0)",
      "requests",
      "tensorflow",
      "opencv-python-headless",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "summary": "A napari SplineDist plugin",
    "support": "https://github.com/DerThorsten/napari-splinedist/issues",
    "twitter": "",
    "version": "0.3.1",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Christopher Nauroth-Kress" }],
    "code_repository": "https://github.com/ch-n/napari-time_series_plotter",
    "description": "# napari-time_series_plotter  [![License](https://img.shields.io/pypi/l/napari-time_series_plotter.svg?color=green)](https://github.com/ch-n/napari-time_series_plotter/raw/main/LICENSE) [![Python Version](https://img.shields.io/pypi/pyversions/napari-time_series_plotter.svg?color=green)](https://python.org) [![PyPI](https://img.shields.io/pypi/v/napari-time_series_plotter.svg?color=green)](https://pypi.org/project/napari-time_series_plotter) [![Anaconda-Server Badge](https://anaconda.org/conda-forge/napari-time-series-plotter/badges/version.svg)](https://anaconda.org/conda-forge/napari-time-series-plotter) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-time-series-plotter)](https://napari-hub.org/plugins/napari-time-series-plotter) [![tests](https://github.com/ch-n/napari-time_series_plotter/workflows/tests/badge.svg)](https://github.com/ch-n/napari-time_series_plotter/actions) [![codecov](https://codecov.io/gh/ch-n/napari-time_series_plotter/branch/main/graph/badge.svg)](https://codecov.io/gh/ch-n/napari-time_series_plotter)   ## Description Napari-time_series_plotter (TSP) is a plugin for the `napari` ndimensional image viewer.   TSP adds live plotting of time-resolved images to napari. With the TSPExplorer widget, you can select and visualize pixel/voxel or ROI mean values from one or multiple image layers as intensity-over-time line plots. The first image dimension is handled as time. TSP supports 3D to nD images (3D: t+2D, nD: t+nD).  The TSPExplorer offers three different plotting modes: Voxel, Shapes, Points --> Voxel mode offers live plotting while moving the cursor over an image layer --> Shapes mode offers shape-based ROI plotting the ROI combination method can be one of [Mean, Median, STD, Sum, Min, Max]; multiple ROIs can be plotted simultaneously --> Points mode offers simultaneous, point-based plotting of multiple voxels  You can modify and save the plots through the canvas toolbar. Plotting powered by `napari-matplotlib`.  ----------------------------------  ## Installation You can either install the latest version via pip or conda.  **pip:**      pip install napari-time-series-plotter  or download the packaged `tar.gz` file from the release assets and install it with           pip install /path/to/file.tar.gz  **conda:**      conda install -c conda-forge napari-time-series-plotter   Alternatively, you can install the plugin directly in the `napari` viewer plugin manager, the napari hub, or the release assets.  <br>  To install the latest development version install directly from the relevant GitHub branch.  ## Usage <p align=\"center\">   <img src=\"https://github.com/ch-n/napari-time_series_plotter/raw/main/napari-time-series-plotter_demo.gif\" alt=\"Demo gif\" /> </p>      - Select the TSPExplorer widget in the `Plugins` tab of the napari viewer - Use the LayerSelector to choose the image layers you want to source for plotting - Select the plotting mode via the options tab (Voxel mode is the default)  Voxel mode: - Move the mouse over the image while holding \"Shift\" - The plotter will display the hovered voxel intensity over time for all selected layers  Shapes mode: - Add one or more shapes to the ROI selection layer - Position it as you need - The plotter will display the combined intensity of the ROI over time for all selected layers     - The shapes are 2D only; 3D ROIs are not supported     - All shapes are on the currently displayed slice     - The ROI combination mode can be selected in the options tab, default: mean  Points mode: - Add one or more points to the Point selection layer - The plotter will display a time series plot for each point on all selected layers     - The points can be on different slices (3D and 4D support only) or images (grid mode)     - Adding or moving points will regenerate the plots  - Set custom title or axe labels in the options tab - Switch between autoscaling and manually defined max and min values of the axes in the options tab - Switch to label truncation in the options tab if your layer names are too long for the figure legend (set max length manually) - Set a scaling factor for the X-axis in the options tab  ## ToDo (help welcome) - [ ] Add Sphinx documentation  ## Version 0.1.0 Milestones - [X] Update to napari-plugin-engine2 [#5](https://github.com/ch-n/napari-time_series_plotter/issues/5) - [X] Update widget GUI [#6](https://github.com/ch-n/napari-time_series_plotter/issues/6) - [ ] Add widget to save pixel/voxel time series to file [#7](https://github.com/ch-n/napari-time_series_plotter/issues/7) - [X] Add ROI and multi-voxel plotting [#14](https://github.com/ch-n/napari-time_series_plotter/issues/14)  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-time_series_plotter\" is free and open-source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  --------------  ## References This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  Images used in the demo gif were taken from [The Cancer Imaging Archive] <br>      DOI: https://doi.org/10.7937/K9/TCIA.2015.VOSN3HN1     Images: 1.3.6.1.4.1.9328.50.16.281868838636204210586871132130856898223             1.3.6.1.4.1.9328.50.16.254461916058189583774506642993503110733  [The Cancer Imaging Archive]: https://www.cancerimagingarchive.net/ [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/ch-n/napari-time_series_plotter/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-time_series_plotter        Description Napari-time_series_plotter (TSP) is a plugin for the napari ndimensional image viewer.  TSP adds live plotting of time-resolved images to napari. With the TSPExplorer widget, you can select and visualize pixel/voxel or ROI mean values from one or multiple image layers as intensity-over-time line plots. The first image dimension is handled as time. TSP supports 3D to nD images (3D: t+2D, nD: t+nD). The TSPExplorer offers three different plotting modes: Voxel, Shapes, Points --> Voxel mode offers live plotting while moving the cursor over an image layer --> Shapes mode offers shape-based ROI plotting the ROI combination method can be one of [Mean, Median, STD, Sum, Min, Max]; multiple ROIs can be plotted simultaneously --> Points mode offers simultaneous, point-based plotting of multiple voxels You can modify and save the plots through the canvas toolbar. Plotting powered by napari-matplotlib.  Installation You can either install the latest version via pip or conda. pip: pip install napari-time-series-plotter  or download the packaged tar.gz file from the release assets and install it with  pip install /path/to/file.tar.gz  conda: conda install -c conda-forge napari-time-series-plotter  Alternatively, you can install the plugin directly in the napari viewer plugin manager, the napari hub, or the release assets.  To install the latest development version install directly from the relevant GitHub branch. Usage     Select the TSPExplorer widget in the Plugins tab of the napari viewer Use the LayerSelector to choose the image layers you want to source for plotting Select the plotting mode via the options tab (Voxel mode is the default)  Voxel mode: - Move the mouse over the image while holding \"Shift\" - The plotter will display the hovered voxel intensity over time for all selected layers Shapes mode: - Add one or more shapes to the ROI selection layer - Position it as you need - The plotter will display the combined intensity of the ROI over time for all selected layers     - The shapes are 2D only; 3D ROIs are not supported     - All shapes are on the currently displayed slice     - The ROI combination mode can be selected in the options tab, default: mean Points mode: - Add one or more points to the Point selection layer - The plotter will display a time series plot for each point on all selected layers     - The points can be on different slices (3D and 4D support only) or images (grid mode)     - Adding or moving points will regenerate the plots  Set custom title or axe labels in the options tab Switch between autoscaling and manually defined max and min values of the axes in the options tab Switch to label truncation in the options tab if your layer names are too long for the figure legend (set max length manually) Set a scaling factor for the X-axis in the options tab  ToDo (help welcome)  [ ] Add Sphinx documentation  Version 0.1.0 Milestones  [X] Update to napari-plugin-engine2 #5 [X] Update widget GUI #6 [ ] Add widget to save pixel/voxel time series to file #7 [X] Add ROI and multi-voxel plotting #14  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-time_series_plotter\" is free and open-source software Issues If you encounter any problems, please file an issue along with a detailed description.  References This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template. Images used in the demo gif were taken from The Cancer Imaging Archive  DOI: https://doi.org/10.7937/K9/TCIA.2015.VOSN3HN1 Images: 1.3.6.1.4.1.9328.50.16.281868838636204210586871132130856898223         1.3.6.1.4.1.9328.50.16.254461916058189583774506642993503110733 ",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari-time-series-plotter",
    "documentation": "https://github.com/ch-n/napari-time_series_plotter#README.md",
    "first_released": "2021-12-01T11:59:13.160554Z",
    "license": "BSD-3-Clause",
    "name": "napari-time-series-plotter",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/ch-n/napari-time_series_plotter",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-11-16T11:56:49.975469Z",
    "report_issues": "https://github.com/ch-n/napari-time_series_plotter/issues",
    "requirements": [
      "napari-plugin-engine (>=0.2.0)",
      "napari-matplotlib",
      "numpy",
      "qtpy",
      "napari ; extra == 'test'",
      "pytest ; extra == 'test'",
      "pytest-qt ; extra == 'test'",
      "pytest-cov ; extra == 'test'"
    ],
    "summary": "A Plugin for napari to visualize pixel values over the first dimension (time -> t+3D, t+2D) as graphs.",
    "support": "https://github.com/ch-n/napari-time_series_plotter/issues",
    "twitter": "",
    "version": "0.0.5",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      {
        "email": "rodriguezsantiago96@gmail.com",
        "name": "Santiago N. Rodriguez Alvarez"
      }
    ],
    "code_repository": "https://github.com/santi-rodriguez/nfinder",
    "description": "# Nfinder Automatic inference of neighboring cells based on their Delaunay triangulation.  ## Dependencies  nfinder was tested with:  - python = 3.8.5 - napari = 0.4.12 - numpy = 1.21.2 - pandas = 1.3.4 - scikit-image = 0.18.3 - scipy = 1.7.1 - importlib-resources 5.4.0   ## Installation  It can be installed with `pip` from PyPI:  ``` pip install nfinder ```   ## Usage For usage examples, please check out the [notebook](https://github.com/santi-rodriguez/nfinder/blob/main/examples.ipynb) in our GitHub repository.",
    "description_content_type": "text/markdown",
    "description_text": "Nfinder Automatic inference of neighboring cells based on their Delaunay triangulation. Dependencies nfinder was tested with:  python = 3.8.5 napari = 0.4.12 numpy = 1.21.2 pandas = 1.3.4 scikit-image = 0.18.3 scipy = 1.7.1 importlib-resources 5.4.0  Installation It can be installed with pip from PyPI: pip install nfinder Usage For usage examples, please check out the notebook in our GitHub repository.",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "nfinder",
    "documentation": "",
    "first_released": "2021-12-06T20:31:59.553762Z",
    "license": "",
    "name": "nfinder",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/santi-rodriguez/nfinder",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-02-05T19:14:23.400655Z",
    "report_issues": "",
    "requirements": null,
    "summary": "Automatic inference of neighboring cells based on their Delaunay triangulation.",
    "support": "",
    "twitter": "",
    "version": "0.3",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "email": "alisterburt@gmail.com", "name": "Alister Burt" }],
    "code_repository": "https://github.com/alisterburt/napari-tomoslice",
    "description": "# napari-tomoslice  [![License](https://img.shields.io/pypi/l/napari-tomoslice.svg?color=green)](https://github.com/alisterburt/napari-tomoslice/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-tomoslice.svg?color=green)](https://pypi.org/project/napari-tomoslice) [![Python Version](https://img.shields.io/pypi/pyversions/napari-tomoslice.svg?color=green)](https://python.org) [![tests](https://github.com/alisterburt/napari-tomoslice/workflows/tests/badge.svg)](https://github.com/alisterburt/napari-tomoslice/actions) [![codecov](https://codecov.io/gh/alisterburt/napari-tomoslice/branch/master/graph/badge.svg)](https://codecov.io/gh/alisterburt/napari-tomoslice)  A napari plugin for visualising and interacting with electron cryotomograms.   ## Installation  You can install `napari-tomoslice` via [pip]:      pip install napari-tomoslice  ## Usage  This plugin provides a user interface for opening electron cryotomograms in  napari as both volumes and slices through volumes.  ![demo](https://user-images.githubusercontent.com/7307488/138575305-b05c4735-9c03-4629-bfb0-9612ea8f26fd.gif)  The plugin can be opened from the `plugins` menu in napari, or with  `napari-tomoslice` at the command line.  ![plugins-menu](https://user-images.githubusercontent.com/7307488/138575015-00ea78d9-02c1-44bc-9034-0c0a7fa8d973.png)  ```yaml Usage: napari-tomoslice [TOMOGRAM_FILE]    An interactive tomogram slice viewer in napari.    Controls:    x/y/z - align normal vector along x/y/z axis    click and drag - shift plane along its normal vector   alt-click - add point on plane (if points layer is active)   o - align plane normal to view direction   [] - decrease/increase plane thickness  Arguments:   [TOMOGRAM_FILE]  Options:   --help                          Show this message and exit.  ```  ## Contributing  Contributions are very welcome.   ## License  Distributed under the terms of the [BSD-3] license, \"napari-tomoslice\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/alisterburt/napari-tomoslice/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-tomoslice      A napari plugin for visualising and interacting with electron cryotomograms. Installation You can install napari-tomoslice via pip: pip install napari-tomoslice  Usage This plugin provides a user interface for opening electron cryotomograms in  napari as both volumes and slices through volumes.  The plugin can be opened from the plugins menu in napari, or with  napari-tomoslice at the command line.  ```yaml Usage: napari-tomoslice [TOMOGRAM_FILE] An interactive tomogram slice viewer in napari. Controls:    x/y/z - align normal vector along x/y/z axis    click and drag - shift plane along its normal vector   alt-click - add point on plane (if points layer is active)   o - align plane normal to view direction   [] - decrease/increase plane thickness Arguments:   [TOMOGRAM_FILE] Options:   --help                          Show this message and exit. ``` Contributing Contributions are very welcome.  License Distributed under the terms of the BSD-3 license, \"napari-tomoslice\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari-tomoslice",
    "documentation": "https://github.com/alisterburt/napari-tomoslice#README.md",
    "first_released": "2021-10-24T00:48:28.316518Z",
    "license": "BSD-3-Clause",
    "name": "napari-tomoslice",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/alisterburt/napari-tomoslice",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-02-01T22:21:41.010473Z",
    "report_issues": "https://github.com/alisterburt/napari-tomoslice/issues",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "napari (==0.4.12)",
      "mrcfile",
      "typer"
    ],
    "summary": "A napari plugin for interacting with electron cryotomograms",
    "support": "https://github.com/alisterburt/napari-tomoslice/issues",
    "twitter": "",
    "version": "0.0.7",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      {
        "email": "sebastian.gonzalez@embl.de",
        "name": "Sebastian Gonzalez-Tirado"
      }
    ],
    "code_repository": "https://github.com/sebgoti/napari-spacetx-explorer",
    "description": "# napari-spacetx-explorer  [![License](https://img.shields.io/pypi/l/napari-spacetx-explorer.svg?color=green)](https://github.com/sebgoti/napari-spacetx-explorer/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-spacetx-explorer.svg?color=green)](https://pypi.org/project/napari-spacetx-explorer) [![Python Version](https://img.shields.io/pypi/pyversions/napari-spacetx-explorer.svg?color=green)](https://python.org) [![tests](https://github.com/sebgoti/napari-spacetx-explorer/workflows/tests/badge.svg)](https://github.com/sebgoti/napari-spacetx-explorer/actions) [![codecov](https://codecov.io/gh/sebgoti/napari-spacetx-explorer/branch/master/graph/badge.svg)](https://codecov.io/gh/sebgoti/napari-spacetx-explorer)  A napari plugin for interactive visualization of decoded spots from spatial transcriptomic data stored as CSV  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.  The plugin code was written by Sebastian Gonzalez-Tirado.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/docs/plugins/index.html --> ## Reader hookspec  `napari-spacetx-explorer` allows the user to open and visualize CSV files that have point-data stored in a given format. The main target is for users who want to analyze decoded spot maps from spatial omics experiments but it can used as well for any other type of coordinate data where each point has assigned a label (e. g. a gene) as a string and the x and y-coordinates of the point's center. The header for these data must be 'target', 'xc', and 'yc', respectively.  ![img.png](https://github.com/sebgoti/napari-spacetx-explorer/raw/main/docs/Read_Hookspec.png)  ## Selecting genes  After loading the gene/target maps it is possible to select specific groups for better visualization. This creates a new \"Points\" layer in napari with the selected groups displayed in different colors.  ![img.png](https://github.com/sebgoti/napari-spacetx-explorer/raw/main/docs/_function_hookspec.png)  ## Loading data in OME.ZARR format  The plugin napari-ome-zarr can be used to display whole-tissue images in addition to the spot maps produced with the  `napari-spacetx-explorer` plugin.  ![img.png](https://github.com/sebgoti/napari-spacetx-explorer/raw/main/docs/_ome_zarr_napari_spacetx_explorer.png)  ## Installation  The easiest installation is via the \"Install/Uninstall Plugins...\" under the Plugins menu in napari.   Another way is through [pip]       pip install napari-spacetx-explorer  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-spacetx-explorer\" is free and open source software  ## Issues  If you encounter any problems or would like some support, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin [file an issue]: https://github.com/sebgoti/napari-spacetx-explorer/issues [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-spacetx-explorer      A napari plugin for interactive visualization of decoded spots from spatial transcriptomic data stored as CSV  This napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template. The plugin code was written by Sebastian Gonzalez-Tirado.  Reader hookspec napari-spacetx-explorer allows the user to open and visualize CSV files that have point-data stored in a given format. The main target is for users who want to analyze decoded spot maps from spatial omics experiments but it can used as well for any other type of coordinate data where each point has assigned a label (e. g. a gene) as a string and the x and y-coordinates of the point's center. The header for these data must be 'target', 'xc', and 'yc', respectively.  Selecting genes After loading the gene/target maps it is possible to select specific groups for better visualization. This creates a new \"Points\" layer in napari with the selected groups displayed in different colors.  Loading data in OME.ZARR format The plugin napari-ome-zarr can be used to display whole-tissue images in addition to the spot maps produced with the  napari-spacetx-explorer plugin.  Installation The easiest installation is via the \"Install/Uninstall Plugins...\" under the Plugins menu in napari. Another way is through pip  pip install napari-spacetx-explorer  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-spacetx-explorer\" is free and open source software Issues If you encounter any problems or would like some support, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 3 - Alpha"],
    "display_name": "napari-spacetx-explorer",
    "documentation": "https://github.com/sebgoti/napari-spacetx-explorer#README.md",
    "first_released": "2021-09-28T13:19:17.894005Z",
    "license": "BSD-3-Clause",
    "name": "napari-spacetx-explorer",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader", "widget"],
    "project_site": "https://github.com/sebgoti/napari-spacetx-explorer",
    "python_version": ">=3.7",
    "reader_file_extensions": ["*"],
    "release_date": "2021-12-10T10:54:48.965377Z",
    "report_issues": "https://github.com/sebgoti/napari-spacetx-explorer/issues",
    "requirements": [
      "napari",
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "pandas"
    ],
    "summary": "visualizer for spatial omic data",
    "support": "https://github.com/sebgoti/napari-spacetx-explorer/issues",
    "twitter": "",
    "version": "0.1.8",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Ruben Lopez" }],
    "code_repository": "https://github.com/rjlopez2/napari-omaas",
    "conda": [],
    "description": "# napari-omaas  [![License BSD-3](https://img.shields.io/pypi/l/napari-omaas.svg?color=green)](https://github.com/rjlopez2/napari-omaas/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-omaas.svg?color=green)](https://pypi.org/project/napari-omaas) [![Python Version](https://img.shields.io/pypi/pyversions/napari-omaas.svg?color=green)](https://python.org) [![tests](https://github.com/rjlopez2/napari-omaas/workflows/tests/badge.svg)](https://github.com/rjlopez2/napari-omaas/actions) [![codecov](https://codecov.io/gh/rjlopez2/napari-omaas/branch/main/graph/badge.svg)](https://codecov.io/gh/rjlopez2/napari-omaas) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-omaas)](https://napari-hub.org/plugins/napari-omaas)  napari-OMAAS stands for Optical Mapping Acquisition and Analysis Software  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/stable/plugins/index.html -->  ## Installation  You can install `napari-omaas` via [pip]:      pip install napari-omaas    To install latest development version :      pip install git+https://github.com/rjlopez2/napari-omaas.git   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-omaas\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/rjlopez2/napari-omaas/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-omaas       napari-OMAAS stands for Optical Mapping Acquisition and Analysis Software  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install napari-omaas via pip: pip install napari-omaas  To install latest development version : pip install git+https://github.com/rjlopez2/napari-omaas.git  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-omaas\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari OMAAS",
    "documentation": "https://github.com/rjlopez2/napari-omaas#README.md",
    "first_released": "2022-08-09T21:49:57.410742Z",
    "license": "BSD-3-Clause",
    "name": "napari-omaas",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader", "writer", "widget", "sample_data"],
    "project_site": "https://github.com/rjlopez2/napari-omaas",
    "python_version": ">=3.8",
    "reader_file_extensions": ["*.sif"],
    "release_date": "2022-08-09T21:49:57.410742Z",
    "report_issues": "https://github.com/rjlopez2/napari-omaas/issues",
    "requirements": [
      "numpy",
      "magicgui",
      "qtpy",
      "sif-parser",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "summary": "napari-OMAAS stands for Optical Mapping Acquisition and Analysis Software",
    "support": "https://github.com/rjlopez2/napari-omaas/issues",
    "twitter": "",
    "version": "0.1.0",
    "visibility": "public",
    "writer_file_extensions": [".npy"],
    "writer_save_layers": ["image*", "labels*", "image"]
  },
  {
    "authors": [
      { "name": "Juan Nunez-Iglesias" },
      { "name": "Abigail McGovern" }
    ],
    "category": { "Workflow step": ["Image Segmentation"] },
    "category_hierarchy": {
      "Workflow step": [
        ["Image Segmentation", "Cell segmentation"],
        ["Image Segmentation", "Region growing", "Watershed segmentation"]
      ]
    },
    "code_repository": "https://github.com/jni/platelet-unet-watershed",
    "conda": [
      { "channel": "conda-forge", "package": "platelet-unet-watershed" }
    ],
    "description": "# platelet-unet-watershed  [![License](https://img.shields.io/pypi/l/platelet-unet-watershed.svg?color=green)](https://github.com/jni/platelet-unet-watershed/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/platelet-unet-watershed.svg?color=green)](https://pypi.org/project/platelet-unet-watershed) [![Python Version](https://img.shields.io/pypi/pyversions/platelet-unet-watershed.svg?color=green)](https://python.org) [![tests](https://github.com/jni/platelet-unet-watershed/workflows/tests/badge.svg)](https://github.com/jni/platelet-unet-watershed/actions) [![codecov](https://codecov.io/gh/jni/platelet-unet-watershed/branch/master/graph/badge.svg)](https://codecov.io/gh/jni/platelet-unet-watershed)  Segment platelets with pretrained unet and affinity watershed  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/docs/plugins/index.html -->  ## Installation  You can install `platelet-unet-watershed` via [pip]:      pip install platelet-unet-watershed  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"platelet-unet-watershed\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin [file an issue]: https://github.com/jni/platelet-unet-watershed/issues [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "platelet-unet-watershed      Segment platelets with pretrained unet and affinity watershed  This napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.  Installation You can install platelet-unet-watershed via pip: pip install platelet-unet-watershed  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"platelet-unet-watershed\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "platelet-unet-watershed",
    "documentation": "https://github.com/jni/platelet-unet-watershed#README.md",
    "first_released": "2021-07-01T06:29:19.185077Z",
    "license": "BSD-3-Clause",
    "name": "platelet-unet-watershed",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/jni/platelet-unet-watershed",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2021-09-20T07:40:31.740406Z",
    "report_issues": "https://github.com/jni/platelet-unet-watershed/issues",
    "requirements": null,
    "summary": "Segment platelets with pretrained unet and affinity watershed",
    "support": "https://github.com/jni/platelet-unet-watershed/issues",
    "twitter": "",
    "version": "0.0.3",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Marc Boucsein" }, { "name": "Robin Koch" }],
    "code_repository": "https://github.com/MBPhys/napari-nd-cropper",
    "conda": [{ "channel": "conda-forge", "package": "napari-nd-cropper" }],
    "description": "# napari-nd-cropper  [![License](https://img.shields.io/pypi/l/napari-medical-image-formats.svg?color=green)](https://github.com/MBPhys/napari-nd-cropper/raw/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-nd-cropper.svg?color=green)](https://pypi.org/project/napari-nd-cropper) [![Python Version](https://img.shields.io/pypi/pyversions/napari-nd-cropper.svg?color=green)](https://python.org)   A napari plugin in order to crop nd-images via different modes:  - Cropping via Drag&Drop interaction box (available for napari releases > 0.4.12) - Cropping of double-clicked regions based on predefined size (Integer or Tuple of integer)  - Cropping based on view  - Cropping via Sliders    ----------------------------------  ## Installation  You can install `napari-nd-cropper` via [pip]:      pip install napari-nd-cropper  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-nd-cropper\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin [file an issue]: https://github.com/MBPhys/napari-nd-cropper/issues [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-nd-cropper    A napari plugin in order to crop nd-images via different modes:  Cropping via Drag&Drop interaction box (available for napari releases > 0.4.12) Cropping of double-clicked regions based on predefined size (Integer or Tuple of integer)  Cropping based on view  Cropping via Sliders    Installation You can install napari-nd-cropper via pip: pip install napari-nd-cropper  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-nd-cropper\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 3 - Alpha"],
    "display_name": "napari-nd-cropper",
    "documentation": "https://github.com/MBPhys/napari-nd-cropper",
    "first_released": "2022-01-12T11:39:06.916260Z",
    "license": "BSD-3-Clause",
    "name": "napari-nd-cropper",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/MBPhys/napari-nd-cropper",
    "python_version": ">=3.9",
    "reader_file_extensions": [],
    "release_date": "2022-01-12T11:39:06.916260Z",
    "report_issues": "https://github.com/MBPhys/napari-nd-cropper/issues",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "napari-tools-menu",
      "qtpy",
      "superqt",
      "magicgui"
    ],
    "summary": "A napari plugin in order to crop nd-images via different modes",
    "support": "https://github.com/MBPhys/napari-nd-cropper/issues",
    "twitter": "",
    "version": "0.1.3",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Martin Weigert" }],
    "code_repository": "https://github.com/maweigert/napari-nlm",
    "conda": [],
    "description": "# napari-nlm  [![License BSD-3](https://img.shields.io/pypi/l/napari-nlm.svg)](https://github.com/maweigert/napari-nlm/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-nlm.svg)](https://pypi.org/project/napari-nlm) [![Python Version](https://img.shields.io/pypi/pyversions/napari-nlm.svg)](https://python.org) [![tests](https://github.com/maweigert/napari-nlm/workflows/tests/badge.svg)](https://github.com/maweigert/napari-nlm/actions) [![codecov](https://codecov.io/gh/maweigert/napari-nlm/branch/main/graph/badge.svg)](https://codecov.io/gh/maweigert/napari-nlm) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-nlm)](https://napari-hub.org/plugins/napari-nlm)  ----------------------------------   GPU accelerated non local means (NLM) denoising plugin for napari (WIP)  * currently only supports single-channel 2D or 3D images * requires a OpenCL capable GPU  ![Screenshot](images/screenshot.jpg)   ## Installation  You can install `napari-nlm` via [pip]:      pip install napari-nlm  ## Usage  1. Open example image `Open Sample > napari-nlm: noisy bricks` 2. Adjust parameters     * `sigma`: denoising strength (the larger sigma, the greater the smoothing)    * `patch_radius`: size of local patches, 2 or 3 is a good default    * `search_radius`: size of search area around each pixel to find similar patches, 7-11 is a good default 3. Denoise by pressing `run`   ## License  Distributed under the terms of the [BSD-3] license, \"napari-nlm\" is free and open source software ",
    "description_content_type": "text/markdown",
    "description_text": "napari-nlm        GPU accelerated non local means (NLM) denoising plugin for napari (WIP)  currently only supports single-channel 2D or 3D images requires a OpenCL capable GPU   Installation You can install napari-nlm via [pip]: pip install napari-nlm  Usage  Open example image Open Sample > napari-nlm: noisy bricks Adjust parameters  sigma: denoising strength (the larger sigma, the greater the smoothing) patch_radius: size of local patches, 2 or 3 is a good default search_radius: size of search area around each pixel to find similar patches, 7-11 is a good default Denoise by pressing run  License Distributed under the terms of the [BSD-3] license, \"napari-nlm\" is free and open source software",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari NLM",
    "documentation": "https://github.com/maweigert/napari-nlm#README.md",
    "first_released": "2022-07-25T22:00:24.500417Z",
    "license": "BSD-3-Clause",
    "name": "napari-nlm",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget", "sample_data"],
    "project_site": "https://github.com/maweigert/napari-nlm",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-07-26T23:46:26.367059Z",
    "report_issues": "https://github.com/maweigert/napari-nlm/issues",
    "requirements": [
      "numpy",
      "magicgui",
      "qtpy",
      "pyopencl (==2022.1.5)",
      "gputools",
      "scikit-image",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "summary": "NLM (non local means) denoising",
    "support": "https://github.com/maweigert/napari-nlm/issues",
    "twitter": "",
    "version": "0.0.4",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      { "email": "liuhanjin-sc@g.ecc.u-tokyo.ac.jp", "name": "Hanjin Liu" }
    ],
    "code_repository": "https://github.com/hanjinliu/napari-multitask",
    "description": "# napari-multitask  Multitasking on napari.  ![](https://github.com/hanjinliu/napari-multitask/raw/main/Figs/output.gif)  Layers and opened dock widgets are stored in the task panels below. Switch your tasks at any time.  # Installation  ``` pip install napari-multitask ```   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-multitask Multitasking on napari.  Layers and opened dock widgets are stored in the task panels below. Switch your tasks at any time. Installation pip install napari-multitask",
    "development_status": [],
    "display_name": "napari-multitask",
    "documentation": "",
    "first_released": "2021-12-06T08:07:30.574504Z",
    "license": "BSD-3-Clause",
    "name": "napari-multitask",
    "npe2": false,
    "operating_system": [],
    "plugin_types": ["widget"],
    "project_site": "",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2021-12-13T06:43:24.186430Z",
    "report_issues": "",
    "requirements": ["magic-class (>=0.5.11)"],
    "summary": "Multitasking in napari",
    "support": "",
    "twitter": "",
    "version": "0.0.2",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "email": "dkrentzel@pm.me", "name": "Daniel Krentzel" }],
    "code_repository": "https://github.com/krentzd/napari-pdf-reader",
    "description": "# PDF reader for napari Reads PDF files into napari   ",
    "description_content_type": "text/markdown",
    "description_text": "PDF reader for napari Reads PDF files into napari",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari-pdf-reader",
    "documentation": "https://github.com/krentzd/napari-pdf-reader#README.md",
    "first_released": "2021-11-05T16:24:49.069732Z",
    "license": "MIT",
    "name": "napari-pdf-reader",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader"],
    "project_site": "https://github.com/krentzd/napari-pdf-reader",
    "python_version": ">=3.7",
    "reader_file_extensions": ["*"],
    "release_date": "2021-11-05T16:24:49.069732Z",
    "report_issues": "https://github.com/krentzd/napari-pdf-reader/issues",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "pillow",
      "pdf2image"
    ],
    "summary": "Reader for PDF files",
    "support": "https://github.com/krentzd/napari-pdf-reader/issues",
    "twitter": "",
    "version": "0.0.1a3",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Dr. Andrew Annex" }],
    "code_repository": "https://github.com/AndrewAnnex/napari-rioxarray",
    "conda": [],
    "description": "# napari-rioxarray  [![License BSD-3](https://img.shields.io/pypi/l/napari-rioxarray.svg?color=green)](https://github.com/AndrewAnnex/napari-rioxarray/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-rioxarray.svg?color=green)](https://pypi.org/project/napari-rioxarray) [![Python Version](https://img.shields.io/pypi/pyversions/napari-rioxarray.svg?color=green)](https://python.org) [![tests](https://github.com/AndrewAnnex/napari-rioxarray/workflows/tests/badge.svg)](https://github.com/AndrewAnnex/napari-rioxarray/actions) [![codecov](https://codecov.io/gh/AndrewAnnex/napari-rioxarray/branch/main/graph/badge.svg)](https://codecov.io/gh/AndrewAnnex/napari-rioxarray) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-rioxarray)](https://napari-hub.org/plugins/napari-rioxarray)  A rioxarray plugin for napari supporting GDAL raster datatypes  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/plugins/index.html -->  ## Installation  You can install `napari-rioxarray` via [pip]:      pip install napari-rioxarray    To install latest development version :      pip install git+https://github.com/AndrewAnnex/napari-rioxarray.git   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-rioxarray\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/AndrewAnnex/napari-rioxarray/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-rioxarray       A rioxarray plugin for napari supporting GDAL raster datatypes  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install napari-rioxarray via pip: pip install napari-rioxarray  To install latest development version : pip install git+https://github.com/AndrewAnnex/napari-rioxarray.git  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-rioxarray\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "Rioxarray Plugin",
    "documentation": "https://github.com/AndrewAnnex/napari-rioxarray#README.md",
    "first_released": "2022-09-01T02:25:14.120179Z",
    "license": "BSD-3-Clause",
    "name": "napari-rioxarray",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader"],
    "project_site": "https://github.com/AndrewAnnex/napari-rioxarray",
    "python_version": ">=3.8",
    "reader_file_extensions": [
      "*.tif",
      "*.tiff",
      "*.img",
      "*.vrt",
      "*.IMG",
      "*.cub",
      "*.TIFF",
      "*.lbl",
      "*.fits",
      "*.TIF",
      "*.CUB",
      "*.LBL",
      "*.FITS"
    ],
    "release_date": "2022-09-01T02:25:14.120179Z",
    "report_issues": "https://github.com/AndrewAnnex/napari-rioxarray/issues",
    "requirements": [
      "numpy",
      "napari",
      "rioxarray",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'"
    ],
    "summary": "A rioxarray plugin for napari supporting GDAL raster datatypes",
    "support": "https://github.com/AndrewAnnex/napari-rioxarray/issues",
    "twitter": "",
    "version": "0.0.1",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Hippolyte Verdier" }],
    "code_repository": "https://github.com/hippover/palmari",
    "conda": [{ "channel": "conda-forge", "package": "palmari" }],
    "description": "  <!-- This file is designed to provide you with a starting template for documenting the functionality of your plugin. Its content will be rendered on your plugin's napari hub page.  The sections below are given as a guide for the flow of information only, and are in no way prescriptive. You should feel free to merge, remove, add and  rename sections at will to make this document work best for your plugin.   ## Description  This should be a detailed description of the context of your plugin and its  intended purpose.  If you have videos or screenshots of your plugin in action, you should include them here as well, to make them front and center for new users.   You should use absolute links to these assets, so that we can easily display them  on the hub. The easiest way to include a video is to use a GIF, for example hosted on imgur. You can then reference this GIF as an image.  ![Example GIF hosted on Imgur](https://i.imgur.com/A5phCX4.gif)  Note that GIFs larger than 5MB won't be rendered by GitHub - we will however, render them on the napari hub.  The other alternative, if you prefer to keep a video, is to use GitHub's video embedding feature.  1. Push your `DESCRIPTION.md` to GitHub on your repository (this can also be done as part of a Pull Request) 2. Edit `.napari/DESCRIPTION.md` **on GitHub**. 3. Drag and drop your video into its desired location. It will be uploaded and hosted on GitHub for you, but will not be placed in your repository. 4. We will take the resolved link to the video and render it on the hub.  Here is an example of an mp4 video embedded this way.  https://user-images.githubusercontent.com/17995243/120088305-6c093380-c132-11eb-822d-620e81eb5f0e.mp4  ## Intended Audience & Supported Data  This section should describe the target audience for this plugin (any knowledge, skills and experience required), as well as a description of the types of data supported by this plugin.  Try to make the data description as explicit as possible, so that users know the format your plugin expects. This applies both to reader plugins reading file formats and to function/dock widget plugins accepting layers and/or layer data. For example, if you know your plugin only works with 3D integer data in \"tyx\" order, make sure to mention this.  If you know of researchers, groups or labs using your plugin, or if it has been cited anywhere, feel free to also include this information here.  ## Quickstart  This section should go through step-by-step examples of how your plugin should be used. Where your plugin provides multiple dock widgets or functions, you should split these out into separate subsections for easy browsing. Include screenshots and videos wherever possible to elucidate your descriptions.   Ideally, this section should start with minimal examples for those who just want a quick overview of the plugin's functionality, but you should definitely link out to more complex and in-depth tutorials highlighting any intricacies of your plugin, and more detailed documentation if you have it.  ## Additional Install Steps (uncommon) We will be providing installation instructions on the hub, which will be sufficient for the majority of plugins. They will include instructions to pip install, and to install via napari itself.  Most plugins can be installed out-of-the-box by just specifying the package requirements over in `setup.cfg`. However, if your plugin has any more complex dependencies, or  requires any additional preparation before (or after) installation, you should add  this information here.  ## How to Cite  Many plugins may be used in the course of published (or publishable) research, as well as during conference talks and other public facing events. If you'd like to be cited in a particular format, or have a DOI you'd like used, you should provide that information here. -->  Process your SPT-PALM recordings with PALMari's all-inclusive pipeline: spot detection, sub-pixel localization, tracking & more. Start quickly with default parameters or customize your pipeline, and run it on entire folder of microscope recordings.  ## Getting Help  Email Hippolyte Verdier : hverdier@pasteur.fr ",
    "description_content_type": "text/markdown",
    "description_text": " Process your SPT-PALM recordings with PALMari's all-inclusive pipeline: spot detection, sub-pixel localization, tracking & more. Start quickly with default parameters or customize your pipeline, and run it on entire folder of microscope recordings. Getting Help Email Hippolyte Verdier : hverdier@pasteur.fr",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "PALMari",
    "documentation": "https://palmari.readthedocs.io/en/latest/",
    "first_released": "2022-05-06T17:58:23.787615Z",
    "license": "\"CeCILL\"",
    "name": "palmari",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/hippover/palmari",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-10-24T11:26:27.264616Z",
    "report_issues": "https://github.com/hippover/palmari/issues",
    "requirements": [
      "click",
      "dask (>=2022.1.0)",
      "dask-image (>=2021.12.0)",
      "imageio-ffmpeg",
      "magicgui (>=0.5.0)",
      "matplotlib (>=3.5)",
      "munkres",
      "napari",
      "nd2reader",
      "numpy",
      "pandas",
      "pyyaml",
      "sklearn",
      "scikit-image (>=0.18.3)",
      "tiffile",
      "toml",
      "tqdm",
      "trackpy (>=0.5.0)",
      "tox ; extra == 'testing'",
      "PyQt5 ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'"
    ],
    "summary": "A pipeline for PALM movies analysis (pre-processing, localization, drifft correction, tracking)",
    "support": "https://github.com/hippover/palmari/issues",
    "twitter": "",
    "version": "0.2.8",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Dr. Andrew Annex" }],
    "code_repository": "https://github.com/AndrewAnnex/napari-pdr-reader",
    "conda": [],
    "description": "# napari-pdr-reader  [![License BSD-3](https://img.shields.io/pypi/l/napari-pdr-reader.svg?color=green)](https://github.com/AndrewAnnex/napari-pdr-reader/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-pdr-reader.svg?color=green)](https://pypi.org/project/napari-pdr-reader) [![Python Version](https://img.shields.io/pypi/pyversions/napari-pdr-reader.svg?color=green)](https://python.org) [![tests](https://github.com/AndrewAnnex/napari-pdr-reader/workflows/tests/badge.svg)](https://github.com/AndrewAnnex/napari-pdr-reader/actions) [![codecov](https://codecov.io/gh/AndrewAnnex/napari-pdr-reader/branch/main/graph/badge.svg)](https://codecov.io/gh/AndrewAnnex/napari-pdr-reader) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-pdr-reader)](https://napari-hub.org/plugins/napari-pdr-reader)  A reader plugin for Napari for PDS data powered by the PDR library  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/plugins/index.html -->  ## Installation  You can install `napari-pdr-reader` via [pip]:      pip install napari-pdr-reader    To install latest development version :      pip install git+https://github.com/AndrewAnnex/napari-pdr-reader.git   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-pdr-reader\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/AndrewAnnex/napari-pdr-reader/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-pdr-reader       A reader plugin for Napari for PDS data powered by the PDR library  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install napari-pdr-reader via pip: pip install napari-pdr-reader  To install latest development version : pip install git+https://github.com/AndrewAnnex/napari-pdr-reader.git  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-pdr-reader\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "PDS reader plugin for Napari",
    "documentation": "https://github.com/AndrewAnnex/napari-pdr-reader#README.md",
    "first_released": "2022-07-14T15:49:45.814517Z",
    "license": "BSD-3-Clause",
    "name": "napari-pdr-reader",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader", "sample_data"],
    "project_site": "https://github.com/AndrewAnnex/napari-pdr-reader",
    "python_version": ">=3.9",
    "reader_file_extensions": [
      "*.fits",
      "*.img",
      "*.IMG",
      "*.FITS",
      "*.lbl",
      "*.LBL"
    ],
    "release_date": "2022-07-14T15:49:45.814517Z",
    "report_issues": "https://github.com/AndrewAnnex/napari-pdr-reader/issues",
    "requirements": [
      "astropy",
      "dustgoggles",
      "napari",
      "numpy",
      "pandas",
      "pdr",
      "pds4-tools",
      "pillow",
      "pvl",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "tox ; extra == 'testing'"
    ],
    "summary": "A reader plugin for Napari for PDS data powered by the PDR library",
    "support": "https://github.com/AndrewAnnex/napari-pdr-reader/issues",
    "twitter": "",
    "version": "0.0.1",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Andrey Aristov" }],
    "code_repository": "https://github.com/aaristov/napari-segment",
    "description": "# napari-segment  [![License](https://img.shields.io/pypi/l/napari-segment.svg?color=green)](https://github.com/aaristov/napari-segment/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-segment.svg?color=green)](https://pypi.org/project/napari-segment) [![Python Version](https://img.shields.io/pypi/pyversions/napari-segment.svg?color=green)](https://python.org) [![tests](https://github.com/aaristov/napari-segment/workflows/tests/badge.svg)](https://github.com/aaristov/napari-segment/actions) [![codecov](https://codecov.io/gh/aaristov/napari-segment/branch/main/graph/badge.svg)](https://codecov.io/gh/aaristov/napari-segment) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-segment)](https://napari-hub.org/plugins/napari-segment)  Interactively segment organoids/spheroids/aggregates in brightfield/fluorescence from nd2 multipositional stack. ----------------------------------  ![image](https://user-images.githubusercontent.com/11408456/201948817-255717a6-5f5c-45a2-ae01-2e0cbb1e29e8.png)   ## Installation  ```pip install napari-segment```  or  From napari plugin  ![image](https://user-images.githubusercontent.com/11408456/201949692-33f94eaf-ac43-44dd-8c21-e9f9a460c5b2.png)   ## Usage for segmentation  1. Drag your nd2 file into napari (otherwise try the Sample data from File / Open Sample / napari-segment) 2. Lauch Plugins -> napari-segment: Segment multipos 3. Select the brightfield channel 4. The data is lazily loaded from nd2 dataset and dynamically segmented in the viewer. 5. Binning 1-8 allows to skip small features and focus on bigger objects, also makes processing faster. ![image](https://user-images.githubusercontent.com/11408456/201701163-70c4af51-8a3a-42a0-adb9-32f0114eb49d.png) 6. Various preprocessing modes allow segmentation of different objects: ![image](https://user-images.githubusercontent.com/11408456/201701809-f16a23ea-d14a-4b38-8b8c-08a113416509.png)    - Invert: will use the dark shadow around aggregate - best for very old aggregates , out of focus (File / Open Sample / napari-segment / Old aggregate)      ![image](https://user-images.githubusercontent.com/11408456/201701950-efd86fae-d85b-471c-bb44-a0e328e26adc.png)    - Gradient: best for very sharp edges, early aggregates, single cells (File / Open Sample / napari-segment / Early aggregate)       ![image](https://user-images.githubusercontent.com/11408456/201705697-5d0d0643-44b6-4cb9-9208-4a29dd899d8c.png)         - Gauss diff: Fluorescence images   The result of preprocessing will be shown in the \"Preprocessing\" layer. 7. Smooth, Theshold and Erode parameters allow you to adjust the preliminary segmentation -> they all will appear in the \"Detections\" layer as outlines     ![image](https://user-images.githubusercontent.com/11408456/201703675-cff6bac1-bb2a-4d45-963f-6e6d00309c77.png)  8. Min/max diameter and eccentricity allow you to filter out unwanted regions -> the good regions will appear in the \"selected labels\" layer as filled areas.  ![image](https://user-images.githubusercontent.com/11408456/201703754-2c83a8d6-70c2-444a-8e30-54a39c901cd0.png) ![image](https://user-images.githubusercontent.com/11408456/201707025-9121f0dc-3939-48f0-ae75-884891be8d66.png)   9. Once satisfied, click \"Save the params!\" - it will automatically create file.nd2.params.yml file, so you can recall how the segmentation was done. Next time you open the same dataset, the parameters will be loaded automatically from this file.   10. Next section is for quantifying the sizes. Pixel size will be retrieved automatically from metadata. If not: update it manually and click Update plots to see the correct sizes. Click on any suspected value to see the corresponding frame and try to adjust the above parameters.   ![image](https://user-images.githubusercontent.com/11408456/201704881-b2303b9a-50c6-49c7-80ff-a6099cc2a151.png)  11. If impossible to get good results with automatic pipeline, click Clone for manual correction: this will create an editable \"Manual\" layer which you can edin with built-in tools in napari. Click \"Update plots\" to see the updated values.   12. \"Save csv!\" will generate a csv file with regionprops.    ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-segment\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/aaristov/napari-segment/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-segment       Interactively segment organoids/spheroids/aggregates in brightfield/fluorescence from nd2 multipositional stack.  Installation pip install napari-segment or From napari plugin  Usage for segmentation  Drag your nd2 file into napari (otherwise try the Sample data from File / Open Sample / napari-segment) Lauch Plugins -> napari-segment: Segment multipos Select the brightfield channel The data is lazily loaded from nd2 dataset and dynamically segmented in the viewer. Binning 1-8 allows to skip small features and focus on bigger objects, also makes processing faster.   Various preprocessing modes allow segmentation of different objects:    Invert: will use the dark shadow around aggregate - best for very old aggregates , out of focus (File / Open Sample / napari-segment / Old aggregate)     Gradient: best for very sharp edges, early aggregates, single cells (File / Open Sample / napari-segment / Early aggregate)     Gauss diff: Fluorescence images   The result of preprocessing will be shown in the \"Preprocessing\" layer. Smooth, Theshold and Erode parameters allow you to adjust the preliminary segmentation -> they all will appear in the \"Detections\" layer as outlines     Min/max diameter and eccentricity allow you to filter out unwanted regions -> the good regions will appear in the \"selected labels\" layer as filled areas.      Once satisfied, click \"Save the params!\" - it will automatically create file.nd2.params.yml file, so you can recall how the segmentation was done. Next time you open the same dataset, the parameters will be loaded automatically from this file.    Next section is for quantifying the sizes. Pixel size will be retrieved automatically from metadata. If not: update it manually and click Update plots to see the correct sizes. Click on any suspected value to see the corresponding frame and try to adjust the above parameters.       If impossible to get good results with automatic pipeline, click Clone for manual correction: this will create an editable \"Manual\" layer which you can edin with built-in tools in napari. Click \"Update plots\" to see the updated values.    \"Save csv!\" will generate a csv file with regionprops.    Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-segment\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "Segment organoid",
    "documentation": "https://github.com/aaristov/napari-segment#README.md",
    "first_released": "2022-10-05T21:56:23.425221Z",
    "license": "BSD-3-Clause",
    "name": "napari-segment",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader", "writer", "widget", "sample_data"],
    "project_site": "https://github.com/aaristov/napari-segment",
    "python_version": ">=3.8",
    "reader_file_extensions": ["*.nd2", "*.tif", "*.npy"],
    "release_date": "2022-12-05T12:51:24.245652Z",
    "report_issues": "https://github.com/aaristov/napari-segment/issues",
    "requirements": [
      "dask",
      "imageio-ffmpeg",
      "matplotlib",
      "napari",
      "nd2",
      "numpy",
      "pytest-qt",
      "scikit-image",
      "zarr"
    ],
    "summary": "Segment organoids and measure intensities",
    "support": "https://github.com/aaristov/napari-segment/issues",
    "twitter": "",
    "version": "0.3.7",
    "visibility": "public",
    "writer_file_extensions": [".npy"],
    "writer_save_layers": ["image", "labels*", "image*"]
  },
  {
    "authors": [{ "email": "cwood1967@gmail.com", "name": "Chris Wood" }],
    "code_repository": "https://github.com/cwood1967/napari-nikon-nd2",
    "description": "# napari-nikon-nd2  [![License](https://img.shields.io/pypi/l/napari-nikon-nd2.svg?color=green)](https://github.com/cwood1967/napari-nikon-nd2/blob/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-nikon-nd2.svg?color=green)](https://pypi.org/project/napari-nikon-nd2) [![Python Version](https://img.shields.io/pypi/pyversions/napari-nikon-nd2.svg?color=green)](https://python.org) [![tests](https://github.com/cwood1967/napari-nikon-nd2/workflows/tests/badge.svg)](https://github.com/cwood1967/napari-nikon-nd2/actions) [![codecov](https://codecov.io/gh/cwood1967/napari-nikon-nd2/branch/main/graph/badge.svg)](https://codecov.io/gh/cwood1967/napari-nikon-nd2)  Opens Nikon ND2 files into napari. This plugin uses the [nd2reader] and [pims] python packages.   ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/docs/plugins/index.html -->  ## Installation  You can install `napari-nikon-nd2` via [pip]:      pip install napari-nikon-nd2  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [Apache Software License 2.0] license, \"napari-nikon-nd2\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  ## Credits  This [napari] plugin was created using [Napari Delta Vision Reader] and the [Allen Institute IO] plugin as examples.   [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin [file an issue]: https://github.com/cwood1967/napari-nikon-nd2/issues [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ [nd2reader]: https://github.com/rbnvrw/nd2reader [pims]: https://github.com/soft-matter/pims [Allen Institute IO]: https://github.com/AllenCellModeling/napari-aicsimageio [Napari Delta Vision Reader]: https://github.com/tlambert03/napari-dv  ",
    "description_content_type": "text/markdown",
    "description_text": "napari-nikon-nd2      Opens Nikon ND2 files into napari. This plugin uses the nd2reader and pims python packages.   This napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.  Installation You can install napari-nikon-nd2 via pip: pip install napari-nikon-nd2  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the Apache Software License 2.0 license, \"napari-nikon-nd2\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description. Credits This napari plugin was created using Napari Delta Vision Reader and the Allen Institute IO plugin as examples.",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "napari-nikon-nd2",
    "documentation": "",
    "first_released": "2021-02-03T21:21:34.976957Z",
    "license": "Apache-2.0",
    "name": "napari-nikon-nd2",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader"],
    "project_site": "https://github.com/cwood1967/napari-nikon-nd2",
    "python_version": ">=3.6",
    "reader_file_extensions": ["*"],
    "release_date": "2021-02-03T21:55:34.674256Z",
    "report_issues": "",
    "requirements": ["napari-plugin-engine (>=0.1.4)", "numpy", "nd2reader"],
    "summary": "Opens Nikon ND2 files into napari.",
    "support": "",
    "twitter": "",
    "version": "0.1.3",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "email": "hthieu@illinois.edu", "name": "Hieu Hoang" }],
    "code_repository": null,
    "description": "# napari-pram  [![License](https://img.shields.io/pypi/l/napari-pram.svg?color=green)](https://github.com/hthieu166/napari-pram/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-pram.svg?color=green)](https://pypi.org/project/napari-pram) [![Python Version](https://img.shields.io/pypi/pyversions/napari-pram.svg?color=green)](https://python.org) [![tests](https://github.com/hthieu166/napari-pram/workflows/tests/badge.svg)](https://github.com/hthieu166/napari-pram/actions) [![codecov](https://codecov.io/gh/hthieu166/napari-pram/branch/main/graph/badge.svg)](https://codecov.io/gh/hthieu166/napari-pram) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-pram)](https://napari-hub.org/plugins/napari-pram)  Plugin for PRAM data annotation and processing.  ![PRAM Demo](https://raw.githubusercontent.com/hthieu166/napari-pram/main/docs/figs/demo.jpg)  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/plugins/stable/index.html -->  ## Usage  ### Open `napari-pram` toolbox:  On the toolbar, select ``[Plugins] > napari-pram: Open PRAM's toolbox``  ### Load PRAM image and annotations:  Press <kbd>Command/Control</kbd> + <kbd>O</kbd>:  - Select `*.json` files for annotations (from either [VGG Annotator](https://www.robots.ox.ac.uk/~vgg/software/via/) or `napari-pram`) - Select `*.png` files for PRAM image  ### Annotate - Press <kbd>Annotate</kbd> - Click the plus-in-circle icon on the top-left panel and start editing  ### Run PRAM particles detector - Select a proper threshold between 1 (ultra sensitive) - 10 (less sensitive) - Press <kbd>Run Detector</kbd>  ### Evaluate - Press <kbd>Evaluate</kbd> - Hide/Unhide true positive/ false postive/false negative layers  ### Load new image - Press <kbd>Clear All</kbd> to remove all layers  ### Export to JSON - Press <kbd>Save to File</kbd> to export all annotations, predictions from the algorithm to a JSON file ## Installation Following this [tutorial](https://napari.org/tutorials/fundamentals/quick_start.html) to install `napari`.   Alternatively, you can follow my instructions as follows:  You will need a python environment. I recommend [Conda](https://docs.conda.io/en/latest/miniconda.html). Create a new environment, for example:          conda create --name napari-env python=3.7 pip   Activate the new environment:      conda activate napari-env   Install [napari](https://napari.org/tutorials/fundamentals/installation) via [pip]:      pip install napari[all]  Then you can finally install our plugin `napari-pram` via [pip]:      pip install napari-pram  Alternatively, the plugin can be installed using napari-GUI  ``[Plugins] > Install/Uninstall Plugins`` and search for `napari-pram`  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [MIT] license, \"napari-pram\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-pram       Plugin for PRAM data annotation and processing.   This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Usage Open napari-pram toolbox: On the toolbar, select [Plugins] > napari-pram: Open PRAM's toolbox Load PRAM image and annotations: Press Command/Control + O:  - Select *.json files for annotations (from either VGG Annotator or napari-pram) - Select *.png files for PRAM image Annotate  Press Annotate Click the plus-in-circle icon on the top-left panel and start editing  Run PRAM particles detector  Select a proper threshold between 1 (ultra sensitive) - 10 (less sensitive) Press Run Detector  Evaluate  Press Evaluate Hide/Unhide true positive/ false postive/false negative layers  Load new image  Press Clear All to remove all layers  Export to JSON  Press Save to File to export all annotations, predictions from the algorithm to a JSON file  Installation Following this tutorial to install napari.  Alternatively, you can follow my instructions as follows: You will need a python environment. I recommend Conda. Create a new environment, for example: conda create --name napari-env python=3.7 pip  Activate the new environment: conda activate napari-env  Install napari via pip: pip install napari[all]  Then you can finally install our plugin napari-pram via pip: pip install napari-pram  Alternatively, the plugin can be installed using napari-GUI [Plugins] > Install/Uninstall Plugins and search for napari-pram Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the MIT license, \"napari-pram\" is free and open source software Issues If you encounter any problems, please [file an issue] along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari PRAM",
    "documentation": "",
    "first_released": "2022-04-29T22:32:47.607040Z",
    "license": "MIT",
    "name": "napari-pram",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader", "writer", "widget", "sample_data"],
    "project_site": "",
    "python_version": ">=3.7",
    "reader_file_extensions": ["*.json", "*.png"],
    "release_date": "2022-05-03T04:27:54.896702Z",
    "report_issues": "",
    "requirements": [
      "numpy",
      "magicgui",
      "qtpy",
      "opencv-python",
      "scipy",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "summary": "plugin for PRAM data annotation and processing",
    "support": "",
    "twitter": "",
    "version": "0.1.3",
    "writer_file_extensions": [".npy"],
    "writer_save_layers": ["image", "labels*", "image*"]
  },
  {
    "authors": [{ "name": "Niklas Netter" }],
    "code_repository": "https://github.com/gatoniel/napari-validate-random-label-predictions",
    "description": "# napari-validate-random-label-predictions  [![License BSD-3](https://img.shields.io/pypi/l/napari-validate-random-label-predictions.svg?color=green)](https://github.com/gatoniel/napari-validate-random-label-predictions/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-validate-random-label-predictions.svg?color=green)](https://pypi.org/project/napari-validate-random-label-predictions) [![Python Version](https://img.shields.io/pypi/pyversions/napari-validate-random-label-predictions.svg?color=green)](https://python.org) [![tests](https://github.com/gatoniel/napari-validate-random-label-predictions/workflows/tests/badge.svg)](https://github.com/gatoniel/napari-validate-random-label-predictions/actions) [![codecov](https://codecov.io/gh/gatoniel/napari-validate-random-label-predictions/branch/main/graph/badge.svg)](https://codecov.io/gh/gatoniel/napari-validate-random-label-predictions) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-validate-random-label-predictions)](https://napari-hub.org/plugins/napari-validate-random-label-predictions)  Validate separate instances of label predictions manually  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/stable/plugins/index.html -->  ## Installation  You can install `napari-validate-random-label-predictions` via [pip]:      pip install napari-validate-random-label-predictions    To install latest development version :      pip install git+https://github.com/gatoniel/napari-validate-random-label-predictions.git   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-validate-random-label-predictions\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/gatoniel/napari-validate-random-label-predictions/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-validate-random-label-predictions       Validate separate instances of label predictions manually  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install napari-validate-random-label-predictions via pip: pip install napari-validate-random-label-predictions  To install latest development version : pip install git+https://github.com/gatoniel/napari-validate-random-label-predictions.git  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-validate-random-label-predictions\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari validate random label predictions",
    "documentation": "https://github.com/gatoniel/napari-validate-random-label-predictions#README.md",
    "first_released": "2022-11-23T15:54:46.355629Z",
    "license": "BSD-3-Clause",
    "name": "napari-validate-random-label-predictions",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/gatoniel/napari-validate-random-label-predictions",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-11-23T15:54:46.355629Z",
    "report_issues": "https://github.com/gatoniel/napari-validate-random-label-predictions/issues",
    "requirements": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "summary": "Validate separate instances of label predictions manually",
    "support": "https://github.com/gatoniel/napari-validate-random-label-predictions/issues",
    "twitter": "",
    "version": "0.0.1",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      { "email": "talley.lambert@gmail.com", "name": "Talley Lambert" }
    ],
    "code_repository": "https://github.com/tlambert03/napari-ndtiffs",
    "description": "# napari-ndtiffs  [![License](https://img.shields.io/pypi/l/napari-ndtiffs.svg?color=green)](https://raw.githubusercontent.com/tlambert03/napari-ndtiffs/master/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-ndtiffs.svg?color=green)](https://pypi.org/project/napari-ndtiffs) [![Python Version](https://img.shields.io/pypi/pyversions/napari-ndtiffs.svg?color=green)](https://python.org) [![tests](https://github.com/tlambert03/napari-ndtiffs/workflows/tests/badge.svg)](https://github.com/tlambert03/napari-ndtiffs/actions) [![codecov](https://codecov.io/gh/tlambert03/napari-ndtiffs/branch/master/graph/badge.svg)](https://codecov.io/gh/tlambert03/napari-ndtiffs)  napari plugin for nd tiff folders with optional OpenCl-based deskewing.  Built-in support for folders of (skewed) lattice light sheet tiffs.  ![napari-ndtiffs demo](https://github.com/tlambert03/napari-ndtiffs/raw/master/demo.gif)  ----------------------------------  *This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.*  ## Features  - Drag and drop a folder of tiffs onto napari window to view easily    - (currently designed to detect  lattice light sheet tiffs, but easily     adjustable) - If lattice `Settings.txt` file is found, will deskew automatically (only if   necessary) - Lazily loads dataset on demand.  quickly load preview your data. - Handles `.zip` archives as well!  Just directly compress your tiff folder,   then drop it into napari. - All-openCL deskewing, works on GPU as well as CPU, falls back to scipy if   pyopencl is unavailable.  It would not be hard to support arbitrary filenaming patterns!  If you have a folder of tiffs with a consistent naming scheme and would like to take advantage of this plugin, feel free to open an issue!  ## Installation  You can install `napari-ndtiffs` via [pip]:  ```shell pip install napari-ndtiffs ```  To also install PyOpenCL (for faster deskewing):  ```shell pip install napari-ndtiffs[opencl] ```  ## Usage  In most cases, just drop your folder onto napari, or use `viewer.open(\"path\")`  ### Overriding parameters  You can control things like voxel size and deskewing angle as follows:  ```python from napari_ndtiffs import parameter_override import napari  viewer = napari.Viewer() with parameter_override(angle=45, name=\"my image\"):     viewer.open(\"path/to/folder\", plugin=\"ndtiffs\") ```  Valid keys for `parameter_override` include:  - **dx**: (`float`) the pixel size, in microns - **dz**: (`float`)the z step size, in microns - **deskew**: (`bool`) whether or not to deskew, (by default, will deskew if angle > 0, or if a lattice metadata file is detected that requires deskewing)  - **angle**: (`float`) the angle of the light sheet relative to the coverslip - **padval**: (`float`) the value with which to pad the image edges when deskewing (default is 0) - **contrast_limits**: (`2-tuple of int`) (min, max) contrast_limits to use when viewing the image - **name**: (`str`) an optional name for the image  ### Sample data  Try it out with test data: [download sample data](https://www.dropbox.com/s/up4ywrn2sckjunc/lls_mitosis.zip?dl=1)  You can unzip if you like, or just drag the zip file onto the napari window.  Or, from command line, use:  ```bash napari path/to/lls_mitosis.zip ```  ## Debugging  To monitor file io and deskew activity, enter the following in the napari console:  ```python import logging logging.getLogger('napari_llsfolder').setLevel('DEBUG') ```   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-ndtiffs\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin [file an issue]: https://github.com/tlambert03/napari-ndtiffs/issues [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-ndtiffs      napari plugin for nd tiff folders with optional OpenCl-based deskewing. Built-in support for folders of (skewed) lattice light sheet tiffs.   This napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template. Features  Drag and drop a folder of tiffs onto napari window to view easily  (currently designed to detect  lattice light sheet tiffs, but easily     adjustable) If lattice Settings.txt file is found, will deskew automatically (only if   necessary) Lazily loads dataset on demand.  quickly load preview your data. Handles .zip archives as well!  Just directly compress your tiff folder,   then drop it into napari. All-openCL deskewing, works on GPU as well as CPU, falls back to scipy if   pyopencl is unavailable.  It would not be hard to support arbitrary filenaming patterns!  If you have a folder of tiffs with a consistent naming scheme and would like to take advantage of this plugin, feel free to open an issue! Installation You can install napari-ndtiffs via pip: shell pip install napari-ndtiffs To also install PyOpenCL (for faster deskewing): shell pip install napari-ndtiffs[opencl] Usage In most cases, just drop your folder onto napari, or use viewer.open(\"path\") Overriding parameters You can control things like voxel size and deskewing angle as follows: ```python from napari_ndtiffs import parameter_override import napari viewer = napari.Viewer() with parameter_override(angle=45, name=\"my image\"):     viewer.open(\"path/to/folder\", plugin=\"ndtiffs\") ``` Valid keys for parameter_override include:  dx: (float) the pixel size, in microns dz: (float)the z step size, in microns deskew: (bool) whether or not to deskew, (by default, will deskew if angle > 0, or if a lattice metadata file is detected that requires deskewing)  angle: (float) the angle of the light sheet relative to the coverslip padval: (float) the value with which to pad the image edges when deskewing (default is 0) contrast_limits: (2-tuple of int) (min, max) contrast_limits to use when viewing the image name: (str) an optional name for the image  Sample data Try it out with test data: download sample data You can unzip if you like, or just drag the zip file onto the napari window. Or, from command line, use: bash napari path/to/lls_mitosis.zip Debugging To monitor file io and deskew activity, enter the following in the napari console: python import logging logging.getLogger('napari_llsfolder').setLevel('DEBUG') Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-ndtiffs\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 3 - Alpha"],
    "display_name": "napari-ndtiffs",
    "documentation": "https://github.com/tlambert03/napari-ndtiffs#README.md",
    "first_released": "2020-05-06T00:05:33.846416Z",
    "license": "BSD-3-Clause",
    "name": "napari-ndtiffs",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["reader"],
    "project_site": "https://github.com/tlambert03/napari-ndtiffs",
    "python_version": ">=3.7",
    "reader_file_extensions": ["*"],
    "release_date": "2021-06-24T19:23:22.169470Z",
    "report_issues": "https://github.com/tlambert03/napari-ndtiffs/issues",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "dask[array]",
      "python-dateutil",
      "scipy",
      "tifffile"
    ],
    "summary": "napari plugin for nd tiff folders with OpenCl deskew",
    "support": "https://github.com/tlambert03/napari-ndtiffs/issues",
    "twitter": "",
    "version": "0.1.2",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Talley Lambert" }],
    "code_repository": "https://github.com/tlambert03/pycudadecon",
    "description": "# pyCUDAdecon  This package provides a python wrapper and convenience functions for [cudaDecon](https://github.com/scopetools/cudaDecon), which is a CUDA/C++ implementation of an accelerated Richardson Lucy Deconvolution algorithm<sup>1</sup>.  * CUDA accelerated deconvolution with a handful of artifact-reducing features. * radially averaged OTF generation with interpolation for voxel size   independence between PSF and data volumes * 3D deskew, rotation, general affine transformations * CUDA-based camera-correction for [sCMOS artifact correction](https://llspy.readthedocs.io/en/latest/camera.html)   ### Install  The conda package includes the required pre-compiled libraries for Windows and Linux. See GPU driver requirements [below](#gpu-requirements)  ```sh conda install -c conda-forge pycudadecon ```  *macOS is not supported*  ### 📖   &nbsp; [Documentation](http://www.talleylambert.com/pycudadecon)   ### GPU requirements  This software requires a CUDA-compatible NVIDIA GPU. The underlying cudadecon libraries have been compiled against different versions of the CUDA toolkit. The required CUDA libraries are bundled in the conda distributions so you don't need to install the CUDA toolkit separately.  If desired, you can pick which version of CUDA you'd like based on your needs, but please note that different versions of the CUDA toolkit have different GPU driver requirements:  To specify a specific cudatoolkit version, install as follows (for instance, to use `cudatoolkit=10.2`)  ```sh conda install -c conda-forge pycudadecon cudatoolkit=10.2 ```  | CUDA | Linux driver | Win driver | | ---- | ------------ | ---------- | | 10.2 | ≥ 440.33     | ≥ 441.22   | | 11.0 | ≥ 450.36.06  | ≥ 451.22   | | 11.1 | ≥ 455.23     | ≥ 456.38   | | 11.2 | ≥ 460.27.03  | ≥ 460.82   |   If you run into trouble, feel free to [open an issue](https://github.com/tlambert03/pycudadecon/issues) and describe your setup.   ## Usage   The [`pycudadecon.decon()`](https://pycudadecon.readthedocs.io/en/latest/deconvolution.html#pycudadecon.decon) function is designed be able to handle most basic applications:  ```python from pycudadecon import decon  # pass filenames of an image and a PSF result = decon('/path/to/3D_image.tif', '/path/to/3D_psf.tif')  # decon also accepts numpy arrays result = decon(img_array, psf_array)  # the image source can also be a sequence of arrays or paths result = decon([img_array, '/path/to/3D_image.tif'], psf_array)  # see docstrings for additional parameter options ```  For finer-tuned control, you may wish to make an OTF file from your PSF using [`pycudadecon.make_otf()`](https://pycudadecon.readthedocs.io/en/latest/otf.html?highlight=make_otf#pycudadecon.make_otf), and then use the [`pycudadecon.RLContext`](https://pycudadecon.readthedocs.io/en/latest/deconvolution.html?highlight=RLContext#pycudadecon.RLContext) context manager to setup the GPU for use with the [`pycudadecon.rl_decon()`](https://pycudadecon.readthedocs.io/en/latest/deconvolution.html?highlight=RLContext#pycudadecon.rl_decon) function.  (Note all images processed in the same context must have the same input shape).  ```python from pycudadecon import RLContext, rl_decon from glob import glob import tifffile  image_folder = '/path/to/some_images/' imlist = glob(image_folder + '*488*.tif') otf_path = '/path/to/pregenerated_otf.tif'  with tifffile.TiffFile(imlist[0]) as tf:     imshape = tf.series[0].shape  with RLContext(imshape, otf_path, dz) as ctx:     for impath in imlist:         image = tifffile.imread(impath)         result = rl_decon(image, ctx.out_shape)         # do something with result... ```  If you have a 3D PSF volume, the [`pycudadecon.TemporaryOTF`](https://pycudadecon.readthedocs.io/en/latest/otf.html?highlight=temporaryotf#pycudadecon.TemporaryOTF) context manager facilitates temporary OTF generation...  ```python  # continuing with the variables from the previous example...  psf_path = \"/path/to/psf_3D.tif\"  with TemporaryOTF(psf) as otf:      with RLContext(imshape, otf.path, dz) as ctx:          for impath in imlist:              image = tifffile.imread(impath)              result = rl_decon(image, ctx.out_shape)              # do something with result... ```  ... and that bit of code is essentially what the [`pycudadecon.decon()`](https://pycudadecon.readthedocs.io/en/latest/deconvolution.html#pycudadecon.decon) function is doing, with a little bit of additional conveniences added in.  *Each of these functions has many options and accepts multiple keyword arguments. See the [documentation](https://pycudadecon.readthedocs.io/en/latest/index.html) for further information on the respective functions.*  For examples and information on affine transforms, volume rotations, and deskewing (typical of light sheet volumes acquired with stage-scanning), see the [documentation on Affine Transformations](https://pycudadecon.readthedocs.io/en/latest/affine.html) ___  <sup>1</sup> D.S.C. Biggs and M. Andrews, Acceleration of iterative image restoration algorithms, Applied Optics, Vol. 36, No. 8, 1997. https://doi.org/10.1364/AO.36.001766 ",
    "description_content_type": "text/markdown",
    "description_text": "pyCUDAdecon This package provides a python wrapper and convenience functions for cudaDecon, which is a CUDA/C++ implementation of an accelerated Richardson Lucy Deconvolution algorithm1.  CUDA accelerated deconvolution with a handful of artifact-reducing features. radially averaged OTF generation with interpolation for voxel size   independence between PSF and data volumes 3D deskew, rotation, general affine transformations CUDA-based camera-correction for sCMOS artifact correction  Install The conda package includes the required pre-compiled libraries for Windows and Linux. See GPU driver requirements below sh conda install -c conda-forge pycudadecon macOS is not supported 📖     Documentation GPU requirements This software requires a CUDA-compatible NVIDIA GPU. The underlying cudadecon libraries have been compiled against different versions of the CUDA toolkit. The required CUDA libraries are bundled in the conda distributions so you don't need to install the CUDA toolkit separately.  If desired, you can pick which version of CUDA you'd like based on your needs, but please note that different versions of the CUDA toolkit have different GPU driver requirements: To specify a specific cudatoolkit version, install as follows (for instance, to use cudatoolkit=10.2) sh conda install -c conda-forge pycudadecon cudatoolkit=10.2 | CUDA | Linux driver | Win driver | | ---- | ------------ | ---------- | | 10.2 | ≥ 440.33     | ≥ 441.22   | | 11.0 | ≥ 450.36.06  | ≥ 451.22   | | 11.1 | ≥ 455.23     | ≥ 456.38   | | 11.2 | ≥ 460.27.03  | ≥ 460.82   | If you run into trouble, feel free to open an issue and describe your setup. Usage The pycudadecon.decon() function is designed be able to handle most basic applications: ```python from pycudadecon import decon pass filenames of an image and a PSF result = decon('/path/to/3D_image.tif', '/path/to/3D_psf.tif') decon also accepts numpy arrays result = decon(img_array, psf_array) the image source can also be a sequence of arrays or paths result = decon([img_array, '/path/to/3D_image.tif'], psf_array) see docstrings for additional parameter options ``` For finer-tuned control, you may wish to make an OTF file from your PSF using pycudadecon.make_otf(), and then use the pycudadecon.RLContext context manager to setup the GPU for use with the pycudadecon.rl_decon() function.  (Note all images processed in the same context must have the same input shape). ```python from pycudadecon import RLContext, rl_decon from glob import glob import tifffile image_folder = '/path/to/some_images/' imlist = glob(image_folder + '488.tif') otf_path = '/path/to/pregenerated_otf.tif' with tifffile.TiffFile(imlist[0]) as tf:     imshape = tf.series[0].shape with RLContext(imshape, otf_path, dz) as ctx:     for impath in imlist:         image = tifffile.imread(impath)         result = rl_decon(image, ctx.out_shape)         # do something with result... ``` If you have a 3D PSF volume, the pycudadecon.TemporaryOTF context manager facilitates temporary OTF generation... python  # continuing with the variables from the previous example...  psf_path = \"/path/to/psf_3D.tif\"  with TemporaryOTF(psf) as otf:      with RLContext(imshape, otf.path, dz) as ctx:          for impath in imlist:              image = tifffile.imread(impath)              result = rl_decon(image, ctx.out_shape)              # do something with result... ... and that bit of code is essentially what the pycudadecon.decon() function is doing, with a little bit of additional conveniences added in. Each of these functions has many options and accepts multiple keyword arguments. See the documentation for further information on the respective functions. For examples and information on affine transforms, volume rotations, and deskewing (typical of light sheet volumes acquired with stage-scanning), see the documentation on Affine Transformations  1 D.S.C. Biggs and M. Andrews, Acceleration of iterative image restoration algorithms, Applied Optics, Vol. 36, No. 8, 1997. https://doi.org/10.1364/AO.36.001766",
    "development_status": ["Development Status :: 4 - Beta"],
    "display_name": "pyCUDAdecon",
    "documentation": "https://pycudadecon.readthedocs.io/en/latest/",
    "first_released": "2022-08-10T22:10:53.720438Z",
    "license": "MIT",
    "name": "pycudadecon",
    "npe2": true,
    "operating_system": [],
    "plugin_types": ["widget"],
    "project_site": "",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-11-07T14:48:45.317086Z",
    "report_issues": "",
    "requirements": [
      "numpy",
      "tifffile",
      "typing-extensions",
      "importlib-metadata ; python_version < \"3.8\"",
      "black ; extra == 'dev'",
      "cruft ; extra == 'dev'",
      "flake8-bugbear ; extra == 'dev'",
      "flake8-docstrings ; extra == 'dev'",
      "flake8-pyprojecttoml ; extra == 'dev'",
      "flake8-typing-imports ; extra == 'dev'",
      "flake8 ; extra == 'dev'",
      "ipython ; extra == 'dev'",
      "isort ; extra == 'dev'",
      "mypy ; extra == 'dev'",
      "pdbpp ; extra == 'dev'",
      "pre-commit ; extra == 'dev'",
      "pydocstyle ; extra == 'dev'",
      "pytest-cov ; extra == 'dev'",
      "pytest ; extra == 'dev'",
      "rich ; extra == 'dev'",
      "pytest (>=6.0) ; extra == 'test'",
      "pytest-cov ; extra == 'test'"
    ],
    "summary": "Python wrapper for CUDA-accelerated 3D deconvolution",
    "support": "",
    "twitter": "",
    "version": "0.4.0",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      { "email": "lukas.migas@yahoo.com", "name": "Lukasz G. Migas" }
    ],
    "code_repository": "https://github.com/lukasz-migas/napari-1d",
    "description": "# napari-plot  [![License](https://img.shields.io/pypi/l/napari-plot.svg?color=green)](https://github.com/lukasz-migas/napari-1d/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-plot.svg?color=green)](https://pypi.org/project/napari-plot) [![Python Version](https://img.shields.io/pypi/pyversions/napari-plot.svg?color=green)](https://python.org) [![tests](https://github.com/lukasz-migas/napari-1d/workflows/tests/badge.svg)](https://github.com/lukasz-migas/napari-1d/actions) [![codecov](https://codecov.io/gh/lukasz-migas/napari-1d/branch/main/graph/badge.svg)](https://codecov.io/gh/lukasz-migas/napari-1d)  Plugin providing support for 1d plotting in napari.  This plugin is in very early stages of development and many things are still in a state of disarray. New features and bug fixes will be coming over the coming months.   ## Note  `napari-plot` provides several custom icons and stylesheets to take advantage of the `Qt` backend. Since it would be a bit busy to add multiple layer lists, I opted to include a toolbar that quickly pulls the layer list whenever requested. Simple use the toolbar to access several commonly accessed elements.  ## Usage  You can use `napari-plot` alongside `napari` where it is embedded as a dock widget. If using this option, controls are relegated to toolbar where you can adjust layer properties like you would do in `napari`.  ![embedded](https://github.com/lukasz-migas/napari-1d/blob/main/misc/embedded.png)  Or as a standalone app where only one-dimensional plotting is enabled. In this mode, controls take central stage and reflect `napari's` own behaviour where layer controls are embedded in the main application.  ![live-view](https://github.com/lukasz-migas/napari-1d/blob/main/misc/napariplot-live-line.gif)  ## Roadmap:  This is only provisional list of features that I would like to see implemented. It barely scratches the surface of what plotting tool should cover so as soon as the basics are covered, focus will be put towards adding more exotic features. If there are features that you certainly wish to be included, please modify the list below or create a [new issue](https://github.com/lukasz-migas/napari-1d/issues/new)  - [ ] Support for new layer types. Layers are based on `napari's` `Layer`, albeit in a two-dimensional setting. Supported and planned layers:   - [x] Line Layer - simple line plot.   - [x] Scatter Layer - scatter plot (similar to `napari's Points` layer).   - [x] Centroids/Segments Layer - horizontal or vertical line segments.   - [x] InfLine Layer - infinite horizontal or vertical lines that span over very broad range. Useful for defining regions of interest.   - [x] Region Layer - infinite horizontal or vertical rectangular boxes that span over very broad range. Useful for defining regions of interest.   - [x] Shapes Layer - `napari's` own `Shapes` layer   - [x] Points Layer - `napari's` own `Points` layer   - [x] Multi-line Layer - more efficient implementation of `Line` layer when multiple lines are necessary.   - [ ] Bar - horizontal and vertical barchart (TODO) - [x] Proper interactivity of each layer type (e.g. moving `Region` or `InfLine`, adding points, etc...) - [x] Intuitive interactivity. `napari-plot` will provide excellent level of interactivity with the plotted data. We plan to support several types of `Tools` that permit efficient interrogation of the data. We currently provide several `zoom` and `select` tools and hope to add few extras in the future.   - [x] Box-zoom - standard zooming rectangle. Simply `left-mouse + drag/release` in the canvas on region of interest   - [x] Horizontal span - zoom-in only in the y-axis by `Ctrl + left-mouse + drag/release` in the canvas.   - [x] Vertical span - span-in only in the x-axis by `Shift + left-mouse + drag/release` in the canvas.   - [x] Rectangle select - rectangle tool allowing sub-selection of data in the canvas. Similar to the `Box-zoom` but without the zooming part.   - [x] Polygon select - polygon tool allowing sub-selection of data in the canvas.   - [x] Lasso select - lasso tool allowing sub-selection of data in the canvas. - [ ] Interactive plot legend - [ ] Customizable axis visuals.   - [x] Plot axis enabling customization of tick/label size and color   - [ ] Support for non-linear scale - [ ] Add convenient plotting interface:   - [ ] Add `.plot` functionality   - [ ] Add `.scatter` functionality   - [ ] Add `.hbar` and `.vbar` functionality   - [ ] Add `.imshow` functionality  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/docs/plugins/index.html -->  ## Installation  You can install `napari-plot` directly from PyPI via:  ```python pip install napari-plot ```  or from the git repo:  ```python git clone https://github.com/lukasz-migas/napari-1d.git cd napari-1d pip install -e '.[all]' ```  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-plot\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin [file an issue]: https://github.com/lukasz-migas/napari-1d/issues [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/   ",
    "description_content_type": "text/markdown",
    "description_text": "napari-plot      Plugin providing support for 1d plotting in napari. This plugin is in very early stages of development and many things are still in a state of disarray. New features and bug fixes will be coming over the coming months.  Note napari-plot provides several custom icons and stylesheets to take advantage of the Qt backend. Since it would be a bit busy to add multiple layer lists, I opted to include a toolbar that quickly pulls the layer list whenever requested. Simple use the toolbar to access several commonly accessed elements. Usage You can use napari-plot alongside napari where it is embedded as a dock widget. If using this option, controls are relegated to toolbar where you can adjust layer properties like you would do in napari.  Or as a standalone app where only one-dimensional plotting is enabled. In this mode, controls take central stage and reflect napari's own behaviour where layer controls are embedded in the main application.  Roadmap: This is only provisional list of features that I would like to see implemented. It barely scratches the surface of what plotting tool should cover so as soon as the basics are covered, focus will be put towards adding more exotic features. If there are features that you certainly wish to be included, please modify the list below or create a new issue  [ ] Support for new layer types. Layers are based on napari's Layer, albeit in a two-dimensional setting. Supported and planned layers: [x] Line Layer - simple line plot. [x] Scatter Layer - scatter plot (similar to napari's Points layer). [x] Centroids/Segments Layer - horizontal or vertical line segments. [x] InfLine Layer - infinite horizontal or vertical lines that span over very broad range. Useful for defining regions of interest. [x] Region Layer - infinite horizontal or vertical rectangular boxes that span over very broad range. Useful for defining regions of interest. [x] Shapes Layer - napari's own Shapes layer [x] Points Layer - napari's own Points layer [x] Multi-line Layer - more efficient implementation of Line layer when multiple lines are necessary. [ ] Bar - horizontal and vertical barchart (TODO) [x] Proper interactivity of each layer type (e.g. moving Region or InfLine, adding points, etc...) [x] Intuitive interactivity. napari-plot will provide excellent level of interactivity with the plotted data. We plan to support several types of Tools that permit efficient interrogation of the data. We currently provide several zoom and select tools and hope to add few extras in the future. [x] Box-zoom - standard zooming rectangle. Simply left-mouse + drag/release in the canvas on region of interest [x] Horizontal span - zoom-in only in the y-axis by Ctrl + left-mouse + drag/release in the canvas. [x] Vertical span - span-in only in the x-axis by Shift + left-mouse + drag/release in the canvas. [x] Rectangle select - rectangle tool allowing sub-selection of data in the canvas. Similar to the Box-zoom but without the zooming part. [x] Polygon select - polygon tool allowing sub-selection of data in the canvas. [x] Lasso select - lasso tool allowing sub-selection of data in the canvas. [ ] Interactive plot legend [ ] Customizable axis visuals. [x] Plot axis enabling customization of tick/label size and color [ ] Support for non-linear scale [ ] Add convenient plotting interface: [ ] Add .plot functionality [ ] Add .scatter functionality [ ] Add .hbar and .vbar functionality [ ] Add .imshow functionality   This napari plugin was generated with Cookiecutter using with @napari's cookiecutter-napari-plugin template.  Installation You can install napari-plot directly from PyPI via: python pip install napari-plot or from the git repo: python git clone https://github.com/lukasz-migas/napari-1d.git cd napari-1d pip install -e '.[all]' Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-plot\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari-plot",
    "documentation": "https://github.com/lukasz-migas/napari-1d#README.md",
    "first_released": "2022-01-09T18:20:25.848886Z",
    "license": "BSD-3-Clause",
    "name": "napari-plot",
    "npe2": false,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/lukasz-migas/napari-1d",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-03-22T15:01:04.475629Z",
    "report_issues": "https://github.com/lukasz-migas/napari-1d/issues",
    "requirements": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "qtpy",
      "qtawesome",
      "napari (<0.4.15,>=0.4.13)",
      "matplotlib",
      "vispy (>=0.9.6)",
      "PySide2 (!=5.15.0,>=5.13.2) ; extra == 'all'",
      "pre-commit (>=2.9.0) ; extra == 'dev'",
      "black (==22.1.0) ; extra == 'dev'",
      "flake8 (==4.0.1) ; extra == 'dev'",
      "PySide2 (!=5.15.0,>=5.13.2) ; extra == 'dev'",
      "pytest ; extra == 'dev'",
      "pytest-qt ; extra == 'dev'",
      "scikit-image ; extra == 'dev'",
      "PyQt5 (!=5.15.0,>=5.12.3) ; extra == 'pyqt'",
      "PyQt5 (!=5.15.0,>=5.12.3) ; extra == 'pyqt5'",
      "PySide2 (!=5.15.0,>=5.13.2) ; extra == 'pyside'",
      "PySide2 (!=5.15.0,>=5.13.2) ; extra == 'pyside2'",
      "PyQt5 (!=5.15.0,>=5.12.3) ; extra == 'qt'",
      "pytest ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "scikit-image ; extra == 'testing'"
    ],
    "summary": "Plugin providing support for 1d plotting in napari.",
    "support": "https://github.com/lukasz-migas/napari-1d/issues",
    "twitter": "",
    "version": "0.1.5",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      { "name": "Computational Microscopy Platform" },
      { "name": "CZ Biohub" }
    ],
    "code_repository": "https://github.com/mehta-lab/recOrder/tree/main/recOrder",
    "conda": [{ "channel": "conda-forge", "package": "recorder-napari" }],
    "description": "# recOrder ![PyPI - Python Version](https://img.shields.io/pypi/pyversions/recOrder-napari) [![Downloads](https://pepy.tech/badge/recOrder-napari)](https://pepy.tech/project/recOrder-napari) [![Python package index](https://img.shields.io/pypi/v/recOrder-napari.svg)](https://pypi.org/project/recOrder-napari) [![Development Status](https://img.shields.io/pypi/status/napari.svg)](https://en.wikipedia.org/wiki/Software_release_life_cycle#Alpha)  This package provides a napari plugin and a command line interface for quantitative label-free microscopy.  In this repository you will find python tools and a napari plugin that allow the user to calibrate microscope hardware, acquire multi-modal data, reconstruct density and anisotropy, and visualize the results.  The acquisition, calibration, background correction, reconstruction, and applications of QLIPP (quantitative label-free imaging with phase and polarization)  are described in the following [E-Life Paper](https://elifesciences.org/articles/55502):  ```bibtex Syuan-Ming Guo, Li-Hao Yeh, Jenny Folkesson, Ivan E Ivanov, Anitha P Krishnan, Matthew G Keefe, Ezzat Hashemi, David Shin, Bryant B Chhun, Nathan H Cho, Manuel D Leonetti, May H Han, Tomasz J Nowakowski, Shalin B Mehta, \"Revealing architectural order with quantitative label-free imaging and deep learning,\" eLife 2020;9:e55502 DOI: 10.7554/eLife.55502 (2020). ```  `recOrder` is to be used alongside a conventional widefield microscope fitted with a universal polarizer (Panel A below).  The universal polarizer allows for the collection of label-free information including the intrinsic anisotropy of the sample and its relative phase (density). These measurements are collected by acquiring data under calibrated, polarization-diverse illumination followed by a computational reconstruction.  The overall structure of `recOrder` is shown in Panel B, highlighting the two different usage modes and their features: graphical user interface (GUI) through a napari plugin, and a command line interface (CLI).  ![Flow Chart](https://github.com/mehta-lab/recOrder/blob/main/docs/images/recOrder_Fig1_Overview.png?raw=true)  ## Dataset  [Slides](https://doi.org/10.5281/zenodo.5135889) and a [dataset](https://doi.org/10.5281/zenodo.5178487) shared during a workshop on QLIPP and recOrder can be found on Zenodo.  [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.5178487.svg)](https://doi.org/10.5281/zenodo.5178487) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.5135889.svg)](https://doi.org/10.5281/zenodo.5135889)  ## Quick Start  (Optional but recommended) install [anaconda](https://www.anaconda.com/products/distribution) and create a virtual environment:  ```sh conda create -y -n recOrder python=3.9 conda activate recOrder ```  > *Apple Silicon users please use*: > > ```sh > CONDA_SUBDIR=osx-64 conda create -y -n recOrder python=3.9 > conda activate recOrder > ```  Install `recOrder-napari`:  ```sh pip install recOrder-napari ```  Open `napari` with `recOrder-napari`:  ```sh napari -w recOrder-napari ```  View command-line help by running  ```sh recOrder.help ```  For more help, see [`recOrder`s documentation](./docs). ",
    "description_content_type": "text/markdown",
    "description_text": "recOrder     This package provides a napari plugin and a command line interface for quantitative label-free microscopy. In this repository you will find python tools and a napari plugin that allow the user to calibrate microscope hardware, acquire multi-modal data, reconstruct density and anisotropy, and visualize the results. The acquisition, calibration, background correction, reconstruction, and applications of QLIPP (quantitative label-free imaging with phase and polarization)  are described in the following E-Life Paper: bibtex Syuan-Ming Guo, Li-Hao Yeh, Jenny Folkesson, Ivan E Ivanov, Anitha P Krishnan, Matthew G Keefe, Ezzat Hashemi, David Shin, Bryant B Chhun, Nathan H Cho, Manuel D Leonetti, May H Han, Tomasz J Nowakowski, Shalin B Mehta, \"Revealing architectural order with quantitative label-free imaging and deep learning,\" eLife 2020;9:e55502 DOI: 10.7554/eLife.55502 (2020). recOrder is to be used alongside a conventional widefield microscope fitted with a universal polarizer (Panel A below).  The universal polarizer allows for the collection of label-free information including the intrinsic anisotropy of the sample and its relative phase (density). These measurements are collected by acquiring data under calibrated, polarization-diverse illumination followed by a computational reconstruction.  The overall structure of recOrder is shown in Panel B, highlighting the two different usage modes and their features: graphical user interface (GUI) through a napari plugin, and a command line interface (CLI).  Dataset Slides and a dataset shared during a workshop on QLIPP and recOrder can be found on Zenodo.   Quick Start (Optional but recommended) install anaconda and create a virtual environment: sh conda create -y -n recOrder python=3.9 conda activate recOrder  Apple Silicon users please use: sh CONDA_SUBDIR=osx-64 conda create -y -n recOrder python=3.9 conda activate recOrder  Install recOrder-napari: sh pip install recOrder-napari Open napari with recOrder-napari: sh napari -w recOrder-napari View command-line help by running sh recOrder.help For more help, see recOrders documentation.",
    "development_status": [],
    "display_name": "recOrder-napari",
    "documentation": "https://github.com/mehta-lab/recOrder/wiki",
    "first_released": "2022-04-19T19:14:03.116915Z",
    "license": "BSD 3-Clause License",
    "name": "recOrder-napari",
    "npe2": true,
    "operating_system": [
      "Operating System :: MacOS",
      "Operating System :: Microsoft :: Windows",
      "Operating System :: POSIX",
      "Operating System :: Unix"
    ],
    "plugin_types": ["reader", "widget"],
    "project_site": "https://github.com/mehta-lab/recOrder",
    "python_version": ">=3.8",
    "reader_file_extensions": ["*.zarr", "*.tif"],
    "release_date": "2022-10-07T22:23:30.962416Z",
    "report_issues": "https://github.com/mehta-lab/recOrder/issues",
    "requirements": [
      "waveorder (==1.0.0rc0)",
      "pycromanager (==0.19.2)",
      "click (>=8.0.1)",
      "pyyaml (>=5.4.1)",
      "tqdm (>=4.61.1)",
      "opencv-python (>=4.5.3.56)",
      "natsort (>=7.1.1)",
      "colorspacious (>=1.1.2)",
      "pyqtgraph (>=0.12.3)",
      "superqt (>=0.2.4)",
      "napari-ome-zarr (>=0.3.2)",
      "qtpy",
      "napari[all]",
      "imageio (!=2.11.0,!=2.22.1)",
      "importlib-metadata",
      "pytest (>=5.0.0) ; extra == 'dev'",
      "pytest-cov ; extra == 'dev'",
      "pytest-qt ; extra == 'dev'",
      "wget (>=3.2) ; extra == 'dev'",
      "tox ; extra == 'dev'",
      "pre-commit ; extra == 'dev'",
      "black ; extra == 'dev'"
    ],
    "summary": "Computational microscopy toolkit for label-free imaging",
    "support": "https://github.com/mehta-lab/recOrder/issues",
    "twitter": "",
    "version": "0.2.0",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Niklas Netter" }],
    "code_repository": "https://github.com/gatoniel/napari-nd2-folder-viewer",
    "description": "# napari-nd2-folder-viewer  [![License BSD-3](https://img.shields.io/pypi/l/napari-nd2-folder-viewer.svg?color=green)](https://github.com/gatoniel/napari-nd2-folder-viewer/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-nd2-folder-viewer.svg?color=green)](https://pypi.org/project/napari-nd2-folder-viewer) [![Python Version](https://img.shields.io/pypi/pyversions/napari-nd2-folder-viewer.svg?color=green)](https://python.org) [![tests](https://github.com/gatoniel/napari-nd2-folder-viewer/workflows/tests/badge.svg)](https://github.com/gatoniel/napari-nd2-folder-viewer/actions) [![codecov](https://codecov.io/gh/gatoniel/napari-nd2-folder-viewer/branch/main/graph/badge.svg)](https://codecov.io/gh/gatoniel/napari-nd2-folder-viewer) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-nd2-folder-viewer)](https://napari-hub.org/plugins/napari-nd2-folder-viewer)  Look through separate nd2 files in one viewer.  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/stable/plugins/index.html -->  ## Installation  You can install `napari-nd2-folder-viewer` via [pip]:      pip install napari-nd2-folder-viewer    To install latest development version :      pip install git+https://github.com/gatoniel/napari-nd2-folder-viewer.git   ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-nd2-folder-viewer\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/gatoniel/napari-nd2-folder-viewer/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-nd2-folder-viewer       Look through separate nd2 files in one viewer.  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install napari-nd2-folder-viewer via pip: pip install napari-nd2-folder-viewer  To install latest development version : pip install git+https://github.com/gatoniel/napari-nd2-folder-viewer.git  Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-nd2-folder-viewer\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "napari nd2 folder viewer",
    "documentation": "https://github.com/gatoniel/napari-nd2-folder-viewer#README.md",
    "first_released": "2022-08-02T11:44:47.716377Z",
    "license": "BSD-3-Clause",
    "name": "napari-nd2-folder-viewer",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/gatoniel/napari-nd2-folder-viewer",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-12-13T16:00:44.288081Z",
    "report_issues": "https://github.com/gatoniel/napari-nd2-folder-viewer/issues",
    "requirements": [
      "numpy",
      "magicgui",
      "qtpy",
      "pyyaml",
      "marshmallow",
      "desert",
      "nd2 (>=0.4.3)",
      "dask",
      "pandas",
      "openpyxl",
      "julian",
      "napari-animation",
      "scikit-learn",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "summary": "Look through separate nd2 files in one viewer.",
    "support": "https://github.com/gatoniel/napari-nd2-folder-viewer/issues",
    "twitter": "",
    "version": "0.0.8",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [{ "name": "Alexis Japas" }],
    "code_repository": "https://github.com/alexisjapas/napari-vesicles-segmentation",
    "conda": [],
    "description": "# napari-vesicles-segmentation  [![License BSD-3](https://img.shields.io/pypi/l/napari-vesicles-segmentation.svg?color=green)](https://github.com/alexisjapas/napari-vesicles-segmentation/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/napari-vesicles-segmentation.svg?color=green)](https://pypi.org/project/napari-vesicles-segmentation) [![Python Version](https://img.shields.io/pypi/pyversions/napari-vesicles-segmentation.svg?color=green)](https://python.org) [![tests](https://github.com/alexisjapas/napari-vesicles-segmentation/workflows/tests/badge.svg)](https://github.com/alexisjapas/napari-vesicles-segmentation/actions) [![codecov](https://codecov.io/gh/alexisjapas/napari-vesicles-segmentation/branch/main/graph/badge.svg)](https://codecov.io/gh/alexisjapas/napari-vesicles-segmentation) [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-vesicles-segmentation)](https://napari-hub.org/plugins/napari-vesicles-segmentation)  A simple plugin to detect vesicles in cells images.  ----------------------------------  This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.  <!-- Don't miss the full getting started guide to set up your new package: https://github.com/napari/cookiecutter-napari-plugin#getting-started  and review the napari docs for plugin developers: https://napari.org/plugins/index.html -->  ## Installation  You can install `napari-vesicles-segmentation` via [pip]:      pip install napari-vesicles-segmentation    To install latest development version :      pip install git+https://github.com/alexisjapas/napari-vesicles-segmentation.git  ## Usage 1. Open napari 2. Open your data ![usage-open-data](images/usage-open-data.png) 3. Launch the vesicles-segmentation plugin 4. Select the data you want to segment and set the parameters of the segmentation ![usage-setup](images/usage-setup.png)     * **image**: The image to segment vesicles in. The image can be a 2D or 3D temporal stack of images.     * **minimum vesicles size**: The minimum size of the vesicles to detect. Smaller detected vesicles are removed.     * **membrane erosion**: The size of the disk radius used for eroding the cell. This is used to remove the external membrane. This parameter scales when downsizing the image, for more information see 'downsizing ratio' parameter.     * **closing size**: The size of the disk radius used for closing the cell. This is used to fill holes in the cell. This parameter scales when downsizing the image, for more information see 'downsizing ratio' parameter.     * **clip**: If set to zero, no standardization is performed. Otherwise, the standard deviation of the image is set to n_sigma * the standard deviation of the image, the image is standardized and its values are clipped to the range [-1, 1] in order to remove outliers. The higher the value of n_sigma, the less outliers are removed. This operation can lead to a better detection of the cell.     * **downsampling ratio**: The downsampling ratio used for the downsampled image. This is used to speed up the computation. Downsampling the image have impact in reducing the resolution of erosion and closing e.g. for a downsize ratio of 2, setting the erosion size to 3 will result in an erosion size of 6.     * **display cell detection**: If set to True, the cell detection is displayed in the viewer instead of the vesicle detection. 5. Click on the \"Segment\" button to start the segmentation. This can take few seconds or minutes depending on the size of the data. The result is added to the viewer as below. ![usage-segmentation](images/usage-segmentation.png)  ## Contributing  Contributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.  ## License  Distributed under the terms of the [BSD-3] license, \"napari-vesicles-segmentation\" is free and open source software  ## Issues  If you encounter any problems, please [file an issue] along with a detailed description.  [napari]: https://github.com/napari/napari [Cookiecutter]: https://github.com/audreyr/cookiecutter [@napari]: https://github.com/napari [MIT]: http://opensource.org/licenses/MIT [BSD-3]: http://opensource.org/licenses/BSD-3-Clause [GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt [GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt [Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0 [Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt [cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin  [file an issue]: https://github.com/alexisjapas/napari-vesicles-segmentation/issues  [napari]: https://github.com/napari/napari [tox]: https://tox.readthedocs.io/en/latest/ [pip]: https://pypi.org/project/pip/ [PyPI]: https://pypi.org/ ",
    "description_content_type": "text/markdown",
    "description_text": "napari-vesicles-segmentation       A simple plugin to detect vesicles in cells images.  This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.  Installation You can install napari-vesicles-segmentation via pip: pip install napari-vesicles-segmentation  To install latest development version : pip install git+https://github.com/alexisjapas/napari-vesicles-segmentation.git  Usage  Open napari Open your data  Launch the vesicles-segmentation plugin Select the data you want to segment and set the parameters of the segmentation  image: The image to segment vesicles in. The image can be a 2D or 3D temporal stack of images. minimum vesicles size: The minimum size of the vesicles to detect. Smaller detected vesicles are removed. membrane erosion: The size of the disk radius used for eroding the cell. This is used to remove the external membrane. This parameter scales when downsizing the image, for more information see 'downsizing ratio' parameter. closing size: The size of the disk radius used for closing the cell. This is used to fill holes in the cell. This parameter scales when downsizing the image, for more information see 'downsizing ratio' parameter. clip: If set to zero, no standardization is performed. Otherwise, the standard deviation of the image is set to n_sigma * the standard deviation of the image, the image is standardized and its values are clipped to the range [-1, 1] in order to remove outliers. The higher the value of n_sigma, the less outliers are removed. This operation can lead to a better detection of the cell. downsampling ratio: The downsampling ratio used for the downsampled image. This is used to speed up the computation. Downsampling the image have impact in reducing the resolution of erosion and closing e.g. for a downsize ratio of 2, setting the erosion size to 3 will result in an erosion size of 6. display cell detection: If set to True, the cell detection is displayed in the viewer instead of the vesicle detection.   Click on the \"Segment\" button to start the segmentation. This can take few seconds or minutes depending on the size of the data. The result is added to the viewer as below.   Contributing Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request. License Distributed under the terms of the BSD-3 license, \"napari-vesicles-segmentation\" is free and open source software Issues If you encounter any problems, please file an issue along with a detailed description.",
    "development_status": ["Development Status :: 2 - Pre-Alpha"],
    "display_name": "Vesicles Segmentation",
    "documentation": "https://github.com/alexisjapas/napari-vesicles-segmentation#README.md",
    "first_released": "2022-07-08T15:13:49.158166Z",
    "license": "BSD-3-Clause",
    "name": "napari-vesicles-segmentation",
    "npe2": true,
    "operating_system": ["Operating System :: OS Independent"],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/alexisjapas/napari-vesicles-segmentation",
    "python_version": ">=3.8",
    "reader_file_extensions": [],
    "release_date": "2022-07-08T15:13:49.158166Z",
    "report_issues": "https://github.com/alexisjapas/napari-vesicles-segmentation/issues",
    "requirements": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "scipy",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'",
      "scikit-image ; extra == 'testing'",
      "scipy ; extra == 'testing'"
    ],
    "summary": "A simple plugin to detect vesicles in cells images.",
    "support": "https://github.com/alexisjapas/napari-vesicles-segmentation/issues",
    "twitter": "",
    "version": "0.0.1",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  },
  {
    "authors": [
      { "email": "talley.lambert@gmail.com", "name": "Talley Lambert" }
    ],
    "code_repository": "https://github.com/tlambert03/psfmodels",
    "description": "# psfmodels  [![PyPI](https://img.shields.io/pypi/v/psfmodels.svg?color=green)](https://pypi.org/project/psfmodels) [![Python Version](https://img.shields.io/pypi/pyversions/psfmodels.svg?color=green)](https://python.org) [![CI](https://github.com/tlambert03/psfmodels/actions/workflows/ci.yml/badge.svg)](https://github.com/tlambert03/psfmodels/actions/workflows/ci.yml) [![codecov](https://codecov.io/gh/tlambert03/psfmodels/branch/main/graph/badge.svg)](https://codecov.io/gh/tlambert03/psfmodels)  Python bindings for scalar and vectorial models of the point spread function.  Original C++ code and MATLAB MEX bindings Copyright &copy; 2006-2013, [Francois Aguet](http://www.francoisaguet.net/software.html), distributed under GPL-3.0 license. Python bindings by Talley Lambert  This package contains three models:  1. The vectorial model is described in Auget et al 2009<sup>1</sup>. For more information and implementation details, see Francois' Thesis<sup>2</sup>. 2. A scalar model, based on Gibson & Lanni<sup>3</sup>. 3. A gaussian approximation (both paraxial and non-paraxial), using paramters from Zhang et al (2007)<sup>4</sup>.  <small>  <sup>1</sup> [F. Aguet et al., (2009) Opt. Express 17(8), pp. 6829-6848](https://doi.org/10.1364/OE.17.006829)  <sup>2</sup> [F. Aguet. (2009) Super-Resolution Fluorescence Microscopy Based on Physical Models. Swiss Federal Institute of Technology Lausanne, EPFL Thesis no. 4418](http://bigwww.epfl.ch/publications/aguet0903.html)  <sup>3</sup> [F. Gibson and F. Lanni (1992) J. Opt. Soc. Am. A, vol. 9, no. 1, pp. 154-166](https://opg.optica.org/josaa/abstract.cfm?uri=josaa-9-1-154)  <sup>4</sup> [Zhang et al (2007). Appl Opt . 2007 Apr 1;46(10):1819-29.](https://doi.org/10.1364/AO.46.001819)  </small>  ### see also:  For a different (faster) scalar-based Gibson–Lanni PSF model, see the [MicroscPSF](https://github.com/MicroscPSF) project, based on [Li et al (2017)](https://doi.org/10.1364/JOSAA.34.001029) which has been implemented in [Python](https://github.com/MicroscPSF/MicroscPSF-Py), [MATLAB](https://github.com/MicroscPSF/MicroscPSF-Matlab), and [ImageJ/Java](https://github.com/MicroscPSF/MicroscPSF-ImageJ)  ## Install  ```sh pip install psfmodels ```  ### from source  ```sh git clone https://github.com/tlambert03/PSFmodels.git cd PSFmodels pip install -e \".[dev]\"  # will compile c code via pybind11 ```  ## Usage  There are two main functions in `psfmodels`: `vectorial_psf` and `scalar_psf`. Additionally, each version has a helper function called `vectorial_psf_centered` and `scalar_psf_centered` respectively. The main difference is that the `_psf` functions accept a vector of Z positions `zv` (relative to coverslip) at which PSF is calculated. As such, the point source may or may not actually be in the center of the rendered volume. The `_psf_centered` variants, by contrast, do _not_ accecpt `zv`, but rather accept `nz` (the number of z planes) and `dz` (the z step size in microns), and always generates an output volume in which the point source is positioned in the middle of the Z range, with planes equidistant from each other. All functions accept an argument `pz`, specifying the position of the point source relative to the coverslip. See additional keyword arguments below  _Note, all output dimensions (`nx` and `nz`) should be odd._  ```python import psfmodels as psfm import matplotlib.pyplot as plt from matplotlib.colors import PowerNorm  # generate centered psf with a point source at `pz` microns from coverslip # shape will be (127, 127, 127) psf = psfm.make_psf(127, 127, dxy=0.05, dz=0.05, pz=0) fig, (ax1, ax2) = plt.subplots(1, 2) ax1.imshow(psf[nz//2], norm=PowerNorm(gamma=0.4)) ax2.imshow(psf[:, nx//2], norm=PowerNorm(gamma=0.4)) plt.show() ```  ![Image of PSF](fig.png)  ```python # instead of nz and dz, you can directly specify a vector of z positions import numpy as np  # generate 31 evenly spaced Z positions from -3 to 3 microns psf = psfm.make_psf(np.linspace(-3, 3, 31), nx=127) psf.shape  # (31, 127, 127) ```  **all** PSF functions accept the following parameters. Units should be provided in microns unless otherwise stated. Python API may change slightly in the future.  See function docstrings as well.  ``` nx (int):       XY size of output PSF in pixels, must be odd. dxy (float):    pixel size in sample space (microns) [default: 0.05] pz (float):     depth of point source relative to coverslip (in microns) [default: 0] ti0 (float):    working distance of the objective (microns) [default: 150.0] ni0 (float):    immersion medium refractive index, design value [default: 1.515] ni (float):     immersion medium refractive index, experimental value [default: 1.515] tg0 (float):    coverslip thickness, design value (microns) [default: 170.0] tg (float):     coverslip thickness, experimental value (microns) [default: 170.0] ng0 (float):    coverslip refractive index, design value [default: 1.515] ng (float):     coverslip refractive index, experimental value [default: 1.515] ns (float):     sample refractive index [default: 1.47] wvl (float):    emission wavelength (microns) [default: 0.6] NA (float):     numerical aperture [default: 1.4] ```  ## Comparison with other models  While these models are definitely slower than the one implemented in [Li et al (2017)](https://doi.org/10.1364/JOSAA.34.001029) and [MicroscPSF](https://github.com/MicroscPSF), there are some interesting differences between the scalar and vectorial approximations, particularly with higher NA lenses, non-ideal sample refractive index, and increasing spherical aberration with depth from the coverslip.  For an interactive comparison, see the [examples.ipynb](notebooks/examples.ipynb) Jupyter notebook.  ## Lightsheet PSF utility function  The `psfmodels.tot_psf()` function provides a quick way to simulate the total system PSF (excitation x detection) as might be observed on a light sheet microscope (currently, only strictly orthogonal illumination and detection are supported).  See the [lightsheet.ipynb](notebooks/lightsheet.ipynb) Jupyter notebook for examples.   ",
    "description_content_type": "text/markdown",
    "description_text": "psfmodels     Python bindings for scalar and vectorial models of the point spread function. Original C++ code and MATLAB MEX bindings Copyright © 2006-2013, Francois Aguet, distributed under GPL-3.0 license. Python bindings by Talley Lambert This package contains three models:  The vectorial model is described in Auget et al 20091. For more information and implementation details, see Francois' Thesis2. A scalar model, based on Gibson & Lanni3. A gaussian approximation (both paraxial and non-paraxial), using paramters from Zhang et al (2007)4.   1 F. Aguet et al., (2009) Opt. Express 17(8), pp. 6829-6848 2 F. Aguet. (2009) Super-Resolution Fluorescence Microscopy Based on Physical Models. Swiss Federal Institute of Technology Lausanne, EPFL Thesis no. 4418 3 F. Gibson and F. Lanni (1992) J. Opt. Soc. Am. A, vol. 9, no. 1, pp. 154-166 4 Zhang et al (2007). Appl Opt . 2007 Apr 1;46(10):1819-29.  see also: For a different (faster) scalar-based Gibson–Lanni PSF model, see the MicroscPSF project, based on Li et al (2017) which has been implemented in Python, MATLAB, and ImageJ/Java Install sh pip install psfmodels from source sh git clone https://github.com/tlambert03/PSFmodels.git cd PSFmodels pip install -e \".[dev]\"  # will compile c code via pybind11 Usage There are two main functions in psfmodels: vectorial_psf and scalar_psf. Additionally, each version has a helper function called vectorial_psf_centered and scalar_psf_centered respectively. The main difference is that the _psf functions accept a vector of Z positions zv (relative to coverslip) at which PSF is calculated. As such, the point source may or may not actually be in the center of the rendered volume. The _psf_centered variants, by contrast, do not accecpt zv, but rather accept nz (the number of z planes) and dz (the z step size in microns), and always generates an output volume in which the point source is positioned in the middle of the Z range, with planes equidistant from each other. All functions accept an argument pz, specifying the position of the point source relative to the coverslip. See additional keyword arguments below Note, all output dimensions (nx and nz) should be odd. ```python import psfmodels as psfm import matplotlib.pyplot as plt from matplotlib.colors import PowerNorm generate centered psf with a point source at pz microns from coverslip shape will be (127, 127, 127) psf = psfm.make_psf(127, 127, dxy=0.05, dz=0.05, pz=0) fig, (ax1, ax2) = plt.subplots(1, 2) ax1.imshow(psf[nz//2], norm=PowerNorm(gamma=0.4)) ax2.imshow(psf[:, nx//2], norm=PowerNorm(gamma=0.4)) plt.show() ```  ```python instead of nz and dz, you can directly specify a vector of z positions import numpy as np generate 31 evenly spaced Z positions from -3 to 3 microns psf = psfm.make_psf(np.linspace(-3, 3, 31), nx=127) psf.shape  # (31, 127, 127) ``` all PSF functions accept the following parameters. Units should be provided in microns unless otherwise stated. Python API may change slightly in the future.  See function docstrings as well. nx (int):       XY size of output PSF in pixels, must be odd. dxy (float):    pixel size in sample space (microns) [default: 0.05] pz (float):     depth of point source relative to coverslip (in microns) [default: 0] ti0 (float):    working distance of the objective (microns) [default: 150.0] ni0 (float):    immersion medium refractive index, design value [default: 1.515] ni (float):     immersion medium refractive index, experimental value [default: 1.515] tg0 (float):    coverslip thickness, design value (microns) [default: 170.0] tg (float):     coverslip thickness, experimental value (microns) [default: 170.0] ng0 (float):    coverslip refractive index, design value [default: 1.515] ng (float):     coverslip refractive index, experimental value [default: 1.515] ns (float):     sample refractive index [default: 1.47] wvl (float):    emission wavelength (microns) [default: 0.6] NA (float):     numerical aperture [default: 1.4] Comparison with other models While these models are definitely slower than the one implemented in Li et al (2017) and MicroscPSF, there are some interesting differences between the scalar and vectorial approximations, particularly with higher NA lenses, non-ideal sample refractive index, and increasing spherical aberration with depth from the coverslip. For an interactive comparison, see the examples.ipynb Jupyter notebook. Lightsheet PSF utility function The psfmodels.tot_psf() function provides a quick way to simulate the total system PSF (excitation x detection) as might be observed on a light sheet microscope (currently, only strictly orthogonal illumination and detection are supported).  See the lightsheet.ipynb Jupyter notebook for examples.",
    "development_status": ["Development Status :: 3 - Alpha"],
    "display_name": "psfmodels",
    "documentation": "",
    "first_released": "2019-09-15T18:38:17.604354Z",
    "license": "GPL-3.0",
    "name": "psfmodels",
    "npe2": true,
    "operating_system": [],
    "plugin_types": ["widget"],
    "project_site": "https://github.com/tlambert03/psfmodels",
    "python_version": ">=3.7",
    "reader_file_extensions": [],
    "release_date": "2022-04-23T15:54:59.302073Z",
    "report_issues": "",
    "requirements": [
      "numpy",
      "scipy (>=0.14.0)",
      "typing-extensions",
      "black ; extra == 'dev'",
      "flake8 ; extra == 'dev'",
      "flake8-docstrings ; extra == 'dev'",
      "flake8-typing-imports ; extra == 'dev'",
      "ipython ; extra == 'dev'",
      "isort ; extra == 'dev'",
      "mypy ; extra == 'dev'",
      "pre-commit ; extra == 'dev'",
      "pydocstyle ; extra == 'dev'",
      "pytest ; extra == 'dev'",
      "pytest-cov ; extra == 'dev'",
      "tox ; extra == 'dev'",
      "tox-conda ; extra == 'dev'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "tox ; extra == 'testing'",
      "tox-conda ; extra == 'testing'",
      "magicgui ; (platform_system != \"Linux\") and extra == 'testing'",
      "pyside2 ; (platform_system != \"Linux\") and extra == 'testing'",
      "qtpy ; (platform_system != \"Linux\") and extra == 'testing'"
    ],
    "summary": "Scalar and vectorial models of the microscope point spread function (PSF).",
    "support": "",
    "twitter": "",
    "version": "0.3.2",
    "visibility": "public",
    "writer_file_extensions": [],
    "writer_save_layers": []
  }
]
